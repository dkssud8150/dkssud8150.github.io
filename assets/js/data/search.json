[ { "title": "[KOOC] 인공지능 및 기계학습 개론 10주차 - Sampling Based Inference ", "url": "/posts/kooc_week10/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-11-09 22:10:00 +0900", "snippet": "Chapter 10. Sampling Based Inference 목차 basic sampling method Markov chain Monte Carlo apply MCMC to the parameter inference of Bayesian Networks mechanism of rejection sampling mechanism of importance sampling sampling based inference concept of Metropolis-Hastings algorithm mechanism of Gibbs sampling case study of sampling based inference Latent Dirichlet Allocation model collapsed Gibbs Sampling derive Gibbs sampling formula for LDA   10-1. Basic Sampling methods Forward Sampling간단한 Baysian network를 살펴보기 위해 이전에 봤던, Alarm/Call 문제를 살펴보자. 이 때, $ P(E = T|MC = T) $ 의 확률을 구하기 위해 E=T와 MC=T의 경우를 카운트하고, MC=T인 경우를 카운트한다. 반복을 엄청 많이 하여 오차를 줄인다.Gaussian Mixture Model에 대해 forward sampling, 즉 직접 테스트를 통해 카운트하여 확률을 계산하는 방법을 사용하여 확률 그래프를 그려볼 수 있다. GMM에서는 latent factor인 z가 존재하고, 이 z는 π에 의해 mixture distribution을 따르는 지표로서 샘플링된다.따라서 샘플 x는 $ N(\\mu_z,\\sum_z) $ 의 정규분포를 따른다. (=$ P(x|z) = N(x|\\mu_z, \\sum_k) $)  Rejection SamplingRejection sampling이란 많은 반복을 통해 P(E=T|MC=T,A=F) 를 구할 때, 이에 해당하지 않는 샘플들 즉, MC=T이지만, A=F인 경우는 버리고(reject), 다시 반복 샘플링하여 MC=T,A=F인 경우는 카운트한다.그 후, 최종적인 확률은 카운트한 값에서 전체 샘플의 개수를 나눠주면 된다. rejection sampling을 수치적으로 살펴보자. 먼저 x_i 를 샘플링한다. q(x)는 $ N(\\mu_z,\\sum_z) $의 정규분포에서 x_i가 샘플링될 확률을 나타낸다. 즉, target distribution이라 할 수 있다.그 다음, 이 q(x)를 둘러쌀 수 있는 p(x)를 생성하고, p(x)와 q(x)에 상수 m을 곱한 것을 비교하여 상수 u보다 크다면 카운트하고, 그렇지 않다면 카운트하지 않고, 다시 샘플링한다. Q Mixture 부분에서 1/3, 3 부분이 M에 해당하고, 그래프에서 빨간색 선 아래 부분이 rejection region을 나타낸다.첫번째 그래프의 경우는 p(x) 또한 Mixture distribution으로 모델링하여 target distribution과 거의 유사하게 그려진다. 그러나 두번째 Q mixture 그래프의 경우 단순한 p(x)를 normal distribution으로 생성했기 때문에 target distribution과 다소 다른 모습을 볼 수 있다.  Importance Samplingrandom variable에 의해 생성될 함수에 대한 기대값을 게산해보자.$ E(f) = \\int f(z)p(z) dz = \\int f(z)\\frac{p(z)}{q(z)}q(z) dz = \\frac{1}{L}\\sum_{l=1}^L \\frac{p(z^l)}{q(z^l)} f(z^l) $f함수의 기대값 E(f)는 z가 샘플링될 확률 분포 p(z)와 true function인 f(z)를 z에 대해 적분한 것과 같다. 기대값을 정확하게 계산하기 위해서는 true function을 알고 있어야 하므로, f(z)를 알고 있다는 가정이 필요하다.여기에서 q(z) 라는 계산의 편의를 위해 생성한 정규분포를 따르는 q(z) 를 분모, 분자에 각각 곱해준다. 샘플 z에 대해 무한대가 아닌 1부터 샘플의 개수인 L까지로 범위를 정해서 적분을 하게 되면$ \\sum_{l=1}^L \\frac{p(z^l)}{q(z^l)} f(z^l) q(z^l) $인데, $ \\sum_{l=1}^L q(z^l) $ 을 상수 1/L로 근사시키면, 정규화 상수가 된다.적분이 시그마가 되는 이유는 간단하다. 예를 들어, f(x)=a 이라는 함수에 대해 1부터 L까지 적분을 하면, L x f(x) 가 된다. 이는 풀어쓰면 f(x=1) + f(x=2) + … + f(x=L) 이다. 이 때, p(z)/q(z)를 빼고 생각을 해보면, 모든 z값들에 대한 f(z)의 평균을 구하는 식이 되므로, p(z)/q(z)는 중요도 가중치 역할을 수행하게 된다. 만약 p와 q가 normalize하지 않다면, 우리가 직접 정규 분포로 만들어주면 된다. 그 방법으로 p(z)에서 z에 대해 1/z_p 를 하고, q에 대해서도 마찬가지로 1/z_q 해주면 된다. 만약, Z가 1보다 클 때의 확률을 구하고자 한다면, f(z)를 1보다 클 때는 1, 1보다 작을 때를 0으로 하는 indentity function으로 만들어주면 된다.  likelihood weight를 적용한 알고리즘의 동작방식은 다음과 같다. 전체 샘플은 오른쪽의 Baysian network를 따르고 있다고 가정한다. 목적 : P(E=T|MC=T,A=F) 를 구하고자 함. 초기 세팅 SumSW = NormSW = 0 반복 과정 SW = SampleWeight = 1 sample 생성 if, sample == (A=F|B=F,E=F), JC|A=T, MC=T|A=F : - P(A=F|B=F,E=F) = 0.999 - SW = 1 * 0.999 - P(MC=T|A=F) = 0.01 - SW = 1 * 0.999 * 0.01 if, sample in E=T : - SumSW += SW NormSW += SW Return SumSW/NormSW  10-2. Sampling Based InferenceSampling method에서 가장 많이 사용되는 기법이 Gibbs sampling이다. 이 Gibbs sampling은 Metropolis-hastings 알고리즘의 특별한 케이스이다.이전에 배웠던 EM 알고리즘에서 Expectation step에서는 optimization을 통해 Z를 할당했다. 이제는 sampling 기반으로 Z를 할당해도 잘 수행되는지 살펴보고자 한다. Markov Chain그리고, 이전에 배웠던 Forward, rejection, importance sampling에서는 현재의 z와 다음 시간에서의 z는 연관이 없었다. metropolis hastings 알고리즘에서는 버련던 instance(sample)들을 활용하여 다음 z를 샘플링하고자 한다. 이 알고리즘을 위해서는 markov chain 개념을 알아야 한다. markov chain에서 각 노드는 state의 확률 분포를 가진다. z_t가 어느 정도의 확률로 할당이 될 것인지에 대한 확률이다.한 시스템이 3개의 state를 가진다고 가정할 때, 특정 시간 t에서의 확률 P(z_t)는 [a b c] (a,b,c는 확률) 을 가진다.이 a,b,c는 정확한 관찰이라 생각하면 True=1 또는 False=0으로 할당될텐데, 확률론적으로 생각한다면 각각 0~1의 값을 가진다.예를 들어, 앞서 살펴봤던 알람과 콜에 대한 baysian network에서 발생할 수 있는 P(z)는 [P(A=T,JC=T) P(A=T,JC=F) P(A=F,JC=T) P(A=F, JC=F)] 라는 4개의 state를 가진다. 각 state는 확률로서 표현된다. 현재 시간 t에서의의 P(z_t)와 다음 시간 t+1에서의 P(z_t+1) 을 연결하는 matrix인 transition matrix, $ T_{i,j} $ 가 존재하여, P(z_t) 와 transition matrix를 가지고 있다면, 이후 시간에서의 P를 계산할 수 있다.  Markov chain의 특성은 접근 가능성(Accessible), 축소/환원성(Reducibility), 주기성(Periodicity), 일시성(Transience), Ergodicity이 있다. Accessible i ⇢ j : 샘플링할 때, i state에서 j state로 전의(이동)가 될 확률이 0보다 크다. i ⇹ j : i에서 j로도, j에서 i로도 이동이 가능하다면 i와 j는 양방향(communicate)성을 가진다.   Reducibility 만약, i와 j가 양방향이고, i와 j가 모두 전체 state인 S에 포함된다면, markov chain은 더이상 줄일 수 없는 상태, 즉 irreducible하다.   Periodicity 특정 state i에 방문하는 주기(period)가 4,8,12… 라고 한다면, period d는 전체 period의 최대공약수인 4로 간주한다. 만약 주기가 1,3,4,11,13 이라고 한다면 d=1이 되므로, 이 경우를 aperiodic, 즉 주기가 없다고 정의한다.   Transience X_0 일 때, j state가 할당되었는데, 이후 특정시간에서 다시 j state를 방문한다면 이를 재방문(recurrent)이라 정의한다. 만약 이후 다시는 방문하지 않는 경우를 반대로 transient라 한다.   Ergodicity 한 state가 재방문을 하나, 그게 언제인지는 모를 때, 즉 aeriodic할 때, ergodic이라 한다. 모든 state가 ergodic하다면 markov chain은 ergodic하다.   Stationary Distributionmarkov chain의 특수 케이스인 Stationary Distribution이 metropolis-hastings를 동작시키는 핵심 개념이라 할 수 있다.$ RT_i $ 는 state i에 방문한 이후 다시 state i에 방문하는 시간 t1,t2,t3 중 가장 작은 값을 의미한다. 만약 markov chain이 irreducible 하고 ergodic하다면 즉, 특정 state에 대해 재방문을 하고 state i와 j가 양방향인 상황이라 할 수 있다. 이런 상황에서 stationary distribution, π_i를 정의한다.$ \\pi_i = lim_{n⇢\\inf} T_{i,j}^{(n)} = \\frac{1}{E[RT_i]}$이 때, π_i에 대한 조건은 다음과 같다.$ \\pi_i \\geq 0,: \\sum_{i \\in S} π_i = 1, \\pi_j = \\sum_{i \\ in S} \\pi_iT_{i,j} $이 때, S는 state의 개수를 의미하고, 특정 시간 t에서의 π_t 는 latent variable인 z_t와 동일한 역할을 하고, S가 3일 때 π_t = [a, b, c] 의 형태를 가진다. 만약 T를 가지고 있는 상황에서 π를 계산할 수 있다. π와 T를 matrix로 생각을 해보면 다음과 같이 구성할 수 있다.$ \\pi(I_{|S|,|S|} - T + 1{|S|,|S|}) = 1{1, |S|} $이 때, I는 identity matrix, 1은 1로 구성되어 있는 matrix를 의미한다. 이는 위의 조건 두,세번째를 활용하여 구성한 것이다. Reversible Markov chain 또는 balance Equation 이라는 개념이 있다. 오른쪽 위의 코드와 같이 transition matrix(T)가 있을 때 stationary distribution의 특성을 활용하여 π(pi)를 쉽게 구할 수 있다. stationary distribution의 특성 상 πT = π 이므로, 성립되는 것을 볼 수 있다.state i와 j가 있을 때, i에서 j로 transition 하는 확률과 j에서 i로 transition 하는 확률이 같으면 reversible하다 할 수 있고, 같지 않으면 irreversible하다고 할 수 있다.$ \\pi_i T_{i,j} = \\pi_j T_{j,i} $  이 이론을 활용하여 MCMC(Markov Chain Monte Carlo)를 정의한다. 즉, 원래의 Markov Chain은 stationary distribution, π를 찾는 것이었으나, MCMC에서는 π가 주어진 상태에서 transition matrix를 찾는다.state i에서의 z_i에서 z_j로 이동하기 위한 transition matrix를 잘 정의하여 stationary distribution을 잘 생성할 수 있게 샘플링해야 한다. 이 때의 z는 state마다 이어져 있는 형태이다.$ z^{(1)} -&amp;gt; z^{(2)} -&amp;gt; \\dots -&amp;gt; z^{(m)} -&amp;gt; \\dots -&amp;gt; z^{(m+n)} $ 예를 들어, 이전의 baysian network Alarm과 JC,MC에 대해 살펴보자. Alarm과 MC가 Evidence, 관측된 값이라 하고, 나머지 3개를 latent variable, Z라 하자. 이 Z를 모두 반복적으로 샘플링하여 True/False를 정한다.  Metropolis-Hastings Algorithmstationary distribution을 알고 있는 상황에서 어떻게 transition matrix를 잘 만들어 낼 수 있는지에 대한 알고리즘이 Metropolis-Hastings 알고리즘이다.MCMC(Markov Chain Mento Carlo)의 일반적인 알고리즘은 현재의 latent variable, z^t가 있고, 현재의 z^t에서 z*로 이동하고자 하며 이 때 z*에 대한 후보로서 q(z*|z^t)를 생성하고, 이 q_t를 proposal distribution 이라 한다.또한, acceptance probability, α를 정의한다. α에 따라 샘플링한 z*을 받아들일지 버릴지를 결정한다. 받아들인다면 다음 z^t+1은 z*이 되고 버린다면 다시 z^t가 된다. 여기서 중요한 것은, 우리가 얻고자 하는 것은 stationary distribution이므로, 우리가 정의한 q를 잘 설정해주어야 한다. 따라서 Metropolis-Hastings 알고리즘에서는 ratio, r(z*|z^t)를 정의하여 q를 잘 설정하고자 했다.$ r(z|z^t) = \\cfrac{q(z^t|z)P(z)}{q(z|z^t)P(z^t)} $이 때, stationary distribution의 reversible markov chain의 특성을 통해 ratio는 1이 되어야 한다. 만약 ratio가 1보다 작으면 분모가 큰 것이고 이를 잘 생각해보면 z* -&amp;gt; z^t로의 확률은 낮고, z^t -&amp;gt; z*로의 확률이 크다는 것을 알 수 있다. 따라서 ratio를 1로 만들어주기 위해 acceptance probability를 잘 조정해야 한다. 지금은 z^t -&amp;gt; z*에 대한 샘플을 줄이거나 반대를 키워야 하므로 *-&amp;gt;t는 1로, t-&amp;gt;*는 ratio로 지정한다.반대로 1보다 크다면 t-&amp;gt;*은 1로, *-&amp;gt;t는 ratio로 설정한다. 결론적으로 acceptance probability는 다음과 같이 정의된다.$ \\alpha(z|z^t) = min{1, r(z|z^t)} $  t에서 *로의 Transition probability는 $ T_{t,}^{MH} = q(z|z^t) \\alpha(z*|z^t) $ 를 만족한다.우리는 이미 P(z)를 알고 있다고 가정했기에 q(z)에 대해서만 정의해주면 된다. q(z)를 샘플링하는 방법으로 Random walk M-H algorithm 이 있다. 그에 대한 과정은 다음과 같다.z*을 normal distribution, N(z^t, σ^2)에 대해 랜덤 위치로 샘플링한다. 그러면 q(z*|z^t)는 normal distribution식을 따르게 되고, z*을 받아들일지 말지를 결정한 후, 받아들이면 z^t+1 의 위치는 z*가 되고, 그 후 다시 z^t=z*에 대해 무작위로 z*을 샘플링한다. z*의 normal distribution을 생각해봤을 때, σ가 클수록 큰 폭으로 이동되고, σ가 작을수록 이동하는 폭이 작아진다는 것을 알 수 있다. 그 결과가 위의 그래프이다. Gaussian Mixture model에 대해 샘플링하고 있으며, 맨 위가 true distribution이고, 중간이 latent, z^t를 나타내고 있다. 그래프에서 볼 수 있듯이 σ가 작으면 샘플링되는 폭도 작아서 변화하는 폭이 작다.  Gibbs SamplingGibbs sampling이란 Metropolis Hastings의 특별 케이스라 할 수 있다. z^t는 state의 개수만큼 vector형태로 존재하는데, 이에 대해 각 샘플링마다 1개의 state만을 업데이트하고자 했다. 업데이트하는 것을 $ z_k^t $라 했을 때, 나머지 업데이트되지 않는 것은 $ z_{-k}^t $ 로 표현한다. 그렇다면 z*으로 업데이트를 하고 난 후에는 z*과 변화하지 않은 z_{-k}^t가 있을 것이다.이런 과정을 MH 알고리즘에 적용하면$ q(z|z^t) = P(z_k,z_{-k}^t|z_{-k}^t) = P(z_k*|z_{-k}^t) $reversible markov chain을 만족시켜야 하므로 $ P(z^t)q(z|z^t) = P(z)q(z^t|z*) $ 가 되어야 한다. 따라서$ P(z^t)q(z|z^t) = P(z_k^t,z_{-k}^t)P(z_k|z_{-k}^t) = P(z_k^t|z_{-k}^t)P(z_{-k}^t)P(z_k*|z_{-k}^t) $joint probability를 condition probability형태로 변환한 후 이를 다시 joint probability로 변환한다. P(z_k, z_{-k}^t) 는 어차피 z^t만 업데이트했으므로 P(z)와 같으므로 변환하고, *에서 t로 오는 것은 P(z_k^t|z_{-k}^t) 와 같으므로 변환하면 원래의 balance equation을 만족하게 된다.따라서 항상 balance equation을 만족하므로, acceptance probability를 사용하지 않아도 된다. gibbs sampling에서는 acceptance probability를 사용하지 않으므로 받아들일지 말지를 판단하지 않고 무조건 수용한다. Alarm/Call에 대한 baysian network에 Gibbs sampling을 적용해보자. Alarm, Marycall이 관찰되 상황에서 buglary, johncall, earthquake를 latent variable이라 생각했을 때 먼저 Buglary만을 업데이트하고자 한다면 Markov blanket을 통해 alarm과 earthquake만 알고 있다면 나머지는 연관이 없다.또한, johncall을 업데이트할 때, Alarm만 알고 있다면 나머지의 variable에 대해서는 연관성이 없다. Alarm은 이미 알고 있는 값이므로 JC만 true/false를 선택하여 적용하면 업데이트할 수 있다.  gibbs sampling의 개념을 좀 더 자세히 살펴보자. 1번의 step마다 1개의 variable만을 변경시키는 기법으로서 최종적인 과정과 그림은 위 사진의 하단과 같다.  Gibbs sampling의 과정을 자세히 살펴보자. i=1에서의 z_i를 초기화한다. step을 T번 반복한다. 각 step마다 1개의 state만 변경시킨다.  이러한 gibbs sampling의 과정을 그래프화하면 위와 같다. Gaussian Mixture Model에 gibbs sampling을 적용하게 되면, EM(expectation, maximiation step)보다 수렴이 다소 느리다. 그러나 EM은 local optima에 빠지기 쉽지만, Gibbs sampling은 local에 빠질 확률이 EM보다 적다.  10-3. Latent Dirichlet AllocationTopic Modeling자연어처리에서 가장 많이 사용되는 기법이다. 여러 개의 단어들을 주고, 이에 대한 latent variable을 추론하여 각 단어의 의미를 분석한다.  Latent Dirichlet AllocationLatent Dirichlet Allocation은 텍스트 데이터에 대해 soft clustering하고 baysian model을 따른다.오른쪽 baysian network에서 α는 prior에 해당한다. 이 α는 dirichlet distribution을 따르는 형태를 지니고 있다. 이 distribution에서 문저에 대한 assignment인 θ가 존재한다. z는 latent variable을 의미하여 단어별 topic assignment, 즉 어떤 의미를 지니는지에 대한 숨은 의미에 대한 할당이다. β도 prior 정보를 의미하고, φ는 topic별 등장할 단어의 확률로 gaussian mixture model에서 개별 cluster에 대한 gaussian distribution으로 모델링하는 부분이다. K는 topic의 개수를 의미한다. M은 문서의 개수, N는 한 문서에서의 단어의 개수이다. w는 evidence 즉 문서에서의 실제 단어를 의미한다. 특정 문장을 생성하는 프로세스는 다음과 같다. Dir : dirichlet distribution Multi : multinomial distribution α : dirichlet distribution prior, 전체 corpus의 topic distribution에 대한 값이다. θ : 문서 레벨의 topic assignment, 특정 문서에 대한 topic assignment들의 비율을 나타낸다. β : dirichlet distribution prior, 개별 단어들마다 어떤 토픽에 어떤 단어가 얼마나 사용될지에 대한 확률의 사전 지식 φ : 어떤 토픽이 어느 정도의 확률로 등장할지에 대한 값 z : 특정 cluster에 대해 선택되는 latent variable, topic별 assignment를 나타낸다. w : 어떤 주제에서 어떤 단어를 선택할지에 대한 pi를 인자로 받아 하나의 단어를 선정한다. 만약 Z를 안다면, α prior을 활용하여 θ를 구할 수 있고, β prior,z,w들을 통해 pi도 구할 수 있다.따라서 Z를 구하는 것이 topic modeling에 가장 중요한 부분을 차지한다. 그렇다면 Z에 대해 어떻게 잘 할당(allocation)하는지에 대해 알아야 할 것이고, 이것이 Gibbs sampling을 하는 이유이다.확률을 구하기 위해 가장 먼저해야 할 부분은 factorization이다. 모든 변수와 prior를 표현하여 확률을 나타낸다. 이 때, ;는 뒤의 내용이 사전 정보(prior)라는 것을 나타낸다. 앞서 정의했던 texture modeling에 대한 baysian network을 활용하여 factorization한다.$ P(W,Z,\\theta, \\phi; \\alpha, \\beta) = \\prod_{i=1}^K P(\\phi_i;\\beta) \\prod_{j=1}^M P(\\theta_j;\\alpha) \\prod_{l=1}^N P(Z_{j,i}|\\theta_j) P(W_{j,l}|\\phi_{Z_{j,l}}) $먼저 베타는 φ에 영향을 주고, φ는 K개만큼 존재한다. 따라서 P(φ_i;β)를 i=1~K에 대해 곱셈한다. 이는 P(φ|β)P(β)와 같고, α에 대해서도 동일하게 만들어 줄 수 있다. 그리고 나서 Z는 θ에 의해 생성되고, M 공간 안에서 N공간에도 포함되므로 M과 N이라는 값에 대해 꽤 많은 곱셈이 진행된다. 그 후 마지막으로 W에 대해서도 작성해주어야 하므로, W에 대해 작성하는데, φ에 대한 k는 토픽의 개수를 의미하고, 토픽의 할당값은 Z에 의해 결정되므로 φ의 아래첨자는 Z가 된다. 위의 식에서 우리가 최종적으로 계산하기 위해서는 θ와 φ를 제거해야 한다. 그 이유는 θ와 φ 이외의 파라미터는 W,Z,α,β 인데, W는 관측지, Z는 최종적으로 구해야 할 값, α,β는 사전 정보이므로 이것들을 제거하는 것은 옳지 않다. 또한, 추후 Z를 구하고 나면 자연스럽게 θ와 φ를 구할 수 있다. 따라서 식을 간편화하기 위해 θ와 φ를 제거하고, 이러한 방식을 Collapsed Gibbs sampling이라 한다.θ,φ를 제거하기 위해 Marginalization을 수행한다. 전체 joint 확률에 대해 적분을 수행하여 marginalize한다. 이를 위에서 구했던 식을 활용하여 간편화할 수 있다. 4개의 값들은 각각 θ에만 연관되어 있는 것과 φ에만 연관되어 있는 것들이 나뉘어져 있다.$ P(W, Z;\\alpha, \\beta) = \\int_\\theta \\int_\\phi P(W,Z,\\theta,\\phi;\\alpha,\\beta) d\\phi d\\theta = \\int_{\\phi} \\prod_{i=1}^K P(\\phi_i;\\beta) \\prod_{j=1}^M \\prod_{l=1}^N P(W_{j,l;\\phi_{Z_{j,l}}} d\\phi : \\times : \\int_\\theta \\prod_{j=1}^M P(\\theta_j;\\alpha) \\prod_{l=1}^N P(Z_{j,l}| \\theta_j) d\\theta $이렇게 나눌 수 있는 이유는 원래의 식에서 모든 연산은 곱셈이고, θ와φ는 서로 독립적이기 때문에 가능하다. 이러한 2개의 변수에 대한 식을 나눠서 θ에 대한 식을 1번, φ에 대한 식을 2번으로 생각하여 각각을 풀어본다. 먼저 1번에 대한 과정이 위와 같다.φ는 간단하게 3x4 행렬이라 치면 열 방향으로는 topic의 구분, 행방향으로는 단어의 구분으로 구성되어 있다. 즉 하나의 행에는 같은 topic에 대한 확률이고, 하나의 열에는 다양한 topic별 단어들에 대한 확률이 포함되어 있다. 이 때, 하나의 행을 모두 더하면 1이 된다.그래서 φ_i 은 i번째 행에 대한 값들을 의미하고, φ_Z 는 특정 토픽에서의 하나의 단어를 가리키고 있다.φ의 각 원소들은 서로 독립적이므로 계산된 값에서 곱셈을 하고 적분을 하는 것과, 적분을 하고 곱셈을 하는 것은 같으므로 곱셈을 밖으로 내보낼 수 있다. β로 인해 만들어지는 φ는 dirichlet distribution에서 추출되는 확률이고, φ에 의해 만들어지는 W는 multinomial distribution이므로 이 둘을 동일하게 맞춰줘야 한다. 그래서 두 distribution의 정의를 알고 이를 풀어줘야만 한다.α에 대한 dirichlet distribution x에 대한 확률은 다음과 같다.이 때의 K는 dirichlet 의 dimension이고, 이 경우는 세 개의 dimension이 있으므로 k=3이다. φ_i의 dimension은 단어의 크기에 해당한다. 여기서 추가 트릭이 더 필요하다. $ n_{j,r}^i $ 의 개수를 세는 것으로, 이는 j번째 문서에서 전체 단어 R idx 중 r idx의 특정 단어가 i번째 토픽에 할당된 단어의 수 이다. 예를 들어, 100개의 문서에서 kaist라는 단어가 3번 나왔다고 한다면, 5(j)번째 문서에서 kaist가 토픽들에 대해 각각 몇 개의 kaist가 할당되었는지에 대한 개수이다. 따라서 만약 전체 토픽들중 kaist가 17(r)번째 단어이고, 토픽 1(i)에 2번이 할당되었다면, $ n_{j,17}^1 $ = 2 가 된다. $ φ_{i,v} $는 특정 i번쨰 topic에 v번째 단어, 즉 i열 v행 단어의 등장 확률인데, 이 값이 몇번 등장했는지를 세다면 $ φ_{i,v}^{n_{(.),v}^i} $ 가 되고, 이는 모든 문서(M)에 대해 i번째 토픽에 v번째 단어가 등장하는 횟수에 대해 곱셈을 하는 것을 의미한다. 전체 문서의 단어 별로 등장 확률을 등장 횟수를 자승하여 곱해준다. 이는 단어 W_i,j에 대해 한 문서의 단어 갯수만큼 곱한 것에 전체 문서의 수만큼 곱해주는 것과 동일하다. 이렇게 변형하고 나면, 곱셈을 하는 차원이 앞부분과 같아지게 되고, 이를 합쳐준다. 합쳐주고 나면, 이 형태를 다시 $ \\alpha = n_{(.),v}^i + \\beta_v $ 인 dirichlet distribution 형태로 만들어 줄 수 있어 보인다. 이를 위해 분모 분자에 각각 형태에 맞게 곱해준다.dirichlet distribution에 대해 적분하는 형태인데, 이는 즉 확률을 모두 더한다는 것과 같고, 그러면 1이 되어 적분 부분이 사라지게 된다.  이번에는 2번에 대해 식을 풀어주자. 이번에도 theta들은 각각 독립적이므로 θ_j로 다 적분을 한 후 곱셈을 해도 상관없다. 이번에도 동일하게 dirichlet distribution의 식과 n을 통해 식을 간편화한다. 이 때, theta_i는 어떤 문서의 토픽 distribution을 의미하므로 dirichlet distribution에서의 K=dimension은 토픽의 개수가 된다. 그리고 n에서의 단어는 상관하지 않으므로 (.) 가 된다. 이렇게 n에 대해서 적용을 하면 dirichlet distribution에서의 α 를 $ n_{j,(.)}^i + α_i $로 치환하여 식을 간편화할 수 있다. $ {\\color{DarkRed} (이 때, α_k가 아닌 α_i 가 맞음)} $1번식에서와 동일하게 치환을 위해 분모 분자에 각각 알맞게 곱해주고 나면, 적분 부분이 1이 되어 사라진다.  이렇게 θ와 φ를 제거할 수 있게 되었는데, 제거할 수 있었던 가장 큰 이유는 dirichlet distribution과 multinomial distribution의 곱셈이 다시 dirichlet distribution의 형태가 되었음에 있다.이를 간단하게 작성해보면 P(X|θ) x P(θ) 이고, 이 likelihood와 prior의 곱셈이 prior distribution을 만들어내는 과정을 Conjugate prior관계라고 한다. 이 관계를 통해 Collapse Gibbs sampling을 수행할 수 있게 되었다.  이러한 상황에서 target인 Z에 대해 한번의 반복마다 하나의 state에 대해서만 업데이트하는 Gibbs sampling을 수행한다. 그렇기에 m번째 문서의 l번째 토픽 할당에 대한 식으로 변형해주어야 한다. $ P(Z_{(m,l)} = k | Z_{-(m,l)},W;\\alpha, \\beta) $이를 joint로 변형을 해주면 분모에 있는 값은 k에 영향을 받지 않으므로 normalizing constant로 간주할 수 있다. 그 후, prior에 대해서만 변수로 존재하는 것들은 모두 곱셈 밖으로 꺼낼 수 있으며, 이는 상수로 간주할 수 있어 비례 형태로 치환할 수 있다. 그런 다음, 우리는 Gibbs sampling을 적용하므로 m번째 문서의 l번째 토픽에만 관심이 있으므로, 곱셈에서 M=m으로, V=l로 고정시켜본다. 또한, m번째 문서로 고정한 상태에서 단어들에 대해 덧셈을 하는 과정이고, 토픽 할당에 대해 이전의 값에서 현재의 값으로 변화하는 것이므로 이전의 값은 -1, 현재의 값은 1로 할당하게 되어 결국은 상수로 존재하게 된다. 그 후, z에 대해 m번째의 l번째 단어를 제외한 나머지들의 n을 구한다. 그리고는 K에 대한 곱셈을 K=k를 분리하여 식을 세운다. 이 때, k를 분리했을 때는 K=k를 제외한 값에서 K=k에 대한 할당 값이 1개 늘어나는 것이므로 K=k를 제외한 계산식에서 개수를 +1해주면 된다.그 다음, $ \\gamma $ 식의 정의를 활용하고자 한다. $ \\gamma(x) = (x - 1)! $ 이고, 따라서 $ \\gamma(x+1) = x! = (x-1)! \\times x = \\gamma(x) \\times x $ 이므로, 아까 더해주었던 +1을 분리시켜 줄 수 있다. +1에 대한 값을 분리해주고 나면, 결국 K=k를 제외한 계산식과 다시 결합을 할 수 있게 된다. $ {\\color{DarkRed} (이 때도 α_k가 아닌 α_i 가 맞음)} $이러고 나면, 4개의 값들의 곱셈 중 앞의 2개의 값들은 k와 아무런 연관이 없으므로 상수이기에 비례식으로 제거해줄 수 있다. 최종적으로 Z를 샘플링하는 데 n을 사용함으로서 각 단어가 어디에 할당되는지 개수를 세어 특정 l번째 단어가 k에 할당될 확률을 구할 수 있다.  최종적으로 LDA를 프로그래밍적 관점에서 살펴보면, 간단해진다. LDA의 과정은 다음과 같다. 입력 : TextCorpus T, prior α, prior β 무작위로 Z를 초기화한다. 무작위로 할당된 Z를 통해 간 문서별 토픽별 단어별로 n을 계산한다. 아래 과정을 수렴 할 때까지 반복한다. m=1부터 문서의 개수만큼 아래 과정을 반복 l=1부터 m번째 문서에서의 단어의 개수만큼 아래 과정을 만복 $ P(Z_{(m,l) = K}|Z_{-(m,l)},W;\\alpha, \\beta) $ 식을 통해 K를 샘플링한다. Z_(m,l) = k 로 할당되어 있는 단어들에 대해 n을 계산한다. Perplexity 기법을 통해 성능을 평가한다. θ와 φ를 계산한다. - φ : 각 word 별 topic의 probability - θ : 특정 문서에서 단어들의 토픽 할당을 평균내어 생성 θ와 φ 반환  perplexity란 두 개의 모델이 있을 때 비교하는 기법으로 자연어처리에 많이 사용된다. 이를 줄여 PPL이라고도 하는데, PPL 수치가 낮을수록 성능이 좋다. " }, { "title": "[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model ", "url": "/posts/kooc_week9/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-11-09 22:10:00 +0900", "snippet": "Chapter 9. Hidden Markov Model 목차 Hidden Markov model static clustering to the dynamic clustering difference of the graphical model three major questions of HMM evaluation question decoding question learning question link some method forward-backward algorithm to the message passing baum-welch algorithm to the EM algorithm.   9-1. Hidden Markov Model이때까지는 시간에 관계없는 상황에서의 데이터들이었다. 그러나 만약 시간에 따라 변화하는 데이터에 대해서는 어떻게 다룰 수 있는지 살펴보고자 한다.시간에 따라 변화한다는 것은 latent variable이 연관되어 있다는 뜻이고, 이를 그래프화하면 다음과 같다.왼쪽 그래프는 이때까지 다뤄왔던 각 데이터들이 독립적인 관계에 대한 것이고, 오른쪽 그래프가 시간이라는 latent variable이 추가되어 각 포인트마다 관련이 되어 있는 상황이다. 관측값 x는 이산적일수도, 연속적일수도 있다. 이산적이라면, x는 특정 값을 가지고, 연속적이라면 확률 분포를 가진다. 이번 강의에서는 이산적인 경우에 대해서만 다뤄보고자 한다.시간이 1~T까지 흘러감에 따라 x는 x1~xT로 존재한다. latent variable, z의 경우도 이산적일수도 있고, 연속적일수도 있다. latent variable이 연속적인 경우에 사용하는 기법을 Kalman filter 이라 한다. 따라서 이번 강의에서는 z도 이산적인 경우만 다룬다. z는 vector로서 k개의 elements를 가진다. initial probability, P(z1) 은 π1~πk 에 대한 multinomial distribution에 의해 생성된다.그리고 HMM(Hidden Markov Model)에는 2가지 확률이 더 존재한다. Transition Probabilitiesz1에서 z2, z2에서 z3등과 같이 $ z_{i} $에서 $z_{j}$로 가는 관계를 나타내는 확률이다. 수식적으로 보면, t-1에서의 z에 대한 z_t의 condition probability를 나타내고, 이는 a라는 변수에 대한 multinomial distribution로 나타내어진다.이 때, z_t=1이라는 특정 상황에 대한 확률을 $ a_{i,j} $ 로 나타낼 수 있다. i와 j는 z의 k개의 elements에서 i번째 elements와 j번째 elements에 대한 index를 나타낸다.  Emission Probabilities이는 z에서의 x에 대한 관계를 나타낸다. 이 또한, x_t=1이라는 특정 상황에 대한 확률을 $ b_{i,j} $ 로 나타낼 수 있다.  9-2. Forward-Backward ProbabilityHMM의 과정을 자세하게 살펴보기에 앞서, 알고 가야할 개념들이 있다. Many Questions일단, ML(machine Learning)에서 몇가지 과제가 있는데, 이는 주어진 변수들에 따라 달라진다. Evaluation question π, a, b, X 가 주어진 경우 즉, 관측값과 latent factor에 대한 정보도 모두 가지고 있는 상태 P(X|Model,π,a,b)를 찾는 것이 목적 π,a,b,X가 주어졌을 때 학습된 모들에 의해 특정 X가 관측될 확률을 구함.   Decoding question π, a, b, X 가 주어진 경우 $ argmax_Z P(Z|X,M,π,a,b) $ π,a,b,X가 주어졌을 때, latent factor, Z의 확률 중 가장 확률이 높은 Z를 구하는 것, 즉 Z를 학습시키면서 가장 최적의 Z를 찾음.   Learning question X만 주어진 경우 $ argmax_{π, a, b} P(X|M,π,a,b) $ 관측값만 가진 상태에서 특정 X가 관측될 확률을 구함. decoding의 경우 관측값과 GT가 주어지므로 Supervised learning과 유사하고, learning 의 경우 관측값만 주어진 상황에서 확률을 구해야 하는 unsupervised learning과 유사하다.   loaded dice간단한 예시를 들어보자. 기울어진 주사위가 있어서 6이 나올 확률이 1/2이고 나머지는 모두 1/10 확률을 가진다고 하자. 일반적인 주사위는 모두 동일하게 1/6의 확률을 가진다.latent factor로서, 기울어진 주사위(L)을 사용할지 평범한 주사위(F)를 사용할지에 대한 확률이 있고, 처음 L을 사용할 확률을 0.5, L을 사용한 후에 다시 L을 사용할 확률을 0.7, F를 사용한 후 F을 사용할 확률을 0.5라 가정한다.X와 Z가 주어진 상황에서 dataset을 훈련시킬 때, 간편한 계산을 위해 Joint probability를 사용해볼 것이다. baysian network를 생각했을 때, P(X,Z)는 다음과 같이 풀어줄 수 있다.\\[P(X,Z) = P(x_1,...,x_t,z_1,...,z_t) = P(z_1)P(x_1|z_1)P(z_2|z_1)P(x_2|z_2)P(z_3|z_2)P(x_3|z_3)\\]아무것도 가정하지 않고도 joint와 conditinoal probability의 정의에 따라 풀어줄 수 있다.앞서 배운 transition과 emission probability를 적용하면 다음과 같다.$ P(X,Z) = \\pi_{z_1} b_{x1=1,z_1=1} a_{z_1=1,z_2=1} \\dots $  그렇다면, P(166,LLL) 과 P(166,FFF) 에 대한 값을 직접 구할 수 있다. 처음 L을 사용할 확률은 0.5, L을 사용했을 때 1이 나올 확률은 0.1, L을 사용했을 때 6이 나올 확률은 0.5이고, L을 사용한 후에 다시 L을 사용할 확률은 0.7이었으므로 다음과 같이 계산한다.P(166,LLL) = 1/2 x 1/10 x 7/10 x 1/2 x 7/10 x 1/2 = 0.0061 P(166,FFF) = 1/2 x 1/6 x 1/2 x 1/6 x 1/2 x 1/6 = 5.7870e - 04F를 사용했을 때 1이 나올 확률과 6이 나올 확률은 동일하게 1/6이고 처음 F를 사용할 확률은 1/2, F에서 F를 다시 사용할 확률은 0.5였다. 이처럼 직접 구해줄 수는 있지만, 횟수가 늘어남에 따라 경우의 수는 기하급수적으로 증가하므로 큰 횟수에 대해서는 직접 구해주기 힘들다.  이전에 다뤘던 marginalization 기법을 사용하고자 한다.$ P(X|\\theta) = \\sum_Z P(X,Z|\\theta) $ HMM에서는 θ가 아닌, π,a,b이므로 이에 대해 다시 정의한다.$ P(X|\\pi,a,b) = \\sum_Z P(X,Z|\\pi,a,b) $ $ P(X) = \\sum_Z P(X,Z) = \\sum_{z_1} \\dots \\sum_{z_t} P(x_1,…,x_t,z_1,…,z_t) = \\sum_{z_1} \\dots \\sum_{z_t} \\pi_{z_1} \\prod_{t=2}^T a_{z_{t-1},z_t} \\prod_{t=1}^T b_{z_t,x_t}$이는 너무 많은 연산이 들어가므로, 반복적인 연산을 피하기 위해 하나의 시간에 대해서만 계산해보자. 특정 시간 t에 대해 구하기 위해서 t-1에서의 z를 marginalization 처리해준다.$ P(x_1,…,x_t,z_t^k=1) = \\sum_{z_{t-1}} P(x_1,…,x_{t-1},x_t,z_{t=1}, z_t^k=1) = \\sum_{z_{t-1}} P(x_1,…,x_{t-1},z_{t-1})P(z_t^k=1|z_{t-1})P(x_t|z_t^k=1) $이 때, z_t에 대한 확률을 구할 때는 x와는 모두 독립적이고, x_t에 대한 확률을 구할 때는 x_1~t-1에 대한 값들과 z_t-1에 대해서는 독립적이므로 지워줄 수 있다.지워주고 나서 시그마에 관련 없는 값들은 밖으로 빼내고 나면, 이전에 구했던 transition과 emission probabilities를 적용할 수 있다.적용하고 나면$ P(x_1,…,x_t,z_t^k=1) = \\alpha_t^k = b_{k,x_t} \\sum_i \\alpha_{t-1}^i a_{i,k} $ 즉, 시간 t에 대한 확률은 시간 t-1에서의 확률에 값을 곱한 것과 동일한 재귀적인 관계가 형성된다.  Dynamic Programming이 때, dynamic programming이라는 기법이 등장한다. 이는 특정 값을 연산하는 데 있어서 겹치는 값들에 대해서는 재사용(recurrence)하는 방식이다. 예를 들어, 피보나치 수열에서 n=4를 구하기 위해서는 F(3)과 F(2)를 구해야 하고, F(3)을 구하기 위해서는 F(2)와 F(1)을 구해야 한다.이런 Top-down 방식으로 연산하는 것을 Recursion이라 하고, 이 recursion의 단점은 같은 값을 반복적으로 연산하는 것이다.반대로, 아래에서부터 연산을 시작하여 F(4)가 될 때까지 연산하는 방식이 dynamic programming이다. 이런 방식으로 진행하게 되면, 먼저 F(0)을 계산하고 값을 저장한다. 그리고 F(1)을 계산하고 저장하고, F(2)를 연산할 때 저장해둔 F(0)과 F(1)을 가져와 F(2)를 계산한다. 이렇게 연산을 이어나가다가 원래 찾고자 했던 값인 F(4)를 연산하고 나면 과정을 종료한다.dynamic programming의 장점은 반복적인 연산을 하지 않는다는 것이다. 이와 같이 반복적인 연산을 할 때 값을 저장하고 불러오는 과정을 Memoization이라 한다.  Forward Probability Calculation만약 $ \\alpha_t^k $ 를 안다면 Z를 알아낼 필요없이 P(X)를 계산할 수 있다. 따라서 이 $ \\alpha_t^k $ 를 계산하기 위해 Forward Probability를 사용한다. Initialize Iterate until time T $ \\alpha_t^k = b_{k,x_t} \\sum_i \\alpha_{t-1}^i a_{i,k} $ Return $ \\sum_i \\alpha_T^i $ $ \\sum_i \\alpha_t^i = \\sum_i P(x_1,…,x_t,z_t^i = 1) = P(x_1,…,x_t) $  Forward probablity의 한계로는 t가 전체 데이터셋에 대한 T일 필요가 없다는 점이다. 예를 들어 t가 2라고 한다면, z3에 도달하지 않은 채로 x1,x2,z2에 대해 모델링된다. 그렇다 하더라도 문제가 발생하지 않는다. 따라서 전체 데이터셋인 X에 대한 확률도 계산해줘야 한다. 그를 위한 방법이 Backward probability calculation 이다.  Backward Probability Calculation이 때는 P(X,z_t^k=1) 이 아닌 P(z_t^k=1|X) 를 계산한다. 전체 X에 대해 연산하기 위해 앞서 구했던 x_t에서 x_t+1~x_T를 추가한다.$ P(z_t^k=1,X) = P(x_1,…,x_t,z_t^k=1,x_{t+1},…,x_T) = P(x_1,…,x_t,z_t^k=1)P(x_{t+1},…,x_T|x_1,…,x_t,z_t^k=1) $이 때도 x_t+1~x_T과 x_1~x_t는 독립적이므로 제거할 수 있다.$ P(x_1,…,x_t,z_t^k=1)P(x_{t+1},…,x_T|z_t^k=1) $P(x_1,…,x_t,z_t^k=1)는 forward 계산할 때 α_t^k로 지정했다. 그리고 P(x_{t+1},…,x_T|z_t^k=1) 를 β_t^k 로 정의한다. $ P(x_{t+1},…,x_T|z_t^k=1) = \\sum_{z_{t+1}} P(z_{t+1},x_{t+1},…,x_T | z_t^k=1) = \\sum_i P(z_{t+1}^i =1 | z_t^k = 1) P(x_{t+1} | z_{t+1}^i = 1, z_t^k=1) P(x_{t+2},…,x_T|x_{t+1},z_{t+1}^i=1,z_{t}^k=1) $이 떄, x_t+1에 대해 구할 때 z_t+1을 알고 있다면, z_t과는 독립적이 되어 제거되고, 그 뒤에 x_t+1도 x_t+2~x_T와는 독립적이므로 제거할 수 있다.이렇게 다 제거하고 나면 이전에 구했던 a,b,β로 치환할 수 있다.$ P(x_{t+1},…,x_T|z_t^k=1) = \\beta_t^k = \\sum_i a_{k,i}b_{i,x_t} \\beta_{t+1}^i $이 때, β_t+1 의 size는 t+1에서 T로 점점 다가가므로 감소된다.  9-3. Viterbi Decodingdecoding question에서 가장 많이 사용되는 기법이 Viterbi decoding이다. $ \\alpha_t^k $ : forward probability $ \\beta_t^k $ : backward probability$ P(z_t^k=1,X) = \\alpha_t^k \\beta_t^k = (b_{j,x_t}\\sum_i \\alpha_{t-1}^i a_{i,k}) \\times (\\sum_i a_{k,i}b_{i,x_t} \\beta_{t+1}^i) $이렇게 특정 time t에 대한 joint probability를 계산할 수 있고, joint를 통해 conditional probability도 계산할 수 있다. 그러나 이는 특정 시간 t인데, 우리가 구하고자 하는 것은 전체 sequence를 보고 전체 sequence에 대한 latent variable이다.그래서 z_t가 아닌 전체 latent variable인 Z에 대해 구해보고자 하고, 이 과정을 decoding question이라 할 수 있다. $ P(x_1,…,x_{t-1},z_1,…,z_{t-1},x_t,z_t^k=1) $ 를 $V_t^k $라 정의하고, 이는 t-1까지의 가장 확률이 높은 Latent vector을 나타낸다.즉, estimation하고자 하는 latent factor(z_t)가 1이라는 클러스터에 속하고 있는 상황에서 $ x_1,…,x_{t-1} $과 $ z_1,…,z_{t-1}, x_t, z_t^k=1 $ 이 관측될 확률이 최대가 되는 z1~z_t-1을 최적화하겠다는 의미이다.t-1까지의 과정을 dynamic programming을 통해 구한다. V_t^k 의 joint probability를 condition probability로 나타내어 간편화한다.$ V_t^k = max_{z_1,…,z_{t-1}} P(x_t,z_t^k=1|x_1,…,x_{t-1},z_1,…,z_{t-1})P(x_1,…,x_{t-1},z_1,…,z_{t-1}) $이 때, x_t와 z_t^k=1에 대해 구할 때, x1,…,x_t-1,z1,…,z_t-2와는 독립적이므로 제거된다. 이에 따라 maximize하는 값도 앞에는 z1~z_t-1이 아닌 z_t-1로 변환되고, 뒤에는 V_t^k 식과 동일한 형식이므로 z1~z_t-2로 바뀌게 된다.이 때, time t-1의 latent factor가 i라는 라는 클러스터에 속해진다고 가정을 하게되면, 다음과 같이 나타내진다.$ max_{i \\in t-1} P(x_t,z_t^k=1|z_{t-1}^i = 1)V_{t-1}^i = max_{i \\in t-1} P(x_t|z_t^k=1)P(z_{t-1}^i = 1)V_{t-1}^i $forward, backward probability 인 a와 b로 나타내면$ b_{k,x_t} max_{i \\in z_{t-1}} a_{i,k} V_{t-1}^i $ 이 식을 통해 t=T일 때까지 반복하여 V_(t-1)^i 를 업데이트한다.  viterbi decoding에 대한 예제가 하나 있다. 제품을 생산하는 시간을 최적화하는 알고리즘으로, 각 원(station)안에 들어 있는 숫자는 프로세스 시간이고, assembly line이 2개 존재한다. 각 station에서는 같은 line으로 갈수도 있고, 다른 line으로 갈수도 있는데, 다른 line으로 갈 경우 추가 시간이 발생된다.이러한 경우 최소한의 시간이 드는 루트는 어떻게 되는지 확인하는 과정이다.아래의 두 개의 테이블에서 첫번째 table은 각 시간마다 line별 걸린 시간을 나타낸 것이고, 오른쪽 table은 해당 시간에서 이전의 line이 어디였는지를 나타내는 값이다. 해당 시간에서 소요된 시간이 더 적은 line에서 이동된 것으로서, 이 숫자들을 역추적해서 이으면 최적의 루트가 된다.즉, 마지막이 line1에서 끝났으므로 1을 선택하고, t=6에서 2가 되어 있으므로 t=5에서는 line2가 소요된 시간이 적게 걸렸다고 볼 수 있다. t=5에서는 L2에 2로 되어 있으므로 t=4에서도 line2에 위치하는 것이 최적의 루트이다. 이렇게 따라가다보면, 빨간색 화살표의 경로처럼 된다.  이러한 경우를 확률적으로 계산해보자. 일단 먼저 확률을 계산하는 table과 trace에 대한 memoization table이 존재해야 한다. trace란 이전 시간에서 최적의 루트를 나타내는 값이다.  Viterbi Decoding Algorithm Initialize $ V_1^k = b_{k,x_1} \\pi_k $ π라는 초기값을 통해 V_1^k를 계산한다. Iterate until time T $ V_t^k = b_{k,x_t} max_{i \\in z_{t-1}} a_{i,k} V_{t-1}^i $ $ trace_t^k = argmax_{i \\in z_{t-1}} a_{i,k} V_{t-1}^i $ V_t^k 는 앞서 말한 확률 table에 대한 값이고, trace는 trace table에 대한 값이다. Return $ P(X,Z) = max_k V_T^k, z_T^ = argmax_k V_T^k,z_{t-1}^* = trace_t^{z_t^*}$  viterbi decoding에도 한계가 존재한다. a,b가 확률이므로 데이터가 100개 200개와 같이 많은 경우 값이 매우 작아져서 0에 가까워진다.따라서 이러한 곱셈 연산에 log를 씌워 곱셈을 덧셈으로 바꿔 연산을 하게 해야 한다. 최종적으로 이러한 decoding algorithm을 통해 학습된 파라미터(π,a,b)와 X를 통해 새로운 sequence(데이터)에 대해 최적의 클러스터링을 할 수 있게 된다.자연어 처리를 예로 들면, 모델을 학습하여 π,a,b를 얻은 후 새로운 데이터(문장)이 들어왔을 때, viterbi decoding 과정을 통해 pos-tagging(품사 태깅)을 할 수 있다.  9-4. Baum-Welch algorithmdecoding알고리즘은 supervised learning에 해당되었다. π,a,b,X 를 모두 알고 있다는 가정하에 새로운 데이터에 대해 fitting하는 작업을 했다. 이제는 unsupervised learning, 즉 X만 주어진 상황에서 모델을 학습하는 동시에 새로운 데이터에 대해 fitting하는 작업을 해보고자 한다. 이를 수행하기 위한 방법이 Baum-Welch 알고리즘이다. 이 Baum-Welch 알고리즘은 이전에 배웠던 EM 알고리즘과 동작이 거의 동일하다.  clustering을 하는 데 있어서 HMM의 파라미터인 π,a,b는 Forward algorithm과 Viterbi algorithm(decoding)에서 사용된다.그러나 π,a,b는 X와 Z를 알고 있다는 가정 하에 추론이 가능해진다. 그러나 현실에서는 Z를 관찰하는 것은 매우 어렵다. 이 Z를 추론하기 위해 학습을 시킨다면, annotation이 필요하게 된다.만약 Z를 모르고, X만 관찰할 수 있다면 clsutering을 위해서는 Z와 π,a,b를 모두 찾아야 한다. 파라미터들을 최적의 값으로 찾아내기 위해 EM algorith을 사용하고자 한다. EM 알고리즘의 전략은 다음과 같다. X에 대해 π,a,b를 최적화한다. X,π,a,b에 대해 가장 확률이 높은 Z를 찾는다. 이를 반복적으로 작업하여 Z,π,a,b를 찾는다. EM algorithm에 대해 조금 더 자세하게 살펴보자. EM 알고리즘의 순서는 다음과 같다. 무작위 값으로 $ \\theta^0 $ 를 초기화한다. likelihood가 수렴이 될때까지 아래 과정을 반복한다. Expectation step $ q^{t+1}(z) = argmax_q Q(\\theta^t, q) = argmax_q L(\\theta^t, q) = argmin_q KL(q||P(Z|X,\\theta^t)) $ q는 latent variable 할당에 대한 값으로, X와 θ^t가 주어졌을 때, Z를 할당하는 확률 그대로 q를 할당한다. Maximization step $ θ^{t+1} = argmax_θ Q(θ, q^{t+1}) = argmax_θ L(θ, q^{t+1}) $ 할당된 Z와 관측값 X에 대해 MLE를 수행하여 파라미터들을 최적화한다.  이 과정을 HMM(Hidden Markov Model)에 적용하면 아래와 같게 된다. 무작위 값으로 $ π^0, a^0, b^0 $ 를 초기화한다. likelihood가 수렴될 때까지 아래 과정을 반복한다. Expectation step $ q^{t+1}(z) = P(Z|X,π^t, a^t, b^t) $ Maximization step $ π^{t+1}, a^{t+1}, b^{t+1} = argmax_{π,a,b} Q(π,a,b,q^{t+1}) $ 지난 주 KL divergence를 배울 때, Q에 대해 정의했다. ln함수는 concave에 해당하므로 최소값과의 차이이 H(q)와 함께 ln안에 있던 ∑를 밖으로 빼냈다. E는 expectation값 즉, 기대값을 의미하고, 이 기대값 q^t+1(Z)는 Expectation step에서 P(Z|X,π^t, a^t,b^t) 로 정의하기로 했으므로 파라미터의 업데이트 식은 다음과 같이 정의된다. $ π^{t+1}, a^{t+1}, b^{t+1} = argmax_{π,a,b}E_{q^{t+1}(z)} lnP(X,Z|π,a,b) + H(q) $ 이 때, P(X,Z|π,a,b)는 joint를 condition probability으로 바꿈과 동시에 marginalization을 통해$ P(X,Z|π,a,b) = \\pi_{z_1} \\prod_{t=2}^T a_{z_{t-1},z_t} \\prod_{t=1}^T b_{z_t,x_t} = ln\\pi_{z_1} + \\sum_{t=2}^T lna_{z_{t-1},z_t} + \\sum_{t=1}^T lnb_{z_t, x_t} = Q(π,a,b,q^{t+1}) $로 정의된다. Q 함수를 최적화해야 하므로, 이에 대해 미분을 수행하는데, 제약조건이 하나 있다. π를 multinomial distribution을 따른다고 정의했으므로 $ \\sum_i π_i = 1 $ 이다. 따라서 lagrange method를 사용하여 미분해야 한다.$ L(π,a,b,q^{t+1}) = Q(π,a,b,q^{t+1}) - \\lambda(\\sum_{i=1}^k \\pi_i - 1) - \\sum_{i=1}^K \\lambda_{a_i} (\\sum_{j=1}^K a_{i,j} -1) - \\sum_{i=1}^K \\lambda_{b_i} (\\sum_{j=1}^M b_{i,j} -1) $ 이 식에서 π에 대해 편미분을 수행해보자.$ \\frac{dL(π,a,b,q^{t+1})}{dπ_i} = \\frac{d}{dπ_i} \\sum_Z P(Z|X,π^t, a^t, b^t) lnπ_{z_1} - \\lambda_π (\\sum_{i=1}^K π_i - 1) = 0 $이 식은 z_1이 i 일 때만 성립되므로, z_1 = i로 두어 π의 최대값을 찾는다.$ \\frac{P(z_1^i = 1| X,π^t, a^t, b^t)}{π_i} = \\lambda_\\pi $ 이 식을 바로 풀기는 어려우므로 L함수를 $ \\lambda_\\pi $ 에 대해 편미분을 수행한다.$ \\frac{d}{d\\lambda_\\pi} L(\\pi,a,b,q^{t+1}) =\\gt : \\sum_{i=1}^K \\pi_i = 1 $즉, π_i를 i=1부터 K까지 모두 더하면 1이 된다는 것이다.그렇다면 $ \\lambda_\\pi $ 는 분자에 있는 값을 1~K만큼 합한 것과 같고, 이것이 곧 $ π^{t+1} $ 이 된다.이 프로세스를 동일하게 a와 b에 적용하면 우상단의 값들로 추출이 된다. Baum Welch algorithm은 결국 HMM에 대한 learning question이자 HMM을 위한 EM 알고리즘이다." }, { "title": "[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm ", "url": "/posts/kooc_week8/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-10-24 22:10:00 +0900", "snippet": "Chapter 8. K-Means Clustering and Gausssian Mixture Model 목차 Clustering task and K-Means algorithm unsuperveised learning K-Means iterative process limitation of K-Means algorithm Gaussian mixture model multinomial distribution and multivariate Gaussian distribution why mixture model parameter updates are derived from the Gaussian mixture model EM algorithm fundamentals of the EM algorithm how to derive the EM updates of a model   8-1. K-Means algorithm1~6주차에서는 supervised learning에 대해 배웠다.이제는 k-means라는 unsupervised learning을 배워보고자 한다. unsupervised learning이란 label이 존재하지 않은 데이터셋을 활용하여 task를 수행하는 것이다. true value를 모르는 상황에서 패턴을 찾아야 한다. 만약, 우리가 이 데이터셋에 대해 색상 데이터를 가지고 있다면, supervised task가 될 것이다. 그러나 오른쪽과 같이 어떤 군집이 만들어지는지에 대해 모르고, 군집을 찾아야 한다면 이는 unsupervised learning이 된다.  k-means algorithm이란, 잠재적으로 생각하기에 내부적으로 k개의 동력이 있어서 n개의 데이터 포인트들을 clustering하는 기법이다. 위의 데이터들에 대해 k=3개의 동력원이 존재하여 n개의 데이터들이 3개의 동력원들에 의해 군집화된다. 여기서 중요한 것은 K-Means와 K-Nearest Neightbor algorithm은 동작방식이 비슷하나, 다른 기법이다. K-Means는 군집을 찾기 위한 unsupervised task에 사용되는 기법이고, KNN은 supervised task에서 사용되는 기법이다. KNN는 한 데이터의 주변에 가장 가까운 점 k개를 찾아 그에 대해 분류하고, 한 데이터를 주변 점들이 많이 분포되어 있는 값으로 지정해주는 것이다. 그러나 K-Means는 실제 컴퓨터가 보는 정보는 분류되지 않은 검은색 점들이고, 무엇보다 중심점들조차 주어지지 않는다. 이러한 상황에서 중심점들을 찾고, 그에 관련된 데이터들을 분류해야 한다.따라서 K-Means는 크게 두 단계로 구성되어 있다. 첫번째로는 중심점을 찾는 것이고, 그 후에 중심점들과의 거리를 통해 데이터들을 분류한다.  clustering에 대한 식은 다음과 같다. $ r_{nk} $ : a assignment variable of data point to cluster $ \\mu_k $ : location of centroids$ r_{nk} $ 는 cluster에 대한 파라미터로서, 각 데이터 포인트는 1개의 centroid(중심점)에 대해서만 할당된다. 그러므로, 할당된 centroid에 대해서는 1로, 나머지 centroid에 대한 것은 0으로 할당하여 연산한다. 그리고, $ \\mu_k $ 는 중심점의 위치이다.최종적으로 J 를 최소화하는 것이 최적화의 목적이다. 그러나 이 때는 파라미터가 2개이므로, 이러한 경우는 반복적인 최적화를 통해 먼저 $ r_{nk} $를 최적화하고, 최적화된 $ r_{nk} $ 를 통해 $ \\mu_k $를 최적화하고, 다시 최적화된 $ \\mu_k $를 통해 $ r_{nk} $를 최적화하는 방식을 통해 J를 최소화한다.   Expectation 파라미터($ r_{nk} $)에 의해 구해지는 확률을 통해 centroid를 중심으로 한 데이터 포인트들을 할당해준다. Maximization likelihood에 대한 파라미터($ \\mu_k $)를 최대화시킴으로써 centroid 위치를 업데이트한다. $ \\mu_k $ 에 대해 optimization하는 과정은 다음과 같다. 최적화하는 방법은 동일하게 미분하여 =0인 값을 찾는 것이다.결론적으로 구해보면, $ \\mu_k $는 할당된 데이터 포인트들의 평균값이 된다. k-means algorithm의 과정은 다음과 같다.특정 반복 횟수만큼 아래 과정을 반복한다. 데이터들 중 random 하게 n개를 골라 centroid에 대한 파라미터인 $ \\mu_k $ 를 지정한다. $ r_nk $를 가까운 centroid( $ \\mu_k $)에 대해 assign한다. ( 0 또는 1 ) assign된 데이터들의 평균값을 구해 $ \\mu_k $ 를 업데이트한다. 다시 $ r_nk $를 업데이트한다. 이러한 반복 과정을 통해, 점차 최적의 centroid를 찾을 수 있다.  이러한 간단한 k-means algorithm에는 한계가 존재한다. centroid의 개수가 명확하지 않다. centroid의 초기 위치가 잘못 지정되면 잘못된 optimization이 수행될 수도 있다. limitation of distance metrics euclidean distance만으로는 데이터의 특정을 정확하게 파악하기 힘들다. hard clustering r_nk가 0 또는 1이므로 discrete하다는 단점이 있다.    8-2. Gaussian Mixture Model Multinomial Distributiongaussian mixture model을 배우기에 앞서, Multinomial distribution에 대해 다시 한 번 짚고 넘어가자. 먼저 binomial distribution이란, 0 또는 1에 대해서만 나타내어지는 distribution이다. binomial distribution에 있어서 X=(0,0,1,0,0,0) 에 대해 3개를 선택하는 것과 같이, 확률에 대한 식은 다음과 같다.\\[P(X|\\mu) = \\prod_{k=1}^K \\mu_k^{x_k}, \\sum_k x_k = 1\\]이 때의 제약조건(Subject)는 $ \\mu_k \\geq 0 $ $ \\sum_k \\mu_k = 1 $ 이 0 또는 1의 binary한 값을, 0,1,2,3,… 에 대한 값으로 늘린 것이 multinomial distribution 이다. N개의 선택지($x_1$,…,$x_n$)를 가진 데이터셋 D가 있다고 할 때의 확률은 다음과 같다.\\[P(X|\\mu) = \\prod_{n=1}^N \\prod_{k=1}^K \\mu_k^{x_{nk}} = \\prod_{k=1}^K \\mu_k^{\\sum_{n=1}^N x_{nk}} = \\prod_{k=1}^K \\mu_k^{m_k}\\]이 때, $ m_k = \\sum_{n=1}^N x_{nk} $ 이다.MLE, 즉 확률이 최대가 되는 μ를 구하기 위한 식은 다음과 같다. Maximize $ P(X|\\mu) = \\prod_{k=1}^K \\mu_k^{m_k} $ 제약조건 $ \\mu_k &amp;gt;= 0, \\sum_k \\mu_k = 1 $ MLE task에서 제약조건이 존재할 때에는 lagrange function을 활용하여 MLE를 수행할 수 있다. lagrange function, L은 다음과 같다.$ L(μ,m,𝜆) = \\sum_{k=1}^K m_k :ln\\mu_k + \\lambda(\\sum_{k=1}^K \\mu_k - 1) $lagrange function을 μ_k 에 대해 미분하면 이 때, 제약조건인 $ \\sum_k \\mu_k = 1 $를 활용하면이 때, $ \\sum_k \\sum_{n=1}^N x_{nk} = N $ 인 이유는 모든 선택지에 대한 summation인 k와 개별 선택지마다 모든 데이터 포인트가 선택되는지에 대한 summation인 N에 대해 x_nk를 summation하는 것이므로, N이 된다. 이 때 x는 각 instance k에 대해 n개의 선택지가 선택될 확률이다. 결론적으로 MLE에 대한 μ_k는 m_k/N이 된다. 이는 특정 선택지가 선택된 개수 / 관측한 선택지 전체를 의미한다.  Multivariate Gaussian DistributionGaussian Mixture model을 위해 또 다른 재료가 필요하다. 그것이 Multivariate Gaussian Distribution이다. single dimension에서의 Gaussian Distribution 식은 다음과 같다.이를 multi dimension으로 바꿀 때는 variance가 covariance matrix로 변환되어 만들어진다. μ값 또한, 벡터의 형태가 된다.이 multivariate gaussian distribution 식에 대한 MLE를 찾아보자. 먼저 log를 씌워서 식을 단순화시킨다.그리고 이 때, 사용되는 기법이 trace trick 이다. trace trick은 선형 대수에서 등장하는 기법으로Trace trick은 위와 같이, Tr 이라는 기호를 통해 나타낸다. 이렇게 간단해진 식을 미분한다. 변수는 μ와 $\\sum$ 이므로 이 둘에 대해 미분한다. trace_trick에 대해 미분하면 간단하게 값을 구할 수 있다.  covariance matrix에 대한 예시가 있다.covariance_matrix를 다양하게 변화시켜가면서 샘플링한 결과이다. 행렬에서 1행 1열과 2행 2열의 위치의 값은 variation이고, 행렬에서 2행 1열과 1행 2열 위치의 값은 correlation 즉 연관성에 대한 상수이다. 만약 분산이 둘다 1인 상황에서 correlation도 둘다 1이라면, 기울기가 양수인 직선의 형태로 나오게 될 것이다. x방향으로도 분산이 1, y방향으로도 분산이 1이고, 서로 상관관계가 크게 작용하고 있다는 의미이기 때문이다.모든 값을 0으로 두면, 분산이 0이므로 점으로 모이게 될 것이고, 분산은 둘다 존재하나 연관성이 없다면 원의 형태로 둥글게 생긴다.correlation이 음수인 경우는 반대 방향으로 작용될 것이다.  Mixture Model이렇게 구한, Multinomial distribution에 대한 MLE estimation 식과 Multivariate gaussian distribution에 대한 MLE estimation을 융합한 것이 Mixture model이다.mixture model을 히스토그램 측면에서 바라본다면, 여러 개의 데이터가 하나로 모여 있는 것으로 보인다. 그래서 이를 1개의 normal distribution으로 만들면 다소 부정확한 모델이 된다.따라서, 이러한 모델의 경우 여러 개의 normal distribution으로 만들어서 나타내고자 한다. 이를 Mixture distribution이라 한다. mixture distribution에 대한 식은 다음과 같다.\\[P(x) = \\sum_{k=1}^K \\pi_k N(x|\\mu_k, \\sigma_k)\\] $ \\pi_k $ : Mixing coefficients multinomial distribution에 대한 값으로, K개의 옵션이 존재하고 그 중 하나가 선택될 확률을 나타낸다. 이 때의 K는 normal distribution의 개수이다. weight의 역할을 수행 $ \\sum_{k=1}^K \\pi_k = 1 $ 0 &amp;lt;= π_k &amp;lt;= 1 k-means에서의 weight 변수는 0 또는 1의 값을 가지지만, 이 때는 확률적인(stochastic) 값을 가진다. $ N(x|μ_k, σ_k) $ : Mixture component multivariate gaussian distribution에 대한 값으로, 개별 normal distribution를 나타낸다. z라는 새로운 variable이 있다 생각한다면, x에 대한 확률 P(x)는 다음과 같다.\\[P(x) = \\sum_{k=1}^K P(z_k)P(x|z)\\]k개 중 z가 선택될 확률과, z가 주어졌을 때, 즉 어떤 normal distribution을 선택했는지에 대한 값이 주어진 상태에서의 x의 확률, 즉 normal distribution을 나타낸다. Gaussian Mixture Model이제 데이터 포인트들이 multiple multivariate Gaussian Distribution의 mixture distribution에 대해 구성된다고 생각해보자.그러면, P(x)는$ P(x) = \\sum_{k=1}^K \\pi_k N(x|\\mu_k, \\sum_k) $로 나타낼 수 있다. 이때의 $ \\pi_k $는 $ P(z_k = 1) = \\pi_k $ 이므로 P(Z)는 k개 안에서 선택지($ z_k $)를 선택하는 것이므로 확률 P(x)를 다음과 같이 P(Z)에 대해 나타낼 수 있다.$ P(Z) = \\prod_{k=1}^K \\pi_k^{z_k} $ 또한, mixture component인 $ N(x|\\mu_k, \\sum_k) $ 에 대해 $ P(X|z_k = 1) = N(x|\\mu_k, \\sum_k) $ 인데, 이 z_k가 k개 있으므로, 이를 일반화시키면 다음과 같이 나타낼 수 있다.$ P(X|Z) = \\prod_{k=1}^K N(x|\\mu_k, \\sum_k)^{z_k} $  위와 같은 baysian network를 가진 모델이 있다고 생각해보자. 파란색은 파라미터에 대한 노드를 나타낸 것이고, N이라는 박스는 안의 내용들이 N번 반복적으로 생성된다는 의미이다.이 때, conditional probability는 다음과 같이 정의된다.\\[p(z_k=1|x_n) = \\cfrac{P(z_k=1)P(x|z_k=1)}{\\sum_{j=1}^K P(z_j=1)P(x|z_j=1)}\\]로 나타낼 수 있으므로, 위에서 구한 값들을 그대로 대입한다.\\[p(z_k=1|x_n) = \\cfrac{\\pi_k N(x|\\mu_k, \\sum_k)}{\\sum_{j=1}^K \\pi_j N(x|\\mu_j, \\sum_j)}\\] 기존의 MLE를 구하는 방식은 distance를 사용하여 확률을 사용하지 않았다. 이번에는 확률을 가정했으므로, log likelihood를 활용하여 MLE를 하고자 한다.  Expectation and Maximization of GMMGMM은 K-Means algorithm과 비슷하다. k-means에서는 assignment variable과 location of cetroid, 2개의 parameter가 존재한다.GMMM에서는 location of centroid 와 동일한 역할을 하는 μ와 σ로 표현되는 Mixture component, π_k로 나타내지는 multinomial distribution으로 모델링된 값인 Mixing coefficients, k-means의 파라미터들과 동일한 역할을 하는 2개의 parameter가 있다. 이전에 Expectation and Maximization step에 대해 잠깐 살펴보았다. Expectation에서는 data point들을 centroid에 대해 할당하고, Maximization에서는 파라미터들을 업데이트한다.  Expectation step기존에 K-means는 가장 가까운 centroid에 대해 각 데이터 포인트들을 0 또는 1로 할당했지만, 이제는 확률에 대해 계산할 것이므로 확률을 할당한다. 이전에 구했던 marginalized probability을 할당하면 된다.$ \\gamma(z_{nk}) = p(z_k = 1 | x_n) = \\cfrac{P(z_k = 1)P(x|z_k = 1)}{\\sum_{j=1}^K P(z_j=1)P(x|z_j=1)} = \\cfrac{\\pi_k N(x|\\mu_k, \\sum_k)}{\\sum_{j=1}^k \\pi_j N(x|\\mu_j, \\sum_j)} $ x와 파라미터(π,μ,∑) 가 주어졌을 때, $\\gamma(z_{nk})$ 를 계산함. $\\gamma(z_{nk})$ 는 수많은 iteration을 통한 지속저긍로 갱신되는 파라미터에 의해 업데이트된다.   Maximization step그 다음, Maximization step에서는 Expectation에서 계산한 $\\gamma(z_{nk})$ 를 통해 파라미터들을 업데이트한다. 업데이트할 파라미터는 π,μ,∑ 이고, 이에 대한 업데이트 식은 다음과 같다.$ lnP(X|\\pi,\\mu,\\sum) = \\sum_{n=1}^N ln{\\sum_{k=1}^K \\pi_k N(x|\\mu_k, \\sum_k)} $이 파라미터들을 최적화를 하기 위해서는 이 식을 미분한다. 만약 제약조건이 존재한다면 lagrange method를 활용하여 미분한다. 각 파라미터들에 대해 미분하므로 총 3번 수행한다. 이 때, π_k에 대한 미분에서 한 데이터 포인트에 대해 k개의 대안이 존재할 때, r(z_nk)를 k개 다 더하면 1이 되고, 이 1을 N개의 데이터 포인트 개수만큼 더하므로 N이 된다. 그리고, pi_k는 확률인데, 이를 k에 대해 다 더하면 1이 된다. 따라서 λ = -N 이 된다. 이렇게 총 3개의 r(z_nk)에 대한 식이 나오므로 이를 통해 파라미터들을 업데이트하고, 이러한 과정을 Maximization이라 할 수 있다. GMM이 k-means에 대해 soft한 이유는 데이터 포인트들이 확률로 할당되기에 확실한 색상이 아닌 융합된 색상으로 나타내어진다.centroid(μ_k) 또한 gaussian distribution이므로 정확한 위치가 아닌 분포를 따르고 있다. GMM의 속성에 대해 알아보자.GMM의 장점으로는 Soft clustering이 가능해지고, 각각의 분포들에 대해 이전에는 서로의 관계를 알 수 없었으나, GMM에서는 서로의 분포간의 관계를 파악할 수 있으므로 더 많은 정보를 가질 수 있다.단점으로는 연산이 더 많아지고, local maximum에 빠질 수 있다. 또한 k-means에서도 존재했던 centroid의 개수를 지정해주어야 한다는 점이다.  Relation between K-Means and GMMmultivariate gaussian distribution에 대한 식이 있었다. 이 때, 특정 k에 대한 상황일 때, ∑_k = 𝜖Ι 라고 가정해보자. I는 identity matrix를 의미하고, 𝜖는 EM process에 대한 값이다.identity matrix는 역행렬과 형태가 동일하다. 이러한 상황에서 지수함수 안의 값을 살펴보자. 지수함수의 특성상 x가 -inf 로 가면 y는 0에 가까워진다. 𝜖가 0에 가까워질수록 -1/2𝜖는 -inf로 갈 것이다. 이러한 상황에서 데이터 포인트가 centroid와 가깝다면 x - μ_k 는 다소 작으므로 다소 천천히 0에 다가가게 되고, 데이터 포인트가 centroid와 멀다면 x - μ_k 는 크므로 빠르게 0에 다가간다.그렇다면, 특정 상황에서 0 또는 1 로 binary하게 분류할 수 있게 되고, 이러한 경우에는 GMM이 k-means와 동일한 역할을 수행하게 된다.  EM AlgorithmEM 알고리즘에 대해 자세하게 살펴보자.P(X|θ) 가 있을 때, Z라는 hidden variable이 존재한다면, 이에 대해 marginalization을 수행하여$ P(X|\\theta) = \\sum_Z P(X,Z|\\theta) $라는 식이 성립된다. 이 때, q(Z)라는 임의의 변수를 생성하고, 이를 분모에 만들어준다. 그리고 Jensen’s inequality 라는 기법을 활용한다.jensen’s inequality란 y = φ(x) 라는 식이 있을 때, x에 대한 평균값을 함수에 넣어서 얻은 y값과 각 x를 함수로 넣어 얻은 y들의 평균을 비교한다.전자가 더 크다면 concave, 후자가 더 크다면 convex 라 한다.그림에서 특정 함수 f가 있을 때, x1과 x2에 대해 각각 f(x1), f(x2)를 구하여 평균한 것보다 f(x1+x2)/2) 가 더 크면 concave, 더 작으면 convex이다. log(ln) 함수는 concave 에 해당된다.만약에, $ \\sum_Z q(Z) ln \\frac{P(X,Z|\\theta)}{q(Z)}$ 이 커지면, 좌항도 최소값이 커지는 것이므로 전체적으로 값들이 증가될 것이다.$ \\sum_Z q(Z) ln \\frac{P(X,Z|\\theta)}{q(Z)}$ 를 풀어보면 $ \\sum_Z q(Z) ln P(X,Z|\\theta) - q(Z)lnq(Z) $ 가 되고, q(Z)lnq(Z) 는 entropy 식과 동일하므로 entropy, H로 변환할 수 있다.H로 변환하기 위해 q(Z)도 변하므로 이를 E_q(z) 로 표현할 수 있다. 이렇게 변환한 식을 Q(θ,q) 라는 새로운 함수로 정의한다. 이 때, q는 확률밀도함수여야 한다는 제약조건이 있다.  또는, P(X,Z|θ) 에 대해 bayes rule을 적용하여 변환할 수 있다. 이 때, q는 확률밀도함수로 가정했기 때문에 모든 z에 대해 q를 더하면 1이 된다.이렇게 변환하게 되면,$ lnP(X|\\theta) - \\sum_Z{q(Z)ln \\frac{q(Z)}{P(Z|X,\\theta)}} $가 되고, 동일한 lnP(X|θ) 라는 값과 함께 inequality의 원인을 확인할 수 있게 된다. 이 원인을 제거한다면 부등호가 등호가 될 수 있을 것이다.$ \\sum_Z{q(Z)ln \\frac{q(Z)}{P(Z|X,\\theta)}} $ 를 자세히 살펴보게 되면, KL divergence의 형태와 매우 유사하다.KL divergence란 kullback leiber divergence를 줄인 말로, 두 확률 분포를 비교하는 기법이다.비교하는 식은 다음과 같다.$ KL(P||Q) = \\sum_i P(i) ln(\\frac{P(i)}{Q(i)}) $P와 Q라는 확률 분포에 대해 비교하는데, KL(P||Q)와 KL(Q||K) 가 다르다는 비대칭(Non-symmetric)한 특징이 있다. 또다른 특징으로는 KL(P||Q)는 항상 0보다 크다.  jensen’s inequality를 활용하여 표현한 L라는 식에서 θ^t라는 현재 시간에서 가지는 theta를 가져와서 그대로 q^t(Z)로 정의하게 되면 second(KL divergence) term이 0이 된다. 이를 통해 Q를 업데이트하고, 업데이트된 Q를 통해 theta를 최대화하여 theta^(t+1) 을 계산한다. 이를 그래프화하면 다음과 같다. KL이라는 제거할 term이 존재하여 이를 줄이기 위해 q를 최적화한다. 최적화된 q를 통해 다시 theta를 최적화할 수 있고, 최적화된 theta에 대해 다시 KL term이 생겨난다. 생겨난 KL term을 제거하기 위해 다시 q를 최적화한다.이러한 반복을 통해 최적의 값으로 도달한다. 그러나 목표로 하는 곳은 global maxima이지만, 이 과정을 통해 local maxima에 빠질 수 있다. EM 알고리즘을 정리하면 다음과 같다. 목표 : latent variable(Z)이 존재할 때 likelihood를 최대화한다. 과정 무작위의 점으로 theta_0를 초기화한다. likelihood가 수렴될 때까지 아래 과정을 반복한다. Expectation step $ q^{t+1}(z) = argmax_qQ(\\theta^t, q) = argmax_qL(\\theta^t,q) = argmin_qKL(q||P(Z|X,\\theta^t)) $ Z를 할당한다. $ \\gamma(z_{nk}) = p(z_k=1|x_n) = \\frac{P(z_k=1)P(x|z_k=1)}{\\sum_{j=1}^kP(z_j=1)P(x|z_j=1)} = \\frac{\\pi_k N(x|\\mu_k, \\sum_k)}{\\sum_{j=1}^k \\pi_j N(x|\\mu_j, \\sum_j)} $ Maximization step $ \\theta^{t+1} = argmax_{\\theta} Q(\\theta, q^{t+1}) = argmax_{\\theta} L(\\theta, q^{t+1}) $ MLE와 동일한 과정의 최적화 미분하여 파라미터를 최적화한다. " }, { "title": "[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network ", "url": "/posts/kooc_week7/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-10-15 19:10:00 +0900", "snippet": "Chapter 7. Bayesian Network 목차 Probability recover the probability concepts recover the probability theorems recover the concepts of the marginal and the conditional independencies Bayesian networks syntax and semantics of Bayesian Networks how to factorize Bayesian networks calculate a probability with given conditions inference of Bayesian networks calculate parameters of Bayesian networks list the exact inference of Bayesian networks   7-1. theorems of Probabilitybayesian network에 들어가기 전에, probability에 대한 복습을 하고자 한다.  Conditional Probabilityconditional probability란 특정 상황이 주어졌을 때의 확률을 구하는 것이다.예를 들어, A와 B에 대한 상황이 존재한다고 생각해보자. B가 주어졌을 때의 A의 확률을 구한다는 것은 P(F=True)인 영역 속에서 P(H=True)인 영역에 해당한다.이는 P(A = true | B = true) 와 같이 표현한다.\\[P(A = true | B = true) = \\cfrac{P(A,B)}{P(B)}\\]  Joint Probability비슷한 개념으로 Joint Probability라는 것이 있다. P(A = true, B = true) 와 같이 표현할 수 있고, 이는 A = true 와 B = true 인 확률을 나타낸다.\\[P(A = true, B = true) = P(A|B)P(B)\\] 이 두 개념을 활용하여 개별 확률인 marginal probability를 구할 수 있다.\\[P(a) = \\sum_b P(a,b) = \\sum_b P(a|b)P(b)\\] 만약, joint distribution인 P(a,b,c,d) 가 주어졌을 때, 우리는 P(b)에 대해 식을 세울 수 있다.\\[P(b) = \\sum_a \\sum_c \\sum_d P(a,b,c,d)\\]또는, joint distribution을 통해 conditional probability를 구할 수 있다.\\[P(c|b) = \\sum_a \\sum_d P(a,c,d | b) = 1/P(b) \\sum_a \\sum_d P(a,c,d,b)\\]이 때, 1/P(b) 는 normalization constant(정규화 상수) 이다. 이렇게만 보면 joint probablity가 매우 강력한 개념이라 생각이 되지만, joint probability를 구하기 위해서는 개별 확률들을 알아야 하므로, feature의 개수와 개별 경우의 수에 따라 기하급수적으로 파라미터가 증가한다.만약 feature 개수가 4개이고, 개별 feature이 true/false로 구성된다면 총 16-1 개의 파라미터를 구해야 한다. joint probability의 또다른 특징으로는 chain rule 이다.\\[P(a,b,c,...z) = P(a|b,c,...,z)P(b,c,...,z) = P(a|b,c,...,z)P(b|c,...,z)P(c|...,z)...P(z)\\]   Independencevariable A와 B가 독립적일 때는 다음과 같은 관계가 성립된다. P(A|B) = P(A) P(A,B) = P(A)P(B) P(B A) = P(B)  따라서 P(C1,…,Cn) 의 joint distribution을 계산하는 것은 간단하다.\\[P(C_1,...,C_n) = \\prod_{i=1}^n P(C_i)\\]  Conditional Independence &amp;amp; Marginal Independence Marignal independence P(A=true|B=true) &amp;gt; P(A=true) 라면 이 경우는 marginally independent가 아니다. 즉, A와 B는 P(A) = P(A|B) 일때만 독립적이어야 한다. Conditional Independence P(A=true|B=true, C=true) = P(A=true|C=true) 이면, conditional independent하다.   7-2. Bayesian Network이전에 Naive Bayes Classifier을 배웠고, 이에 대한 function을 정의했다.\\[f_{NB}(x) = argmax_{Y=y}P(Y=y) \\prod_{1 \\leq i \\leq d} P(X_i = x_i | Y = y)\\]bayesian network는 graphical notation 한 특성을 가지고 있다. 즉 random variable conditional independence obtain a compact representation of the full joint distribution에 대한 정보가 들어있다.  Syntax(문법)bayesian network는 acyclic(사이클이 존재x), directed graph nodes random variable P(X_i| Parents(X_i)) (e.g. P(X1|Y)) link Direct influence from the parent to the child 네트워크의 구조는 conditional independence를 가정하고 있다.즉, toothache(치통)과 stench(악취)는 충치(cavity)와는 관계가 있지만, weather과는 관계가 없다. 따라서 weather은 variable과는 독립적이고, toothache와 stench는 cavity와 conditionally independent하다. 유명한 예시를 하나 들어보자.도둑(buglary)이 들거나, 지진(earthquake)이 발생했을 때, 알람(Alarm)이 울리는데, 알람이 울리면, 이웃인 John과 Mary가 전화를 하는 상황이다.알람에 대한 확률은 P(A|B,E) 로 나타낼 수 있고, John이 전화하는 것에 대한 확률은 P(J|A), Mary가 전화할 확률은 P(M|A) 이다.이 network에서의 Variable은 Burglary, Earthquake, Alarm, JohnCalls, MaryCalls, 모두이다. 정성적(Qualitative)인 요소로는 인과관계에 대한 사전정보 데이터로부터의 학습 네트워크의 구조 정량적(Quantitative)인 요소로는 조건부 확률 테이블(이미지에서의 우하단 표)  위의 구조는 간단했지만, 이러한 network를 복잡하게 구성할 수도 있다. 이럴 때, 지역적인 variable들에 대한 관계를 정의할 수 있다. Common parent 동일한 parent variable을 가진 다른 variable에 대해서는 독립적이다. 즉, John이 전화할 확률과 Mary가 전화할 확률은 서로 독립적이다. 𝐽 ⊥ 𝑀|𝐴 P(J,M|A) = P(J|A)P(M|A) Cascading 연쇄작용으로서, 연결되어 있는 variable일 때, A가 주어졌다면, B와 M은 독립이다. mary가 전화할 확률은 A를 알고 있다면 B는 필요가 없다. 즉 direct influence에 대한 확률을 안다면 indirect influence variable과는 독립적이다. B ⊥ M|A P(M|B,A) = P(M|A) V-structure B와 E는 공통의 child를 가지는 상황을 V-structure이라 하는데, A가 주어지지 않았다면, B와 M은 관계가 없으므로, 독립적이나, A가 주어진다면, B와 E는 관계가 생기는 것이므로 독립이 되지 않는다. ~ (B ⊥ E|A) P(B,E,A) = P(B)P(E)P(A|B,E)  이러한 복잡한 관계를 쉽게 이해하기 위해 Bayes Ball Algorithm 이라는 개념을 도입한다. 만약 $ X_A ⊥ X_B | X_C $ 를 확인하고자 할 때, 즉 X_C가 주어졌을 때, X_A와 X_B가 독립인지에 대해 판단하고자 할 때 사용할 수 있다.공을 굴린다고 생각해서, 만약 방향이 -&amp;gt; 일 때, 오른쪽 variable이 주어진다면(회색) 공이 굴러가지지 않고, variable이 주어지지 않는다면(무색) 굴러갈 수 있다. 반대로 방향이 &amp;lt;- 이라면, 오른쪽 variable이 주어졌을 때는 굴러가지고, variable이 주어지지 않았을 때는 굴러갈 수 없다. 위의 local structure들에 적용해보자. Common parent두 개의 variable이 공통의 한 개 parent를 가지는데, parent가 주어진다면, 공이 굴러갈 수 없으므로 이 두 variable은 독립적이고, 주어지지 않는다면, 독립이 아니다.  Cascading세 개의 variable이 연속적으로 존재하는데, 중간의 variable이 주어진다면, 나머지 두 개의 variable은 공이 굴러갈 수 없어서 독립이고, 주어지지 않는다면, 공이 굴러갈 수 있으므로 독립이 아니다.  V-structure두 개의 variable이 공통의 한 개 child를 가지는데, child가 주어진다면, V-structure은 common parent와 반대로, 공이 굴러 갈 수 있어서 두 variable은 독립이 아니다. 그러나 주어지지 않는다면 공이 굴러갈 수 없어서 독립이다. 간단한 예시를 들어보자.x1~x6의 variable이 존재하고, 위와 같은 관계로 형성되어 있다고 해보자. x2가 주어졌을 때, x1과 x4의 관계 (x1 ⊥ x4 | x2)x2가 주어졌을 때, x1과 x4는 x2 방향으로는 cascading구조이므로 지나갈 수 없다. x3방향으로 지나간다 하더라도, x1과 x6는 v-structure구조이므로 지나갈 수 없다.따라서 x1과 x4는 독립이다.  x1이 주어졌을 때, x2와 x5의 관계 (x2 ⊥ x5 | x1)x1이 주어졌을 때, x2와 x5는 v-structure 구조이므로, x6가 주어졌을 때만 지나갈 수 있다. x1방향으로 지나간다 하더라도, common parent 구조이므로 x1이 알려져 있지 않아야 지나갈 수 있다.따라서 x1가 주어졌을 때는 x2와 x5는 독립이다.  x2,x3가 주어졌을 때, x1과 x6의 관계 (x1 ⊥ x6 | {x2,x3})x2와 x3가 주어졌을 때, x1과 x6는 x2방향으로는 cascading 구조이므로 x2가 주어졌을 때는 지나갈 수 없다. x3방향으로 지나간다 하더라도, cascading 구조이므로 x3가 주어졌을 때는 지나갈 수 없다.따라서 x2와 x3가 주어졌을 때는 x1와 x6는 독립이다.  x1,x6가 주어졌을 때, x2와 x3의 관계 (x2 ⊥ x3 | {x1,x6})x1과 x6가 주어졌을 때, x2와 x3는 x1방향으로는 common parent 구조이므로 x1이 주어졌을 때는 지나갈 수 없다. x6방향으로 지나간다 하면, x2와 x5가 v-structure 구조이고, x6가 주어졌으므로 지나갈 수 있고, x6,x5,x3가 cascading 구조인 상황에서 x5가 주어지지 않았으므로 x3로 도착할 수 있다.따라서 x1와 x6가 주어졌을 때는 x2와 x3는 독립이 아니다.  이러한 bayes ball algorithm을 활용하여 다양한 기법을 정의할 수 있다. Markov BlanketA라는 특정 variable이 bayesian network안에 존재한다고 할 때, 주변 특정 관계에 있는 variable만 알면, 나머지의 variable에 대해서는 conditional independent하다.특정 관계에는 parents, children, children’s other parents 이다. parents -&amp;gt; cascading 관계에 있는 variable과 관련 children -&amp;gt; cascading 관계에 있는 varable과 관련 children’s other parents -&amp;gt; v-structure 관계에 있는 variable과 관련   D-Seperation(directly-seperated) Y가 주어졌을 때, Z와 X는 d-seperated 이다. (X ⊥ Z | Y)  bayesian network가 있을 때, 이러한 joint probability를 구할 때, conditional independent 를 고려하면 다음과 같이 단순화할 수 있다.\\[P(X) = \\prod_i P(X_i | X_{\\pi_i})\\]\\[P(X1,X2,X3,X4,X5,X6,X7,X8) = P(X1)P(X2)P(X3|X1)P(X4|X2)P(X5|X2)P(X6|X3,X4)P(X7|X6)P(X8|X5,X6)\\]이렇게 나타낼 수 있는 이유는 joint probability와 conditional probability의 관계와, cascading, common parent 등의 구조로 인해 가능하다. 오른쪽과 같이 $ \\mu $ 와 $ \\sigma $ 의 공통의 parent를 가지는, X 들이 있다고 하면, 사각형의 공간을 만들어 간단하게 표현할 수 있다. 수식도 간단하게 나타낼 수 있다.\\[P(D|\\theta) = P(X_1,...,X_N|\\mu, \\sigma) = \\prod_N P(X_1|\\mu, \\sigma)\\]  7-3. Inference on Bayesian Networks1. Likelihood모든 variable, X = {X_1,…,X_N} 이 있을 때, X는 $ X_H$ 와 $ X_V $ 로 나눌 수 있다. $ X_V $는 evidence variable, 즉 관측된 variable이고, $ X_H $ 는 hidden variable, 즉 관측하지 않은 variable이다.evidence value, $ x_V $ 에 대한 확률(Likelihood)은 다음고 같다.\\[P(x_V) = \\sum_{X_H} P(X_H, X_V) = \\sum_{x_1} \\dots \\sum_{x_k} P(x_1, ..., x_k, x_V)\\]bayesian network와 conditional probability table이 존재하면, Joint Probability를 구하는 것이 가장 적합하다. 따라서, 구하고자 하는 것이 아닌 X_H에 대해 marginalization을 수행하여, X_H에 대한 관계를 모두 포함시킨다.도둑과 지진에 대한 알람 task에서의 P(X_H, X_V) 는 다음과 같다.\\[P(X_H, X_V) = P(B)P(E)P(A|B,E)P(M|A)P(J|A) = P(B,E,A,M,J)\\] 2. Conditional ProbabilityB와 M에 대해 주어졌을 때 알람의 conditional probability를 구해보자. hidden variable을 또다시 2가지로 세분화할 수 있다.X_H = {Y,Z} Y : 관측이 되지 않았지만, 관심은 있는 variable (interested hidden variable) Z : 관측도 되지 않았고, 관심도 없는 variable (uninterested hidden variable)B와 M은 관측이 되었으므로 X_V에 해당하고, A에 대해 구하고 싶으므로, Y는 A, 나머지는 Z에 해당한다.그래서 관측된 variable들에 대한 interested hidden variable에 대한 식은 다음과 같다.\\[P(Y|x_V) = \\sum_z P(Y, Z=z | x_V) = \\sum_z \\cfrac{P(Y, Z, x_V)}{P(x_V)} = \\sum_z \\cfrac{P(Y, Z, x_V)}{\\sum_{y,z} P(Y=y, Z=z, x_V)}\\]full joint로 만들기 위해 Z를 삽입하고, P(x_V)에 대해서는 앞서 정의했으므로, x_V 가 주어졌을 때의 Y의 conditional probability는 위와 같다. 3. Most Probable Assignment 마지막으로, P(Y x_V)가 최대가 될 파라미터를 구할 수 있다. ( $argmax_a P(A|B=true, M=true)$) 만약 P(A|B,E) 에 대한 posteriori를 구하면, prediction task에 해당하고, 반대로 P(B,E|A)에 대한 posteriori를 구하면 diagnosis task가 된다.  joint probability를 구하는 것이 중요한데, 이를 구하기 위해서는 너무 많은 곱셈과 덧셈이 존재한다.\\[P(a = true, b=true, mc=true) = \\sum_{JC}\\sum_{E} P(a,b,E,JC,mc) = \\sum_{JC} \\sum_{E} P(JC|a)P(mc|a)P(a|b,E)P(E)P(b)\\] 조금 더 간단하게 구할 수 있는 방법은 없을까? 이러한 확률(P)를 함수의 형태로 생각해보자. 소문자가 evidence variable, 대문자가 hidden variable에 대한 값이다.\\[P(e,jc,mc,B,A) = P(e)\\sum_B P(b)\\sum_A P(a|b,e)P(jc|a)P(mc|a) =&amp;gt; f_E(e)\\sum_B f_B(b) \\sum_A f_A(a,b,e) f_J(a) f_M(a)\\]MC에 대한 함수 $ f_m $ 와 JC에 대한 함수 $ f_j $ 를 곱해서 $ f_{JM} $ 을 만들 수 있다. 그리고, $ f_A $ 와 $ f_JM $ 을 결합하고, A에 대해 marginalization을 수행하게 되면, $ f_{\\bar{A}JM}(b,e) $ 가 된다. 이렇게 계속 단순화시키면, $ f_E\\bar{B}\\bar{A}JM(e) $ 를 만들어 낼 수 있다.  이번에는 간단한 bayesian network인 A &amp;lt;- B &amp;lt;- C &amp;lt;- D 를 정의하고, 이에 대한 full joint를 구하면,$ P(A,B,C,D) = P(A|B)P(B|C)P(C|D)P(D) $가 된다. 이 때, A,B,C,D에 대한 network를 다르게 표현하고자 한다. clique와 separator 개념을 사용하여, 노드(clique)를 A,B/B,C/C,D 로 구성하고, 그 사이에 링크(separator)를 B,C 로 정의한다.그리고나서, potential function 을 정의한다. potential function on nodes\\[\\psi(a,b), \\psi(b,c), \\psi(c,d)\\] potential function on links\\[\\phi(b), \\phi(c)\\] 그 후, potential function을 활용하여 P(A,B,C,D)를 정의한다. 정의하는 방법에는 2가지가 있다. $ P(A,B,C,D) = \\cfrac{\\prod_N \\psi(N)}{\\prod_L \\phi(L)} = \\cfrac{\\psi(a,b)\\psi(b,c)\\psi(c,d)}{\\phi(b)\\phi(c)} $ ψ(a,b) = P(A|B), ψ(b,c) = P(B|C), ψ(c,d) = P(C|D)P(D) φ(b) = 1, φ(c) = 1  $ P(A,B,C,D) = \\cfrac{\\prod_N \\psi(N)}{\\prod_L \\phi(L)} = \\cfrac{\\psi(a,b)\\psi(b,c)\\psi(c,d)}{\\phi(b)\\phi(c)} $ ψ(a,b) = P(A,B), ψ(b,c) = P(B,C), ψ(c,d) = P(C,D) φ(b) = P(B), φ(c) = P(C)  위의 potential function을 clique graph를 적용하게 되면 P(B) = $ \\sum_A \\psi(A,B) $ A에 대해 marginalization을 수행하면 P(B)가 된다. P(B) = $ \\sum_C \\psi(B,C) $ P(B) = φ(B)이 때, 만약 A가 관찰된다면, $ \\phi(B) $ 값도 바뀔 것이고, 그렇다면, 그 뒤에 있는 B,C/C/C,D 에 대한 것들도 모두 바뀌게 된다. 이를 belief propagation이라 한다. belief를 전파(propagation)하기 위해서는 Absorption(update) rule 을 적용해야 한다. update된 separator와 update된 clique를 다음과 같이 정의한다. φ^(B) = $ \\sum_A \\psi^(A,B) $ ψ^(B,C) = $ \\psi(B,C)\\frac{\\phi^(B)}{\\phi(B)} $ 이렇게 정의가 되면, 다음과 같이 적용된다.이를 local consistency, 지역적 일관성이라 한다.  이제 실제 예제에 적용할 것인데, 우리는 conditional probability는 알고 있지만, joint probability는 모르는 상태이므로, 일전에 정의했었던 P(A,B,C,D)에서 conditional probability를 활용한 방식을 사용할 것이다.  P(b) 를 구해보자. $ \\phi^*(b) = \\sum_a \\psi(a,b) = 1 $ absorption rule을 수행할 때, φ*(b)를 구하기 위해서 a에 대해 marginalization을 하면 된다. 이렇게 되면, 모든 case에 대한 확률이 되므로 1이 된다. $ \\psi^(b,c) = \\psi(b,c)\\frac{\\phi^(b)}{\\phi(b)} = P(b|c)P(c) \\times 1 = P(b,c) $ 앞서 ψ*에 대해 정의했으므로 그대로 정의하고, 위에서 b separator에 대한 값은 둘다 1. conditional probability의 정의에 따라 joint probability로 정의된다. $ \\phi^{**}(b) = \\sum_c \\psi(b,c) = \\sum_c P(b,c) = P(b) $ 이번에는 B,C 에서 A,B로 진행해본다. 동일하게 c에 대해 marginalization을 수행하여 apsorption rule이 적용된 b를 계산한다. $ \\psi^(a,b) = \\psi(a,b)\\frac{\\phi^{**}(b)}{\\phi^(b)} = \\frac{P(a|b)P(b)}{1} = P(a,b) $ ψ*에 대해 앞서 정의했으므로, 그대로 정의하여 사용한다. $ \\phi^{**}(b) = \\sum_a \\psi^(a,b) = P(b) $ A,B에서 B,C로 한번 더 진행해보면 동일한 값이 나오는 것을 확인할 수 있다.   P(b|a=1,c=1) 를 구해보자.관측이 있는 상태에서 알지 못하는 variable에 대한 확률을 구하고, 추가로 확률이 최대가 되는 값을 찾는 것이 중요하다. $ \\phi^*(b) = \\sum_a \\psi(a,b) \\delta(a=1) = P(a = 1|b) $ 동일하게 a에 대해 marginalization을 수행하지만, a=1이라는 관측이 이미 있었으므로 a=1일 때의 값만 사용한다. 이로 인해, 모든 case라서 나온 1이 아닌, a=1일때의 확률 $ \\psi^(b,c) = \\psi(b,c)\\frac{\\phi^(b)}{\\phi(b)} = P(b|c=1)P(c=1)\\frac{P(a=1|b)}{1} $ 앞서 absorption rule에서 정의한 것을 사용하되, c=1이라는 관측이 있었으므로 이로 제한하여 사용한다. $ \\phi^{**}(b) = \\sum_c \\psi(b,c) \\delta(c=1) = ψ^*(b,c) = P(b|c=1)P(c=1)P(a=1|b) $ B,C에서 A,B로 진행하므로 φ**는 c를 marginalization을 수행한 것과 단다. 따라서 위에 것이 그대로 내려온다.«««&amp;lt; HEAD:_posts/Classlog/KOOC/2022-10-15-kooc_week7.md $ \\psi^(a,b) = \\psi(a,b)\\frac{\\phi^{**}(b)}{\\phi^(b)} = P(a=1|b) \\frac{P(b|c=1)P(c=1)P(a=1|b)}{P(a=1|b)} = P(b|c=1)P(c=1)P(a=1|b) $ absorption rule에서 정의한 것을 그대로 사용한다. $ \\phi^{***}(b) = \\sum_a \\psi^*(a,b) \\delta(a=1) = P(b|c=1)P(c=1)P(a=1|b) $ A,B에서 다시 B,C로 이동을 시켜보면 동일한 값이 나온다는 것을 알 수 있다. 이렇게 구해진 seperator로 P(b|a=1,c=1) 를 구할 수 있다.  " }, { "title": "[KOOC] 인공지능 및 기계학습 개론 5,6주차 - Support Vector Machine, Training, Regularization ", "url": "/posts/kooc_week56/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-10-05 00:01:00 +0900", "snippet": "Chapter 5. Support Vector Machine 목차 Support Vector Machine Classifier maximum margin idea of the SVM formulation of the optimization problem soft-margin and penalization penalization term difference between the log-loss and the hinge-loss kernel trick primal problem and the dual problem of SVM types of kernels apply the kernel trick to SVM and logistic regression   5-1. Support Vector Machinedecision boundary은 모델의 성능에 영향을 많이 끼친다. 따라서 decision boundary를 잘 지정하는 것이 중요하다. 위의 그래프처럼 빨간색 점과 파란색 점으로 이루어진 데이터가 있고, 그에 대한 decision boundary를 구하고자 한다면, 어떻게 그어야 잘 그어질 수 있을까? 대충 빨간색과 파란색이 잘 구별되도록 그을 수 있겠지만, 그렇게 하면 새로운 데이터에 대해 분별력이 떨어질 수도 있다. 따라서 이를 잘 설정할 수 있도록 margin을 설정하여 그려본다. 예를 들어, 가장 하단의 빨간색 점 2개를 지나는 빨간색 직선을 긋고, 그 직선과 동일한 기울기를 가진 가장 상단의 파란색 점을 지나는 파란색 직선을 또 그려본다. 그리고는, 이 두 직선과 같은 거리를 가진 직선인 초록색 선을 그린다. 그러면 초록색 선 기준으로 현재 데이터에 대해 알맞는 직선 중 가장 먼 거리에 위치한 직선은 각각 이 빨간색 직선과 파란색 직선이 될 것이다.수식적으로 바라본다면, decision boundary 직선의 방정식은 아래 방향을 향한 법선 벡터 w, bias 텀 b를 가지고, 직선 위의 점을 fitting했을 때 wx + b = 0로 나타낼 수 있고, 파란색 점들을 positive case, 빨간색 점들을 negative case라 할 때, positive point들은 wx + b &amp;gt; 0, negative point들은 wx + b &amp;lt; 0 이 된다. 이 때, positive case를 +1, negative case를 -1로 가정하고, confidence level을 $ (wx_i + b)y_i $ 라고 정의를 하면, confidence level은 항상 양수가 될 것이고, 이 confidence를 높이는 것을 목표로 할 수 있다.decision boundary와 가장 근접해 있는 점과의 수직 거리를 margin이라 나타낼 수 있다. 빨간색 직선과 파란색 직선이 maximum margin을 가진 decision boundary라 할 수 있다. wx + b = f(x)로 가정했을 때, 직선 위의 점 x_p에 대해서는 wx_p + b = 0, 또, positive point인 $ x_pos $ 들에 대해서는 wx_pos + b &amp;gt; a , a &amp;gt; 0 이 성립된다.임의의 점 X에 대해 decision boundary와의 margin(distance)를 구해보자. 직선 위의 점 x_p 와 X의 거리를 r이라 할 때, decision boundary의 법선 벡터인 w를 통해 X를 구할 수 있다. 그 후, margin인 r를 구하기 위해 직선의 방정식에 대입한다.\\[x = x_p + r \\cfrac{w}{||w||}\\]\\[f(x) = w \\cdot x + b = w(x_p + r \\cfrac{w}{||w||} + b = w x_p + b + r \\cfrac{w \\cdot w }{||w||} = r||w||\\]  margin $ r = \\frac{a}{||w||} $ 로 나타낼 수 있고, margin이 최대가 되는 w와 b를 값을 구하기 위해 optimization을 수행하면 $ argmax_{w,b} 2r = \\cfrac{a}{   w   } $ 로 정의할 수 있다. 이 때, 2r이 이유는 margin은 위 아래 양쪽으로 거리가 있는데 r은 한 방향에 대한 거리만 정의했으므로 2를 곱한다. 사실 2는 상수이므로 중요한 값은 아니다. 아까 confidence level에 대한 식을 $ (wx_i + b)y_i &amp;gt;= a $ 라 정의했는데, a는 임의의 수이므로 1로 지정해볼 수 있다. a를 1로 하게 되면 $ argmax_{w,b} 2r = \\cfrac{1}{   w   } $ 이 될 것이고, 분모, 분자를 바꾸어서 $ argmin_{w,b} 2r =   w   $ 로 할 수 있다. 이 때, w는 개별 요소를 살펴보면 w1, w2, w3… 이므로 $   w   = \\sqrt{w_1^2 + w_2^2} $ 와 같은 quadratic problem으로 정의된다. 이 w에 대해 최적화해야 하는데, matlab에서는 linear programming, quadratic programming과 같은 optimization 기법이 존재하므로 이를 사용하여 최적화를 수행하면 된다.  그러나, 우리가 가진 데이터는 완벽하게 나뉘어질 수 있는 상황이지만, 만약에 빨간색 점이 파란색 점들 사이에 존재하게 되면, 완벽한 decision boundary를 만들 수 없다.이러한 완벽한 decision boundary에 대한 margin을 hard margin이라 해보자. hard margin인 경우에는 어떠한 에러도 존재해서는 안된다. 실제 현실은 error가 항상 존재하므로, 에러를 일부 인정하는 margin을 soft margin이라 한다. soft margin을 하게 되면, decision boundary를 선형으로 유지하면서 decision boundary를 찾을 수 있다. 또는 soft margin을 가정하지 않고, decision boundary를 비선형으로 그어주면 hard margin을 유지하면서 완벽한 decision boundary를 만들 수 있다.그래서 soft margin과 비선형 decision boundary를 만들 수 있게 해주는 kernel trick에 대해 배워보고자 한다.  5-2. Soft Marginerror case를 허용하는 decision boundary를 구성하고자 한다.  아까 전, 구했던 optimization 식 $ min_{w,b}   w   $ 에 error 개수와 constant 항을 추가하여 최적화한다. \\[min_{w,b}||w|| + C \\times N_{error}\\]C는 패널티와 같은 항으로, loss function에 해당한다.loss function에는 zero-one loss function이 있다. 이는 상단에서 아래로 이동한다고 생각할 때, loss를 0으로 하다가 decision boundary를 지나가는 순간인 wx + b가 0 이하로 되는 순간 loss를 1로 하는 것이다.그러나 이는 너무 단순하므로, Hinge loss라고 하는 function을 사용할 수도 있다. 파란색 직선과 decision boundary의 margin은 항상 1이다. 동일하게 상단에서 아래로 이동한다고 생각할 때, 파란색 선을 지날 때부터 loss가 error case와의 거리에에 따라 점차 증가하는 선을 따라간다.이를 수식적으로 표현하기 위해 slack variable이라는 개념을 도입한다. 즉 오분류되었을 때, 오류의 정도를 slack으로 관리한다는 것이다. C는 slack parameter를 얼마 정도의 강도로 적용할 것인지에 대한 값이다. slack($\\xi_j$) 를 다 합한 것을 최소화하는데, 원래 $ (wx_i + b)y_i &amp;gt;= 1 $ 이었으나 여기에 slack 을 추가하여 $ (wx_i + b)y_i &amp;gt;= 1 - \\xi_i :,: \\xi_i &amp;gt;= 0 $ 로 만들어준다. 이렇게하면 quadratic programming을 수행하는데에는 적합하나, C라는 파라미터가 추가되었기에 더 복잡해질 수가 있다. 이렇게 slack variable을 추가하여 decision boundary를 결정짓는 모델을 soft-margin SVM이라 한다. 그래프와 같이 파란색 점이 어디에 위치하냐에 따라 slack variable 값이 달라지는데, 파란색 선보다 위에서는 0, desicion boundary와 파란색 선 사이에서는 0~1 사이의 값, decision boundary보다 아래로 내려가게 되면 1보다 큰 slack variable을 가지게 된다. 이러한 slack variable을 추가함으로써 다소 soft한 모델을 만들 수 있게 되었다. 최종적인 패널티값은 C x $ \\sum_j \\xi_j $ 이다. logistic function에 대한 loss function은 log loss가 있다. 예전에 optimization을 위해 log를 씌워 연산을 수행했다. argmax 안에 log부분이 logistic function인데, 이 부분을 풀어보면, $ Y_iX_i\\theta - log(1 + e^{X_i\\theta})$ 가 되는데, 여기의 log부분이 아까 배웠던 slack variable과 비슷한 역할을 한다. 이 log부분은 그래프에서의 파란색 형태의 loss function을 가지고 있다. logistic function과 SVM을 비교하기는 힘드나, SVM이 더 최근에 나왔다.  이 때, C값을 어떻게 결정짓냐에 따라 모델이 큰 차이를 보인다. C가 작으면, 패널티가 작아져서 decision boundary를 넘어가더라도, 영향이 크지 않게 된다. 그래서 C를 키웠더니 C가 특정값 이상이 되면 decision boundary가 고정이 된다. 그렇다면, C를 아주 크게 잡아놓고 진행을 하는 것이 무조건 좋을 수 있으나, 새로운 데이터에 대해 decision boundary가 얼마나 신뢰도가 큰지에 대해 다시 생각을 해봐야 하므로 많은 시행착오가 필요하다.  5-3. Kernel Trick이때까지는 linear한 decision boundary에 대해 공부했다. soft-margin decision boundary는 decision boundary를 넘어가는 점에 대해 error term으로 처리하고 선형으로 decision boundary를 설정했다. 그러나 만약 한 개의 점만이 아닌, 연속적으로 불규칙한, 즉 선형으로 설명할 수 없는 데이터가 있다고 하면, error term으로 설명하기 어렵다.일전에 linear regression에서 단순히 linear regression만이 아닌, 임의로 차수를 높여 non-linear regression을 수행해보았다.이러한 테크닉을 동일하게 적용하여 non-linear한 decision boundary를 구할 수도 있다. 간단하게 얘기해서, decision boundary를 구할 때는 (x1, x2)에 대해 식을 세웠다. 이 x1, x2를 임의로 제곱, 세제곱하거나 두 값을 곱하는 등의 테크닉을 통해 차수를 높여준다. 그렇게 하여 오른쪽 하단에 위치한 그래프처럼 non linear 한 decision boundary를 구할 수 있다. 위에서는 3차까지 늘렸지만, 이를 무한대까지 늘리는 방법이 있을까? 이를 위해 사용하는 기법이 kernel trick이다. 이 kernel trick을 사용하기 위해서는 optimization을 정확하게 이해하고 있어야 한다. 간략하게 얘기하면 prime problem과 dual problem이 있을 때, 이 두개가 같은 성질을 가진 문제라고 정의하고 optimization을 할 수 있다. SVM의 경우 primal problem에 해당하고, kernel을 도입하기 위해서는 이 prime을 dual problem로 바꿔줘야 한다. SVM은 quadratic programming을 통해 optimization을 수행했다. 이를 dual problem으로 바꾸기 위해서는 Lagrange method 라는 것을 잘 이해해야 한다.SVM에서 optimization을 수행하는 식을 아래와 같이 정의할 수 있다. $ min_x f(x) $ s.t. $ g(x) \\leq 0,: h(x) = 0 $이 때, Lagrange method에서의 Prime Function L에 대해 $ L(x, \\alpha, \\beta) = f(x) + \\alpha g(x) + \\beta h(x) $ 로 정의하여 g(x)와 h(x)를 f(x)로 포함시킬 수 있다. 이 때 단순하게 포함시키는 것이 아닌 alpha와 beta라는 Lagrange Multiplier 라는 상수를 정의하여 포함시키는 것이다. 이 때의 alpha는 0보다 크거나 같아야 한다.이를 통해 Lagrange Dual Function d를 Lagrange Prime Function을 활용하여 정의할 수 있다.\\[d(\\alpha, \\beta) = inf_{x \\in X} L(x, \\alpha, \\beta) = min_x L(x, \\alpha, \\beta)\\]이 식을 통해, optimization ($ max_{\\alpha \\geq 0, \\beta} L(x, \\alpha, \\beta) $)을 풀 때, 특정 범위 내에서는 f(x)와 동일하게 동작하는 성질이 있다. 따라서 optimization이 되어 있는 Lagrange Prime Function을 f(x)와 동일하게 사용하겠다, 즉 f(x) 대신 optimization 식을 쓰겠다는 것이 primal problem에서 dual problem으로 넘어가는 과정이다. 정리하면, Primal problem에서의 optimization 식은 $ g(x) \\leq 0, h(x) = 0 $ 일 때, $ min_x f(x) $ 이고, 이를 Lagrange Prime Function으로 변환하면 $ min_x max_{\\alpha \\geq 0, \\beta} L(x, \\alpha, \\beta) $ 가 된다.Dual Problem 관점에서 바라보면, dual function을 정의하여 optimization을 수행한다. optimization 식은 $ \\alpha &amp;gt; 0 $ 일 때, $ max_{\\alpha &amp;gt; 0, \\beta} d(\\alpha, \\beta) $ 이고, 이를 앞서 정의했던 식을 통해 d를 치환하면 $ max_{\\alpha \\geq 0, \\beta} min_x L(x, \\alpha, \\beta) $ 이 된다. 여기서 가장 중요한 것은 Strong duality 라는 개념이 만족되어야 한다. dual function에서 나온 solution과 원래 가지고 있었던 Prime function이 같게 되어야 한다는 것이다.\\[d* = max_{\\alpha \\geq 0, \\beta} min_x L(x, \\alpha, \\beta) = min_x max_{\\alpha \\geq 0, \\beta} L(x, \\alpha, \\beta) = p*\\] 이 strong duality를 보장해주는 조건이 바로 KKT Condition 이다. 즉 지금까지는 Prime Function으로 문제를 풀어왔는데, 이를 Dual Function으로 풀고자 하고, 이 dual function으로 만들고자 할 때 만족해야 하는 조건이 KKT Condition이다.KKT condition $ \\triangledown L(x^, \\alpha^, \\beta^*) = 0 $ $ \\alpha^* \\geq 0 $ $ g(x^*) \\leq 0 $ $ h(x^*) = 0 $ $ \\alpha* g(x^*) = 0 $  이 strong duality와 kkt condition은 어려운 개념이므로 따로 공부를 해야 할 것이다. 이 강좌에서는 중요한 부분이 아니므로 빠르게 넘어간다.  SVM에 Lagrange dual problem을 적용해보고자 한다. 먼저 SVM에서의 optimization은 $ (wx_i + b)y_i \\geq 1 $ 에 대해 $ min_{w,b} ||w|| $ 이다. 이 때, 1을 좌항으로 넘겨 g(x)를 정의하고, ||w|| 를 f(x)로 정의한다. 이 때, Prime Problem에서의 optimization은 $\\alpha_j \\geq 0 $ 에 대해 $ min_{w,b}max_{\\alpha \\geq 0, \\beta} \\frac{1}{2}w \\cdot w - \\sum_j \\alpha_j [(wx_j + b)y_i - 1] $ 이다. 이 min과 max의 위치를 바꿔주어 $ max_{\\alpha \\geq 0, \\beta}min_{w,b} \\frac{1}{2}w \\cdot w - \\sum_j \\alpha_j [(wx_j + b)y_i - 1] $ 로 하고, KKT Condition을 가져와서 optimization을 단순화할 것이다. KKT에서 사용할 조건들은 다음과 같다. $ \\triangledown L(x^, \\alpha^, \\beta^*) = 0:=&amp;gt;: \\cfrac{\\partial L(w,b,a)}{\\partial w} = 0, \\cfrac{\\partial L(w,b,\\alpha )}{\\partial b} = 0:=&amp;gt;: w = \\sum_j \\alpha_j x_jy_j,:\\sum_j \\alpha_j y_j = 0 $ $ \\alpha_i ((wx_i + b)y_i - 1) = 0 $ 그 후, Lagrange Prime Function 을 풀어보자.푸는 과정에서 위에서 정의했던 KKT의 조건들을 사용했다. 이렇게 최종적인 식을 구했는데, 첫번째 항은 linear한 항이고, 두번째 항은 square한 항이므로 quadratic programming을 적용하여 alpha를 구할 수 있다.만약, $ \\alpha_j $를 알고 있다면, w와 b를 쉽게 구할 수 있다. 그러나 이렇게 구했지만 아직 linear한 decision boundary에 머물러 있다.  다음으로 넘어가기 전에 kernel 이라는 것을 먼저 살펴보자. kernel은 두 벡터의 내적(inner product)로 계산하는데, 이 두 벡터는 다른 space 상의 vector를 의미한다.$ K(x_i, x_j) = \\varphi(x_i) \\cdot \\varphi(x_j) $x_i와 x_j를 다른 차원으로 보낸 후의 내적을 한 것이 kernel이다. 이런 식으로 정의할 수 있는 kerenl의 종류는 다양하다. Polynomial(homogeneous) $ k(x_i, x_j) = (x_i \\cdot x_j)^d:,:: (d : degree) $ Polynomial(inhomogeneous) $ k(x_i, x_j) = (x_i \\cdot x_j + 1)^d $ Gaussian kernel function (Radial Basis Function) $ k(x_i, x_j) = exp(- \\gamma ||x_i - x_j||^2) $ Hyperbolic tangent (Sigmoid Function) $ k(x_i, x_j) = tanh(kx_i \\cdot x_j + c) $ 이러한 kernel들이 처음 정의했던 $ \\varphi $ 에 대한 식이 되는지 살펴보자.degree가 1일 때의 polynomial을 구한다면,\\[K(&amp;lt;x_1, x_2&amp;gt;, &amp;lt;z_1, z_2&amp;gt;) = &amp;lt;x_1^2, \\sqrt{2}x_1x_2, x_2^2&amp;gt; \\cdot &amp;lt;z_1^2, \\sqrt{2}z_1z_2, z_2^2&amp;gt; = (x_1z_1 + x_2z_2) = (x \\cdot z)^2\\] degree가 2일 때의 polynomial을 구하면,\\[K(&amp;lt;x_1, x_2&amp;gt;, &amp;lt;z_1, z_2&amp;gt;) = &amp;lt;x_1^2, \\sqrt{2}x_1x_2, x_2^2&amp;gt; \\cdot &amp;lt;z_1^2, \\sqrt{2}z_1z_2, z_2^2&amp;gt; = x_1^2z_1^2 + 2x_1x_2z_1z_2x_2^2z_2^2 = (x_1z_1 + x_2z_2)^2 = (x \\cdot z)^2\\] 따라서 degree가 n일 때의 polynomial kernel function은 $ K(&amp;lt;x_1,x_2&amp;gt;, &amp;lt;z_1, z_2&amp;gt;) = (x \\cdot z)^n $ 이다. x와 z를 먼저 내적해서 다른 차원으로 보내는 것이나, 다른 차원으로 먼저 보낸 후 내적을 한 것이나 결과가 같게 된다. 그러면 10차원 100차원 무한대 차원까지 증가시켰을 때의 계산도 간편하게 할 수 있다.  이제 다시 SVM으로 돌아가서 optimization식에 x_i, x_j를 Kernel을 적용하여 nonlinear하게 진행해보자.$ max_{a \\geq 0} \\sum_j \\alpha_j - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j \\varphi(x_i), \\varphi(x_j) = max_{a \\geq 0} \\sum_j \\alpha_j - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $또한, w,b에 대해서도 정의할 수 있다.$ w = \\sum_{i=1}^N \\alpha_i y_i \\varphi(x_i) $$ b = y_i - \\sum_{i=1}^N \\alpha_i y_i \\varphi(x_i) \\varphi(x_j) $ 이 때, w에 대해서는 kernel trick을 사용할 수가 없다. 그러나 전체적으로 다시 생각해보면, 우리의 최종 목표는 w와 b를 구하는 것이 아니라, 이 w와 b를 통해 만들어진 decision boundary를 통해 분류를 잘 하는 것이다. 즉, w와 x,b가 있는 식에 x를 집어넣어서 +가 나오면 positive case, -가 나오면 negative case로 분류하기 위함이다.그래서 다시 처음으로 돌아가서 wx + b라는 식에서 x를 다른 차원으로 보내어 식을 세워보면 $ w \\varphi(x) + b $ 이 되는데, 이는 kernel trick을 사용할 수 없다. 그러나 이 식을 풀어보면\\[sign(w \\varphi(x) + b) = sign(\\sum_{i=1}^N \\alpha_i y_i \\varphi(x_i) \\cdot \\varphi(x_i) + y_j - \\sum_{i=1}^N \\alpha_i y_i \\varphi(x_i) \\varphi(x_j)) = sign(\\sum_{i=1}^N \\alpha_i y_i K(x_i, x) + y_j - \\sum_{i=1}^N \\alpha_i y_i K(x_i, x_j))\\]이 때, sign은 안에 식이 양수이면, 1 음수이면 -1, 0이면 0으로 반환해준다. &amp;gt; 0 : 1 &amp;lt; 0 : -1 = 0 : 0 KKT condition에 의해 생성된 식에 의해 결국 kernel trick을 사용할 수 있게 된다. 예전처럼 w,b를 직접적으로 구할 수는 없지만 위의 식을 통해 classifier를 할 수 있다. 4차원에 대한 kernel을 적용한 예와 RBF라는 알고리즘을 적용한 예시이다.  logistic regression에 kernel을 적용한 예이다. Chapter 6. Training / Testing and Regularization 목차 concept of bias and variance overfitting, underfitting to segment bias, variance of error trade-off between bias and variance concept of Occam’s razor cross-validation various performance metrics for supervised machine learning concept of regularization apply regularization to Linear regression Logistic regression Support Vector Machine   6-1. Concept of bias and varianceNaive Bayes, Logistic Regression, SVM 등 다양한 classifer를 배웠고, SVM의 경우 아직도 많이 사용되는 ML 알고리즘이다. 더 나은 ML 알고리즘이란 성능이 좋은 것일텐데, 이를 어떻게 평가할 수 있을까? 성능에 영향을 끼치는 것은 정확도와 데이터셋이다.정확도는 precision/Recall이나 F-Measure 값을 통해 성능을 평가할 수 있다. 데이터셋은 True/False의 비율이 어떤지, 분산이 작은지에 따라 데이터셋의 타당성을 평가할 수 있다. 머신러닝에서 Training과 Testing이라는 것이 존재한다. Training이란 사전 정보와 이전 경험을 통해 모델의 파라미터를 학습시키는 과정이다. Decision tree나 SVM 등과 같은 ML 알고리즘이 현재에 잘 사용되지 않는 이유는 훈련 데이터셋에 존재하지 않는 값에 대해서는 잘 분류를 하지 못하기 때문이다. Testing은 학습시킨 ML 알고리즘과 파라미터를 테스트하는 과정이다. 훈련하는 과정에서 파라미터들이 training 데이터셋에 대해 regression을 하는데, training 데이터셋에 너무 많이 fitting 되어 있는 모델을 overfitting이라 하고, 너무 적게 fitting 되어 있는 모델을 underfitting이라 한다. 그렇다면, 얼마나 complex하게 model을 만들어야 할까? 훈련된 ML 알고리즘의 estimation error, Eout은 approximation error, Ein와 데이터의 variance로 인해 발생되는 error의 합보다 크다.\\[E_{out} \\leq E_{in} + \\Omega\\] 어떤 모델을 정의해보자. f : true function g : ML training function $ g^{(D)} $ : dataset D를 통해 학습된 function D : dataset $ \\bar{g} $ : 수많은 모든 데이터셋을 관찰하였을 때의 평균적인 hypothesis $ \\bar{g}(x) = E_D [g^{(D)}(x)]$ $ E_D $는 무한히 많은 데이터셋을 의미한다. single instance of dataset D에 대한 Error를 $ E_{out}(g^{(D)}(x)) = E_x[(g^{(D)}(x) - f(x))^2] $ 라 정의할 수 있다. Ex는 expected error를 의미한다.무한히 많은 수의 데이터셋 D에 대한 expected error는 $ E_D[E_{out}(g^{(D)}(x))] = E_D[E_x[(g^{(D)}(x) - f(x))^2]] = E_x[E_D[(g^{(D)}(x) - f(x))^2]] $Ex 안 부분을 간편하게 표현하기 위해 $ \\bar{g}(x) $ 를 추가하여 표현한다. 그리고 나서, 제곱을 풀어준다.\\[E_D[(g^{(D)}(x) - f(x))^2] = E_D[(g^{(D)}(x) - \\bar{g}(x) + \\bar{g}(x) - f(x))^2] = E_D[(g^{(D)}(x) - \\bar{g}(x))^2 + (\\bar{g}(x) - f(x))^2 + 2(g^{(D)}(x) - \\bar{g}(x))(\\bar{g}(x) - f(x))] = E_D[(g^{(D)}(x) - \\bar{g}(x))^2] + (\\bar{g}(x) - f(x))^2 + E_D[2(g^{(D)}(x) - \\bar{g}(x))(\\bar{g}(x) - f(x))]\\]이 때, $ g^{(D)}(x) $ 를 무한대로 학습시킨 것의 평균이 g_bar이므로, $ \\bar{g}(x) = E_D [g^{(D)}(x)] $ 라 정의했다 따라서, $ E_D[2(g^{(D)}(x) - \\bar{g}(x))(\\bar{g}(x) - f(x))] = 0 $ 이다. 최종적인 error는 다음과 같다.\\[E_D[E_out(g^{(D)}(x))] = E_x[E_D[(g^{(D)}(x) - \\bar{g}(x))^2] + (\\bar{g}(x) - f(x))^2]\\] 이 식에서 Veriance 와 Bias를 정의한다. $ Variance(x) = E_D[(g^{(D)}(x) - \\bar{g}(x))^2] $ $ Bias^2(x) = (\\bar{g}(x) - f(x))^2 $다양한 모든 데이터셋을 알고 있나는 가정하에 만든 hypothesis인 g_bar는 $ g^{(D)}(x) $ 를 무한대로 증가시킨 것이므로, 이 둘의 차이를 variance라 표현한다. 그리고, 아무리 approximation을 해도 true function과는 반드시 다를 것임에는 분명하므로, 우리가 가진 모델의 한계점에 의해 생길 수 있는 error를 Bias라 표현할 수 있다.이 때, bias를 줄이기 위해 g_bar의 차수를 무수히 높여서 bias를 줄인다면 variance가 증가하므로 bias와 variance는 trade-off 관계이다.variance를 줄이기 위해서는 더 많은 데이터를 수집해야 하고, bias를 줄이기 위해서는 더욱 더 복잡한 모델을 만들어야 한다.  6-2. Performance measurementtrue function을 sin(2*pi*x)라 가정하고, 이에 대한 bias와 variance를 생각해보자.점 2개를 사용하여 linear regression을 한다고 가정할 때, 기울기를 가진 직선으로 regression을 하는 방법과 constant한 기울기가 존재하지 않은 y = c 형태의 함수로 regression 할수도 있을 것이다. 전자의 경우가 초록색 선일 것이고, 후자가 빨간색 선이다. 이 경우는 점 2개가 적절하게 구성되어 초록색 선이 적합할 수 있지만, 만약에 x = 0.1 과 0.2 위치에서의 점 2개로 데이터가 구성된다면 우상향되는 직선이 estimated될텐데, 이러한 경우 x = 0.7 부분에서 실제 true function과의 error가 매우 클 것이다. 그렇다면, 이러한 경우는 빨간색 선처럼 y = c로 투영하는 것이 안전할 수 있다.그렇다면 어떤 기준으로 regression을 해야 할까?초록색 선과 빨간색 선에 대한 각 bias와 variance를 살펴보면, 빨간색 선의 경우 bias가 다소 높지만, variance는 작다. 초록색 선의 경우 bias는 다소 작지만, variance가 높다.따라서 이 둘의 balance가 중요할 것이다.  Occam’s RazorOccam’s Razor라는 개념이 있다. 이는 비슷한 error를 가진 hypothesis에 대해서는 가장 단순한 모델을 선정하라는 것이다. 우리가 true function을 모른 채로 아래 3가지의 hypothesis가 존재하고, 3개 모두 error가 동일하다면, 가장 단순한 underfitting인 hypothesis를 선정하는 것이 가장 적합하다.   Cross Validation이전에 $ \\bar{g}(x) $ 를 생성하기 위해서는 무수히 많은 데이터셋이 필요했다. 그러나 현실에서 우리는 무수히 많은 데이터셋을 생성할 수 없으므로, 가지고 있는 데이터셋을 N개의 subset으로 분할하여 사용하여 최대한 $ \\bar{g}(x) $ 와 비슷하게 구성할 것이다. 이를 위해 사용되는 개념이 N-fold cross validation 이다. N개의 subset으로 나눈 후, N-1개의 subset은 training에 사용하고, 1개는 testing에 사용한다. 이 과정을 총 N번, 즉 모든 subset에 대해 1번씩 test를 진행한다.가장 최소단위로 subset을 구성한다면 당연히 instance가 1개인 경우이므로 이러한 경우를 LOOCV(Leave One Out Cross Validation) 이라 한다.   Performance Measure method현실에서 우리는 true function을 알수가 없고, 무수히 많은 데이터셋을 생성할 수 없기에 average hypothesis도 계산할 수 없다. 그러므로 우리는 bias와 variance를 성능 평가에 사용할수가 없다.그러나 반드시 성능을 평가해야 하므로, 다른 방법을 사용한다. Precision and Recall첫번째 방법으로는 precision and recall 이 있다. 예를 들어, 오른쪽 표처럼 현실에서의 true와 false가 존재하고, 우리가 true 또는 false라 예측한 값인 positive/negative가 존재할 때, 다음과 같이 case를 나눌 수 있다.만약 우리의 task가 spam filter와 VIP classifer 두 가지가 존재한다면, spam filter의 경우 안전이 우선시 되어야 할 것이다. VIP classifer의 경우는 정확도가 우선시되어야 한다. spam filter은 안전, 즉 스팸이 아닌 것(False)을 스팸이라 분류(Positive)하는 경우를 가장 예방해야 하고, VIP classifier는 VIP(True)를 아니라고 분류(Negative)하는 경우를 가장 예방해야 한다. 따라서 전자는 FP를, 후자는 FN을 줄이는 것을 우선시해야 한다. 평가 지표로서 precision과 recall을 다음과 같이 정의할 수 있다. Precision : TP / (TP + FP) recall : TP / (TP + FN)filtering은 Precision을, recall은 Recall을 사용하여 평가하는 것이 좋다.  F-Measureprecision과 recall의 밸런스를 측정하는 것도 중요하므로 그에 대해 수식을 다음과 같이 정의할 수 있다. $ F_b-Measure = (1 + b^2) \\times (Precision \\times Recall) / (b^2 \\times Precision + Recall) $ $ F_1-Measure = (1 + 1^2) \\times (Precision \\times Recall) / (1^2 \\times Precision + Recall) $b는 precision에 대한 중요도로, b를 낮게 할지, 높게 할지에 따라 recall을 강조할지, precision을 강조할지 선택할 수 있다.  ROC curveROC(Receiver Operating Characteristic) curve는 True Positive Rate와 False Positive Rate에 대한 값을 그래프로 나타낸 것이다. ROC 커브가 좌상단에 붙을수록 더 좋은 분류기이다.ROC curve에서 볼 수 있는 특성은 크게 3가지가 있다. True Positive Rate와 False Positive Rate ROC Curve위의 한 점이 의미하는 것은? ROC Curve의 휜 정도가 의미하는 것은?먼저 FPR(False Positive Rate)와 TPR(True Positive Rate)는 ROC curve에서 각각 x,y축에 표시되는 값이다. FPR과 TPR은 위에 나온 precision과 recall에 대한 표에 나온 F/T/P/N과 동일한 값을 의미한다.그 다음, ROC Curve를 살펴보면, 그래프 위에 하나의 점이 존재한다. 이 점은 decision boundary를 의미한다.ROC Curve의 휜 정도는 구별하는 정도를 의미한다. 즉 커브가 많이 휠수록 더 잘 분류한다는 의미가 된다. 참고 : https://angeloyeo.github.io/2020/08/05/ROC.html   6-3. Model Regularizationregularization이란, 모델을 너무 완벽하게 fit하지 않겠다, 즉 training dataset에 대해 정확도를 줄이기 위한 기법이다. 이를 통해 test에 대한 잠재적 fit을 증가시킬 수 있다. regularization에는 L1, L2 등의 방법이 있다. L1 regularization(Lasso regularization) : $ \\lambda |w| $ L2 regularization(Ridge regularization) : $ \\frac{\\lambda}{2} ||w||^2 $이외에도 Bridge라는 방법도 존재하긴 하나, 가장 많이 사용되는 것은 L2 이다. 그 이유는 아래 그래프에서 좌상단에 parameter space가 존재하고, 최적화를 할 때마다 영역이 점점 커진다고 생각할 때, 중간의 도형과 paramter space가 접하는 지점에 대해 optimization이 진행된다. 따라서 Lasso의 경우는 4개의 꼭지점에서만 parameter가 존재할 수 밖에 없으므로 제한적이다. 그러나 ridge의 경우는 원이므로 다양한 paramter가 존재할 수 있게 된다. ridge loss function : $ \\frac{1}{2} \\sum_{n=0}^N (train_n - g(x_n, w))^2 + \\frac{\\lambda}{2}||w||^2 $ lasso loss function : $ \\frac{1}{2} \\sum_{n=0}^N (train_n - g(x_n, w))^2 + \\lambda|w| $  ridge regularization을 활용하여 linear regression을 살펴보자. w를 계산하기 위해 w에 대해 loss function을 편미분하면 다음과 같다.\\[\\frac{d}{dw} E(w) = 0\\]\\[\\frac{d}{dw} E(w) = \\frac{d}{dw} (\\frac{1}{2} ||train - Xw ||^2 \\frac{\\lambda}{2}||w||^2)\\]이를 정리하여 미분하고, w에 대해 정리한다.\\[w = (X^TX + \\lambda I)^{-1}X^T \\cdot train\\]train은 y label을 의미한다. regularization이 적용되지 않았을 때와 적용되었을 때의 차이를 살펴보면 위의 그래프와 같다. 모델의 복잡도를 높이지 않고도 bias는 약간 증가하고, variance가 상당히 감소되었음을 알 수 있다. regularization에 $ \\lambda $ 를 조절하여 강도를 조절 할 수 있다. $ \\lambda $ 가 너무 작으면 variance는 높고, $ \\lambda $ 가 너무 작으면 variance도 너무 작아진다. 따라서 적합한 regularization의 강도를 찾아야 하므로 이에 대해 수많은 실험을 통해 최적화를 수행해야 한다.   regularization of Logistic Regressionlogistic regression에 대해서도 regularization을 추가할 수 있다.\\[argmax_\\theta \\sum_{i=1}^m log(p(y_i \\| x_i, \\theta)) - \\alpha R(\\theta)\\] \\[L1 : R(\\theta) = ||\\theta||_1 = \\sum_{i=1}^n |\\theta_i |\\]\\[L2 : R(\\theta) = ||\\theta||_2^2 = \\sum_{i=1}^n \\theta_i^2\\]logistic regression을 수행할 때, closed form이 아니기에 approximation을 수행해야 했다. 따라서 regularization에 대해서도 gradient descent/ascent 를 수행하여 최적화를 할 수 있다.  regularization and SVMSVM에서는 regularization과 유사한 기능을 하는 term이 있었다. 그것은 C로 지난 주에 C를 조절하면서 decision boundary가 어떻게 변하는지를 살펴보았었다. C는 soft-margin을 감안했을 때 추가되는 값이므로 soft-margin을 가정하면 자동적으로 regularization이 추가된다고 생각할 수 있다." }, { "title": "[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression ", "url": "/posts/kooc_week34/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-09-29 01:40:00 +0900", "snippet": "Chapter 3. Motivation and Basics 목차 optimal classification optimal predictor concept of Bayes risk concept of desicion boundary Naive Bayes Classifier what is classifier? Bayesian version of linear classifier naive assumption naive classifier of text mining bag-of-words   3-1. Decision Boundaryoptimal predictor 이란 이때까지 해왔던 f(X), 즉 $ \\hat{f} $ 이 실제 Y와 같지 않을 확률이 최소가 되는 것을 f* 라는 것이다. 즉 우리가 가정한 값이 틀릴 확률이 최소가 되도록 하고자 한다. 이는 우리가 가정한 값이 정답일 확률이 최대가 되도록 한다는 것과 동일하다. 여기 점선과, 실선 총 4개의 값에 대한 그래프가 있다고 해보자. 이 때, 특정 X에 대해 점선은 점선끼리 더해서 1이 되어야 할 것이고, 실선은 실선끼리 더해서 1이 되어야 할 것이다. 이 둘 중 어느 것이 더 나은 것인지에 대한 측정을 해보아야 한다.4개의 선들이 모두 만나는 지점을 Xm이라 했을 때, 이 Xm보다 조금 앞인 Xn의 위치에서 보면 점선의 경우 초록색과 빨간색이 차이가 별로 크지 않다. 그러나 실선의 경우 점선보다는 다소 크다. 즉, 점선의 경우 명확하게 맞는지 틀린지를 구분하게 해주지 못한다는 뜻이다. Xm의 앞부분을 보면 초록색이 true라 판단하므로 빨간색 아래의 영역은 error에 해당된다. 그래서 점선의 빨간색 아랫부분과 실선의 빨간색 아랫부분의 차이가 두 모델의 차이이자 Bayes Risk라 불리는 값이 된다.따라서 Bayes optimal classifier 라는 것은 Bayes risk라는 값의 크기를 줄이는 최적화를 수행하는 과정을 나타낸다. 이 때, P(Y = y | X = x ) 를 prior 정보를 조금 더 활용할 수 있도록 Bayes theorem을 통해 변환할 수 있다. Prior 정보를 알아낼 때는 MLE나 MAP 등의 방법으로 데이터셋에서 추출하여 얻을 수 있다. 그리고 class conditional density = P(X = x | Y = y) 는 Y를 먼저 특정한 후, 그곳에서 x가 나올 확률을 구할 수 있다. 그러나 이러한 방식은 x의 feature가 1개 이상이고, 서로 상호적이라면 density를 추정하기 어려울 것이다. 3-2. Naive Bayes Classifier따라서, 이러한 상호적인 관계를 무시하겠다는 것이 Naive Bayes Classifier이다. 각각의 feature들이 독립적이라는 가정하에 확률을 구해 추정한다.지난 번에 배웠던 이 데이터를 가지고, 함수를 최적화해보자. 각 x의 feature인 x1,x2,x3,x4,x5,x6가 있고, label인 y=yes가 존재한다. 이 때, prior는 y에 대해서 구하는 것이므로 yes에 대한 prior는 3/4가 된다. 즉, prior를 구하기 위해서 필요한 파라미터는 k-1(이 때 k=2), 1개이다. 그 후, P(X=x|Y=y) 를 구해야 하는데, 이를 구하기 위해서 고려해야할 파라미터의 개수는 각 feature마다 2가지의 선택지만 존재한다고 해도 64개의 값이 더해지면 1이 되어야 한다는 특성으로 이해 63개의 값만 구하면 된다. 그리고 y의 값도 고려해줘야 하므로 (2^d - 1)^k 개의 파라미터가 필요하다. 2개의 선택지만 가정한다 해도 (64-1)*2 = 126 개의 파라미터가 필요해진다. 우리가 직접 모델을 추정하기 위해서는 (2^d - 1)^k 를 잘 해결해야 간편하게 모델을 추정할 수 있다. 이를 위해 conditional independence, 즉 각 feature 별 독립을 가정하여 연산을 줄이고자 한다.예를 들어, 상사인 commander가 한명 있고, 2명의 officer가 있다고 할 때, 만약 A가 명령을 듣지 못한다해도 B가 수행하는 행동에 따라서 행동할 수 있을 것이다. 이는 독립적이지 않다는 의미가 된다. 그러나 만약 B가 어떻게 행동하든지 상사의 명령을 무조건 들을 수 있어서 따르게 된다면 B의 행동에 영향을 받지 않는다. 이럴 때는 독립적이게 된다. marginal independence라는 것은 상사가 어떻게 말하든 듣지 않고, 행동을 하는 상황을 일컫는다. 아까의 데이터셋으로 돌아와 P(X=x|Y=y)를 구하기 위해서는 (2^d -1)^k 개의 파라미터가 필요했다. 그러나 우리가 독립적이라는 것을 가정하게 되면 $ P(X=x|Y=y) = P(X)P(Y) $ 로 계산이 가능하다. 그렇게 되면, 개별 feature들 간 곱셈으로 변하게 되어, (2-1)dk 개의 파라미터 수로 줄어든다. 최종적으로 Naive Bayes Classifier 에 의해 구해진 f는 다음과 같다.\\[f_{NB}(x) = argmax_{Y=y} P(Y=y) \\prod_{1 \\leq i \\leq d} P(X_i = x_i | Y = y)\\] naive bayes classifier는 생성하기 쉽다. 확률만 MLE, MAP를 통해 잘 구해줄 수 있다면 구하기 쉽다. 그러나 그만큼 문제점은 많이 존재한다. 각 feature간이 독립적이라는 것 자체가 부정확한 가정이며, 값이 존재하지 않은 feature가 있다면 0으로 수렴하는 값이 추론될 수 있게 될 것이다.  Chapter 4. Logistic Regression 목차 Logistic Regression Classifier why the logistic regression is better than linear regression logistic function logistic regression classifier approximation approach for open form solutions Gradient Descent Algorithm tailor expansion gradient descent / ascent algorithm difference between Naive Bayes and logistic regression similarity of two classifier difference of two classifier performance of two classifier   4-1. Logistic Regression이러한 그림에서 선들이 겹치는 구간인 Xm에서 결정, desicion이 변화하므로 이 지점을 desicion boundary라 부른다. 점선의 경우 1차원적으로 증가하지만, 실선의 경우 S 커브 형태를 가진다. 이 S커브를 sigmoid function 이라 부른다. 이러한 sigmoid function은 확률이란 것이 0~1로 값이 매칭되어야 하고, tail 부분에서 거의 일정한 경향이 있으므로 최적화하기 쉽다. 또한, sigmoid function은 desicion boundary의 근처에서 급격하게 변화하는 모습을 보인다. 이전에 사용했던 신용평가 데이터셋을 사용하여 Sigmoid function을 만드는 방법을 설명하고자 한다.A15라는 feature를 사용하여 예측을 했을 때, 왼쪽 그래프의 형태가 된다. x축은 X=A15인 feature이고, y축은 예측 클래스인 0 또는 1이다. 왼쪽 그래프를 보면, 다소 작은 값의 feature를 가진 데이터들은 0, 예외도 존재하긴 하나 대체로 큰 feature 값을 가진 데이터는 1로 구성되어 있는 것을 볼 수 있다.조금 더 자세하게 보기 위해 x를 log에 씌워 관찰해본다. log를 씌운 그래프는 오른쪽 그래프가 되고, log를 씌운다는 것은 급격한 변화를 감소시켜 자세하게 보고자 한다는 것이다. 오른쪽 그래프를 볼 때, 방금 가설을 세웠던 작은값들은 0, 다소 큰 값은 1이라는 것이 더 확실하게 바라볼 수 있다. 이 때, 어떤 지점에 Desicion boundary를 그어야 잘 분류할 수 있는지 확인해볼 필요가 있다. 이러한 A15에 대한 데이터를 linear function으로 투영한 결과가 빨간색 점인데, 이 값들은 0~1을 넘어서는 결과가 되므로 잘못된 함수가 된다. 그리고는 sigmoid function의 한 종류인 logistic function을 사용하여 fitting 해보고자 한다. 그 결과가 왼쪽 그래프의 초록색 그래프이다.이 경우는 decision boundary가 매우 앞에 위치하고 있다. 이를 더 자세하게 보기 위해 log를 씌워서 본다. 이 때, 보면 linear regression도 지수함수적인 그래프로 구성된다. 그리고 초록색 그래프의 경우 급격한 변화가 자세하게 보인다.linear regression의 경우 y값이 0.5를 지나는 것이 decision boundary가 되므로 그 선이 그어져 있고, logistic regression의 경우 급격한 변화가 생기는 부분인 y값이 0.5를 지나는 부분이 decision boundary이므로 그 선이 마찬가지로 그어져 있다. 회색 선을 확인해봤을 때, logistic regression에 대한 decision boundary가 더 분류가 잘 될 것이다.  이제 logisitic function에 대해 자세하게 살펴보자.먼저 sigmoid function은 x가 $ -\\infty \\sim \\infty $ 의 범위를 가진다. 그리고 항상 증가하는 형태를 가져야 한다. tanh(x) 나 argtan(x) 가 sigmoid function의 종류 중 하나이다.그리고, logistic function은 sigmoid 와 비슷한 형태를 가지는데, y의 범위가 sigmoid는 -1~1이지만, logistic의 경우 0~1의 y범위를 가진다. 함수는 다음과 같이 정의된다.\\[f(x) = \\cfrac{1}{1+e^{-x}}\\] 그리고, 이 logistic function을 역함수 한 것을 logit function이라 부르고, 그 형태는 오른쪽 그래프와 같다. logit function을 통해 A15에 대한 데이터를 fitting 해보면, 다음과 같다. logit function은 $ f(x) = log(\\cfrac{x}{1-x}) $ 의 형태를 가지는데, 이를 x를 p와 fitting하고, y를 A15라는 feature에 fitting을 하게 되면 $ x = log(\\cfrac{y}{1-y}) $ 가 된다. 오른쪽 그래프는 A15가 x축, p가 y축이다.그 후, 이 function을 원래 데이터에 맞게 rescaling과 translation을 해야 하므로, recale에 대한 변수를 a, translation에 대한 변수를 b라 지정하여 $ ax + b = log(\\cfrac{p}{1-p}) $ 로 나타낼 수 있다.또한, linear regression에서 수행했던 matrix로의 변환인 $ X\\theta $ 와 같이 ax + b도 1차함수형태이므로 이를 matrix형태인 $ X\\theta $ 로 변환할 수 있다. p는 P(Y|X) 즉, X가 주어진 상황에서 Y의 확률인데, 이를 logit 함수의 형태로 regression을 한다하여 logistic regression이라 한다. 따라서 최종적인 형태는 다음과 같다.\\[X\\theta = log(\\cfrac{P(Y|X)}{1-P(Y|X)})\\] logistic regression은 binomial or multinomial outcome을 예측하는 확률적인 classifier이다. 이러한 상황에서 bernoulli experiment에 logistic function을 사용해보자.$ P(y|x) = \\mu(x)^y (1-\\mu(x))^{1-y} $ 인 상황에서 y가 1인 상황일 때의 $ \\mu $ 를 logistic function 인 $ \\mu(x) = \\cfrac{1}{1 + e^{-\\theta^Tx}} $ 로 변환할 수 있다. 이를 위의 식인 $ X\\theta $ 에 대입하여 정리해보면 다음과 같다.\\[P(Y|X) = \\cfrac{e^{X\\theta}}{1+e^{X\\theta}}\\]supervised learning인 상황에서 X, Y는 값을 알고 있으니 최적화해야 하는 부분은 theta가 된다. 기존에 사용했던 MLE 개념을 차용할 것인데, 예전에 사용했던 $ \\hat{\\theta} = argmax_{\\theta}P(D|\\theta) $ 는 D, supervised learning 개념이 들어가 있지 않은 간편화된 데이터셋에 대한 식이었다. 이를 더 고도화하여 condition에 따른 MLE, 즉 Maximum Conditional Likelihood Estimation(MCLE)를 구한다. 동일하게 MLE를 구하지만, 주어진 feature이 존재하는 상황에서 클래스에 대한 확률을 구한다.동일하게 log를 취하여 연산을 하고, $ log(\\prod_{1 \\leq i \\leq N}) = \\sum_{1 \\leq i \\leq N} $ 로 바꿀 수 있다. 그리고 P에 대해서도 이전에 사용했던 방식을 통해 구체화시킨다. $ P(Y_i | X_i;\\theta) = \\mu(X_i)^{Y_i} (1-\\mu(X_i))^{1-Y_i} $ 로 될 것이고, 여기에 log를 씌우면 간단하게 식을 세울 수 있다.이 때, $ log(\\cfrac{\\mu(X_i)}{1-\\mu(X_i)}) $ 가 이전에 사용했던 $ X\\theta $ 에 대한 식과 동일한 형태이므로 이를 치환한다. 이 때 X_i에 대한 값이므로 $ X_i \\theta $ 로 치환한다. 또한, $ \\mu(x) = \\cfrac{e^{X\\theta}}{1+e^{X\\theta}} $ 이므로 이 식을 사용하여 $ log(1-\\mu(X_i)) $ 도 치환한다. 최종적으로 theta_hat은 다음과 같다.\\[\\hat{\\theta} = argmax_{\\theta} \\sum_{1 \\leq i \\leq N} {Y_iX_i\\theta - log(1 + e^{X_i\\theta})}\\] 최대가 되는 theta를 찾기 위해 개별 theta(theta_j)에 대해 미분을 한다. $ Y_iX_i\\theta $를 theta_j에 대해 미분하면, theta_j에 해당하는 값 이외의 theta_1,theta_2…는 모두 사라지게 될 것이다. 그러면 결국 j에 대한 값만 남게 되므로 $ Y_iX_{i,j} $ 만 남게 된다. 그 후 X_(i,j) 에 대해 묶어줄 수 있고, 그에 따라 생겨나는 식은 또 다시 P(y=1|x) 에 대한 것으로 변환해줄 수 있게 된다. 그러나 이렇게 변환하면 간단하게 업데이트를 할 수 없는 open form 형태가 되므로 근사를 취해야만 한다. 4-2. Gradient method근사를 취하기 위한 방법으로는 gradient를 활용하는 것이다. gradient method를 사용할 때의 가장 기초는 Taylor Expansion 이다. Taylor series는 함수를 무한대의 함수의 합으로 근사화할 수 있다는 것이다.\\[f(x) = f(a) + \\cfrac{f&#39;(a)}{1!}(x-a) + \\cfrac{f&#39;&#39;(a)}{2}(x-a)^2 + \\cdots = \\sum_{n=0}^\\infty \\cfrac{f^{(n)}(a)}{n!}(x-a)^n\\] 이러한 taylor expansion을 활용하여 gradient descent/ascent algorithm을 구현해본다. gradient method는 함수 f(x)와 첫 시작 파라미터인 x1이 주어졌을 때, 올바른 방향으로 가기 위해 f(x)의 값을 높이거나/줄이거나하는 파라미터를 반복적으로 움직여주는 기법이다. 따라서 이 기법에 필요한 값들은 방향/속도이다. 얼마나 빠르게 이동하던 상관없이 방향이 맞으면 언젠가는 정답에 가까워지게 되므로 더 중요한 것은 방향이 된다. Big-Oh Notation 을 활용하여 taylor expansion을 정의하면 $ f(x) = f(a) + \\cfrac{f’(a)}{1!}(x-a) + O(||x-a||^2) $ 이 된다. 이 때, a=x1(initial parameter), x = x1 + hu (h=speed, u=unit direction)을 통해 x를 업데이트를 한다고 하면$ f(x_1 + hu) = f(x_1) + hf’(x_1)u + h^2O(1) $ 의 식이 된다. 이 때, h가 작은 값이라는 가정하에 O(1)는 상수이므로 근사를 통해 제거하고, f(x1)를 좌항으로 넘긴다.이 때, f(x)를 증가시킬수도 있고, 감수시킬수도 있다. $ argmin_u {f(x1 + hu) - f(x1)} $ 이란, 즉 증가량을 최소화하는 u를 찾는 것이고, 이는 기울기가 감소되는 방향(gradient descent)으로 진행됨을 나타낸다. 그리고 이 식은 이미 위에서 구현을 했다. u는 감소하는 unit vector이므로 $ u = - \\cfrac{f’(x_1)}{|f’(x_1)|} $ 로 나타낼 수 있다.그렇다면, 처음 값 x1 이 아닌 특정 x_t에 대한 업데이트는 다음과 같다.\\[x_{t+1} = x_t + hu* = x_t - h \\cfrac{f&#39;(x_1)}{f(x_1)}\\]여기서 unit vector에 -를 +로 바꿔주면 gradient ascent로 될 것이다. 이때까지는 이론을 배웠으니, 실제 적용 사례를 살펴보자. f(x1,x2)에 대한 함수가 존재하고, 이에 대해 미분을 하면 (1,1)의 지점에서 최소값이라는 것이 구해질 것이다. 단순하게 생각해봐도 f(x1,x2)에서 오른쪽 텀에 있는 값들이 모두 square(제곱)처리가 되어 있으므로 항상 양수가 나오게 될 것이다. 그러면 이 function에서의 최소값은 당연히 0이 되어야 하므로 쉽게 (1,1)이라는 지점을 찾아낼 수 있다.이 함수에 단순한 미분이 아닌 gradient descent method를 사용해보자.순서 choose random initial point $ x_0 = (x_1^0, x_2^0) = (-1.3, 0.9) $ partial derivative vector at the point $ f’(x^0) = (\\cfrac{\\partial}{\\partial x_1} f(x_1, x_2) , \\cfrac{\\partial}{\\partial x2} f(x_1, x_2)) = (-415.4, -158) $ select speed, h h = 0.001 update the point $ x^1 = x^0 - h \\cfrac{f’(x^0)}{|f’(x^0)|} = (-1.2991, 0.9004) $ iteration 2~4 for x^i #!/usr/bin/env bash# gradient descent code make soon.# 코드 추가 예정.  자 그러면, 이제 gradient ascent method와 logistic regression을 결합하면 어떻게 될까?gradient ascent에 대한 식이 argmax 형태로 나타나 있다. 이 때 argmax부분의 안에 들어있는 식을 f(theta) 라 했을 때, 각각의 theta_j에 대해 편미분을 수행한다. 수행한 결과는 gradient method를 배우기 전에 구해보았다.그리고, gradient method에서의 업데이트 식은 $ \\theta_{t+1} = \\theta_t + h u* = \\theta_t + h \\cfrac{f’(\\theta_t)}{|f’(\\theta_t)|} $ 가 되는데, 여기 u*를 logistic function을 적용하면, theta_j에 대한 업데이트 식은 다음과 같이 구성된다. $$ \\theta_j^{t+1} = \\theta_j^t + h \\cfrac{\\partial f(\\theta^t)}{\\partial \\theta_j^t} = \\theta_j^t + h{\\sum_{1 \\leq i \\leq N}X_{i,j}(Y_i - P(Y=1 X_i;\\theta^t))} 그리고, 이 때 ,P(y=1|x)를 치환할 수 있으므로,\\[\\theta^t + \\cfrac{h}{C}{\\sum_{1 \\leq i \\leq N} X_{i,j}(Y_i - \\cfrac{e^{X_i \\theta^t}}{1 + e^{X_i \\theta^t}})}\\]이 때 ,C 는 unit vector로의 정규화 상수이다.  4-3. difference between naive bayes and logistic regression Gaussian Naive Bayes기존에 배웠던 Naive Bayes 방식은 단순히 2개의 class에 대한 classifier이었다. 그러나 logistic regression은 continuous feature에 대해 다뤄지므로 이 둘을 비교하기 위해 Naive Bayes를 gaussian distribution을 따른다는 가정하에 continuous 한 function으로 만들어줄 수 있다. 기존의 Navie Bayes classifier function 은 $ f_{NB}(x) = argmax_{Y=y}P(Y=y) \\prod_{1 \\leq i \\leq d}P(X_i = x_i | Y = y) $ 로 정의된다. 이를 평균이 $ \\mu $, 분산이 $ \\rho^2 $ 인 gaussian distribution에 대해 정의하면,\\[P(X_i | Y, \\mu, \\rho^2) = \\cfrac{1}{\\rho \\sqrt{2 \\pi}}e^{-\\cfrac{(X_i - \\mu)^2}{2 \\rho^2}}\\]그리고 prior, P(Y = y)는 MLE 또는 MAP를 통해 구해줄 수 있으므로 상수 $ \\pi_1 $ 로 나타낼 수 있다.따라서 Gaussian Naive Bayes function은 Gaussian distribution을 feature 개수만큼 곱한 $ P(Y)\\prod_{1 \\leq i \\leq d}P(X_i|Y) = \\pi_k \\prod_{1 \\leq i \\leq d} \\cfrac{1}{\\rho_k^iC} e^{-\\frac{1}{2}(\\frac{X_i - \\mu_k^i}{\\rho_k^i})^2} $ 가 된다.이 때, C는 sqrt(2pi)_i 를 feature개수인 d개를 모두 곱한 값을 나타낸다.  이제, Gaussian Naive bayes function과 logistic regression이 같아질 수 있는가를 증명해본다. naive Bayes assumption에서 $ P(Y=y|X) = \\cfrac{P(X|Y=y)P(Y=y)}{P(X)} $ 인데, 여기서 logistic regression 의 경우 좌항에서 바로 치환하여 연산을 했고, Gaussian Naive Bayes 모델의 경우 우항으로 변환 후 연산하여 function을 구했다. 그래서 이 둘의 관계가 정확하게 어떻게 구현될지 정의해볼 것이다. GNB(Gaussian Naive Bayes), $ \\cfrac{P(X|Y=y)P(Y=y)}{P(X)} = \\cfrac{P(X|Y=y)P(Y=y)}{P(X|Y=y)P(Y=y) + P(X|Y=n)P(Y=n)} $ 으로 정의할 수 있다. 이 때, n은 Y에 포함되어 있지 않은 X들을 의미한다.여기서, 이 P(X|Y=y)는 feature의 개수에 따라 파라미터가 기하급수적으로 증가하게 되므로, feature에 대해 각각을 표현해주기 위해 $ \\prod $ 형태로 표현한다. 그 후, P(Xi|Y=y) 가 gaussian distribution을 따른다고 가정했기 때문에, P(Y=y|X)는 다음과 같이 정의된다.\\[P(Y=y|X) = \\frac{\\pi_1 \\prod_{1 \\leq i \\leq d} \\frac{1}{\\rho_1^iC} e^{-\\frac{1}{2}(\\frac{X_i - \\mu_1^i}{\\rho_1^i})^2}}{\\pi_1 \\prod_{1 \\leq i \\leq d} \\frac{1}{\\rho_i^iC} e^{-\\frac{1}{2}(\\frac{X_i - \\mu_1^i}{\\rho_1^i})^2} + \\pi_2 \\prod_{1 \\leq i \\leq d} \\cfrac{1}{\\rho_2^iC} e^{-\\frac{1}{2}(\\frac{X_i - \\mu_2^i}{\\rho_2^i})^2}} = \\frac{1}{1+\\frac{\\pi_2 \\prod_{1 \\leq i \\leq d} \\frac{1}{\\rho_2^iC} e^{-\\frac{1}{2}(\\frac{X_i - \\mu_2^i}{\\rho_2^i})^2}}{\\pi_1 \\prod_{1 \\leq i \\leq d} \\frac{1}{\\rho_1^iC} e^{-\\frac{1}{2}(\\frac{X_i - \\mu_1^i}{\\rho_1^i})^2}}}\\] 이 때, $\\rho_2^i = \\rho_1^i $ 라 가정하면,\\[P(Y=y|X) = \\cfrac{1}{1+e^{-\\cfrac{1}{2(\\rho_1^i)^2}\\sum_{1\\leq i \\leq d}{2(\\rho_2^i - \\rho_1^i)X_i + {\\mu_1^i}^2 - {\\mu_2^i}^2} + log\\pi_2 - log\\pi_1}}\\]이 때, sumation 안에 있는 $ 2(\\mu_2^i - \\mu_1^i)X_i $ 가 linear regression에서 표현되는 $ X\\theta $ 와 같은 형태이다.  최종적으로, 우리가 Naive Bayes classifier를 구하기 위해 가정한 것들에는 여러 가지가 있다. $ P(Y=y|X) = \\cfrac{1}{1+e^{-\\cfrac{1}{2(\\rho_1^i)^2}\\sum_{1\\leq i \\leq d}{2(\\rho_2^i - \\rho_1^i)X_i + {\\mu_1^i}^2 - {\\mu_2^i}^2} + log\\pi_2 - log\\pi_1}} $ Assumption Naive Bayes assumption Gaussian distribution for P(X|Y) Bernoulli distribution for P(Y) num of parameter : 4d + 1Naive Bayes classifier 식에 있는 파라미터는 총 각 feature 마다의 variance 2개, mean 2개씩이므로 4d, 그리고 prior 1개이다. prior는 binomial 인 상황에서 true에 대한 확률을 알고 있으면 false는 구할 수 있으므로 1개이다. 그에 반해 Logistic Regression은 가정한 것이 1개 뿐이다. $ P(Y|X) = \\cfrac{1}{1+e^{-\\theta^Tx}} $ Assumption fitting to logistic function num of parameter : d + 1그리고, 파라미터도 개별 feature에 대한 theta와 bias term 이므로 d+1개 뿐이다. 단편적으로 파라미터의 개수만 봤을 때는 logistic regression이 좋지만, 우리가 조정할 수 있는 값이 많다는 자유도 측면에서 봤을 때는 naive bayes가 더 좋다. 따라서 개인의 task에 맞는 모델을 사용해야 할 것이다.   Generative modelNaive Bayes classifier 처럼, P(Y|X)를 P(X|Y)P(Y)/P(X) 로 정의하여 추론하는 방법을 Generative model이라 한다. prior정보를 활용할 수 있고, Bayesian 형태이며, joint probability 에 대해 모델링한다는 점이 장점이다.  Discriminative modelLogistic Regression 처럼, P(Y|X)를 곧바로 추론하는 방법을 Discriminative model 이라 한다. 이는 conditional probability 에 대해 모델링하는 것이 특징이다." }, { "title": "[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy ", "url": "/posts/kooc_week12/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-09-27 01:40:00 +0900", "snippet": "Chapter 1. Motivation and Basics 목차 Motivation Machine Learning, AI, Datamining What is Machine Learning? MLE MAP Basics Probability Distribution And some Rules   1-1. Motivation  Machine Learning에는 학습 방법에 따라 supervised learning, unsupervised learning, reinforcement learning 이 있다. 먼저 supervised learning에는 요약하거나 예측과 같은 것들이 이에 해당하고, 요약하고 정리하고 군집을 찾는 것이 unsupervised learning에 해당한다. 마지막으로 어떤 것이 지능적이고, 원하는 계획인가에 대한 학습이 reinforcement learning이다.더 자세하게 살펴보자.  supervised learning사람이나, 기계가 지정한 가이드가 있는 것이 supervised라 할 수 있다. 예를 들어, 스팸 필터링은 매우 많은 데이터들에 의해 어떤 것을 필터링할지 판단한다. 또한, 자동 카테고리화도 수많은 데이터의 축적에 의해 분류하는 모델을 생성하는 것이다.또는, 특정 값에 대해 가격을 예측하는 것과 같은 것들은 regression, 또는 prediction 도 supervised learning task에 해당한다.  unsupervised learning그에 반해, 순수히 주어진 데이터에 의해 기계가 군집을 찾고, 패턴을 찾도록 하는 것이 unsupervised learning이다. 또는 잠재적인 현상을 분석할 때도 활용한다. 예를 들어 신문기사가 엄청 많을 때, 주제를 10개로 추려보는 것은 직접 가이드를 제작해주기 힘들어서, 기계가 직접 군집을 찾아야 하므로 unsupervised learning에 해당한다. 1-2. What consists of Machine Learning?Thumbtack, 압정을 활용하여 게임을 해보자. 동전과 달리, 압정은 앞과 뒤가 50:50이라 하기 어렵다. 그래서 앞 또는 뒤가 나올 확률을 직접 구해보고자 한다. 가장 먼저 해야 할 방법은 직접 던져보는 것이다. 뾰족한 부분이 아래일 때를 Head, 뾰족한 부분이 위일 때를 Tail 이라 하고, 5번을 던져봤을 때 Head가 2번 Tail이 3번 나온다면, 확률은 각각 2/5, 3/5 인 것은 당연한 결과다. 그러나 이 2/5, 3/5라는 값은 사실 그리 간단하게 구해지는 것이 아니다. 이 2/5, 3/5로 구하는 것은 Binomial distribution이라 하는데, 이는 뚜렷한, 이산적인 사건에 대한 확률 분포를 말한다. 즉, Head가 나오든, Tail이 나오든 2가지의 경우만 존재하므로 Binomial distribution이라 한다.먼저 head가 나올 경우, 다음에 Tail이 나올 때 앞서 나온 값이 영향을 주지는 않으므로 독립적인 상황이다. 만약 Head가 나올 경우를 P(H) = Theta, Tail이 나올 경우를 P(T) = 1 - Theta 라 하자. 이럴 때, 방금 전 상황인 HHTHT에 대한 확률은 $ P(HTTHT) = \\theta (1-\\theta) (1-\\theta) \\theta (1-\\theta) = \\theta^2 (1-\\theta)^3 $ 이다. 전체 Data를 D, Head가 나온 횟수를 a_H, 반대의 경우를 a_T라 한다면 theta, 즉 head가 나올 확률은 다음과 같다.\\[P(D | \\theta) = \\theta^{a_H}(1-\\theta)^{a_T}\\]이러한 추정값이 최적의 $ \\theta $ 라는 것을 증명해야 하는데, 이를 증명하는 것이 확률이라 할 수 있다. 이를 증명할 때 사용하는 대표적인 방법에는 MLE, MAP이 있다. MLE(Maximum Likelihood Estimation)관측된 데이터의 확률이 최대가 되는 값 theta를 찾는 방법이 MLE이다. MLE를 수식적으로 나타내면 다음과 같다.\\[\\hat{\\theta} = argmax_{\\theta} P(D|\\theta)\\]즉, P(D|theta)가 최대가 되는 theta를 theta hat이라 표현한다는 것이다. 여기에 위에서 구한 식을 대입하면\\[\\hat{\\theta} = argmax_{\\theta} P(D|\\theta) = argmax_{\\theta} \\theta^{a_H}(1-\\theta)^{a_T}\\] 이를 구할 때, 쉽게 구하는 방법은 log를 씌우는 것이다. log를 씌운 값이 최대가 되면, 그 log 안의 값도 최대가 된다는 특성이 있다.\\[\\hat{\\theta} = argmax_{\\theta} ln\\theta^{a_H}(1-\\theta)^{a_T} = argmax_{\\theta} (a_H ln\\theta + a_T ln(1-\\theta))\\] 그 후, 미분을 통해 최솟값을 찾는다. 그러면 theta_hat = a_H / (a_T + a_H) 가 된다.  그런데, 만약 5번이 아닌 50번을 수행해서 동일하게 head가 나올 확률이 0.6이 나온다면, 효율상 5번이 더 좋다고 생각할 수 있다. 여기에는 조금의 오류가 존재한다. 우리가 이때까지 구한 것은 정답(Ground Truth)이 아니라 추론(estimation)값이므로, 오류가 항상 존재한다. 정답인 $ \\theta* $ 와 $ \\hat{\\theta} $ 사이의 오차값과 error boundary($ \\epsilon $) 에 대한 수식이 있다.\\[P(|\\hat{\\theta} - \\theta*| \\geq \\epsilon) \\leq 2e^{-2N\\epsilon^2}\\]정답과 추론의 차이가 특정 에러보다 클 확률은 우항보다 작다는 것이다. N은 trial, 즉 반복횟수인데, N이 커질수록 우항은 작아지므로 오차가 특정 error boundary보다 클 확률도 작아지게 될 것이다.이러한 방식이 PAC(Probably Approximate Correct)이라 하는데, 즉 아마도 특정 확률과 오차범위 안에서 존재하는 theta를 추정하는 기법이다.  MAPMLE에서 구한 확률은 사전정보가 포함되어 있지 않다. 그러나 만약 사전정보를 가미하여 확률을 구하고자 한다면 다른 방식을 통해 구해야 할 것이다. Bayes라는 학자는 다음과 같이 확률에 대한 식을 정의했다.\\[P(\\theta | D) = \\cfrac{P(D | \\theta)P(\\theta)}{P(D)} \\: == \\: Posterior = \\cfrac{Likelihood \\: x \\: Prior \\: Knowledge}{Normalizing\\:Constant}\\]theta가 주어졌을 때 data를 관측할 확률 x theta에 대한 사전정보 / 데이터를 관측할 확률 를 하면 데이터가 주어졌을 때 theta의 확률을 구할 수 있다. 이 떄 우리는 theta에 대해 구하고 있으므로, P(D)는 상수가 된다. 그리고 P(D|theta)는 이전에 구했으므로 P(theta)만 지정해둔다면 p(theta|D)를 구할 수 있다. 또한, Bayes는 P(theta)를 binomial distribution이 아닌 Beta Distribution을 통해 구하고자 했다.\\[P(\\theta) = \\cfrac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)},\\: B(\\alpha,\\beta) = \\cfrac{r(\\alpha)r(\\beta)}{r(\\alpha+\\beta)},\\:r(\\alpha) = (\\alpha - 1)!\\]이 때, B(alpha, beta)는 theta와 관련이 없으므로 상수 취급할 수 있다. 그렇다면, P(theta|D)는\\[P(\\theta|D) \\propto P(D|\\theta)P(\\theta) \\propto \\theta^{a_H}(1-\\theta)^{a_T} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} = \\theta^{a_H+\\alpha-1} (1-\\theta)^{a_T+\\beta-1}\\]이 식은 P(D|theta) 와 유사한 것으로 보아, theta_hat은 쉽게 추정할 수 있다.\\[\\hat{\\theta} = \\cfrac{a_H+\\alpha-1}{a_H+\\alpha+a_T+\\beta-2}\\] 이 때, MLE와 MAP의 값이 다를 수 있다. 그러나 만약 반복 횟수를 증가시킨다면, alpha와 beta에 비해 a_H와 a_T는 증가하여 비중이 커질 것이므로 같아질 수 있다.  1-3. BasicsNormal Distribution가장 많이 사용되는 기본적인 확률분포의 형태이다. 이 평균(mean) 또는 분산(variance)를 통해 함수의 모양을 바꿀 수 있을 것이다. Normal distribution에는 양쪽에 롱테일, 즉 무한대로 가는 긴 꼬리가 존재한다. Beta Distributionbeta에서는 롱테일이 존재하지 않는다. 따라서 확률을 모델링할 때 0과 1이라는 확실한 범위가 존재하므로 beta distribution을 사용할 수 있다. Binomial Distribution이 binomial distribution은 앞서 배운 함수 형태를 가지고 있고, 이산적인(discrete) 함수 형태를 가지고 있다. Multinomial Distributionbinomial distribution은 2가지의 경우만 판단하지만, 만약 2가지 이상의 선택지가 존재한다면 더 다양한 예제를 판단할 수 있어야 한다. 대화를 할 때, 수많은 단어 중 하나를 선택하므로 수많은 데이터를 가진 이산적인 함수를 사용한다.  Chapter 2. Fundamentals of Machine Learning 목차 classical method overview rule based approach classical statistics approach information theory approach rule based machine learning how to find generalized rules decision tree how to create decision tree weakness of decision tree given new dataset linear regression how to infer a parameter set   2-1. Rule Based Machine Learning Overviewperfect world라고 가정해보자. 즉 관측에 대한 에러가 존재하지 않고, 관측한 정보가 전부라 가정하는 것이다. 이러한 상황에서 날씨에 대해 관측하여 밖으로 나가 운동을 할지에 대한 태스크를 진행한다.  function approximationfunction approximation이란 몇 개의 데이터가 있을 때, 어떤 함수에 입력하면 나오는 결과가 정답인 함수를 추정하는 것이다. 그래서 머신러닝이란 더 나은 함수를 생성하는 기법이 된다.데이터의 개수, instance,X 가 존재하고, 그 instance에 대한 feature가 O=&amp;lt;Sunny, Warm, Normal, Strong, Warm, Same&amp;gt; 이라는 값을 가지고, 원하는 정답인 Y=yes 라 해보자. 이러한 instance가 여러 개 존재할 때, dataset D가 될 것이다. 데이터를 통해 가설, H를 세워 X를 집어넣을 떄 Y가 나오는 함수를 생성할 수 있다. 예를 들어 sunny, warm, same만 일치한다면 나머지에는 상관없이 yes라는 Y를 출력하는 h_i를 가정할 수 있다. 이는 가설한 것이므로 수많은 가설이 존재할 수 밖에 없다.그리고, 우리가 진짜 원하는 함수인, target function, c 가 있다면, 항상 c(X) = Y라는 식이 참이 된다. 이 c는 알 수 없으므로 추론하여 알아내야만 한다. 그래서 데이터의 개수가 많을수록 우리가 원하는 정답인 Y를 정확하게 추출할 수 있게 된다.그래서 instance가 x1,x2,x3가 있고, instance에 대한 가설인 h1,h2,h3가 있다고 해보자. 그렇다면, 가설이 정확한지 아닌지를 판단하기 위해서는 h1,h2,h3에 각각의 instance를 집어넣어 정답이 잘 추론되었는지 역계산해야 한다. 위의 h1,h2,h3를 살펴보면, h1은 x1,x2,x3가 모두 yes라는 값을 추출하는 가설이 되고, h2와 h3는 각각 2개씩만 yes라는 값을 추출하는 가설이다. 그러면 h1은 다소 일반적인 가설이므로 generalized function이라 할 수 있고, h2와 h3는 h1보다 specificed function이라 할 수 있다.  Find-S Algorithm그래서 정답의 가설인 c를 잘 추론하기 위한 알고리즘으로 Find-S 라는 알고리즘이 있다. 이 알고리즘은 먼저, 가장 specific한 가설인 H = &amp;lt;None, None, None, None, None, None&amp;gt; 라는 H를 세운다. 그 후 각 instance마다 각 feature들을 현재의 가설과 비교하여 업데이트한다.if x is positive For feature in O if h_i in H == feature_i pass else update to more generalize처음에는 모두 None이었다가 x1에 의해 H에 있는 모든 feature들을 업데이트하고, 다음 x2에서 각 feature들과 H에 있는 feature들을 비교하여 같지 않으면 둘다 포함이 되도록 업데이트한다. 이러한 방법으로 하면 가설을 확정지을 수는 있지만, 만약 이렇게 나온 h1,2,3,4가 정답과 다를수도 있을 것이다. 그래서 생각한 방법이 범위를 정하여 그 범위 안에 존재하는 가설은 모두 정답의 후보라 할 수 있게 하는 알고리즘을 생각했다.  Version SpaceVS(Version Space)란 가능한 가설의 집합을 나타내고, 이를 구하기 위해서 가장 일반적인 가설과 가장 구체적인 가설을 정의하여 범위를 좁혀나가는 Candidate Elimination Algorithm을 사용해보고자 한다. 일반적인 가설을 G, 구체적인 가설을 S 라 하고, dataset에 포함되어 있는 각 instance마다 값을 비교해나가며 업데이트한다.if y of x is positive # update first S, before Gif y of x is negative # update first G, before Sy값이 참이라고 한다면, S의 범위를 x에 존재하는 feature들을 커버할 수 있을만큼만 일반적으로 업데이트해야 하는데, 만약 G에 대해 거짓이 나오게 된다면 G를 한 단계 일반적인 방향으로 떨어뜨려야 한다. 반대로, 거짓이라면 G의 범위를 커버할 수 있을만큼만 구체적으로 업데이트해야 하고, 만약 S에서 참이라 한다면 S를 한 단계 구체적인 방향으로 올려야 한다. 구체적인 동작방식은 다음과 같다.출력값이 yes라고 한다면, S를 먼저 업데이트하므로 G0==G1==G2가 될 것이다. 그리고 G3에서 same의 경우 마지막 instance에서 forest가 change이지만, yes가 나오는 결과로 인해 forest라는 feature는 y에 대해 관련이 없다라고 업데이트가 되야 하므로, 후보에서 제외가 된다. 이 때, 만약 새로운 instance가 등장했고, 한가지의 instance인 세번째 값이 Specific boundary에는 만족하지 않으나, General boundary에는 만족하는 값이라면, 즉 VS안의 수많은 가설 중 하나라면 판단하기 어려우므로 이러한 rule based 방식은 적합하지 않다. perfect world에서는 rule based가 정확하지만, real world에서는 정확하지 않다.  2-2. Desicion Treeperfect world에서는 정확하나 real world에서는 불안정한 이유는 바로 노이즈 때문이다. real world에서는 노이즈는 항상 존재하는데, 이러한 노이즈를 제거하기 위해 수많은 연구들이 진행되어 왔다. 그 중 하나가 decision tree이다.sky가 sunny라고 하면, temp를 고려하여 warm일 때만 다음을 고려하고, 그렇게 feature들을 모두 고려하는 방식이 desicion tree이다. 조금 더 구체적인 예시를 위해 흔히 사용되는 uci dataset을 사용해보고자 한다. uci 데이터셋 중에서 credit approval, 신용평가에 대한 데이터셋을 사용할 것이다. 총 690개의 instance가 존재하고, 307개는 positive, 나머지는 negative로 구성되어 있다. 그리고 신용평가에 사용되는 feature들은 15개가 존재한다. 여기서 A1에 대해서만 판단하여 a를 항상 negative, b는 항상 positive를 준다고 판단했을 때, 실제 결과는 a에 대해 112개는 negative, 98개는 positive이고, b에 대해서는 206개는 positive, 262개는 negative로 구성되게 된다. 그렇다면 a의 98개와 b의 262개는 틀린 값이 된다. 이 때, ? 는 don’t care가 아니라, 데이터가 존재하지 않는 instance에 대한 값이다. A9에 대해 t를 positive, f에 대해 negative를 주게 된다면, A1보다는 적게 틀리게 되겠지만 에러는 존재하게 된다.  2-3. Entropy and Information GainEntropy불확실성(uncertainty), 즉 에러를 줄이기 위해 어떤 속성이 좋은지를 측정하기 위해 Entropy를 사용한다. entropy, H는 다음과 같이 구할 수 있다. 이 때, entropy가 높을수록 불확실성도 높아진다.\\[H(X) = -\\sum_X P(X = x)log_bP(X = x)\\]  Conditional Entropy조건부를 적용한 상황에서 entropy도 구할 수 있다. 우리는 특정 feature가 주어졌을 때의 entropy를 구하고 싶어 한다. 그래서 X가 주어졌을 때, Y에 대한 entropy는 다음과 같다.\\[H(Y|X) = \\sum_X P(X = x)H(Y|X = x) = \\sum_X P(X = x)(\\sum_Y P(Y = y | X = x)log_bP(Y = y | X = x))\\] Information Gaininformation gain은 또 다른 측정기법이다. 우리는 A1 또는 A9에 대한 entropy를 구할 수도 있지만, A1에서 a,b,? 각각의 클래스에 대한 entropy도 구할 수 있을 것이다.+,- 각각에 대한 확률을 구하여 A1에 대한 entropy를 구할 수 있다.\\[P(Y = +) = \\cfrac{307}{307 + 383}, \\: P(Y = -) = \\cfrac{383}{307 + 383}\\] 그렇다면, A1 또는 A9이라는 조건을 주고 conditional entropy를 측정한다면 H(Y|A1) 과 같은 형태를 띌 것이다. 여기서 우리가 구해봐야 할 것은 단지 conditional entropy가 아니라, 이를 활용하여 원래의 entropy인 H(Y)에 비해 불확실성이 얼마나 줄어드는지를 측정해야 한다. 그래서 이러한 값이 information gain이라 하는 값이 되고, 이는 원래의 값에서 conditional entropy를 빼주면 된다.\\[IG(Y, A_i) = H(Y) - H(Y|A_i)\\]당연히 IG가 클수록 불확실성이 그만큼 줄어들었다는 것이므로 더 좋은 지표가 될 것이다. 이렇게 각 클래스별로 information gain을 가지고, 최적의 루트를 정할 수 있다. 이렇게 decision tree를 만드는 방식에는 여러 가지가 있는데, 대표적으로 ID3 algorithm이 있다. 먼저 하나의 클래스를 정해서 오픈 노드를 생성한다. 이에 대해 최적의 루트를 판단하기 위해 위에서 배운 Information gain을 활용하여 어느 방향이 더 좋은지 확인한다.  여기서 tree가 깊을수록 좋을 것이라 생각할 수 있지만, 현실은 그렇지 않다. 왜냐하면 이러한 tree는 주어진 데이터셋에 대해 너무 많이 최적화되기 떄문이다. 이를 overfitting이라 하고, 그래서 최적의 tree 깊이를 잘 정해줘야 한다.  2-4. Linear Regression지금까지는 Rule 기반의 모델을 구축했지만, 이제는 조금 더 통계 기반의 모델을 구축해보고자 한다. 머신러닝이란 더 나은 함수를 추정하는 것인데, 그 중 Linear Regression은 선형으로 decision boundary를 추정해보는 것이다.rule 기반의 모델에서는 뚜렷한 구별 형태로 가설을 세웠지만, 이제는 함수 형태로 가설해볼 것이다. 지금은 선형의 모델을 구성할 예정이므로, 가중치(weight)인 theta를 잘 정의하여 최적화한다.가설 h는 $ \\hat{f}(x;\\theta) = \\theta_0 + \\sum_{i=1}^n \\theta_i x_i = \\sum_{i=0}^n \\theta_i x_i $ 로 정의할 수 있다. 이때, n은 feature의 개수이다. 이러한 h를 matrix 형태로 변환해보면 다음과 같다.따라서 $ \\hat{f} = X\\theta $ 가 된다. 그리고, real world에서는 노이즈가 항상 존재하므로, error term인 e를 추가하여 정답의 f를 정의한다. 그리고 우리는 이 error term을 최소인 가설이 최적의 가설이라 판단할 수 있다. 그러므로 e가 최소가 되는 theta를 구한다.\\[f = X\\theta + e = Y\\]\\[\\hat{\\theta} = argmin_{\\theta} (f-\\hat{f})^2 = argmin_{\\theta} (Y - X\\theta)^2\\]이 때, 이 식을 행렬의 특성을 활용하여 다시 나타내면 다음과 같다.\\[argmin_{\\theta} (Y - X\\theta)^T (Y-X\\theta) = argmin_{\\theta} (Y-X\\theta)^T (Y-X\\theta) = argmin_{\\theta} (\\theta^T X^T X\\theta - 2\\theta^TX^TY + Y^TY) = argmin_{\\theta} (\\theta^TX^TX\\theta - 2 \\theta^T X^T Y)\\]우리가 최소화시키고자 하는 값은 theta이므로 theta와 관련이 없는 Y^T Y 는 제거할 수 있다. MLE를 구할 때 사용했던 테크닉을 그대로 사용하여 argmin 안에 있는 값을 미분하여 최소가 되는 theta를 구한다.\\[\\theta = (X^TX)^{-1} X^TY\\] 이를 통해 값을 추정할 수 있다. 빨간색 점이 GT 즉 정답인 Y값이고, 파란색 점들이 우리가 선형으로 추정한 Y^ 이다. 선형으로 추정을 하니 생각보다 잘 맞지 않는 모습을 보이고 있다. 따라서 이러한 linear 특성을 수정하여 비선형 가설을 세우고자 한다.원래의 식에서 x value에 제곱, 세제곱 네제곱… 을 취하여 새로운 variable을 생성한다.이를 통해 새로운 곡선을 만들었는데, 뒤에 오는 값들이 거의 희박하게 존재하는데도 이들을 위해 더 깊게 만들어야 하는지에 대한 의문이 든다. 따라서 최적의 깊이를 정하는 것이 또 다시 중요해진다." }, { "title": "[KOOC] 인공지능 및 기계학습 개론 - Abstract ", "url": "/posts/kooc_start/", "categories": "Classlog, kooc", "tags": "kooc", "date": "2022-09-27 00:40:00 +0900", "snippet": "KOOC란?KOOC(KAIST Massive Open Online Course)는 KAIST 무료 온라인 강좌를 말하는 것으로, KAIST 교수님들의 강의를 무료로 수강할 수 있는 플랫폼입니다. KOOC에는 다양한 주제로 강좌가 열려 있고, 온라인으로 진행되는 강좌이므로 언제 어디서든 강의를 들을 수 있습니다. 저는 그 중, 인공지능을 전공할 때 필수적인 개념들이 담겨져 있는, AI 전공자에게는 필수코스인 문인철 교수님의 인공지능 및 기계학습 개론(https://kooc.kaist.ac.kr/machinelearning1_17/joinLectures/9738)을 들어보려고 합니다.강좌는 각 주차별 약 2시간정도의 분량이 총 6주차로 구성되어 있습니다. 내용은 MLE, MAP, Rule Based Algorithm, Naive Bayes Classifier, Decision Tree와 같이 머신러닝의 기초를 먼저 배우고, Support Vector Machine부터 딥러닝에 자주 등장하는 Over-fitting, Cross Validation 등도 함께 배웁니다.인공지능을 공부하는 분들에게는 꼭 추천드리는 강좌이므로, 한번씩은 수강해보시길 추천드립니다." }, { "title": " [ROS2 프로그래밍] ROS 패키지 설계 및 토픽 뜯어보기 ", "url": "/posts/package/", "categories": "Classlog, ROS2", "tags": "ROS2", "date": "2022-07-27 01:23:00 +0900", "snippet": "ROS 패키지 제작ROS 패키지 설계목적별로 나누어 노드 단위의 프로그램을 작성하고, 노드와 노드간의 데이터 통신을 설계해야 한다. 이번 예제 패키지는 토픽, 서비스, 액션으로 구성하고자 한다.총 4개의 노드로 구성되며, 각 노드에서는 1개 이상의 토픽 퍼블리셔, 토픽 서브스크라이브, 서비스 서버, 서비스 클라이언트, 액션 서버, 액션 클라이언트가 존재한다. 중앙에 있는 노드는 다른 노드들과의 연동을 해야 하므로 가장 핵심적인 역할이다.각각의 노드, 토픽, 서비스, 액션은 고유의 이름을 가지고 있다. argument : arithmetic_argument 토픽 이름으로 현재 시간과 변수 a,b를 퍼블리시한다. operator : arithmetic_operator 서비스 이름으로 calculator 노드에게 연산자(+-*/)를 서비스 요청값으로 보낸다. calculator 토픽이 생성된 시간과 변수 a,b를 arithmetic_argument 이름의 토픽을 서브스크라이브한다. 받은 변수 a,b와 operator 노드로부터 요청값으로 받은 연산자를 통해 계산하고(a 연산자 b), operator 노드에게 결괏값을 arithmetic_operator 이름으로 서비스 응답값을 보낸다. checker 노드로부터 액션 목표값(① action goal)을 받은 후부터 저장된 변수(a,b,연산자)를 가지고 연산한 값을 합한다. 그리고 연산이 끝난 계산식을 arithmetic_checker 이름으로 액션 피드백(② action feedback)을 checker 노드로 보낸다. 연산값의 합이 액션 목표값을 넘기면 최종 연산 합계를 arithmetic_checker 이름으로 액션 결괏값(③ action result)을 checker 노드로 보낸다. checker : 연산값의 합계의 한계치를 arithmetic_checker 액션 이름으로 액션 목표값으로 전달한다.  ROS2 CPP 패키지 제작 034 ROS2 패키지 설계 (C++) 035 토픽 프로그래밍 (C++)ROS2 패키지 설정🔥 주의 : 아래 파일들은 코드를 설명하기 위해서만 보고, 추후 git clone을 수행할 예정이므로 패키지를 직접 생성하지 않기를 바란다. 만약 패키지를 동일한 이름으로 생성할 경우 빌드에 문제가 생기므로, 다른 이름으로 생성하여 커스터마이징하거나, clone한 것을 사용하길 바란다.  package.xml&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;?xml-model href=&quot;http://download.ros.org/schema/package_format3.xsd&quot; schematypens=&quot;http://www.w3.org/2001/XMLSchema&quot;?&amp;gt;&amp;lt;!-- ROS2 패키지 설정파일의 package format이 세번째 버전이므로 3 --&amp;gt;&amp;lt;package format=&quot;3&quot;&amp;gt; &amp;lt;!-- 패키지 이름 --&amp;gt; &amp;lt;name&amp;gt;topic_service_action_rclcpp_example&amp;lt;/name&amp;gt; &amp;lt;version&amp;gt;0.2.0&amp;lt;/version&amp;gt; &amp;lt;!-- 설명 --&amp;gt; &amp;lt;description&amp;gt;ROS 2 rclcpp example package for the topic, service, action&amp;lt;/description&amp;gt; &amp;lt;maintainer email=&quot;jhyoon@todo.todo&quot;&amp;gt;jhyoon&amp;lt;/maintainer&amp;gt; &amp;lt;!-- 라이선스 --&amp;gt; &amp;lt;license&amp;gt;Apache License 2.0&amp;lt;/license&amp;gt; &amp;lt;author email=&quot;passionvirus@gmail.com&quot;&amp;gt;Pyo&amp;lt;/author&amp;gt; &amp;lt;author email=&quot;routiful@gmail.com&quot;&amp;gt;Darby Lim&amp;lt;/author&amp;gt; &amp;lt;!-- 빌드툴 : ament_cmake --&amp;gt; &amp;lt;buildtool_depend&amp;gt;ament_cmake&amp;lt;/buildtool_depend&amp;gt; &amp;lt;!-- dependency --&amp;gt; &amp;lt;!-- ros2에서 사용하는 cpp툴 --&amp;gt; &amp;lt;depend&amp;gt;rclcpp&amp;lt;/depend&amp;gt; &amp;lt;!-- cpp툴에서 action 사용하기 위함 --&amp;gt; &amp;lt;depend&amp;gt;rclcpp_action&amp;lt;/depend&amp;gt; &amp;lt;!-- std 패키지 사용 --&amp;gt; &amp;lt;depend&amp;gt;std_msgs&amp;lt;/depend&amp;gt; &amp;lt;!-- 토픽,서비스,액션 인터페이스 사용하는 패키지 --&amp;gt; &amp;lt;depend&amp;gt;msg_srv_action_interface_example&amp;lt;/depend&amp;gt; &amp;lt;!-- 사용하고자 하는 Lint 패키지, 테스트 코드를 위한 의존성 패키지 --&amp;gt; &amp;lt;test_depend&amp;gt;ament_lint_auto&amp;lt;/test_depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_lint_common&amp;lt;/test_depend&amp;gt; &amp;lt;export&amp;gt; &amp;lt;!-- build type : ament_cmake --&amp;gt; &amp;lt;build_type&amp;gt;ament_cmake&amp;lt;/build_type&amp;gt; &amp;lt;/export&amp;gt;&amp;lt;/package&amp;gt;  CMakeLists.txtCMakelist에는 크게 cmake 설정, 의존성 명시, 빌드, 설치, 테스트, ament package 매크로 설정으로 구성되어 있다.# Set minimum required version of cmake, project name and compile optionscmake_minimum_required(VERSION 3.5)# project nameproject(topic_service_action_rclcpp_example)# C언어의 버전을 명시하지 않으면 C99를 기본으로 사용if(NOT CMAKE_C_STANDARD) set(CMAKE_C_STANDARD 99)endif()# C++의 버전을 명시하지 않으면 C++ 14를 기본으로 사용if(NOT CMAKE_CXX_STANDARD) set(CMAKE_CXX_STANDARD 14)endif()# GNU를 기본으로 사용하지만, Clang 컴파일러를 사용할 수도 있다.if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES &quot;Clang&quot;) add_compile_options(-Wall -Wextra -Wpedantic)endif()# Find dependencies# REQUIRED : 패키지를 찾고, 없으면 에러가 나도록 하는 옵션find_package(ament_cmake REQUIRED)find_package(rclcpp REQUIRED)find_package(msg_srv_action_interface_example REQUIRED)find_package(rclcpp_action REQUIRED)# header 파일을 가져오기 위한 디렉토리include_directories(include)# Build# 실제로 실행하는 파일 (node_name file_directory)add_executable(argument src/arithmetic/argument.cpp)# 프로그램 실행을 위해 필요한 의존성 패키지ament_target_dependencies(argument msg_srv_action_interface_example rclcpp)add_executable(calculator src/calculator/main.cpp src/calculator/calculator.cpp)# calculator에서는 action도 진행되기에 의존성 패키지에 추가ament_target_dependencies(calculator msg_srv_action_interface_example rclcpp rclcpp_action)add_executable(checker src/checker/main.cpp src/checker/checker.cpp)ament_target_dependencies(checker msg_srv_action_interface_example rclcpp rclcpp_action)add_executable(operator src/arithmetic/operator.cpp)ament_target_dependencies(operator msg_srv_action_interface_example rclcpp)# Installinstall(TARGETS argument calculator checker operator DESTINATION lib/${PROJECT_NAME})# 여기서 볼 수 있듯 launch 와 param은 share 폴더 아래에 저장해야 한다.install(DIRECTORY launch param DESTINATION share/${PROJECT_NAME})# Test# test시 필요한 의존성 패키지들, 추후 test시 colcon test 를 통해 사용 가능# ament_lint_auto_find_test_dependenices를 통해 cpplint라는 코드 스타일을 점검해주는 패키지를 사용if(BUILD_TESTING) find_package(ament_lint_auto REQUIRED) ament_lint_auto_find_test_dependencies()endif()# Macro for ament package# ament 패키지를 위한 매크로 함수를 적어주어야 함ament_package()  코드 다운로드 및 빌드source ~/robot_ws/install/local_setup.bash는 ~/.bashrc에 없으면 추가하고, 있으면 추가하지 않아도 된다.$ cd ~/robot_ws/src$ git clone https://github.com/robotpilot/ros2-seminar-examples.git$ cd ~/robot_ws &amp;amp;&amp;amp; colcon build --symlink-installStarting &amp;gt;&amp;gt;&amp;gt; msg_srv_action_interface_exampleStarting &amp;gt;&amp;gt;&amp;gt; logging_rclpy_exampleFinished &amp;lt;&amp;lt;&amp;lt; logging_rclpy_example [1.26s]Starting &amp;gt;&amp;gt;&amp;gt; my_first_ros_rclcpp_pkgFinished &amp;lt;&amp;lt;&amp;lt; my_first_ros_rclcpp_pkg [13.8s]Starting &amp;gt;&amp;gt;&amp;gt; my_first_ros_rclpy_pkg...$ echo &#39;source ~/robot_ws/install/local_setup.bash&#39; &amp;gt;&amp;gt; ~/.bashrc$ source ~/.bashrc$ . ~/robot_ws/install/local_setup.bash. ~/robot_ws/install/local_setup.bash 를 하지 않으면 패키지를 찾지 못한다는 에러가 뜨니 주의하자. 빌드가 끝나고, 문제가 없다면 ~/robot_ws/install/topic_service_action_rclcpp_example 폴더 안에 우리가 작성한 ROS 인터페이스를 사용하기 위한 파일들이 저장될 것이다.$ ls ~/robot_ws/installCOLCON_IGNORE logging_rclpy_example rqt_example tf2_rclpy_examplelocal_setup.bash msg_srv_action_interface_example setup.bash time_rclcpp_examplelocal_setup.ps1 my_first_ros_rclcpp_pkg setup.ps1 time_rclpy_examplelocal_setup.sh my_first_ros_rclpy_pkg setup.sh topic_service_action_rclcpp_example_local_setup_util_ps1.py rclcpp_tutorial setup.zsh topic_service_action_rclpy_example_local_setup_util_sh.py rclpy_tutorial testbot_descriptionlocal_setup.zsh ros2env tf2_rclcpp_example$ ls ~/robot_ws/install/topic_service_action_rclcpp_example/lib/topic_service_action_rclcpp_example/argument calculator checker operator$ ls ~/robot_ws/install/topic_service_action_rclcpp_example/share/topic_service_action_rclcpp_example/cmake/ launch/ local_setup.sh package.dsv package.xmlenvironment/ local_setup.bash local_setup.zsh package.ps1 package.zshhook/ local_setup.dsv package.bash package.sh param/이와 같이 argument, operator, calculator, checker와 같은 실행 스크립트가 생성되었다. 그리고 share 폴더 안에는 launch폴더와 param 폴더가 생성되었고, 각 폴더 안에는 arithmetic.launch.py와 arithmetic_config.yaml가 위치한다.$ ls ~/robot_ws/install/topic_service_action_rclcpp_example/share/topic_service_action_rclcpp_example/launch/arithmetic.launch.py$ ls ~/robot_ws/install/topic_service_action_rclcpp_example/share/topic_service_action_rclcpp_example/param/arithmetic_config.yaml  간단한 노드 실행 calculator 노드 실행이 노드는 토픽 서브스크라이버, 서비스 서버, 액션 서버 역할을 수행한다.$ ros2 run topic_service_action_rclcpp_example calculator[INFO]: Run calculator실행했지만, 아직 받는 토픽이나 서비스, 액션이 없으므로 동작이 되지 않는다.  argument 노드 실행이 노드는 토픽 퍼블리셔 역할을 한다. 이를 실행하면 퍼블리시하고 있는 argument a, argument b가 표시될 것이고, 이를 통해 calculator 노드에서는 argument a, argument b와 함께 수신 받은 시간 정보가 출력될 것이다.$ ros2 run topic_service_action_rclcpp_example argument[INFO]: Published argument_a 4.73[INFO]: Published argument_b 7.43[INFO]: Published argument_a 1.55[INFO]: Published argument_b 6.38[INFO]: Published argument_a 7.09[INFO]: Published argument_b 5.89[INFO]: Published argument_a 5.90[INFO]: Published argument_b 8.21ros2 run topic_service_action_rclcpp_example calculator[INFO]: Run calculator[INFO]: Timestamp of the message: sec 1658859718 nanosec 500974826[INFO]: Subscribed argument a: 4.73[INFO]: Subscribed argument b: 7.43[INFO]: Timestamp of the message: sec 1658859719 nanosec 500985640[INFO]: Subscribed argument a: 1.55[INFO]: Subscribed argument b: 6.38[INFO]: Timestamp of the message: sec 1658859720 nanosec 501005719[INFO]: Subscribed argument a: 7.09[INFO]: Subscribed argument b: 5.89[INFO]: Timestamp of the message: sec 1658859721 nanosec 500910222[INFO]: Subscribed argument a: 5.90[INFO]: Subscribed argument b: 8.21[INFO]: Timestamp of the message: sec 1658859722 nanosec 501017918[INFO]: Subscribed argument a: 2.57[INFO]: Subscribed argument b: 8.50[INFO]: Timestamp of the message: sec 1658859723 nanosec 500904549[INFO]: Subscribed argument a: 4.43[INFO]: Subscribed argument b: 4.86  operator 노드 실행이 노드는 서비스 클라이언트 역할을 한다. calculator 노드에게 랜덤으로 선택한 연산자를 서비스 요청값으로 보내고, 연산된 결과값을 받아 터미널 창에 표시한다. 실제 계산식은 calculator 노드가 실행 중인 창에서 확인할 수 있다.$ ros2 run topic_service_action_rclcpp_example operator[INFO]: Result 13.79Press Enter for next service call.[INFO]: Result 9.57Press Enter for next service call.[INFO]: Result 0.30Press Enter for next service call.[INFO]: Result 0.71Press Enter for next service call.$ ros2 run topic_service_action_rclcpp_example calculator[INFO]: 2.224688 + 7.350139 = 9.57483[INFO]: 2.224688 / 7.350139 = 0.302673[INFO]: Timestamp of the message: sec 1658859884 nanosec 500955814[INFO]: Subscribed argument a: 5.25[INFO]: Subscribed argument b: 7.38[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644Entor 키를 통해 서비스를 보낼 수 있다.  checker 노드 실행마지막으로 이 노드는 먼저 연산값의 합계 한계치를 액션 목표값으로 calculator 노드에 전달한다. 이후 checker 노드는 calculator 노드에게 액션 피드백을 받는데, 그 피드백은 각 연산과 그 결과의 string타입이다. 지정한 연산값의 합계가 목표 합계를 넘기면 checker 노드는 액션 결과값으로 calculator 노드로부터 최종 연산 합계를 전달받는다.$ ros2 run topic_service_action_rclcpp_example checkergoal_total_sum : 50[INFO]: Action goal accepted.[INFO]: Action feedback:[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: Action feedback:[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: Action feedback:[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: Action feedback:[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644$ ros2 run topic_service_action_rclcpp_example calculator[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644[INFO]: 5.247090 / 7.383574 = 0.710644동일한 logging이 출력되는데, 이는 목표 합계를 넘겼을 때, calculator 노드가 checker 노드에게 최종 연산 합계를 전달하기 때문이다. 합계의 한계치는 기본으로 50으로 설정되어 있다. 이를 수정하고자 한다면 checker 노드를 실행시킬 때 실행 인자로 -g 100 이라 입력하면 GOAL_TOTAL_SUM 이라는 합계 한계치 인자가 100으로 할당된다.ros2 run topic_service_action_rclcpp_example checker -g 100  launch 파일 실행launch 파일을 통해 argument 노드와 calculator 노드를 한번에 실행시킬 수 있다. launch파일은 arithmetic.launch.py 파일이다.ros2 launch topic_service_action_rclcpp_example arithmetic.launch.py  ROS 토픽 뜯어보기이제는 위의 패키지에서 토픽에 대해 더 자세히 뜯어보고자 한다. 토픽은 비동기식 단방향 메시지 송수신 방식으로 메시지를 발행하는 퍼블리셔와 메시지를 구독하는 서브스크라이버 간의 통신이다. 이는 1:1 통신을 기본으로 하지만, 1:N, N:1, N:N도 가능하다.위에서 토픽은 argument 노드에서 calculator 노드로의 argument a, argument b, 토픽을 생성한 시간에 대한 메시지 전달이다. 따라서 퍼블리셔와 서브스크라이버를 직접 작성해보고자 한다.코드는 소스 파일과 헤더 파일이 존재하고, 각각 src 폴더와 include 폴더에 위치한다.퍼블리셔 topic_service_action_rclcpp_example/src/arithmetic/argument.cpp topic_service_action_rclcpp_example/include/arithmetic/argument.hpp 서브스크라이버 topic_service_action_rclcpp_example/src/arithmetic/argument.cpp topic_service_action_rclcpp_example/include/arithmetic/argument.hpp 퍼블리셔토픽 퍼블리셔 노드는 argument 노드이다. Node 설정 QoS 설정 create_publisher 설정 (timer 설정) 퍼블리시 함수 작성  헤더 파일#ifndef ARITHMETIC__ARGUMENT_HPP_#define ARITHMETIC__ARGUMENT_HPP_// 시간을 다루는 라이브러리#include &amp;lt;chrono&amp;gt;// 동적 메모리를 다루는 라이브러리#include &amp;lt;memory&amp;gt;// 문자열을 다루는 라이브러리#include &amp;lt;string&amp;gt;// 서로다른 도메인을 다루는 라이브러리#include &amp;lt;utility&amp;gt;// rclcpp API를 담고 있는 rclcpp 헤더파일#include &quot;rclcpp/rclcpp.hpp&quot;// 만든 인터페이스를 담고 있는 헤더파일#include &quot;msg_srv_action_interface_example/msg/arithmetic_argument.hpp&quot;// rclcpp의 Node 클래스를 상속받는 Argument 클래스class Argument : public rclcpp::Node{public: using ArithmeticArgument = msg_srv_action_interface_example::msg::ArithmeticArgument; // Argument 클래스의 생성자는 rclcpp의 NodeOptions를 인자로 받는다. NodeOptions에는 context, arguments, intra process communication, parameter, allocator와 같은 Node 생성을 위한 다양한 옵션이 존재 explicit Argument(const rclcpp::NodeOptions &amp;amp; node_options = rclcpp::NodeOptions()); virtual ~Argument();private: void publish_random_arithmetic_arguments(); void update_parameter(); // 토픽 메시지에 담을 랜덤 변수의 범위 float min_random_num_; float max_random_num_; // publisher와 timerbase 멤버변수가 선언되어 있다. rclcpp::Publisher&amp;lt;ArithmeticArgument&amp;gt;::SharedPtr arithmetic_argument_publisher_; rclcpp::TimerBase::SharedPtr timer_; rclcpp::Subscription&amp;lt;rcl_interfaces::msg::ParameterEvent&amp;gt;::SharedPtr parameter_event_sub_; rclcpp::AsyncParametersClient::SharedPtr parameters_client_;};#endif // ARITHMETIC__ARGUMENT_HPP_  소스 파일// C언어 표준 인풋 아웃풋 라이브러리#include &amp;lt;cstdio&amp;gt;#include &amp;lt;memory&amp;gt;#include &amp;lt;string&amp;gt;#include &amp;lt;utility&amp;gt;// 랜덤 숫자 생성 라이브러리#include &amp;lt;random&amp;gt;#include &quot;rclcpp/rclcpp.hpp&quot;/// 프로그램 실행시 넘겨받은 인자를 다루는 ROS2 라이브러리#include &quot;rcutils/cmdline_parser.h&quot;// 위에서 생성한 argument 헤더 파일#include &quot;arithmetic/argument.hpp&quot;using namespace std::chrono_literals;// Argument 클래스, 부모 클래스인 rclcpp:Node를 선언해주었는데, 첫번째 인자에는 노드 이름을, 두번째 인자에는 노드 옵션 변수를 명시한다.Argument::Argument(const rclcpp::NodeOptions &amp;amp; node_options): Node(&quot;argument&quot;, node_options), min_random_num_(0.0), max_random_num_(0.0){ // QoS는 QoS 라이브러리를 이용하여 depth를 10으로 한다. this-&amp;gt;declare_parameter(&quot;qos_depth&quot;, 10); int8_t qos_depth = this-&amp;gt;get_parameter(&quot;qos_depth&quot;).get_value&amp;lt;int8_t&amp;gt;(); // 랜덤값 중 최소값로 0설정 this-&amp;gt;declare_parameter(&quot;min_random_num&quot;, 0.0); min_random_num_ = this-&amp;gt;get_parameter(&quot;min_random_num&quot;).get_value&amp;lt;float&amp;gt;(); // 랜덤값 중 최대값로 9설정 this-&amp;gt;declare_parameter(&quot;max_random_num&quot;, 9.0); max_random_num_ = this-&amp;gt;get_parameter(&quot;max_random_num&quot;).get_value&amp;lt;float&amp;gt;(); this-&amp;gt;update_parameter(); /* - History 옵션: KeepLast(depth : 10) - Reliability 옵션: reliable - Durability 옵션: volatile */ const auto QOS_RKL10V = rclcpp::QoS(rclcpp::KeepLast(qos_depth)).reliable().durability_volatile(); // QoS는 publisher를 초기화할 때 두번째 인자로 들어가고, 첫번째 인자는 메시지 통신에 사용될 토픽명 arithmetic_argument_publisher_ = this-&amp;gt;create_publisher&amp;lt;ArithmeticArgument&amp;gt;(&quot;arithmetic_argument&quot;, QOS_RKL10V); // 1초당 1번씩 publisher_random_arithmetic_arguments 멤버함수가 호출하도록 설정 timer_ = this-&amp;gt;create_wall_timer(1s, std::bind(&amp;amp;Argument::publish_random_arithmetic_arguments, this));}Argument::~Argument(){}// timer에 의해 1초당 1번씩 호출void Argument::publish_random_arithmetic_arguments(){ std::random_device rd; std::mt19937 gen(rd()); // ROS2 파라미터를 통해 얻은 숫자가 min(0) 과 max(9) 사이의 랜덤한 값으로 숫자를 생성 std::uniform_real_distribution&amp;lt;float&amp;gt; distribution(min_random_num_, max_random_num_); // msg_srv_action_interface_example 패키지에 있는 msg 인터페이스를 선언 msg_srv_action_interface_example::msg::ArithmeticArgument msg; // time-stamp msg.stamp = this-&amp;gt;now(); // argument a msg.argument_a = distribution(gen); // argument b msg.argument_b = distribution(gen); // 토픽 발행 arithmetic_argument_publisher_-&amp;gt;publish(msg); // logging RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Published argument_a %.2f&quot;, msg.argument_a); RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Published argument_b %.2f&quot;, msg.argument_b);}void Argument::update_parameter(){ parameters_client_ = std::make_shared&amp;lt;rclcpp::AsyncParametersClient&amp;gt;(this); while (!parameters_client_-&amp;gt;wait_for_service(1s)) { if (!rclcpp::ok()) { RCLCPP_ERROR(this-&amp;gt;get_logger(), &quot;Interrupted while waiting for the service. Exiting.&quot;); return; } RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;service not available, waiting again...&quot;); } auto param_event_callback = [this](const rcl_interfaces::msg::ParameterEvent::SharedPtr event) -&amp;gt; void { for (auto &amp;amp; changed_parameter : event-&amp;gt;changed_parameters) { if (changed_parameter.name == &quot;min_random_num&quot;) { auto value = rclcpp::Parameter::from_parameter_msg(changed_parameter).as_double(); min_random_num_ = value; } else if (changed_parameter.name == &quot;max_random_num&quot;) { auto value = rclcpp::Parameter::from_parameter_msg(changed_parameter).as_double(); max_random_num_ = value; } } }; parameter_event_sub_ = parameters_client_-&amp;gt;on_parameter_event(param_event_callback);}// helpvoid print_help(){ printf(&quot;For argument node:\\n&quot;); printf(&quot;node_name [-h]\\n&quot;); printf(&quot;Options:\\n&quot;); printf(&quot;\\t-h Help : Print this help function.\\n&quot;);}int main(int argc, char * argv[]){ // cmdline_parser 라이브러리 함수를 사용하여, 만약 명령 인자에 -h 가 있을 경우 print_help 함수를 호출하고 종료 if (rcutils_cli_option_exist(argv, argv + argc, &quot;-h&quot;)) { print_help(); return 0; } // 초기화 rclcpp::init(argc, argv); // Argument 클래스를 인스턴스화 auto argument = std::make_shared&amp;lt;Argument&amp;gt;(); // publish_random_arithmetic_arguments 실행 rclcpp::spin(argument); // ctrl+c와 같은 시그널을 통해 노드 종료를 종료 rclcpp::shutdown(); return 0;}파라미터와 관련된 부분은 추후 다루도록 한다. 서브스크라이버토픽 서브스크라이버 노드는 calculator 노드이다. calculator의 소스 코드는 토픽 서브스크라이버, 서비스 서버, 액션 서버를 모두 포함하고 있어서 매우 길어서 토픽 서브스크라이버와 관련된 부분만 살펴보도록 한다.calculator 클래스는 토픽 퍼블리셔 노드와 마찬가지로 rclcpp::node를 상속하고, 생성자에서 calculator라는 노드 이름으로 초기화된다. 그리고 위에서와 동일하게 QoS를 생성한다.서브스크라이버는 create_subscription을 통해 초기화되고, 해당 함수는 토픽명과 QoS, 콜백함수를 인자로 받는다. 첫번째, 두번째 인자는 Argument 클래스와 동일하게 넣고, 콜백함수는 std::bind가 아닌 람다 표현식을 사용했다. 콜백함수를 보면 인자를 통해 수신받은 메시지에 접근하여 멤버 변수에 저장한다.  Node 설정 QoS 설정 create_subscription 설정 서브스크라이브 함수 작성  소스파일 // QoS 설정 - history : KeepLast, Reliability : reliable, Durability : volatile const auto QOS_RKL10V = rclcpp::QoS(rclcpp::KeepLast(qos_depth)).reliable().durability_volatile(); // 서브스크라이버 초기화, [토픽명, QoS] arithmetic_argument_subscriber_ = this-&amp;gt;create_subscription&amp;lt;ArithmeticArgument&amp;gt;( &quot;arithmetic_argument&quot;, QOS_RKL10V, [this](const ArithmeticArgument::SharedPtr msg) -&amp;gt; void { argument_a_ = msg-&amp;gt;argument_a; argument_b_ = msg-&amp;gt;argument_b; // logging - timestamp RCLCPP_INFO( this-&amp;gt;get_logger(), &quot;Subscribed at: sec %ld nanosec %ld&quot;, msg-&amp;gt;stamp.sec, msg-&amp;gt;stamp.nanosec); // logging - argument a, argument b RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Subscribed argument a : %.2f&quot;, argument_a_); RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Subscribed argument b : %.2f&quot;, argument_b_); } );  실행실행하기 전에 CMakeLists.txt를 확인해서 add_executable 태그에 알맞게 들어가있는지 확인한다.add_executable(argument src/arithmetic/argument.cpp)add_executable(calculator src/calculator/main.cpp src/calculator/calculator.cpp)첫번째 인자는 실행명, 그 다음 인자부터는 실행할 소스파일이다. 실행은 run과 launch 두가지 방식이 있다.$ ros2 run topic_service_action_rclcpp_example calculator$ ros2 run topic_service_action_rclcpp_example argument $ ros2 launch topic_service_action_rclcpp_example arithmetic.launch.py 두 방법 모두 동작하는 것은 동일하다.   ROS2 python 패키지 제작 028 ROS2 패키지 설계 (python) 029 토픽 프로그래밍 (python)ROS2 패키지 설정 package.xml (패키지 설정 파일)topic_service_action_rclpy_example 패키지의 설정 파일은 다음과 같다.&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;?xml-model href=&quot;http://download.ros.org/schema/package_format3.xsd&quot; schematypens=&quot;http://www.w3.org/2001/XMLSchema&quot;?&amp;gt;&amp;lt;package format=&quot;3&quot;&amp;gt; &amp;lt;name&amp;gt;topic_service_action_rclpy_example&amp;lt;/name&amp;gt; &amp;lt;version&amp;gt;0.2.0&amp;lt;/version&amp;gt; &amp;lt;description&amp;gt;ROS 2 rclpy example package for the topic, service, action&amp;lt;/description&amp;gt; &amp;lt;maintainer email=&quot;jhyoon@gmail.com&quot;&amp;gt;Pyo&amp;lt;/maintainer&amp;gt; &amp;lt;license&amp;gt;Apache License 2.0&amp;lt;/license&amp;gt; &amp;lt;author email=&quot;passionvirus@gmail.com&quot;&amp;gt;Pyo&amp;lt;/author&amp;gt; &amp;lt;author email=&quot;routiful@gmail.com&quot;&amp;gt;Darby Lim&amp;lt;/author&amp;gt; &amp;lt;!-- ros2에서 사용되는 python툴 --&amp;gt; &amp;lt;depend&amp;gt;rclpy&amp;lt;/depend&amp;gt; &amp;lt;depend&amp;gt;std_msgs&amp;lt;/depend&amp;gt; &amp;lt;!-- 토픽, 서비스, 액션 인터페이스를 사용하기 위한 의존성 패키지 --&amp;gt; &amp;lt;depend&amp;gt;msg_srv_action_interface_example&amp;lt;/depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_copyright&amp;lt;/test_depend&amp;gt; &amp;lt;!-- python과 연동시키기 간편한 flake를 사용하기 위한 test 툴 --&amp;gt; &amp;lt;test_depend&amp;gt;ament_flake8&amp;lt;/test_depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_pep257&amp;lt;/test_depend&amp;gt; &amp;lt;!-- python에 사용될 test툴 --&amp;gt; &amp;lt;test_depend&amp;gt;python3-pytest&amp;lt;/test_depend&amp;gt; &amp;lt;export&amp;gt; &amp;lt;build_type&amp;gt;ament_python&amp;lt;/build_type&amp;gt; &amp;lt;/export&amp;gt;&amp;lt;/package&amp;gt;  setup.py (파이썬 패키지 설정 파일)#!/usr/bin/env python3import globimport osfrom setuptools import find_packagesfrom setuptools import setuppackage_name = &#39;topic_service_action_rclpy_example&#39;share_dir = &#39;share/&#39; + package_namesetup( name=package_name, version=&#39;0.2.0&#39;, packages=find_packages(exclude=[&#39;test&#39;]), data_files=[ (&#39;share/ament_index/resource_index/packages&#39;, [&#39;resource/&#39; + package_name]), (share_dir, [&#39;package.xml&#39;]), (share_dir + &#39;/launch&#39;, glob.glob(os.path.join(&#39;launch&#39;, &#39;*.launch.py&#39;))), (share_dir + &#39;/param&#39;, glob.glob(os.path.join(&#39;param&#39;, &#39;*.yaml&#39;))), ], install_requires=[&#39;setuptools&#39;], zip_safe=True, author=&#39;Pyo, Darby Lim&#39;, author_email=&#39;passionvirus@gmail.com, routiful@gmail.com&#39;, maintainer=&#39;Pyo&#39;, maintainer_email=&#39;passionvirus@gmail.com&#39;, keywords=[&#39;ROS&#39;], classifiers=[ &#39;Intended Audience :: Developers&#39;, &#39;License :: OSI Approved :: Apache Software License&#39;, &#39;Programming Language :: Python&#39;, &#39;Topic :: Software Development&#39;, ], description=&#39;ROS 2 rclpy example package for the topic, service, action&#39;, license=&#39;Apache License, Version 2.0&#39;, tests_require=[&#39;pytest&#39;], entry_points={ &#39;console_scripts&#39;: [ &#39;argument = topic_service_action_rclpy_example.arithmetic.argument:main&#39;, &#39;operator = topic_service_action_rclpy_example.arithmetic.operator:main&#39;, &#39;calculator = topic_service_action_rclpy_example.calculator.main:main&#39;, &#39;checker = topic_service_action_rclpy_example.checker.main:main&#39;, ], },)살펴볼 부분은 data_files와 entry_points 이다.data_files 이 패키지에서 사용되는 파일들을 기입하여 함께 배포한다. 주로 resource 폴더 내에 있는 ament_index를 위한 패키지의 이름의 빈 파일이나, package.xml, .launch.py, .yaml 등을 기입한다. 이를 통해 빌드 후 해당 파일들이 설치 폴더에 추가된다. 이 패키지에서는 arithmetic.launch.py 런치 파일과 arithmetic_config.yaml 파라미터 파일이 사용되므로 아래와 같이 해당 파일들의 설정을 추가한다.data_files=[ (&#39;share/ament_index/resource_index/packages&#39;, [&#39;resource/&#39; + package_name]), (share_dir, [&#39;package.xml&#39;]), (share_dir + &#39;/launch&#39;, glob.glob(os.path.join(&#39;launch&#39;, &#39;*.launch.py&#39;))), (share_dir + &#39;/param&#39;, glob.glob(os.path.join(&#39;param&#39;, &#39;*.yaml&#39;))), ], entry_points entry_points는 설치하여 사용할 실행 가능한 콘솔 스크립트 이름과 호출 함수를 기입한다. ros2 run과 같은 노드 실행 명령어를 통해 각 노드를 실행할 것이므로 entry_points에 추가한다.entry_points={ &#39;console_scripts&#39;: [ &#39;argument = topic_service_action_rclpy_example.arithmetic.argument:main&#39;, &#39;operator = topic_service_action_rclpy_example.arithmetic.operator:main&#39;, &#39;calculator = topic_service_action_rclpy_example.calculator.main:main&#39;, &#39;checker = topic_service_action_rclpy_example.checker.main:main&#39;, ], },  코드 다운로드 및 빌드위의 git 클론을 하게 되면, rclcpp 뿐만 아니라 rclpy 패키지도 함께 들어있다.~/robot_ws/install/topic_service_action_rclpy_example 폴더가 존재하는지 확인한다.~/robot_ws/install$ lsCOLCON_IGNORE logging_rclpy_example rqt_example tf2_rclpy_examplelocal_setup.bash msg_srv_action_interface_example setup.bash time_rclcpp_examplelocal_setup.ps1 my_first_ros_rclcpp_pkg setup.ps1 time_rclpy_examplelocal_setup.sh my_first_ros_rclpy_pkg setup.sh topic_service_action_rclcpp_example_local_setup_util_ps1.py rclcpp_tutorial setup.zsh topic_service_action_rclpy_example_local_setup_util_sh.py rclpy_tutorial testbot_descriptionlocal_setup.zsh ros2env tf2_rclcpp_example 간단한 노드 실행 calculator 노드 실행아직 주고 받는 토픽, 서비스, 액션이 없으니 대기 상태이다.$ ros2 run topic_service_action_rclpy_example calculator  argument 노드 실행토픽 퍼블리셔 역할을 하는 노드이다. 실행하고 나면 퍼블리시하고 있는 argument a, argument b가 표시된다. 그리고, calculator 노드를 실행시킨 터미널에서도 토픽을 수신받은 시간에 대한 정보와 argument a, argument b가 표시된다.$ ros2 run topic_service_action_rclpy_example argument[INFO]: Published argument a: 4.0[INFO]: Published argument b: 5.0[INFO]: Published argument a: 1.0[INFO]: Published argument b: 9.0[INFO]: Published argument a: 0.0...$ ros2 run topic_service_action_rclpy_example calculator[INFO]: Timestamp of the message: builtin_interfaces.msg.Time(sec=1659430104, nanosec=525228141)[INFO]: Subscribed argument a: 4.0[INFO]: Subscribed argument b: 5.0[INFO]: Timestamp of the message: builtin_interfaces.msg.Time(sec=1659430105, nanosec=525153321)[INFO]: Subscribed argument a: 1.0[INFO]: Subscribed argument b: 9.0[INFO]: Timestamp of the message: builtin_interfaces.msg.Time(sec=1659430106, nanosec=525057834)[INFO]: Subscribed argument a: 0.0[INFO]: Subscribed argument b: 1.0...  operator 노드 실행서비스 클라이언트 역할을 하는 노드를 실행해보자. 그러면 calculator 노드에게 랜덤으로 선택한 연산자를 서비스 요청값으로 보내고, 연산된 결과값을 반환받아 터미널에 표시한다. 실제 계산식은 calculator 노드가 실행 중인 창에서 확인할 수 있다.$ ros2 run topic_service_action_rclpy_example operator[INFO]: Result: 3.0Press Enter for next service call.[INFO]: Result: 14.0Press Enter for next service call.[INFO]: Result: 0.6666666865348816Press Enter for next service call....$ ros2 run topic_service_action_rclpy_example calculator[INFO]: Timestamp of the message: builtin_interfaces.msg.Time(sec=1659430234, nanosec=525041106)[INFO]: Subscribed argument a: 6.0[INFO]: Subscribed argument b: 2.0[INFO]: 6.0 / 2.0 = 3.0[INFO]: Timestamp of the message: builtin_interfaces.msg.Time(sec=1659430238, nanosec=525190661)[INFO]: Subscribed argument a: 2.0[INFO]: Subscribed argument b: 7.0[INFO]: 2.0 * 7.0 = 14.0[INFO]: Timestamp of the message: builtin_interfaces.msg.Time(sec=1659430240, nanosec=525128381)[INFO]: Subscribed argument a: 2.0[INFO]: Subscribed argument b: 3.0[INFO]: 2.0 / 3.0 = 0.6666666666666666  checker 노드 실행마지막으로 checker 노드는 연산값의 합계의 한계치를 액션 목표값으로 전달하고, calculator 노드는 이를 받은 후부터의 연산값을 합하여 액션 피드백으로 각 연산 계산식을 보낸다. 지정한 목표 합계를 넘기면 액션 결과값으로 최종 연산 합계를 보낸다.$ ros2 run topic_service_action_rclpy_example checker[INFO]: Action goal accepted.[INFO]: Action feedback: [&#39;9.0 + 8.0 = 17.0&#39;][INFO]: Action feedback: [&#39;9.0 + 8.0 = 17.0&#39;, &#39;9.0 + 8.0 = 17.0&#39;][INFO]: Action feedback: [&#39;9.0 + 8.0 = 17.0&#39;, &#39;9.0 + 8.0 = 17.0&#39;, &#39;9.0 + 8.0 = 17.0&#39;][INFO]: Action succeeded![INFO]: Action result(all formula): [&#39;9.0 + 8.0 = 17.0&#39;, &#39;9.0 + 8.0 = 17.0&#39;, &#39;9.0 + 8.0 = 17.0&#39;][INFO]: Action result(total sum): 51.0합계 한계치는 기본적으로 50으로 지정되어 있고, 이를 수정하려면 checker 노드 실행할 때 -g 인자를 통해 직접 할당할 수 있다.ros2 run topic_service_action_rclpy_example checker -g 100  launch 파일 실행argument 노드와 calculator 노드를 한번에 실행시키고자 한다면 launch파일을 실행하면 된다.ros2 launch topic_service_action_rclpy_example arithmetic.launch.py  ROS2 토픽 뜯어보기동일하게 토픽에 대해 자세하게 뜯어볼 것이다.퍼블리셔 topic_service_action_rclpy_example/topic_service/action_rclpy_example/arithmetic/argument.py 서브스크라이버 topic_service_action_rclpy_example/topic_service/action_rclpy_example/calculator/calculator.py 퍼블리셔토픽 퍼블리셔 역할을 하는 argument 노드이다. Node 설정 QoS 설정 create_publisher 설정 퍼블리시 함수 작성 import randomfrom msg_srv_action_interface_example.msg import ArithmeticArgumentfrom rcl_interfaces.msg import SetParametersResult# ros2에서 사용하는 python 모듈import rclpy# 노드from rclpy.node import Node# 파라미터from rclpy.parameter import Parameter# QoS 패키지from rclpy.qos import QoSDurabilityPolicyfrom rclpy.qos import QoSHistoryPolicyfrom rclpy.qos import QoSProfilefrom rclpy.qos import QoSReliabilityPolicy# rclpy.node 모듈의 Node 클래스를 상속한다.class Argument(Node): def __init__(self): # 생성자 - argument 노드 이름으로 초기화 super().__init__(&#39;argument&#39;) # 파라미터 설정 self.declare_parameter(&#39;qos_depth&#39;, 10) qos_depth = self.get_parameter(&#39;qos_depth&#39;).value self.declare_parameter(&#39;min_random_num&#39;, 0) self.min_random_num = self.get_parameter(&#39;min_random_num&#39;).value self.declare_parameter(&#39;max_random_num&#39;, 9) self.max_random_num = self.get_parameter(&#39;max_random_num&#39;).value self.add_on_set_parameters_callback(self.update_parameter) # QoS 설정 ### RELIABLE : 데이터 수신에 집중, 유실 시 재전송 &amp;lt;-&amp;gt; BEST_EFFORT : 데이터 송신에 집중, 전송 속도 중시 ### KEEP_LAST : 정해진 메시지 큐 사이즈(depth)만큼 데이터를 보관 &amp;lt;-&amp;gt; KEEP_ALL : 모든 데이터를 보관 ### VOLATILE : 서브스크라이버 생성되기 전의 데이터는 삭제 &amp;lt;-&amp;gt; TRANSIENT_LOCAL : 서브스크라이버 생성되기 전의 데이터도 보관 QOS_RKL10V = QoSProfile( reliability=QoSReliabilityPolicy.RELIABLE, history=QoSHistoryPolicy.KEEP_LAST, depth=qos_depth, durability=QoSDurabilityPolicy.VOLATILE) # 퍼블리셔 선언, (토픽 타입, 이름, QoS 설정) self.arithmetic_argument_publisher = self.create_publisher( ArithmeticArgument, &#39;arithmetic_argument&#39;, QOS_RKL10V) # 함수 실행 주기, 1초에 1번 실행하도록 설정 # 이전 설정은 모두 퍼블리시를 위한 설정이고, publish_random_arithmetic_argments가 실제 토픽 발행 부분 self.timer = self.create_timer(1.0, self.publish_random_arithmetic_arguments) # 메시지 타입은 arithmeticArgument, timestamp, 랜덤의 argument a,b 저장 def publish_random_arithmetic_arguments(self): msg = ArithmeticArgument() # timestamp 지정 msg.stamp = self.get_clock().now().to_msg() # 랜덤의 argument a 지정 msg.argument_a = float(random.randint(self.min_random_num, self.max_random_num)) # 랜덤의 argument b 지정 msg.argument_b = float(random.randint(self.min_random_num, self.max_random_num)) # 토픽 발행 self.arithmetic_argument_publisher.publish(msg) # logging self.get_logger().info(&#39;Published argument a: {0}&#39;.format(msg.argument_a)) self.get_logger().info(&#39;Published argument b: {0}&#39;.format(msg.argument_b)) def update_parameter(self, params): for param in params: if param.name == &#39;min_random_num&#39; and param.type_ == Parameter.Type.INTEGER: self.min_random_num = param.value elif param.name == &#39;max_random_num&#39; and param.type_ == Parameter.Type.INTEGER: self.max_random_num = param.value return SetParametersResult(successful=True)def main(args=None): rclpy.init(args=args) try: argument = Argument() # 1초에 1번씩 함수 실행 try: rclpy.spin(argument) # Ctrl+C와 같은 시그널이 들어오면 종료 except KeyboardInterrupt: argument.get_logger().info(&#39;Keyboard Interrupt (SIGINT)&#39;) finally: argument.destroy_node() finally: rclpy.shutdown()if __name__ == &#39;__main__&#39;: main() 서브스크라이버토픽 서브스크라이버 역할을 하는 calculator 노드이다. calculator의 소스 코드는 토픽 서브스크라이버, 서비스 서버, 액션 서버를 모두 포함하고 있어서 매우 길어서 토픽 서브스크라이버와 관련된 부분만 살펴보도록 한다.calculator 클래스는 rclpy.node 모듈의 Node 클래스를 상속하고 있으며, 생성자에서 calculator 라는 노드 이름으로 초기화된다. 그 뒤에는 QoSProfile 클래스를 사용하여 서브스크라이버의 QoS 설정을 해준다.  Node 설정 QoS 설정 create_subscription 설정 서브스크라이브 함수 작성 import rclpyfrom rclpy.executors import MultiThreadedExecutor# Node 클래스를 상속class Calculator(Node): def __init__(self): # calculator 이름으로 초기화 super().__init__(&#39;calculator&#39;) # 변수 초기화 self.argument_a = 0.0 self.argument_b = 0.0 self.callback_group = ReentrantCallbackGroup() (일부 코드 생략) # QoS 설정 QOS_RKL10V = QoSProfile( reliability=QoSReliabilityPolicy.RELIABLE, history=QoSHistoryPolicy.KEEP_LAST, depth=qos_depth, durability=QoSDurabilityPolicy.VOLATILE) # 서브스크라이버 설정 ### 퍼블리셔와 동일하게 (토픽 타입(ArithmeticArgument), 토픽 이름, QoS 설정) + callback함수 ### get_arithmetic_argument : 퍼블리셔로부터 메시지를 서브스크라이브할 때마다 실행되는 함수 ### callback_group = ReentrantCallbackGroup : 콜백함수를 병렬로 실행할 수 있게 해주는 multithread 기능이 있다. 지정해주지 않아도 되는데, 지정하지 않으면 MultuallyExclusiveCallbackGroup이 기본으로 설정된다. 이는 한번에 하나의 콜백함수만을 실행하도록 허용하는 것이고, ReenttrantCallbackGroup은 제한없이 콜백함수를 병렬로 실행시켜준다. callback_group 설정으로는 create_subscription(), create_service(), ActionServer(), create_timer() 에서 사용된다. self.arithmetic_argument_subscriber = self.create_subscription( ArithmeticArgument, &#39;arithmetic_argument&#39;, self.get_arithmetic_argument, QOS_RKL10V, callback_group=self.callback_group) # callback 함수 : ArithmeticArgument 타입의 메시지의 arithmetic_argument라는 토픽을 서브스크라이브 하면 실행된다. def get_arithmetic_argument(self, msg): # 서브스크라이브한 msg의 argument a와 b를 멤버 변수에 저장 self.argument_a = msg.argument_a self.argument_b = msg.argument_b # logging self.get_logger().info(&#39;Subscribed at: {0}&#39;.format(msg.stamp)) self.get_logger().info(&#39;Subscribed argument a: {0}&#39;.format(self.argument_a)) self.get_logger().info(&#39;Subscribed argument b: {0}&#39;.format(self.argument_b))def main(args=None): rclpy.init(args=args) try: calculator = Calculator() # multithreadExecutor가 있는데, 이는 스레드 풀(thread pool)을 사용하여 콜백을 실행하는 것인데, num_threads로 스레드 수를 지정할 수 있고, 지정하지 않을 경우 multiprocessing.cpu_count()를 통해 시스템에서 가용할 수 있는 스레드 수를 지정받는다. 둘다 해당되지 않으면 단일 스레드를 사용한다. excutor는 콜백이 병렬로 발생하도록 허용하여 ReentrantCallbackGroup과 함께 사용하여 콜백함수를 병렬로 실행할 수 있게 한다. executor = MultiThreadedExecutor(num_threads=4) # 위에서 설정한 executor를 calculator 노드에 추가 executor.add_node(calculator) try: executor.spin() except KeyboardInterrupt: calculator.get_logger().info(&#39;Keyboard Interrupt (SIGINT)&#39;) finally: executor.shutdown() calculator.arithmetic_action_server.destroy() calculator.destroy_node() finally: rclpy.shutdown()if __name__ == &#39;__main__&#39;: main() 실행실행하기 전에 setup.py를 다시 한 번 점검하자. entry_points={ &#39;console_scripts&#39;: [ &#39;argument = topic_service_action_rclpy_example.arithmetic.argument:main&#39;, &#39;operator = topic_service_action_rclpy_example.arithmetic.operator:main&#39;, &#39;calculator = topic_service_action_rclpy_example.calculator.main:main&#39;, &#39;checker = topic_service_action_rclpy_example.checker.main:main&#39;, ], },entry_points에 argument.py의 main함수 실행와 calculator.py의 main함수 실행하는 부분이 담겨져 있어야 한다. $ ros2 launch topic_service_action_rclcpp_example arithmetic.launch.py[INFO] [launch]: All log files can be found below /home/jhyoon/.ros/log/2022-08-02-18-33-29-121351-LAPTOP-FCNUC3SV-442[INFO] [launch]: Default logging verbosity is set to INFO[INFO] [argument-1]: process started with pid [444][INFO] [calculator-2]: process started with pid [446][calculator-2] [INFO]: Run calculator[argument-1] [INFO]: Published argument_a 7.62[argument-1] [INFO]: Published argument_b 1.43[calculator-2] [INFO]: Timestamp of the message: sec 1659432810 nanosec 256482232[calculator-2] [INFO]: Subscribed argument a: 7.62[calculator-2] [INFO]: Subscribed argument b: 1.43[argument-1] [INFO]: Published argument_a 8.76[argument-1] [INFO]: Published argument_b 0.60[calculator-2] [INFO]: Timestamp of the message: sec 1659432811 nanosec 256391424[calculator-2] [INFO]: Subscribed argument a: 8.76[calculator-2] [INFO]: Subscribed argument b: 0.60 " }, { "title": "[ROS2 이론] turtlesim 실행", "url": "/posts/turtlesim/", "categories": "Classlog, ROS2", "tags": "ROS2", "date": "2022-07-26 04:23:00 +0900", "snippet": " 개발 환경을 설정하는 것은 해당 블로그를 확인해보길 바란다.   007 패키지 설치와 노드 실행Turtlesim 이란? 추가 예정.   Turtlesim 패키지 설치turtlesim은 ROS의 기본 기능을 가지고 있어 튜토리얼로 많이 사용된다.sudo apt install ros-foxy-turtlesimturtlesim 패키지에 포함된 노드가 어떤 것들이 있는지 확인해보자.$ ros2 pkg executables turtlesimturtlesim draw_squareturtlesim mimicturtlesim turtle_teleop_keyturtlesim turtlesim_node draw_square : 사각형 모양으로 turtle을 움직이게 하는 노드 mimic : 유저가 지정한 토픽으로 동일 움직임의 turtlesim_node를 복수개 실행시킬 수 있는 노드 turtle_teleop_key : turtlesim_node를 움직이게 하는 속도 값을 퍼블리시하는 노드 turtlesim_node : turtle_teleop_key로부터 속도 값을 토픽으로 받아 움직이게 하는 간단 2D 시뮬레이터 노드 Turtlesim 패키지 노드 실행turtlesim_node 노드와 turtle_teleop_key 노드를 실행시키면 파란색 창에 거북이가 보인다. turtle_teleop_key 노드를 실행시키면 화살표키로 turtlesim_node 노드의 거북이를 움직일 수 있게 된다.ros2 run turtlesim turtlesim_noderos2 run turtlesim turtle_teleop_key  키보드로 움직일 수도 있는데, 눌려진 키보드의 키 값이 어떻게 메시지로 전달되는지 확인해봐야 한다. 키 값은 linear velocity와 angular velocity가 포함된 geometry_msgs 패키지의 Twist 메시지 형태로 보내지고 있다. 그렇기에 직접 터미널로 토픽을 발행하여 조종해보자.그 전에 노드, 토픽, 서비스, 액션에 대해 알고 있어야 한다.  008 ROS2 노드와 데이터 통신노드와 메시지 통신은 앞서 글에서 배웠기 때문에, 간단히 설명했다. 노드(node) 최소 단위의 실행 가능한 프로세스 각 노드는 서로 유기적으로 message로 연결되어 사용된다. 메시지 통신(message communication) 노드와 노드 사이에 입력과 출력 데이터를 서로 주고받게 설계해야 한다. 주고 받는 데이터를 메시지(message)라 한다. 주고받는 방식을 메시지 통신(message communication)이라 한다. 주고받는 통신 방법에 따라 토픽, 서비스, 액션, 파라미터로 나뉜다.   토픽그림에서처럼 NodeA - NodeB, NodeA - NodeC 처럼 비동기식 단방향 메시지 송수신 방식을 토픽(topic)이라 하고, 메시지를 발간하는 Publisher, 메시지를 구독하는 Subscriber가 존재한다.  서비스NodeB - NodeC 처럼 동기식 양방향 메시지 송수신 방식을 서비스(Service)라 하고, 서비스를 요청(request)하는 쪽을 service client라 하고, 서비스를 응답(response)하는 쪽을 service server라 한다.  액션NodeA - NodeB 처럼 비동기식 + 동기식 양방향 메시지 송수신 방식을 액션(action)이라 하고, 액션 목표(Goal)을 지정하는 action client와 액션 목표를 받아 특정 태스크를 수행하면서 중간 결과값에 해당하는 액션 피드백(Feedback)과 최종 결과값에 해당하는 액션 결과(result)를 전송하는 action server간의 통신을 액션(action)이라 볼 수 있다.즉 Action Goal -&amp;gt; Action Feedback -&amp;gt; Action Result 로 진행된다.액션의 구현 방식을 더 자세히 살펴보면 토픽과 서비스의 혼합이라 볼 수 있는데, 액션 목표와 액션 결과를 전달하는 방식은 서비스와 같고, 액션 피드백은 토픽과 같은 메시지 전송 방식이다. 그래서 피드백의 퍼블리셔는 Action Server라 할 수 있고, 피드백의 서브스크라이버는 Action client와 service client라 할 수 있다.  파라미터각 노드에 파라미터 관련 parameter server를 실행시켜 외부의 parameter client 간의 통신으로 파라미터를 변경하는 것으로 서비스와 동일하다. 단 노드 내 매개변수 또는 글로벌 매개변수를 서비스 메시지 통신 방법을 사용하여 노드 내부 또는 외부에서 쉽게 지정(set)하거나 변경할 수 있고, 쉽게 가져와(get) 사용할 수 있다. 이제 turtlesim에서 실행되는 통신을 살펴보자. 먼저 어떤 노드들이 실행되고, 어떤 토픽, 어떤 서비스와 액션이 있는지 확인해봐야 한다.$ ros2 node list/teleop_turtle/turtlesim$ ros2 topic list/parameter_events/rosout/turtle1/cmd_vel/turtle1/color_sensor/turtle1/pose$ ros2 service list/clear/kill/reset/spawn/teleop_turtle/describe_parameters/teleop_turtle/get_parameter_types/teleop_turtle/get_parameters/teleop_turtle/list_parameters/teleop_turtle/set_parameters/teleop_turtle/set_parameters_atomically/turtle1/set_pen/turtle1/teleport_absolute/turtle1/teleport_relative/turtlesim/describe_parameters/turtlesim/get_parameter_types/turtlesim/get_parameters/turtlesim/list_parameters/turtlesim/set_parameters/turtlesim/set_parameters_atomically$ ros2 action list/turtle1/rotate_absolute 그리고 rqt_graph로 시각화할 수 있다.rqt_graph turtlesim 노드 정보먼저 노드를 자세히 살펴보고자 한다. turtlesim의 노드는 총 2개로 구성되어 있다.$ ros2 node list/teleop_turtle/turtlesimturtlesim_node 노드 파일은 turtlesim이라는 노드명으로 실행되어 있고, turtle_teleop_key 노드 파일은 teleop_turtle 이라는 노드명으로 실행되어 있다.이 때, 동일 노드를 여러 개 실행하고자 하면 이전과 동일하게 ros2 run turtlesim turtlesim_node로 실행할 수 있는데, 이렇게 하면 동일한 노드 이름으로 생성된다.만약 다른 노드 이름으로 설정하고자 한다면 아래 명령어로 실행한다.ros2 run turtlesim turtlesim_node __node:=new_turtle 노드 이름만 변경되었고, 토픽인 turtle1/cmd_vel은 동일하다. 만약 teleop_turtle 노드를 이용하여 거북이를 움직이면 두 개의 노드의 거북이가 동일하게 움직인다. 이는 동일한 토픽을 이용하기 때문이다. 이 토픽 또한 토픽명을 변경하거나 name_space를 통해 바꿀 수 있다.$ ros2 node list/new_turtle/teleop_turtle/turtlesim  노드의 정보를 확인하기 위해서는 ros2 node info 명령어를 사용해야 한다. 이를 통해 퍼블리셔, 서브스크라이버, 서비스, 액션, 파라미터 정보를 확인할 수 있다.$ ros2 node info /turtlesim/turtlesim Subscribers: /parameter_events: rcl_interfaces/msg/ParameterEvent /turtle1/cmd_vel: geometry_msgs/msg/Twist Publishers: /parameter_events: rcl_interfaces/msg/ParameterEvent /rosout: rcl_interfaces/msg/Log /turtle1/color_sensor: turtlesim/msg/Color /turtle1/pose: turtlesim/msg/Pose Service Servers: /clear: std_srvs/srv/Empty /kill: turtlesim/srv/Kill /reset: std_srvs/srv/Empty /spawn: turtlesim/srv/Spawn /turtle1/set_pen: turtlesim/srv/SetPen /turtle1/teleport_absolute: turtlesim/srv/TeleportAbsolute /turtle1/teleport_relative: turtlesim/srv/TeleportRelative /turtlesim/describe_parameters: rcl_interfaces/srv/DescribeParameters /turtlesim/get_parameter_types: rcl_interfaces/srv/GetParameterTypes /turtlesim/get_parameters: rcl_interfaces/srv/GetParameters /turtlesim/list_parameters: rcl_interfaces/srv/ListParameters /turtlesim/set_parameters: rcl_interfaces/srv/SetParameters /turtlesim/set_parameters_atomically: rcl_interfaces/srv/SetParametersAtomically Service Clients: Action Servers: /turtle1/rotate_absolute: turtlesim/action/RotateAbsolute Action Clients:$ ros2 node info /teleop_turtle/teleop_turtle Subscribers: /parameter_events: rcl_interfaces/msg/ParameterEvent Publishers: /parameter_events: rcl_interfaces/msg/ParameterEvent /rosout: rcl_interfaces/msg/Log /turtle1/cmd_vel: geometry_msgs/msg/Twist Service Servers: /teleop_turtle/describe_parameters: rcl_interfaces/srv/DescribeParameters /teleop_turtle/get_parameter_types: rcl_interfaces/srv/GetParameterTypes /teleop_turtle/get_parameters: rcl_interfaces/srv/GetParameters /teleop_turtle/list_parameters: rcl_interfaces/srv/ListParameters /teleop_turtle/set_parameters: rcl_interfaces/srv/SetParameters /teleop_turtle/set_parameters_atomically: rcl_interfaces/srv/SetParametersAtomically Service Clients: Action Servers: Action Clients: /turtle1/rotate_absolute: turtlesim/action/RotateAbsolute  turtlesim 토픽 009 ROS2 토픽 (topic)토픽은 비동기식 단방향 메시지 송수신 방식으로 메시지 형태로 메시지를 발행하는 퍼블리셔(publisher)와 메시지를 구동하는 서브스크라이버(subscriber) 간의 통신이라 볼 수 있다. 이는 1:1 통신을 기본으로 하지만, 1:N도 가능하고, 구성에 따라 N:1, N:N 통신도 가능하다.하나의 노드가 퍼블리셔와 서브스크라이버를 동시에 수행할 수도 있다. 원한다면 자신이 발행한 토픽을 셀프 구독할 수 있게 구성할 수도 있다.  토픽 목록 확인ros2 run turtlesim turtlesim_noderos2 run turtlesim turtle_teleop_key$ ros2 node info /turtlesim/turtlesim Subscribers: /turtle1/cmd_vel: geometry_msgs/msg/Twist Publishers: /turtle1/color_sensor: turtlesim/msg/Color /turtle1/pose: turtlesim/msg/Pose...토픽만을 보면 위와 같을 것이다. turtlesim 노드 subscriber geometry_msgs/msg/Twist 형태의 메시지인 turtle1/cmd_vel publisher turtlesim/msg/color_sensor 메시지 형태인 turtle1/color_sensor turtlesim/msg/Pose 메시지 형태인 turtle1/pose  간단한 리스트로 확인하기 위해서는 다음과 같이 명령어를 실행하면 된다. 동작 중인 모든 노드들의 토픽 정보를 확인할 수 있다. 이 때, -t는 메시지의 형태도 함께 표시하기 위한 옵션이다.$ ros2 topic list -t/parameter_events [rcl_interfaces/msg/ParameterEvent]/rosout [rcl_interfaces/msg/Log]/turtle1/cmd_vel [geometry_msgs/msg/Twist]/turtle1/color_sensor [turtlesim/msg/Color]/turtle1/pose [turtlesim/msg/Pose]  토픽 정보 확인하나의 토픽을 더 자세히 확인해보자.$ ros2 topic info /turtle1/cmd_velType: geometry_msgs/msg/TwistPublisher count: 1Subscription count: 1메시지 타입은 Twist, 퍼블리셔 1개, 서브스크라이버 1개로 구성되어 있다.  토픽 내용 확인토픽의 내용을 확인해보자. teleop_key를 실행한 터미널에서 방향키를 눌러 거북이를 움직이게 되면, linear 와 angular 값이 담긴 토픽이 발행되고 있는 것을 볼 수 있다.$ ros2 topic echo /turtle1/cmd_vellinear: x: 2.0 y: 0.0 z: 0.0angular: x: 0.0 y: 0.0 z: 0.0---linear: x: 0.0 y: 0.0 z: 0.0angular: x: 0.0 y: 0.0 z: 2.0---linear에 x,y,z , angular에 x,y,z 총 6개의 값으로 구성되어 있고, linear.x 값은 1.0m/s 단위로 구성되어 있다.모든 메시지는 meter, second, degree, kg 등 SI 단위를 기본으로 사용한다.  메시지 크기 확인메시지의 대역폭, 즉 송수신받는 토픽 메시지의 크기를 확인해보고자 한다. 크기 확인은 ros2 topic bw로 토픽의 초당 대역폭을 확인해볼 수 있다.$ ros2 topic bw /turtle1/cmd_velSubscribed to [/turtle1/cmd_vel]35 B/s from 2 messages Message size mean: 52 B min: 52 B max: 52 B26 B/s from 2 messages Message size mean: 52 B min: 52 B max: 52 B21 B/s from 2 messages Message size mean: 52 B min: 52 B max: 52 B18 B/s from 2 messages Message size mean: 52 B min: 52 B max: 52 B15 B/s from 2 messages Message size mean: 52 B min: 52 B max: 52 B13 B/s from 2 messages Message size mean: 52 B min: 52 B max: 52 B방향키를 통해 실행을 해보면 약 35 B/s 정도의 대역폭을 가지고 있고, 토픽을 보내지 않으면 값이 계속 감소된다.  토픽 주기 확인토픽의 전송 주기를 확인하기 위해서는 ros2 topic hz를 사용해야 한다.$ ros2 topic hz /turtle1/cmd_velaverage rate: 0.449 min: 1.016s max: 3.439s std dev: 1.21158s window: 2average rate: 0.293 min: 1.016s max: 5.792s std dev: 1.94967s window: 3/turtle1/cmd_vel 토픽을 발행하면 약 0.3 Hz를 가지고 있다. 이는 약 0.0003초에 한번씩 토픽을 발행한다.  토픽 지연 시간 확인토픽은 RMW 및 네트워크 장비를 거치기 때문에, latency가 반드시 존재한다. 지연 시간을 체크하는 방식은 메시지 내 header라는 stamp 메시지를 사용하여 체크한다.$ ros2 topic delay /TOPIC_NAME그러나 turtlesim에서는 stamp가 없기에 테스트는 수행하지 못했다.  토픽 발행토픽을 발행하는 방법은 ros2 topic pub &amp;lt;topic-name&amp;gt; &amp;lt;msg type&amp;gt; &quot;&amp;lt;args&amp;gt;&quot; 로 발행할 수 있다.이 때, --once옵션을 사용하여 단 한번만 발행을 할 수 있다.ros2 topic pub --once /turtle1/cmd_vel geometry_msgs/msg/Twist &quot;{linear: {x: 2.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.8}}&quot;이 때, linear은 m/s 단위이고, angular는 rad/s 단위이다. 지속적인 발행을 원한다면 –once 옵션을 제거하고 –rate옵션을 사용하여 Hz 단위로 발행한다.ros2 topic pub --rate 1 /turtle1/cmd_vel geometry_msgs/msg/Twist &quot;{linear: {x: 2.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.8}}&quot;  거북이를 조종할 때 ← ↑ ↓ → 키를 사용하여 움직일 수 있는데, 터미널에도 나오듯이 F를 기준으로 G,B,V,C,D,E,R,T를 사용해서도 거북이를 움직일 수 있다.R키를 누르면, 1.57 radian 방향인 π/2, 즉 위 방향을 바라보게 된다.  bag 파일 저장토픽을 파일 형태로 저장할 수 있다. 이 때 저장하는 파일의 형태가 bag 이고, 저장된 토픽은 다시 불러와 동일한 타이밍에 재생할 수 있다. 이를 rosbag이라 한다.명령하는 방법은 ros2 bag record &amp;lt;topic_name1&amp;gt; &amp;lt;topic_name2&amp;gt; &amp;lt;topic_name3&amp;gt; 와 같이 사용하여 원하는 토픽만 기록할 수 있다.ros2 bag record &amp;lt;topic_name1&amp;gt; &amp;lt;topic_name2&amp;gt; &amp;lt;topic_name3&amp;gt; -a를 사용하면 전체 토픽을 저장할 수 있다.ros2 bag record -a 원하는 이름이 있다면 -o 를 사용하여 이름을 지정한다.ros2 topic pub --rate 1 /turtle1/cmd_vel geometry_msgs/msg/Twist &quot;{linear: {x: 2.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.8}}&quot;### ros2 bag record -o &amp;lt;file-name&amp;gt; &amp;lt;topic-name&amp;gt;$ ros2 bag record -o test.bag turtle1/cmd_vel[INFO]: Opened database &#39;test.bag/test.bag_0.db3&#39; for READ_WRITE.[INFO]: Listening for topics...[INFO]: Subscribed to topic &#39;/turtle1/cmd_vel&#39;[INFO]: All requested topics are subscribed. Stopping discovery...  bag 파일 정보 확인저장된 bag파일의 정보를 확인할 수 있다.$ ros2 bag info test.bagFiles: test.bag_0.db3Bag size: 16.8 KiBStorage id: sqlite3Duration: 1.984sStart: Jul 26 2022 14:43:24.586 (1658814204.586)End: Jul 26 2022 14:43:26.571 (1658814206.571)Messages: 9Topic information: Topic: /turtle1/cmd_vel | Type: geometry_msgs/msg/Twist | Count: 9 | Serialization Format: cdr  bag 파일 재생저장된 bag 파일을 재생하여 토픽을 발행할 수 있다.$ ros2 bag play test.bag[INFO]: Opened database &#39;test.bag/test.bag_0.db3&#39; for READ_ONLY.  ROS 인터페이스ROS 노드 간에 데이터를 주고받을 때 토픽, 서비스, 액션이 사용되는데, 이 때 사용되는 데이터의 형태를 ROS 인터페이스(interface)라 한다. ROS 인터페이스에는 ROS2에 새롭게 추가된 IDL(Interface Definition Language)와 ROS1부터 존재하던 msg, srv, action 등이 있다.즉, 토픽, 서비스, 액션은 각각 msg, srv, action interface를 사용하고 있으며 정수, 소수, bool 등과 같은 단순 자료형을 기본으로 하고, 메시지 안에 메시지를 품는 구조도 있다. 메시지 인터페이스 (message interface, msg)지금까지 다루던 토픽의 형태가 msg 분류의 데이터 형태이다. 예를 들어 Twist 데이터 형태를 자세히 보면 Vector3 linear와 Vector3 angular 가 있는데, 이것이 메시지 안에 메시지를 품고 있는 형태이다. Vector3안에는 또 다시 float64 형태의 x,y,z 값이 존재한다. geometry_msgs/msgs/Twist Vector3 linear float64 x float64 y float64 z Vector3 angular float64 x float64 y float64 z  $ ros2 interface show geometry_msgs/msg/TwistVector3 linearVector3 angular$ ros2 interface show geometry_msgs/msg/Vector3float64 xfloat64 yfloat64 z interface 명령어에는 show 이외에도 list, package, packages, proto가 있다. list : 현재 개발 환경의 모든 msg, srv, action 메시지를 보여준다. package + &amp;lt;package name&amp;gt; : 지정한 패키지에 포함된 인터페이스를 보여준다. packages : msg, srv, action 인터페이스를 담고 있는 패키지의 목록을 보여준다. proto + &amp;lt;interface&amp;gt; : 특정 인터페이스 형태를 입력하면 인터페이스의 기본 형태를 표시해준다. $ ros2 interface listMessages: action_msgs/msg/GoalInfo action_msgs/msg/GoalStatus action_msgs/msg/GoalStatusArray ...Services: action_msgs/srv/CancelGoal composition_interfaces/srv/ListNodes composition_interfaces/srv/LoadNode ...Actions: action_tutorials_interfaces/action/Fibonacci example_interfaces/action/Fibonacci tf2_msgs/action/LookupTransform turtlesim/action/RotateAbsolute토픽에 해당되는 msg 인터페이스 이외에도 서비스, 액션 메시지도 존재하고 이를 srv, action 인터페이스라 한다. $ ros2 interface packagesaction_msgsaction_tutorials_interfaces... $ ros2 interface package turtlesimturtlesim/srv/Spawnturtlesim/msg/Poseturtlesim/action/RotateAbsoluteturtlesim/srv/Killturtlesim/srv/TeleportAbsoluteturtlesim/srv/SetPenturtlesim/srv/TeleportRelativeturtlesim/msg/Color이 때, msg는 토픽, srv는 service, action은 action 인터페이스에 해당한다. $ ros2 interface proto geometry_msgs/msg/Twist&quot;linear: x: 0.0 y: 0.0 z: 0.0angular: x: 0.0 y: 0.0 z: 0.0&quot;  Turtlesim 서비스 010 ROS2 서비스 (service) 서비스는 동기식 양방향 메시지 송수신 방식으로 서비스의 요청(request)을 하는 쪽을 service client라 하며, 요청받은 서비스를 수행 후 서비스의 응답(response)을 하는 쪽을 service server라 한다.결국 서비스는 특정 요청을 하는 클라이언트와 요청받은 일을 수행한 후 결괏값을 전달하는 서버 간의 통신이다. 서비스 요청 및 응답 또한, msg 인터페이스의 변형인 srv 인터페이스라 한다. 서비스는 동일 서비스에 대해 복수의 클라이언트를 가질 수 있다. 단 응답은 서비스 요청이 있었던 클라이언트에 대해서만 응답하는 형태로, 위의 그림에서 Node C의 클라이언트가 Node B의 서버에게 요청을 했다면, B의 서버는 요청받은 서비스를 수행한 후 Node C의 클라이언트에게만 응답을 하게 된다.  서비스 목록 확인turtlesim 패키지를 통해 서비스 목록을 살펴보도록 한다. 목록을 살펴볼 때는 topic이랑 동일하게 ros2 service list로 확인할 수 있다. $ ros2 run turtlesim turtlesim_node[INFO]: Starting turtlesim with node name /turtlesim[INFO]: Spawning turtle [turtle1] at x=[5.544445], y=[5.544445], theta=[0.000000]$ ros2 service list/clear/kill/reset/spawn/turtle1/set_pen/turtle1/teleport_absolute/turtle1/teleport_relative/turtlesim/describe_parameters/turtlesim/get_parameter_types/turtlesim/get_parameters/turtlesim/list_parameters/turtlesim/set_parameters/turtlesim/set_parameters_atomically파라미터가 붙어있는 부분을 제외한 7가지의 서비스에 대해 자세하게 알아보자.  서비스 형태 확인서비스 형태 확인 명령어는 ros2 service type &amp;lt;service name&amp;gt; 이다. 그러나 더 쉽게 확인하는 방법은 ros2 service list -t를 사용할 수 있다.$ ros2 service type /clearstd_srvs/srv/Empty$ ros2 service type /killturtlesim/srv/Kill$ ros2 service list -t/clear [std_srvs/srv/Empty]/kill [turtlesim/srv/Kill]/reset [std_srvs/srv/Empty]/spawn [turtlesim/srv/Spawn]/turtle1/set_pen [turtlesim/srv/SetPen]/turtle1/teleport_absolute [turtlesim/srv/TeleportAbsolute]/turtle1/teleport_relative [turtlesim/srv/TeleportRelative]...clear 서비스는 std_srvs/srv/Empty 형태이고, kill 서비스는 turtlesim/srv/Kill 형태임을 확인할 수 있다.  서비스 찾기서비스 형태를 통해 서비스명을 찾을 수 있다. ros2 service find &amp;lt;service type&amp;gt; 와 같이 서비스 형태를 넣으면 서비스명을 찾는다. 서비스 요청실제 서비스 서버에게 서비스를 요청(request)해본다. 서비스 요청은 ros2 service call &amp;lt;service_name&amp;gt; &amp;lt;service_type&amp;gt; &quot;&amp;lt;arguments&amp;gt;&quot;를 통해 보낼 수 있다. /clear 서비스는 turtlesim 노드를 동작할 때 표시되는 이동 궤적을 지우는 서비스이다.$ ros2 run turtlesim turtle_teleop_key $ ros2 service call /clear std_srvs/srv/Emptywaiting for service to become available...requester: making request: std_srvs.srv.Empty_Request()response:std_srvs.srv.Empty_Response()뒤에 argument 부분이 없는 이유는 해당 서비스가 아무런 내용이 없는 형태로도 사용이 가능하기 때문이다. /kill 서비스는 죽이고자 하는 거북이 이름을 서비스 요청의 내용을 입력하면 거북이가 사라지게 된다.$ ros2 service call /kill turtlesim/srv/Kill &quot;name: &#39;turtle1&#39;&quot;waiting for service to become available...requester: making request: turtlesim.srv.Kill_Request(name=&#39;turtle1&#39;)response:turtlesim.srv.Kill_Response() 이번에는 /reset 서비스를 해보자. 이 서비스는 단어 그대로 모든 것을 리셋하여 거북이를 처음 상태로 되돌린다.$ ros2 service call /reset std_srvs/srv/Emptywaiting for service to become available...requester: making request: std_srvs.srv.Empty_Request()response:std_srvs.srv.Empty_Response() 이제 /set_pen 을 통해 거북이의 궤적의 색과 크기를 지정해보자. r,g,b, width, offset을 통해 변경이 가능하다/$ ros2 service call /turtle1/set_pen turtlesim/srv/SetPen &quot;{r: 255, g: 255, b: 255, width: 10}&quot; 마지막으로 /spawn 서비스를 사용하여 해당 위치 및 자세에 맞게 거북이를 추가한다. 이름 옵션을 지정하지 않으면 자동으로 지정된다.$ ros2 service call /spawn turtlesim/srv/Spawn &quot;{x: 5.5, y: 9, theta: 1.57, name: &#39;leonardo&#39;}&quot;waiting for service to become available...requester: making request: turtlesim.srv.Spawn_Request(x=5.5, y=9.0, theta=1.57, name=&#39;leonardo&#39;)response:turtlesim.srv.Spawn_Response(name=&#39;leonardo&#39;)  서비스 인터페이스서비스 또한 토픽과 마찬가지로 인터페이스를 가지고 있고, 파일로는 srv파일을 가르킨다. 서비스 인터페이스는 메시지 인터페이스의 확장형이라 볼 수 있다.$ ros2 interface show turtlesim/srv/Spawn.srvfloat32 xfloat32 yfloat32 thetastring name # Optional. A unique name will be created and returned if this is empty---string nameSpawn.srv 에는 float32 형태의 x,y,theta, string 형태의 name 이라는 데이터가 들어 있다. 여기서 특이한 것은 ---인데, 이는 구분자라 하여 요청과 응답을 나누어 사용하기 위한 구분이라 할 수 있다. 즉 구분자 위의 x,y,theta, name 부분은 서비스 요청에 해당하여 클라이언트에서 서버로 전송하는 값이다. 아래의 name은 응답 부분에 해당하여 지정된 서비스 요청을 수행하고, name 데이터를 클라이언트에 전송한다.Spawn 서비스를 사용하려면 위치인 x,y와 자세인 theta, 거북이 이름인 name를 통해 거북이를 생성하고, 응답 부분의 name에 신규 거북이의 이름이 저장된다.  Turtlesim 액션 011 ROS2 액션 (action) 비동기식 + 동기식 양방향 메시지 송수신 방식으로 액션 목표(goal)을 지정하는 액션 클라이언트와 액션 목표를 받아 특정 태스크를 수행하면서 중간 결괏값에 해당되는 액션 피드백과 최종 결괏값에 해당되는 액션 결과를 전송하는 맥션 서버 간의 통신이라 할 수 있다. 더 자세히 들여다보면, 액션 클라이언트는 서비스 클라이언트 3개와 서브스크라이버 2개로 구성되어 있고, 액션 서버는 서비스 서버 3개와 퍼블리셔 2개로 구성되어 있다. 액션 데이터는 action 인터페이스라 한다. ROS1에서의 액션은 목표, 피드백, 결과 값을 토픽으로만 주고 받았는데, ROS2에서는 토픽과 서비스 방식을 혼합하여 사용했다. 그 이유는 토픽으로만 액션을 구성했을 때는 비동기식 방식으로 구성되는데 반해, ROS2는 서비스와 토픽 방식의 혼합으로 인한 동기식 방식이므로 원하는 타이밍에 적절한 액션을 수행하게 되었다.ROS에서는 목표 상태(goal_state)라는 것이 존재하는데, 이는 목표 값을 전달한 후의 상태 머신을 구동하여 액션의 프로세스를 쫓는다. 여기서 상태머신(Goal State Machine)은 액션 목표 전달의 이후 액션의 상태 값을 액션 클라이언트에게 전달하여 액션이 원할하게 동작할 수 있도록 한다.  상태 머신 구조 위에서 G,B,V,C,D,E,R,T 키를 사용하는 것이 rotate_absolute 액션을 수행하는 것이고, 이 키들은 액션의 목표 값을 전달하는 목적으로 사용된다. F 액션 목표를 취소하는 키이다.$ ros2 node info /turtlesim/turtlesim ... Action Servers: /turtle1/rotate_absolute: turtlesim/action/RotateAbsolute Action Clients:turtlesim 노드는 액션 서버 역할을 하고, turtlesim/action/RotateAbsolute 이라는 action 인터페이스를 사용하는 /turtle1/rotate_absolute 라는 이름의 액션 서버임을 확인할 수 있다. 만약 위의 키들로 액션을 수행하게 되면, ros2 run turtlesim turtlesim_node를 실행한 터미널에 아래와 같이 표시된다.$ ros2 node info /teleop_turtle/teleop_turtle ... Action Servers: Action Clients: /turtle1/rotate_absolute: turtlesim/action/RotateAbsoluteteleop_turtle 노드가 액션 클라이언트 역할을 한다.  [INFO]: Rotation goal completed successfully그러나 전달되는 과정에서 F 키를 눌러 액션 목표를 취소하게 되면 취소되었다고 표시되고, 수행하던 행동을 멈춘다.[INFO]: Rotation goal canceled rotate_absolute 액션을 더 세분화하면 5가지로 구성된다./turtle1/rotate_absolute/_action/send_goal: turtlesim/action/RotateAbsolute_SendGoal/turtle1/rotate_absolute/_action/cancel_goal: action_msgs/srv/CancelGoal/turtle1/rotate_absolute/_action/status: action_msgs/msg/GoalStatusArray/turtle1/rotate_absolute/_action/feedback: turtlesim/action/RotateAbsolute_FeedbackMessage/turtle1/rotate_absolute/_action/get_result: turtlesim/action/RotateAbsolute_GetResult추가적인 내용은 추후 설명한다고 한다.  액션 정보 확인ros2 action info 를 통해 액션의 정보를 확인할 수 있는데, 여기에는 액션의 이름, 액션의 서버와 클라이언트 노드의 이름과 개수를 확인할 수 있다.$ ros2 action info /turtle1/rotate_absoluteAction: /turtle1/rotate_absoluteAction clients: 1 /teleop_turtleAction servers: 1 /turtlesim  액션 목표(action goal) 전달ros2 action send_goal &amp;lt;action name&amp;gt; &amp;lt;action type&amp;gt; &quot;&amp;lt;value&amp;gt;&quot;를 통해 action을 보낼 수 있다.$ ros2 action send_goal /turtle1/rotate_absolute turtlesim/action/RotateAbsolute &quot;{theta: 1.5708}&quot;Waiting for an action server to become available...Sending goal: theta: 1.5708Goal accepted with ID: 760369ad7f7f4cc8af2618fa688b65f6Result: delta: -0.16000032424926758Goal finished with status: SUCCEEDED거북이를 12시 방향인 1.57 radian 값을 목표로 주게 되면 전달한 목표 값과 액션 목표의 UID(Unique ID), 시작 위치부터 결괏값에 해당하는 위치까지의 변위 값인 delta를 결과로 보여준다. 그리고 전달 상태를 표시하게 된다.여기에 --feedback이라는 옵션을 주면, 거북이가 목표값까지의 남은 회전량을 표시해준다.$ ros2 action send_goal /turtle1/rotate_absolute turtlesim/action/RotateAbsolute &quot;{theta: 1.5708}&quot; --feedbackWaiting for an action server to become available...Sending goal: theta: 1.5708Feedback: remaining: 1.554800033569336Goal accepted with ID: 21a4fdd053f84ea1935a9b3c582c9ac6Feedback: remaining: 1.5388000011444092Feedback: remaining: 1.5227999687194824Feedback: remaining: 1.5067999362945557...Feedback: remaining: 0.05079972743988037Feedback: remaining: 0.03479969501495361Feedback: remaining: 0.018799662590026855Result: delta: -1.536000370979309Goal finished with status: SUCCEEDED  액션 인터페이스토픽, 서비스와 마찬가지로 액션도 인터페이스를 가지고 있다. action 파일이 이에 해당되고, 액션 인터페이스는 메시지 및 서비스 인터페이스의 확장형이라 볼 수 있다.ros2 interface show &amp;lt;action name&amp;gt; 명령어로 액션 인터페이스의 정보를 확인할 수 있는데, float32 형태의 theta, delta, remaining 이라는 세 개의 데이터가 있다. 여기도 마찬가지로 ---라는 구분자가 있고, 맨 위부터 액션 목표(goal), 액션 결과(result), 액션 피드백(feedback)으로 나누어져 있다. 즉, theta는 액션 목표, delta는 액션 결과, 액션 피드백(feedback)이다. 모든 데이터는 radian 단위를 사용한다.$ ros2 interface show turtlesim/action/RotateAbsolute.action# The desired heading in radiansfloat32 theta---# The angular displacement in radians to the starting positionfloat32 delta---# The remaining rotation in radiansfloat32 remaining  ROS2 토픽/서비스/액션 정리 012 ROS2 토픽/서비스/액션 정리 및 비교  토픽, 서비스, 액션 비교   토픽 (topic)​ 서비스 (service) 액션 (action) 연속성 연속성 일회성 복합 (토픽+서비스) 방향성 단방향 양방향 양방향 동기성 비동기 동기 동기 + 비동기 다자간 연결 1:1, 1:N, N:1, N:N(publisher:subscriber) 1:1(server:client) 1:1(server:client) 노드 역할 발행자 (publisher), 구독자 (subscriber) 서버 (server), 클라언트 (client) 서버 (server), 클라언트 (client) 동작 트리거 발행자 클라언트 클라언트 인터페이스 msg 인터페이스 srv 인터페이스 action 인터페이스 CLI 명령어 ros2 topic, ros2 interface ros2 service, ros2 interface ros2 action, ros2 interface 사용 예 센서 데이터, 로봇 상태, 로봇 좌표, 로봇 속도 명령 등 LED 제어, 모터 토크 On/Off, IK/FK 계산, 이동 경로 계산 등 목적지로 이동, 물건 파지, 복합 태스크 등   msg, srv, action 인터페이스 비교   msg 인터페이스 srv 인터페이스 action 인터페이스 확장자 *.msg *.srv *.action 데이터 토픽 데이터 (data) 서비스 요청 (request), 서비스 응답 (response) 액션 목표 (goal), 액션 결과 (result), 액션 피드백 (feedback) 형식 fieldtype1 fieldname1, fieldtype2 fieldname2, fieldtype3 fieldname3 fieldtype1 fieldname1, fieldtype2 fieldname2, fieldtype3 fieldname3, fieldtype4 fieldname4 fieldtype1 fieldname1, fieldtype2 fieldname2, fieldtype3 fieldname3, fieldtype4 fieldname4, fieldtype5 fieldname5, fieldtype6 fieldname6 사용 예 [geometry_msgs/msg/Twist], Vector3 linear, Vector3 angular [turtlesim/srv/Spawn.srv], float32 x, float32 y , float32 theta , string name — string name [turtlesim/action/RotateAbsolute.action], float32 theta, float32 delta, float32 remaining   " }, { "title": "[ROS2 프로그래밍] ROS 프로그래밍 코드 스타일 및 프로그래밍 기초", "url": "/posts/helloworld/", "categories": "Classlog, ROS2", "tags": "ROS2", "date": "2022-07-26 01:35:00 +0900", "snippet": " 023 ROS 프로그래밍 규칙 (코드 스타일)ROS2 프로그래밍 규칙 (코드 스타일)협업 프로그래밍 작업 시 일관된 규칙을 만들고, 이를 준수하여야 코드가 꼬이거나 문제가 생기지 않는다.ROS가 지원하느 프로그래밍 언어는 다양하지만, 가장 많이 사용되는 C++과 python만 살펴보자. C++ styleROS2 Developer Guide 및 ROS2 Code Style에서 다루는 C++ 코드 스타일은 오픈소스 커뮤니티에서 가장 많이 사용되는 Google C++ Style Guide를 사용한다. 이름 규칙 타입, 클래스, 구조체, 열거형 : CamelCased 파일, 패키지, 인터페이스, 네임스페이스, 변수, 함수, 메소드 : snake_case 상수, 매크로 : ALL_CAPITALS 전역 변수는 g_ 접두어를 붙인다. 클래스 멤버 변수는 마지막에 _를 추가한다. 공백 : 기본 들여쓰기는 space 2개를 사용한다. 함수, if, for 등의 괄호는 아래 줄로 넘겨서 시작한다.if (a == 0) { a = 1;} python styleROS2 Developer Guide 및 ROS2 Code Style에서 다루는 python 코드 스타일은 pythib 이름 규칙 타입, 클래스 : CamelCased 파일, 패키지, 인터페이스, 변수, 함수, 메소드 : snake_case 상수 : ALL_CAPITALS 공백 : 기본 들여쓰기는 space 4개를 사용한다.  ROS2 프로그래밍 기초 - pythonROS2에는 python에서 작업할 수 있도록 도와주는 rclpy가 있다.패키지 생성ROS2 패키지 생성 명령어는 다음과 같다. ros2 pkg create [패키지이름] --build-type [빌드 타입] --dependencies [의존하는패키지1] [의존하는패키지2]..cd ~/robot_ws/src/ros2 pkg create rclpy_tutorial --build-type ament_python --dependencies rclpy std_msgs 이를 실행하고 나면 디렉토리는 다음과 같게 된다.src └── rclpy_tutorial ├── rclpy_tutorial └── \\__init__.py ├── resource └── rclpy_tutorial ├── test └── test_ ├── test_copyright.py ├── test_flake8.py └── test_pep257.py ├── package.xml ├── setup.cfg └── setup.py이렇게 기본적으로 생성된 파일은 패키지 설정 파일인 package.xml, 파이썬 패키지 설정 파일인 setup.py, 파이썬 패키지 환경 설정 파일인 setup.cfg로 구성된다. package.xml패키지 설정 파일은 사용할 RCL(ROS2 Client Libraries)에 따라 달라지는데, C++은 build type으로 ament_cmake가 사용되고, python은 ament_python으로 설정한다. 그 외에는 각기 다른 개발 환경에 맞춘 의존성 패키지를 설정해준다.&amp;lt;!-- package.xml --&amp;gt;&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;?xml-model href=&quot;http://download.ros.org/schema/package_format3.xsd&quot; schematypens=&quot;http://www.w3.org/2001/XMLSchema&quot;?&amp;gt;&amp;lt;package format=&quot;3&quot;&amp;gt; &amp;lt;name&amp;gt;rclpy_tutorial&amp;lt;/name&amp;gt; &amp;lt;version&amp;gt;0.0.0&amp;lt;/version&amp;gt; &amp;lt;description&amp;gt;TODO: Package description&amp;lt;/description&amp;gt; &amp;lt;maintainer email=&quot;jhyoon@todo.todo&quot;&amp;gt;jhyoon&amp;lt;/maintainer&amp;gt; &amp;lt;license&amp;gt;TODO: License declaration&amp;lt;/license&amp;gt; &amp;lt;!-- packages --&amp;gt; &amp;lt;depend&amp;gt;rclpy&amp;lt;/depend&amp;gt; &amp;lt;depend&amp;gt;std_msgs&amp;lt;/depend&amp;gt; &amp;lt;!-- dependencies --&amp;gt; &amp;lt;test_depend&amp;gt;ament_copyright&amp;lt;/test_depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_flake8&amp;lt;/test_depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_pep257&amp;lt;/test_depend&amp;gt; &amp;lt;test_depend&amp;gt;python3-pytest&amp;lt;/test_depend&amp;gt; &amp;lt;export&amp;gt; &amp;lt;build_type&amp;gt;ament_python&amp;lt;/build_type&amp;gt; &amp;lt;/export&amp;gt;&amp;lt;/package&amp;gt;  setup.py기본적으로 구성되어 있는 코드는 아래와 같다.from setuptools import setuppackage_name = &#39;rclpy_tutorial&#39;setup( name=package_name, version=&#39;0.0.0&#39;, packages=[package_name], data_files=[ (&#39;share/ament_index/resource_index/packages&#39;, [&#39;resource/&#39; + package_name]), (&#39;share/&#39; + package_name, [&#39;package.xml&#39;]), ], install_requires=[&#39;setuptools&#39;], zip_safe=True, maintainer=&#39;jhyoon&#39;, maintainer_email=&#39;jhyoon@todo.todo&#39;, description=&#39;TODO: Package description&#39;, license=&#39;TODO: License declaration&#39;, tests_require=[&#39;pytest&#39;], entry_points={ &#39;console_scripts&#39;: [ ], },) 파이썬 패키지 설정 파일은 entry_points 옵션의 console_scripts를 사용하는 부분이 중요하다. 이 부분에서 helloworld_publisher와 helloworld_subscriber 콘솔 스크립트는 각각 rclpy_tutorial.heeloworld_publisher 모듈과 rclpy_tutorial.helloworld_subscriber 모듈의 main 함수가 호출되게 된다. 이를 통해 ros2 run 또는 ros2 launch 로 해당 스크립트를 실행시킬 수 있다.from setuptools import find_packagesfrom setuptools import setuppackage_name = &#39;rclpy_tutorial&#39;setup( name=package_name, version=&#39;0.0.2&#39;, packages=find_packages(exclude=[&#39;test&#39;]), data_files=[ (&#39;share/ament_index/resource_index/packages&#39;, [&#39;resource/&#39; + package_name]), (&#39;share/&#39; + package_name, [&#39;package.xml&#39;]), ], install_requires=[&#39;setuptools&#39;], zip_safe=True, author=&#39;Mikael Arguedas, Pyo&#39;, author_email=&#39;mikael@osrfoundation.org, pyo@robotis.com&#39;, maintainer=&#39;Pyo&#39;, maintainer_email=&#39;pyo@robotis.com&#39;, keywords=[&#39;ROS&#39;], classifiers=[ &#39;Intended Audience :: Developers&#39;, &#39;License :: OSI Approved :: Apache Software License&#39;, &#39;Programming Language :: Python&#39;, &#39;Topic :: Software Development&#39;, ], description=&#39;ROS 2 rclpy basic package for the ROS 2 seminar&#39;, license=&#39;Apache License, Version 2.0&#39;, tests_require=[&#39;pytest&#39;], entry_points={ &#39;console_scripts&#39;: [ &#39;helloworld_publisher = rclpy_tutorial.helloworld_publisher:main&#39;, &#39;helloworld_subscriber = rclpy_tutorial.helloworld_subscriber:main&#39;, ], },)  setup.cfg파이썬 패키지 환경 설정 파일에서는 자신의 패키지 이름이 그대로 작성되어 있는지 확인해야 하며, 추후 colcon을 이용하여 빌드하게 되면 /home/&amp;lt;username&amp;gt;/robot_ws/install/&amp;lt;package-name&amp;gt;/lib/&amp;lt;package-name&amp;gt; 에 실행 파일이 생성된다.[develop]script-dir=$base/lib/rclpy_tutorial[install]install-scripts=$base/lib/rclpy_tutorial 노드 생성 퍼블리셔 노드 생성퍼블리션 노드 소스 코드 파일은 robot_ws/src/rclpy_tutorial/rclpy_tutorial/ 위치에 helloworld_publisher.py 파일을 생성한다. 아래의 코드는 helloworld 메시지를 보내주기 위한 코드이다.#!/usr/bin/env python# -*- coding: utf8 -*-import rclpyfrom rclpy.node import Node # 퍼블리셔의 QoS 설정을 위해 QoSProfile 클래스를 사용from rclpy.qos import QoSProfile # 퍼블리싱하는 메시지 타입은 std_msgs.msg의 String이므로 importfrom std_msgs.msg import String # rclpy의 Node 클래스를 상속하여 사용class HelloworldPublisher(Node): def __init__(self): # Node 클래스의 이름을 helloworld_publisher라 지정 super().__init__(&#39;helloworld_publisher&#39;) # 퍼블리시할 데이터 버퍼에 10개까지 저장 qos_profile = QoSProfile(depth=10) # 퍼블리셔 노드 생성 (msg type, topic name, QoS) self.helloworld_publisher = self.create_publisher(String, &#39;helloworld&#39;, qos_profile) # 콜백 함수 실행 시 사용되는 타이머로 지정한 값마다 콜백함수를 실행 (timer_period_sec, 발행을 실행할 함수) self.timer = self.create_timer(1, self.publish_helloworld_msg) self.count = 0 # callback function, callback함수를 구현할 때는 멤버함수, lambda, 지역 함수 등으로 선언이 가능하다. 이 때는 멤버함수 사용 def publish_helloworld_msg(self): # 메시지 타입 - String msg = String() # 메시지의 data 입력 msg.data = &#39;Hello World: {0}&#39;.format(self.count) # 메시지 발행 self.helloworld_publisher.publish(msg) # print와 비슷한 함수로 기록용 self.get_logger().info(&#39;Published message: {0}&#39;.format(msg.data)) self.count += 1def main(args=None): # 초기화 rclpy.init(args=args) # node라는 이름으로 클래스 생성 node = HelloworldPublisher() try: # 노드를 spin, 즉 지정된 콜백함수를 실행 rclpy.spin(node) except KeyboardInterrupt: # ctrl+c와 같은 인터럽트 시그널을 받으면 반복을 끝냄 node.get_logger().info(&#39;Keyboard Interrupt (SIGINT)&#39;) finally: # 노드 소멸 node.destroy_node() # 노드 종료 rclpy.shutdown() if __name__ == &#39;__main__&#39;: main()  서브스크라이버 노드 작성퍼블리셔 노드와 같은 디렉토리인 /home/&amp;lt;username&amp;gt;/robot_ws/install/&amp;lt;package-name&amp;gt;/lib/&amp;lt;package-name&amp;gt; 에 helloworld_subscriber.py 파일을 생성한다.#!/usr/bin/env python# -*- coding: utf8 -*-import rclpyfrom rclpy.node import Nodefrom rclpy.qos import QoSProfilefrom std_msgs.msg import Stringclass HelloworldSubscriber(Node): def __init__(self): super().__init__(&#39;Helloworld_subscriber&#39;) # 서브스크라이버 데이터를 버퍼에 10개까지 저장 qos_profile = QoSProfile(depth=10) self.helloworld_subscriber = self.create_subscription( String, &#39;helloworld&#39;, self.subscribe_topic_message, qos_profile) # 메시지 타입, 토픽 이름, 콜백 함수, QoS def subscribe_topic_message(self, msg): # 데이터를 받으면 logging self.get_logger().info(&#39;Received message: {0}&#39;.format(msg.data)) def main(args=None): # 초기화 rclpy.init(args=args) # 클래스 생성 node = HelloworldSubscriber() try: # 콜백함수 실행 rclpy.spin(node) except KeyboardInterrupt: # 시그널 시 정지 node.get_logger().info(&#39;Keyboard Interrupt (SIGINT)&#39;) finally: # 노드 소멸 node.destroy_node() # 노드 종료 rclpy.shutdown() if __name__ == &#39;__main__&#39;: main()서브스크라이버의 코드는 퍼블리셔와 거의 동일하다. 빌드ROS2에서는 colcon으로 빌드를 한다. 우선 소스 코드가 있는 workspace로 이동하고 colcon build 명령어를 통해 전체를 빌드한다. 여기에 빌드 옵션을 추가할 수 있는데, 특정 패키지만 선택하여 빌드하고자 할 때는 --packages-select, symlink를 사용하려면 --symlink-install 옵션을 사용한다.(워크스페이스내의 모든 패키지 빌드하는 방법) $ cd ~/robot_ws &amp;amp;&amp;amp; colcon build --symlink-install(특정 패키지만 빌드하는 방법)$ cd ~/robot_ws &amp;amp;&amp;amp; colcon build --symlink-install --packages-select [패키지 이름1] [패키지 이름2] [패키지 이름N](특정 패키지 및 의존성 패키지를 함께 빌드하는 방법)$ cd ~/robot_ws &amp;amp;&amp;amp; colcon build --symlink-install --packages-up-to [패키지 이름]따라서 위에 생성한 패키지만 빌드한다.$ cd ~/robot_ws$ colcon build --symlink-install --packages-select rclpy_tutorialStarting &amp;gt;&amp;gt;&amp;gt; rclpy_tutorialFinished &amp;lt;&amp;lt;&amp;lt; rclpy_tutorial [0.81s]Summary: 1 package finished [1.00s]특정 패키지의 첫 빌드 시에는 빌드 후 아래 명령어를 통해 환경 설정 파일을 불러와서 실행 가능한 패키지의 노드 설정들을 해줘야 빌드된 노드를 실행할 수 있다.$ . ~/robot_ws/install/local_setup.bash 실행각 노드를 실행할 때는 ros2 run명령어를 통해 실행한다. 패키지 전체를 실행할 때는 ros2 launch 명령어를 사용할 수 있지만, 아직 launch를 생성하지 않았기에 우선 run으로 실행한다.$ ros2 run rclpy_tutorial helloworld_subscriber[INFO]: Received message: Hello World: 0[INFO]: Received message: Hello World: 1[INFO]: Received message: Hello World: 2[INFO]: Received message: Hello World: 3[INFO]: Received message: Hello World: 4...$ ros2 run rclpy_tutorial helloworld_publisher[INFO]: Published message: Hello World: 0[INFO]: Published message: Hello World: 1[INFO]: Published message: Hello World: 2[INFO]: Published message: Hello World: 3[INFO]: Published message: Hello World: 4... $ rqt_graph rqt_graph  ROS2 프로그래밍 기초 - C++ROS2에는 python과 마찬가지로 c++을 위해 제작된 rclcpp가 있다. 코드도 앞서 배운 python과 거의 동일하다.패키지 생성$ cd ~/robot_ws/src/$ ros2 pkg create rclcpp_tutorial --build-type ament_cmake --dependencies rclcpp std_msgsROS에서 C++을 사용하기 위한 클라이언트 라이브러리 rclcpp를 사용한다. 이렇게 하면 ROS1과 동일한 디렉토리 경로를 확인할 수 있다.src └── rclpy_tutorial ├── include └── rclcpp_tutorial ├── src ├── CMakeLists.txt └── package.xml 생성된 package.xml, CMakeLists.txt은 각각 패키지 설정 파일과 빌드 설정 파일이다. package.xmlC++이라면 build type으로 ament_cmake를 사용한다.&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;?xml-model href=&quot;http://download.ros.org/schema/package_format3.xsd&quot; schematypens=&quot;http://www.w3.org/2001/XMLSchema&quot;?&amp;gt;&amp;lt;package format=&quot;3&quot;&amp;gt; &amp;lt;name&amp;gt;rclcpp_tutorial&amp;lt;/name&amp;gt; &amp;lt;version&amp;gt;0.0.1&amp;lt;/version&amp;gt; &amp;lt;description&amp;gt;ROS 2 rclcpp basic package for the ROS 2 seminar&amp;lt;/description&amp;gt; &amp;lt;maintainer email=&quot;jhyoon@todo.todo&quot;&amp;gt;jhyoon&amp;lt;/maintainer&amp;gt; &amp;lt;license&amp;gt;Apache License 2.0&amp;lt;/license&amp;gt; &amp;lt;buildtool_depend&amp;gt;ament_cmake&amp;lt;/buildtool_depend&amp;gt; &amp;lt;!-- packages --&amp;gt; &amp;lt;depend&amp;gt;rclcpp&amp;lt;/depend&amp;gt; &amp;lt;depend&amp;gt;std_msgs&amp;lt;/depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_lint_auto&amp;lt;/test_depend&amp;gt; &amp;lt;test_depend&amp;gt;ament_lint_common&amp;lt;/test_depend&amp;gt; &amp;lt;export&amp;gt; &amp;lt;build_type&amp;gt;ament_cmake&amp;lt;/build_type&amp;gt; &amp;lt;/export&amp;gt;&amp;lt;/package&amp;gt;  CMakeLists.txtCMakeLists파일을 아래와 같이 수정한다.cmake_minimum_required(VERSION 3.5)project(rclcpp_tutorial)# Default to C99if(NOT CMAKE_C_STANDARD) set(CMAKE_C_STANDARD 99)endif()# Default to C++14if(NOT CMAKE_CXX_STANDARD) set(CMAKE_CXX_STANDARD 14)endif()if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES &quot;Clang&quot;) add_compile_options(-Wall -Wextra -Wpedantic)endif()# find dependenciesfind_package(ament_cmake REQUIRED)find_package(rclcpp REQUIRED)find_package(std_msgs REQUIRED)# Buildadd_executable(helloworld_publisher src/helloworld_publisher.cpp)ament_target_dependencies(helloworld_publisher rclcpp std_msgs)add_executable(helloworld_subscriber src/helloworld_subscriber.cpp)ament_target_dependencies(helloworld_subscriber rclcpp std_msgs)# Installinstall(TARGETS helloworld_publisher helloworld_subscriber DESTINATION lib/${PROJECT_NAME})if(BUILD_TESTING) find_package(ament_lint_auto REQUIRED) # the following line skips the linter which checks for copyrights # uncomment the line when a copyright and license is not present in all source files #set(ament_cmake_copyright_FOUND TRUE) # the following line skips cpplint (only works in a git repo) # uncomment the line when this package is not in a git repo #set(ament_cmake_cpplint_FOUND TRUE) ament_lint_auto_find_test_dependencies()endif()# Macro for ament packageament_package() 노드 생성 퍼블리셔 노드 생성퍼블리셔 노드는 ~/robot_ws/src/rclcpp_tutorial/src/ 폴더에 helloworld_publisher.cpp 파일을 생성한다.#include &amp;lt;chrono&amp;gt;#include &amp;lt;functional&amp;gt;#include &amp;lt;memory&amp;gt;#include &amp;lt;string&amp;gt;// Node 클래스를 사용하기 위한 rclcpp 헤더 파일#include &quot;rclcpp/rclcpp.hpp&quot; // 메시지 타입인 String 선언#include &quot;std_msgs/msg/string.hpp&quot; // 추후 500ms, 1s 와 같이 시간을 가시성 있게 문자로 표현하기 위한 namespaceusing namespace std::chrono_literals; // rclcpp의 Node 클래스를 상속하여 사용class HelloworldPublisher : public rclcpp::Node {public: // Node 클래스의 생성자를 호출, 노드 이름을 helloworld_publisher로 지정, count_는 0으로 초기화 HelloworldPublisher() : Node(&quot;helloworld_publisher&quot;), count_(0) { // QoS 설정을 위해 KeepLast 형태로 depth를 10으로 설정하여 퍼블리시할 데이터를 버퍼에 10개까지 저장 auto qos_profile = rclcpp::QoS(rclcpp::KeepLast(10)); // node클래스의 create_publisher함수를 이용하여 퍼블리셔 설정, 메시지 타입으로 String, 토픽 이름으로 helloworld, QoS helloworld_publisher_ = this-&amp;gt;create_publisher&amp;lt;std_msgs::msg::String&amp;gt;( &quot;helloworld&quot;, qos_profile); // 콜백 함수를 수행, period=1초, 1초마다 지정한 콜백함수를 실행 timer_ = this-&amp;gt;create_wall_timer( 1s, std::bind(&amp;amp;HelloworldPublisher::publish_helloworld_msg, this)); }private: // 콜백 함수 void publish_helloworld_msg() { // String 타입으로 msg 선언 auto msg = std_msgs::msg::String(); // 메시지 데이터를 입력 msg.data = &quot;Hello World: &quot; + std::to_string(count_++); // logging, RCLCPP_XXX 계열의 함수는 print와 비슷 RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Published message: &#39;%s&#39;&quot;, msg.data.c_str()); // publishing helloworld_publisher_-&amp;gt;publish(msg); } // private 변수 rclcpp::TimerBase::SharedPtr timer_; rclcpp::Publisher&amp;lt;std_msgs::msg::String&amp;gt;::SharedPtr helloworld_publisher_; size_t count_;};int main(int argc, char * argv[]){ // rclcpp 초기화 rclcpp::init(argc, argv); // 클래스 생성 auto node = std::make_shared&amp;lt;HelloworldPublisher&amp;gt;(); // 콜백 함수 실행 rclcpp::spin(node); // ctrl+c와 같은 인터럽트 시그널 예외 상황에서 노드 종료 rclcpp::shutdown(); return 0;}이때도, 멤버함수로 콜백함수를 선언했다. 만약 lambda로 구현하고자 한다면 publish_helloworld_msg 함수를 삭제하고 위의 HelloworldPublisher 클래스 생성자 구문의 timer_ = this-&amp;gt;create_wall_timer() 함수에 람다 표현식을 추가하면 된다. timer_ = this-&amp;gt;create_wall_timer( 1s, [this]() -&amp;gt; void { auto msg = std_msgs::msg::String(); msg.data = &quot;Hello World2: &quot; + std::to_string(count_++); RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Published message: &#39;%s&#39;&quot;, msg.data.c_str()); helloworld_publisher_-&amp;gt;publish(msg); } );  서브스크라이버 노드 생성동일하게 ~/robot_ws/src/rclcpp_tutorial/src/에 helloworld_subscriber.cpp 파일을 생성한다.#include &amp;lt;functional&amp;gt;#include &amp;lt;memory&amp;gt;#include &quot;rclcpp/rclcpp.hpp&quot;#include &quot;std_msgs/msg/string.hpp&quot;// bind 함수의 대체자 역할을 위해 _1로 선언using std::placeholders::_1; // rclcpp의 Node클래스를 상속하여 사용class HelloworldSubscriber : public rclcpp::Node {public: // Node 클래스의 생성자를 호출하고 노드 이름을 helloworld_subscriber로 지정 HelloworldSubscriber() : Node(&quot;Helloworld_subscriber&quot;) { // QoS depth 10으로 하여 버퍼에 10개 저장 auto qos_profile = rclcpp::QoS(rclcpp::KeepLast(10)); // 구독할 토픽의 메시지 타입과 토픽의 이름, QoS 설정, 수신받은 메시지를 처리할 콜백함수를 기입 helloworld_subscriber_ = this-&amp;gt;create_subscription&amp;lt;std_msgs::msg::String&amp;gt;( &quot;helloworld&quot;, qos_profile, std::bind(&amp;amp;HelloworldSubscriber::subscribe_topic_message, this, _1)); }private: // 콜백함수 void subscribe_topic_message(const std_msgs::msg::String::SharedPtr msg) const { // logging RCLCPP_INFO(this-&amp;gt;get_logger(), &quot;Received message: &#39;%s&#39;&quot;, msg-&amp;gt;data.c_str()); } // private 변수로 사용되는 helloworld_subscriber_ 선언 rclcpp::Subscription&amp;lt;std_msgs::msg::String&amp;gt;::SharedPtr helloworld_subscriber_; };int main(int argc, char * argv[]){ rclcpp::init(argc, argv); auto node = std::make_shared&amp;lt;HelloworldSubscriber&amp;gt;(); rclcpp::spin(node); rclcpp::shutdown(); return 0;} 빌드rclpy를 빌드할 때와 동일하다.$ cd ~/robot_ws$ colcon build --symlink-install --packages-select rclcpp_tutorialStarting &amp;gt;&amp;gt;&amp;gt; rclcpp_tutorialFinished &amp;lt;&amp;lt;&amp;lt; rclcpp_tutorial [11.8s]Summary: 1 package finished [12.0s]마지막으로 동일하게 bash파일을 실행시켜준다.$ . ~/robot_ws/install/local_setup.bash $ ros2 run rclcpp_tutorial helloworld_subscriber[INFO]: Received message: &#39;Hello World: 0&#39;[INFO]: Received message: &#39;Hello World: 1&#39;[INFO]: Received message: &#39;Hello World: 2&#39;[INFO]: Received message: &#39;Hello World: 3&#39;[INFO]: Received message: &#39;Hello World: 4&#39;...$ ros2 run rclcpp_tutorial helloworld_publisher[INFO]: Published message: &#39;Hello World: 0&#39;[INFO]: Published message: &#39;Hello World: 1&#39;[INFO]: Published message: &#39;Hello World: 2&#39;[INFO]: Published message: &#39;Hello World: 3&#39;[INFO]: Published message: &#39;Hello World: 4&#39;...  ROS2 토픽, 서비스, 액션 인터페이스 살펴보기 추가 예정 027 토픽, 서비스, 액션 인터페이스" }, { "title": "[ROS2 이론] 목차 및 개요", "url": "/posts/introduction/", "categories": "Classlog, ROS2", "tags": "ROS2", "date": "2022-07-25 18:35:00 +0900", "snippet": "표윤석 박사님의 네이버 카페 글을 참고로 하여 ROS2를 공부하고, 기록하고자 합니다. 기본 URL : https://cafe.naver.com/openrt/24070 github : https://github.com/robotpilot/ros2-seminar-examples 목차는 기본적으로 이론/프로그래밍으로 나뉘어져 있고, 심화 프로그래밍과 기타 등이 존재합니다. 저는 이론/프로그래밍을 위주로 공부할 예정입니다.책으로도 존재하니 책으로 보고 싶으신 분들은 책으로 공부하시면 될 것 같습니다. 위의 참고 자료는 온라인 강의로 사용된 PDF파일 600페이지 분량이라고 합니다. 참고 교재 - ROS 로봇 프로그래밍 또한, 저는 일단 wsl2로 진행할 예정이니 오리지널과 다소 차이가 있을 수 있습니다. ROS2는 윈도우에서도 사용이 가능하긴 하나 본래의 ROS가 ubuntu에서 주로 사용하기에 저도 ubuntu 환경에서 사용하기 위해 wsl2를 사용하는 것입니다. wsl2에 ROS2 설치 참고 자료 : https://keep-steady.tistory.com/45  why ROS2? 003 왜? ROS 2로 가야하는가? 004 ROS 2의 중요 컨셉과 특징 005 ROS1과 2의 차이점을 통해 알아보는 ROS2의 특징ROS1을 배우지 않았다면, 그냥 ROS2를 배워도 된다. 그러나 ROS1을 사용하던 사람이 ROS2를 사용할 이유는 잘 모를 수 있다.ROS2는 2014년 3월에 개발을 시작하여 2015년도에 알파 버전이 릴리즈되었으며, 1년 동안 8번의 릴리즈가 발표되었고, 2017년에 베타 버전이 작업되어 2017년 12월에 첫 공식 버전이 나왔다. 지금은 총 6개의 공식 배포판이 나와있다.ROS1에서 ROS2를 갈아타야하는 가장 큰 이유는 ROS1의 최후의 버전이 발표되었기 때문이다. ROS1의 마지막 버전인 Noetic Ninjemys는 2020년 05월 23일에 릴리즈되었다. 이 릴리즈가 마지막 버전이라는 선언을 했다.ROS1은 Linux만을 주로 지원하고 있었는데, ROS2는 Linux, windows, macOS 등 다양하게 지원하고 있다.  ROS1과 ROS2의 차이점  1. 개발 환경 설정 001 ROS 2 개발 환경 구축위에서 말했다시피 먼저 WSL2에서 ROS2를 설치하는 과정을 먼저 설명하고자 한다. wsl2가 설치되어 있다는 가정하에 시작한다.ubuntu 환경 설정sudo apt update &amp;amp;&amp;amp; sudo apt upgrade  지역 변수 설정$ sudo locale-gen en_US en_US.UTF-8$ sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8$ export LANG=en_US.UTF-8  지역 변수 확인$ localeLANG=en_US.UTF-8LANGUAGE=LC_CTYPE=&quot;en_US.UTF-8&quot;LC_NUMERIC=&quot;en_US.UTF-8&quot;...  ROS2 환경 변수 설정$ vim ~/.bashrcvim이나 nano, gedit 등 자신이 원하는 편집기를 사용하면 된다. 기존에 존재했던 것들은 그대로 두고, 아래 부분을 추가한다. source, export는 모두 추가해주어야 하며, alias는 자신이 원하는 대로 추가하면 된다. 내용 ```markdownsource /opt/ros/foxy/setup.bashsource ~/robot_ws/install/local_setup.bashsource /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bashsource /usr/share/vcstool-completion/vcs.bashsource /usr/share/colcon_cd/function/colcon_cd.shexport _colcon_cd_root=~/robot_wsexport ROS_DOMAIN_ID=7export ROS_NAMESPACE=robot1export RMW_IMPLEMENTATION=rmw_fastrtps_cpp# export RMW_IMPLEMENTATION=rmw_connext_cpp# export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp# export RMW_IMPLEMENTATION=rmw_gurumdds_cpp# export RCUTILS_CONSOLE_OUTPUT_FORMAT=&#39;[{severity} {time}] [{name}]: {message} ({function_name}() at {file_name}:{line_number})&#39;export RCUTILS_CONSOLE_OUTPUT_FORMAT=&#39;[{severity}]: {message}&#39;export RCUTILS_COLORIZED_OUTPUT=1export RCUTILS_LOGGING_USE_STDOUT=0export RCUTILS_LOGGING_BUFFERED_STREAM=1alias cw=&#39;cd ~/robot_ws&#39;alias cs=&#39;cd ~/robot_ws/src&#39;alias ccd=&#39;colcon_cd&#39;alias cb=&#39;cd ~/robot_ws &amp;amp;&amp;amp; colcon build --symlink-install&#39;alias cbs=&#39;colcon build --symlink-install&#39;alias cbp=&#39;colcon build --symlink-install --packages-select&#39;alias cbu=&#39;colcon build --symlink-install --packages-up-to&#39;alias ct=&#39;colcon test&#39;alias ctp=&#39;colcon test --packages-select&#39;alias ctr=&#39;colcon test-result&#39;alias rt=&#39;ros2 topic list&#39;alias re=&#39;ros2 topic echo&#39;alias rn=&#39;ros2 node list&#39;alias killgazebo=&#39;killall -9 gazebo &amp;amp; killall -9 gzserver &amp;amp; killall -9 gzclient&#39;alias af=&#39;ament_flake8&#39;alias ac=&#39;ament_cpplint&#39;alias testpub=&#39;ros2 run demo_nodes_cpp talker&#39;alias testsub=&#39;ros2 run demo_nodes_cpp listener&#39;alias testpubimg=&#39;ros2 run image_tools cam2image&#39;alias testsubimg=&#39;ros2 run image_tools showimage&#39;```  IDE 설치표윤석 박사님의 블로그에 보면, IDE로 vscode를 사용하는 것을 권장한다. vscode가 linux, window, macOS 모두 사용 가능한 크로스 플랫폼 에디터인 동시에 다양한 프로그래밍 언어를 지원하고 있고, .pcd와 같은 특수한 파일을 볼 수 있는 뷰어도 지원하고 있다.추천해주시는 vscode에 extensions은 정말 많다. 필요한 것은 알아서 설치하길 바란다. 이름 설명 C/C++ C/C++ intellisense, 디버깅 및 코드 검색 CMake CMake 언어 지원 CMake Tools CMake 언어 지원 및 다양한 툴 Python 디버깅, intellisense, 코드 서식 지정, 리팩토딩 등 지원 ROS ROS 개발 지원 URDF URDF/xacro 지원 Colcon Tasks Colcon 명령어를 위한 VScode Task 이 외에도 많으니 https://cafe.naver.com/openrt/25288를 참고하길 바란다.  패키지 설치 및 gpg 설정 및 소스 설정$ sudo apt update &amp;amp;&amp;amp; sudo apt install curl gnupg2 lsb-release -y$ sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg$ echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main&quot; | sudo tee /etc/apt/sources.list.d/ros2.list &amp;gt; /dev/null$ sudo apt update 이 때, PUBKEY 에러가 나면 아래 코드를 실행한다.참고 URL : https://answers.ros.org/question/325039/apt-update-fails-cannot-install-pkgs-key-not-working/?answer=325040#post-id-325040#### remove old key$ sudo apt-key del 421C365BD9FF1F717815A3895523BAEEB01FA116OK#### install new key$ sudo -E apt-key adv --keyserver &#39;hkp://keyserver.ubuntu.com:80&#39; --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654$ sudo apt update 그래도 에러가 나면$ curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -$ sudo apt updateor$ sudo apt-key adv --keyserver &#39;hkp://keyserver.ubuntu.com:80&#39; --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654$ sudo apt update ROS2 패키지 설치$ sudo apt update$ sudo apt install ros-foxy-desktop ros-foxy-rmw-fastrtps* ros-foxy-rmw-cyclonedds* -y  패키지 설치 확인$ echo &quot;source /opt/ros/foxy/setup.bash&quot; &amp;gt;&amp;gt; ~/.bashrc$ source ~/.bahsrc$ ros2 run demo_nodes_cpp talker[INFO] [1658742890.693325028] [talker]: Publishing: &#39;Hello World: 1&#39;[INFO] [1658742891.693441287] [talker]: Publishing: &#39;Hello World: 2&#39;[INFO] [1658742892.693642848] [talker]: Publishing: &#39;Hello World: 3&#39;...$ ros2 run demo_nodes_py listener[INFO] [1612912265.593335793] [listener]: I heard: [Hello World: 3][INFO] [1612912266.576514520] [listener]: I heard: [Hello World: 4][INFO] [1612912267.576780341] [listener]: I heard: [Hello World: 5][INFO] [1612912268.576769156] [listener]: I heard: [Hello World: 6][INFO] [1612912269.577142775] [listener]: I heard: [Hello World: 7]...이 두가지를 동시에 실행했을 때 문제가 없다면 ros2가 잘 설치된 것이다.  ROS2 개발 툴 설치로봇 프로그래밍에 필수로 사용되는 패키지들이다. 개발 환경 설치시 설치해두는 것이 좋다.$ sudo apt update &amp;amp;&amp;amp; sudo apt install -y build-essential cmake git libbullet-dev python3-colcon-common-extensions python3-flake8 python3-pip python3-pytest-cov python3-rosdep python3-setuptools python3-vcstool wget$ python3 -m pip install -U argcomplete flake8-blind-except flake8-builtins flake8-class-newline flake8-comprehensions flake8-deprecated flake8-docstrings flake8-import-order flake8-quotes pytest-repeat pytest-rerunfailures pytest$ sudo apt install --no-install-recommends -y libasio-dev libtinyxml2-dev libcunit1-dev  ROS2 빌드 테스트$ mkdir -p ~/robot_ws/src$ cd ~/robot_ws$ colcon build --symlink-install  확인위 과정을 진행하면 워크스페이스 폴더에 src, build, install, log 폴더가 생성된 것을 확인할 수 있다. colcon이 뭐지?  VSCode의 개발 환경 설정 User settings 설정settings.json 은 VScode의 사용자별 글로벌 환경 설정을 지정하는 파일이다. 이 파일에 기술된 설정들은 모든 작업 공간에 적용되는 미니맵 사용, 세로 제한 줄 표시, 탭 사이즈 등이다. 추가 예정   C/C++ properties 설정 추가 예정   Tasks 설정 추가 예정   Launch 설정 추가 예정  QtCreator 설치QtCreator는 ROS2의 rqt UI를 작성할 때 편리한 툴이다.$ sudo apt install qtcreator -y$ qtcreator  2. ROS2와 Data Distribution Service 006 ROS2와 DDSROS에 중요한 용어 정의 및 메시지, 메시지 통신에 대해 먼저 알아보고자 한다. 메시지 통신은 ROS 프로그래밍에 있어서 ROS1과 2의 공통된 중요한 핵심 개념이다.ROS에는 프로그램의 재사용성을 극대화하기 위해 최소 단위의 실행 가능한 프로세서라고 정의하는 노드 단위로 프로그램을 작성하게 된다. 패키지(package) : 하나 이상의 노드 또는 노드 실행을 위한 정보 등을 묶어 놓은 것 메타패키지(metapackage) : 패키지의 묶음 메시지(message) : 노드와 노드 간 주고 받는 입력과 출력 데이터 주고받는 방식을 메시지 통신이라 한다. 메시지는 int, float, point, bool, string 등 다양한 변수 형태로 존재 데이터 구조 및 메시지의 배열을 정할 수 있다. 메시지를 주고받는 통신 방법에 따라 토픽(topic), 서비스(service), 액션(action), 파라미터(parameter)로 구분된다. 퍼블리셔 : 메시지를 보내는 노드 서브스크라이버 : 메시지를 받는 노드 ROS1은 자체적으로 개발한 TCPROS와 같은 통신 라이브러리를 사용하고 있는 방면, ROS2에서는 OMG(Object Management Group)에 의해 표준화된 DDS(Data Distribution Service)의 퍼블리셔와 서브스크라이브 프로토콜인 DDSI-RTPS(Real Time Publish Subscribe)를 사용한다. DDSI-RTPS는 DDS의 중요 컨셉인 DCPS(Data-Centric Publish-Subscribe), DLRL(Data Local Reconstruction Layer)의 내용을 담아 재정한 통신프로토콜이다. DDS통신을 사용하는 이유는 ROS1과 같이 자체적으로 제작하기보다 산업용 표준인 DDS를 통신 미들웨어로 사용하기 위해서이다.DDS의 사용으로 노드 간 실시간 데이터 전송이 보장되고, 노드 간 동적 검색 기능을 지원하여 ROS1에서 노드를 관리하던 마스터 노드가 없이도 통신이 가능해졌다. 또한, TCP에서의 데이터 손실을 방지하고, 신뢰도를 높였다. DDS란?그렇다면 과연 DDS가 무엇일까? DDS는 한국말로 데이터 분산 시스템인데, OMG 에서 표준을 정하고자 만든 트레이드마크이다. 간단하게는 데이터 통신을 위한 미들웨어이다.OMG DDS Foundation에서 정의한 DDS는 다음과 같다. The Data Distribution Service (DDS™) is a middleware protocol and API standard for data-centric connectivity from the Object Management Group® (OMG®). It integrates the components of a system together, providing low-latency data connectivity, extreme reliability, and a scalable architecture that business and mission-critical Internet of Things (IoT) applications need. In a distributed system, middleware is the software layer that lies between the operating system and applications. It enables the various components of a system to more easily communicate and share data. It simplifies the development of distributed systems by letting software developers focus on the specific purpose of their applications rather than the mechanics of passing information between applications and systems.출처 : https://www.dds-foundation.org/what-is-dds-3/ 즉, DDS는 데이터를 중심으로 연결성을 같는 미들웨어이다. 이 미들웨어는 ISO 7계층에서 호스트 계층에 해당되는 4~7계층에 해당된다. DDS의 특징ROS2의 미들웨어로 사용할 때의 장점을 살펴보자. 산업 표준DDS는 1989년 설립된 비영리 단체인 OMG(Object Management Group)가 관리하는 산업 표준이다. ROS1은 자체적인 TCPROS였기에, ROS2의 DDS 사용은 다양한 산업으로 넓혀갈 수 있는 발판이 될 수 있다.  OS 독립DDS는 Linux, windows, macOS, Android 등 다양한 운영체제를 지원하고 있다.  언어 독립DDS는 미들웨어이므로 그 상위 레벨인 사용자 코드 레벨에서는 DDS 사용을 위해 프로그래밍 언어를 바꿀 필요가 없다. ROS2에서도 DDS를 RMW(ROS Middleware)로 디자인되어, 다양한 언어를 지원하는 ROS 클라이언트 라이브러리(ROS client Library)를 제작하여 멀티 프로그래밍 언어를 지원하고 있다. 각 언어에 맞게 rclpy, rclcpp, rclc, rcljava 와 같은 이름으로 지원한다.  UDP 기반의 전송 방식일반적으로 UDP 기반의 신뢰성 있는 멀티캐스트를 구현하여 시스템이 최신 네트워킹 인프라의 이점을 활용할 수 있다. TCP 대신 UDP 기반의 통신은 여러 목적지로 동시에 데이터를 보낼 수 있다.ROS2에서는 ROS_DOMAIN_ID라는 환경 변수로 도메인을 설정하게 된다. ROS2에서는 전역 공간이라 불리는 DDS Global Space라는 공간에 있는 토픽들에 대해 구독 및 발행을 할 수 있다.  데이터 중심적 기능DDS를 사용하면 제일 많이 보는 말 중 하나는 Data Centric이다. DDS에서도 DCPS(Data Centric Publish Subscribe)라는 개념이 있는데, 이는 적절한 수신자에게 적절한 정보를 효율적으로 전달하는 것을 목표로 하는 발간 및 구독 방식이다. 사용자 입장에서 어떤 데이터인지, 어떤 형식인지, 어떻게 보낼지, 안전하게 보낼지에 대한 기능이 존재한다는 것이다.  동적 검색DDS는 동적 검색(Dynamic Discovery)를 제공한다. 응용 프로그램은 DDS의 동적 검색을 통하여 어떤 토픽이 지정 도메인 영역에 있으며 어떤 노드가 이를 발신하고 수신하는지 알 수 있게 된다. 이는 ROS 프로그래밍 시 데이터를 주고 받을 노드들의 IP주소 및 포트를 미리 입력하거나 따로 구성하지 않아도 되며, 사용하는 시스템 아키텍처의 차이점을 고려할 필요가 없기 때문에, 모든 운영 체제 또는 하드웨어 플랫폼에서 매우 쉽게 작업할 수 있다.ROS1에서는 마스터 노드에 노드의 이름 및 메시지를 찾아서 연결시켜준다. ROS2에서는 마스터 노드가 없기에 노드를 DDS의 Participant 개념으로 취급하게 되었으며, 동적 검색 기능을 이용하여 DDS 미들웨어를 통해 직접 검색하고 노드를 연결할 수 있게 된다.  확장 가능한 아키텍처DDS를 사용함으로써 IoT, 항공 우주 산업과 같은 초대형 시스템에 확장이 가능해졌다. 단일 표준 통신 계층에서 복잡성을 흡수함으로서 분산 시스템 개발을 더욱 단순화시켰다. 수백 수천개의 노드를 관리하거나 여러 대의 로봇, 주변 인프라를 통제하는 IT 기술 등에 적용시킬 수 있게 되었다.  상호 운용성DDS를 사용하고 있는 제품을 사용하면, A라는 회사의 제품을 사용하다가 B라는 회사 제품으로 변경이 가능하다. ROS2를 지원하는 업체는 ADLink, Eclipse, Eprosima, Gurum Network, RTI로 총 5곳이 있다.  서비스 품질(QoS)노드 간의 DDS 통신 옵션을 설정하는 QoS는 퍼블리셔 및 서브스크라이버 등을 선언하고 사용할 때 매개변수처럼 QoS를 사용한다.self.helloworld_publisher = self.create_publisher(String, &#39;helloworld&#39;, qos_profile)DDS 사양상 설정 가능한 QoS 항목은 22가지인데, ROS2에서는 데이터 손실을 방지하여 신뢰도를 우선시(reliable) 할 수 있으며, UDP처럼 통신 속도를 최우선시하여 사용(best effort)할 수 있게 하는 신뢰성(reliability) 기능이 대표적으로 사용된다.그 외에는 다음과 같다. history : 통신 상태에 따라 정해진 사이즈만큼의 데이터를 보관 Durability : 데이터를 수신하는 서브스크라이버가 생성되기 전의 데이터를 사용할지 폐기할지에 대한 설정 Deadline : 정해진 주기 안에 데이터가 발신 및 수신되지 않을 경우 이벤트 함수를 실행시키는 기능 Lifespan : 정해진 주기 안에서 수신되는 데이터만 유효 판정하고 그렇지 않은 데이터는 삭제하는 기능 Liveliness : 정해진 주기 안에서 노드 혹은 토픽의 생사를 확인이 외에도 엄청 많다. 이러한 QoS 설정을 통해 DDS는 적시성, 트래픽 우선순위, 안정성 및 리소스 사용과 같은 데이터를 주고 받는 모든 측면을 사용자가 제어할 수 있게 되었다.  보안DDS를 사용함으로서 보안 성능을 향상시켰다. DDS-Security라는 DDS 보안 사양을 ROS에 적용하여 보안에 대한 이슈를 통신단부터 해결했다. 또한, SROS2(Secure Robot Operating System 2)라는 툴을 개발하여 보안 관련 RCL 서포트 및 보안 관련 프로그래밍을 위한 툴킷을 만들어 배포했다.  RMW 변경 방법기본적으로 실행하게 되면 기본 RMW인 rmw_fastrtps_cpp가 사용된다. 만약 RMW를 변경하여 사용하려면 위에 개발 환경에서 hash(#)처리했던 설정들로 변경하여 사용하면 된다.export RMW_IMPLEMENTATION=rmw_fastrtps_cpp# export RMW_IMPLEMENTATION=rmw_connext_cpp# export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp# export RMW_IMPLEMENTATION=rmw_gurumdds_cpp# export RMW_IMPLEMENTATION=rmw_opensplice_cpp이를 변경하고 나서 노드를 실행하면 된다. rclcpp_tutorial은 0025번 목차를 확인하길 바란다. 단지 아래 내용을 확인하려면 별다른 설정없이 ros2 run demo_nodes_cpp talker으로 실행하면 된다. $ export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp$ ros2 run rclcpp_tutorial helloworld_publisher[INFO]: Published message: Hello World: 0[INFO]: Published message: Hello World: 1[INFO]: Published message: Hello World: 2[INFO]: Published message: Hello World: 3[INFO]: Published message: Hello World: 4...만약 RMW_IMPLEMENTATION 설정을 각각 테스트하고자 한다면, 각 노드 별로 서로 다르게 RMW_IMPLEMENTATION 환경 변수로 선언할 수 있다.$ export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp$ ros2 run rclcpp_tutorial helloworld_publisher[INFO]: Published message: Hello World: 0[INFO]: Published message: Hello World: 1[INFO]: Published message: Hello World: 2[INFO]: Published message: Hello World: 3[INFO]: Published message: Hello World: 4...$ export RMW_IMPLEMENTATION=rmw_fastrtps_cpp$ ros2 run rclcpp_tutorial helloworld_subscriber[INFO]: Received message: Hello World: 0[INFO]: Received message: Hello World: 1[INFO]: Received message: Hello World: 2[INFO]: Received message: Hello World: 3[INFO]: Received message: Hello World: 4...이렇게 실행해서 통신이 되면, 문제 없이 실행된 것이다. Domain 변경 방법UDP 멀티캐스트로 통신이 이루어지기 때문에, 별도의 설정을 하지 않으면 같은 네트워크의 모든 노드가 연결된다. 예를 들어 같은 연구실에서 동일 네트워크를 사용한다면 다른 연구원들의 노드의 데이터에 접근 가능하게 된다.그래서 다른 네트워크를 이용하도록 설정하거나 name space를 변경하여 사용하면 된다. 가장 간단한 방법으로는 DDS의 domain을 변경하는 것이다. 이는 ROS_DOMAIN_ID 라는 환경변수를 변경한다.$ export ROS_DOMAIN_ID=11$ ros2 run rclcpp_tutorial helloworld_publisher[INFO]: Published message: Hello World: 0[INFO]: Published message: Hello World: 1[INFO]: Published message: Hello World: 2[INFO]: Published message: Hello World: 3[INFO]: Published message: Hello World: 4$ export ROS_DOMAIN_ID=12$ ros2 run rclcpp_tutorial helloworld_subscriber$ export ROS_DOMAIN_ID=11$ ros2 run rclcpp_tutorial helloworld_subscriber[INFO]: Received message: Hello World: 89[INFO]: Received message: Hello World: 90...도메인을 다르게 설정했기 때문에, 메시지를 받지 못하다가 같은 ID로 설정하니 구독이 진행되는 것을 확인할 수 있다. DOMAIN_ID는 대체로 0~232까지의 정수를 사용한다.  QoS 테스트위에서 QoS의 특정 중 reliable, reliability 기능을 설명했다. 이에 대해 간단한 테스트를 진행해보고자 한다.이 테스트에서는 tc(traffic control)라는 리눅스 네트워크 트래픽 제어 유틸리티를 사용하여 임의의 데이터 손실(10%)를 만들어 reliability성을 테스트했다. 기본적으로 Reliablility의 기본 설정은 RLIABLE 로 되어 있다. 그렇기에 데이터 손실이 있어도 TCP와 같이 ack로 매번 확인하여 손실된 데이터를 재전송하므로 잃어버리는 데이터가 없다. 그러나 손실이 존재할 때, 터미널 창을 잠시 멈추는 것을 확인할 수 있는데, 이 때 손실된 데이터를 순차적으로 재전송하고 다시 ack 작업을 하기 때문이다. Reliability가 BEST_EFFORT로 되어 있을 때$ sudo tc qdisc add dev lo root netem loss 10%tc의 데이터 손실을 10%로 설정한 후 동일하게 실행해보면, 1부터 순차적으로 데이터를 전송했지만, 서브스크라이버 노드에 데이터를 손실한 채 표시된다. 데이터 손실이 있어도 문제없는 데이터라면 Reliability를 BEST_EFFORT로 설정하면 속도가 더 빨라질 것이다. 마지막으로 tc를 다시 되돌려놓는다.sudo tc qdisc delete dev lo root netem loss 10%" }, { "title": "[데브코스] 18주차 - Camera-LiDAR Calibration ", "url": "/posts/calibration/", "categories": "Classlog, devcourse", "tags": "devcourse, calibration", "date": "2022-07-04 23:20:00 +0900", "snippet": " 2D(카메라)와 3D(라이다) 좌표계를 서로 맞춰주는 것에 대해 설명하고자 한다. 2D 이미지 좌표계와 3D world 좌표계 간의 변환을 하기 위해서는 변환식을 알아야 한다. 변환하는 방법에는 2가지가 있다. 하나는 homography method라고 하여, 3개 이상의 2d 이미지 좌표계와 그에 상응되는 3d 월드 좌표계 8개를 가지고 있다는 가정하에, homography matrix를 구할 수 있다.현재에는 이 방법이 아닌 다른 방법을 위주로 설명하고, 추후 더 추가하고자 하니, homography에 대해 더 자세히 알고자 한다면, openCV를 참고하길 바란다. 또는 지난 블로그를 참고해도 좋다. OpenCV homography tutorial homograghy 블로그 Projection Method변환하는 또다른 방법으로는 projection method가 있다. Projection matrix라고도 하는 카메라 자체 특성인 intrinsic 정보들과 카메라에서 월드 좌표계로의 이동, 회전 변환인 extrinsic를 가지고, 변환할 수 있다. u, v : 이미지 좌표계 U,V,W : 월드 좌표계 K : camera calibration 정보 (intrinsic matrix) R, t : 카메라와 LIDAR 간의 관계 (extrinsic matrix)" }, { "title": "[데브코스] 18주차 - DepthFormer 실행 ", "url": "/posts/depthFormer/", "categories": "Classlog, devcourse", "tags": "devcourse, DepthFormer", "date": "2022-07-02 23:20:00 +0900", "snippet": " 실행 코드 : https://colab.research.google.com/drive/1xR54UirAUw7JcROSKZPl3HHH_hsDVBlh#scrollTo=lZQtCRpI6hN2Miniconda 설치 방법설치 사이트 : https://docs.conda.io/en/latest/miniconda.html참고 사이트 : https://digiconfactory.tistory.com/entry/미니콘다Miniconda-설치하기-파이썬-패키지관리 python버전 별로 설치하거나 최신 버전을 다운받는다.설치 exe파일을 열면 2개의 체크박스가 나오는데, 첫번째는 시스템 환경 변수에 추가할 건지에 대한 것이고, 2번째는 지금 설치하는 python을 시스템 전체 default로 설정할 것인지이다.  WSL2 or Ubuntu miniconda install참고 사이트 : https://smoothiecoding.kr/미니콘다-wsl2-vsc-파이썬/wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh ENTER를 누르고, 사용 설명서가 뜨고, yes를 입력해야 설치가 진행된다. 그러면 설치 경로를 변경하는 창이 뜬다. default값인 /usr/bin/miniconda3 로 정해준다. default값으로 사용하려면 그냥 ENTER를 누르면 된다. conda init을 할거냐는 창이 뜨면, yes라고 친다. 그러면 설치 성공된다.최종적으로 conda를 적용하기 위해 다음 명령어를 친다.source ~/.bashrc   가상환경 interpretervscode에서 코드를 디버깅하기 위해서, 가상환경의 python을 인터프리터로 사용해야 가상환경 내에서 코드를 돌릴 수 있다. 그렇지 않으면 local에 있는 인터프리터로 잡혀서 가상환경 안에 깔았던 패키지들이 인식되지 않는다. 그래서 가상환경에 존재하는 python으로 인터프리터를 바꿔줘야 한다. 먼저 CTRL+SHIFT+P를 눌러 명령창을 켠다. python: select interpreter를 선택한다. 가상환경안에 존재하는 python을 선택한다. conda를 사용했다면, 자신이 설치한 anaconda의 경로를 알아야 한다. 보통은 /usr/bin/anaconda3 또는 /home/&amp;lt;username&amp;gt;/anaconda3 등에 있을 것이다. 그러면 그 안에 존재하는 인터프리터의 경로는 /usr/bin/anaconda3/envs/&amp;lt;envname&amp;gt;/bin/python.exe가 된다.  가상환경 구축참고 사이트 : https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox/blob/main/docs/get_started.md#installation Conda 사용하여 환경 구축 가상 환경 생성conda create -n mde python=3.7conda activate mde  install pytorchconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  MMCV 설치 및 toolbox 설치pip install mmcv-full==1.3.13 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.htmlgit clone https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox.gitcd Monocular-Depth-Estimation-Toolboxpip install -e .  추가 패키지 설치pip install future tensorboard Virtualenv로 가상환경 구축anaconda를 설치하지 않았다면, 다른 패키지로 가상환경을 구축해야 한다. (그러나 이렇게 하면 conda에 대한 에러가 많이 떠서 비추천) virtualenv 패키지 설치pip install virtualenv  가상환경 구축virtualenv MDE --python=python3.7source MDE/bin/activateubuntu인 경우 bin폴더에 activate가 있을 것이고, window인 경우 bin대신 Scripts 에 있다.  pytorch, cudatoolkit 설치cuda 11.6 version 설치pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pinsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600wget https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda-repo-wsl-ubuntu-11-6-local_11.6.2-1_amd64.debsudo dpkg -i cuda-repo-wsl-ubuntu-11-6-local_11.6.2-1_amd64.debsudo apt-key add /var/cuda-repo-wsl-ubuntu-11-6-local/7fa2af80.pubsudo apt-get updatesudo apt-get -y install cuda  conda대신 virtualenv를 사용하게 되면 train.sh 실행 시 바꿔줘야 할 부분이 있다.https://pytorch.org/docs/stable/elastic/run.html$ python -m torch.distributed.launch ...에서 python -m torch.distributed.launch대신 torchrun으로 변경한다.$ torchrun ...  가상환경 나가기deactivate  데이터셋 다운로드데이터셋 디렉토리 구조monocular-depth-estimation-toolbox├── depth├── tools├── configs├── splits├── data│ ├── kitti│ │ ├── input│ │ │ ├── 2011_09_26│ │ │ │ ├── 2011_09_26_drive_0001_sync│ │ │ │ │ ├── image_02│ │ │ │ ├── 2011_09_26_drive_0002_sync│ │ │ │ ├── ...│ │ │ │ ├── 2011_10_03_drive_0047_sync│ │ │ ├── 2011_09_28│ │ │ ├── ...│ │ │ ├── 2011_10_03│ │ ├── gt_depth│ │ │ ├── 2011_09_26_drive_0001_sync│ │ │ │ ├── image_03│ │ │ ├── 2011_09_26_drive_0002_sync│ │ │ ├── ...│ │ │ ├── 2011_10_03_drive_0047_sync| | ├── benchmark_test│ │ │ ├── 0000000000.png│ │ │ ├── 0000000001.png│ │ │ ├── ...│ │ │ ├── 0000000499.png| | ├── benchmark_cam│ │ │ ├── 0000000000.txt│ │ │ ├── 0000000001.txt│ │ │ ├── ...│ │ │ ├── 0000000499.txt│ │ ├── split_file.txt│ ├── nyu│ │ ├── basement_0001a│ │ ├── basement_0001b│ │ ├── ... (all scene names)│ │ ├── split_file.txt│ ├── SUNRGBD│ │ ├── SUNRGBD│ │ │ ├── kv1│ │ │ ├── kv2│ │ │ ├── realsense│ │ │ ├── xtion│ │ ├── split_file.txt│ ├── cityscapes│ │ ├── camera│ │ │ ├── test│ │ │ ├── train│ │ │ ├── val│ │ ├── disparity_trainvaltest│ │ │ ├── disparity│ │ ├── leftImg8bit_trainvaltest│ │ │ ├── leftImg8bit│ │ ├── split_file.txt# data directory formatinput_image: 2011_09_26/2011_09_26_drive_0002_sync/image_02/data/0000000069.png gt_depth: 2011_09_26_drive_0002_sync/proj_depth/groundtruth/image_02/0000000069.png   KITTI 데이터셋 설치KITTI : http://www.cvlibs.net/datasets/kitti/eval_depth_all.php  input 데이터셋 다운로드mkdir inputhttp://www.cvlibs.net/datasets/kitti/raw_data.php위의 사이트에 raw data들을 synced data 형태로 다운로드 해서 2011_09_26과 같이 날짜에 대한 폴더를 생성하고, 그 안에 데이터들을 넣어준다.  추가적인 데이터를 다운받으려면 Dataset annotated depth maps data set 을 다운로드하여 데이터를 gt_depth에 넣어준다. curl로 gt_depth 데이터셋 다운로드cd Monocular-Depth-Estimation-Toolboxmkdir datacd datacurl https://s3.eu-central-1.amazonaws.com/avg-kitti/data_depth_annotated.zip -o ./kitti.zipjar xvf ./kitti.zipmkdir gt_depthmv ./train/* ./gt_depth/ 추후 코드에서 input 데이터와 gt_depth 데이터를 비교할 때 사용하는데, input데이터를 image_02, gt_depth 데이터는 image_03 데이터를 사용하므로 gt_depth를 다운하고 그 데이터를 분할해줘도 된다. 위의 gt_depth를 다운받아보고, 자신의 kitti_eigen_test.txt 파일에 맞게 분할해주면 된다. 나의 경우 txt파일에 0002_sync, 0013,0020에 대한 gt 파일이 지정되어 있었는데, gt_depth에는 해당 번호들이 없어서 input데이터에서 복사해왔다.  benchmark_cam 다운로드http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_predictionvaildation and test set을 다운받고, prediction 폴더만 data폴더에 옮긴다.data - kitti - test_depth_prediction_anonymous/intrinsics -&amp;gt; benchmark_cam - test_depth_prediction_anonymous/image -&amp;gt; benchmark_test  split_file downloadhttps://www.kaggle.com/datasets/qikangdeng/kitti-split-and-eigen-split?resource=download&amp;amp;select=eigen_train_files.txt해당 파일을 다운받고, data/kitti/폴더에 넣는다. 그리고 자기가 가지고 있는 데이터에 맞게 파일을 수정해준다. 나는 02,09,13,20 에 대해서만 사용할 예정이므로 나머지는 다 지웠다.  NYU 데이터셋 다운로드NYU : https://drive.google.com/u/0/uc?id=1AysroWpfISmm-yRFGBgFTrLy6FjQwvwP&amp;amp;export=downloadcd datamkdir nyucd nyunyu 폴더에 위의 zip파일을 다운로드한다.unzip sync.zip &amp;amp;&amp;amp; sudo rm -rf sync.zip또는jar xvf sync.zip &amp;amp;&amp;amp; sudo rm -rf sync.zipzip파일을 압축 풀면, sync 폴더가 생성되고, 그 안에 dasement_0001a, dasement_0001b… 등이 들어있다.mv sync/* ./ &amp;amp;&amp;amp; sudo rm -rf sync  split 파일들은 해당 주소를 확인한다.nyu_train.txt와 nyu_test.txt 파일을 다운받거나 복사해서 data/nyu 위치에 넣는다.  SUN RGB-D 데이터셋 다운로드SUN RGB-D : https://rgbd.cs.princeton.edu/해당 사이트로 들어가 Data → SUNRGBD V1을 다운로드한다.mkdir SUNRGBDcd SUBRGBDcurl https://rgbd.cs.princeton.edu/data/SUNRGBD.zip -o sun_rgbd.zipunzip sun_rgbd.zip  Cityscapes 데이터셋 다운로드cityscapes : https://www.cityscapes-dataset.com/downloads/해당 사이트로 들어가 leftImg8bit_trainvaltest.zip (11GB)를 다운받는다.mkdir cityscapescd cityscapes  실행DepthFormer Trainbash ./tools/dist_train.sh configs/depthformer/depthformer_swinl_22k_w7_kitti.py 1 --work-dir work_dirs/saves/depthformer/depthformer_swinl_22k_w7_kitti sh [./tools/dist_train.sh] ${CONFIG_FILE} ${GPU_NUM} —work_dir [work_dir]  DepthFormer Test single gpuspython ./tools/test.py ./configs/depthformer/depthformer_swinl_22k_w7_kitti.py ./checkpoints/depthformer_swinl_22k_kitti.pth --show-dir depthformer_swinl_22k_w7_kitti_resultshow-dir은 test 결과를 저장할 디렉토리 경로다.  no gpusCUDA_VISIBLE_DEVICES=-1 python tools/test.py configs/depthformer/depthformer_swinl_22k_w7_kitti.py checkpoints/depthformer_swinl_22k_kitti.pth --show  Custom dataset inference 해보기custom dataset에 대해 depth를 추론해보고자 한다.  configs/depthformer/에 있는 자신이 사용할 pretrained model에 대한 py파일을 복사하여 custom으로 이름을 바꿔준 뒤, _base_에 _base_/datasets/kitti.py 와 같은 kitti나 nyu 등을 custom으로 변경한다. data/custom/rgb 폴더를 생성하고, 추론할 RGB 데이터를 넣는다. test 파일 실행python ./tools/test.py ./configs/depthformer/depthformer_swinl_22k_w7_custom.py ./checkpoints/depthformer_swinl_22k_kitti.pth --show-dir depthformer_swinl_22k_w7_custom_result 여기서 ModuleNotFoundError가 뜬다면, 등록된 경로를 확인해봐야 한다.print(sys.path)[&#39;/home/ubuntu/Monocular-Depth-Estimation-Toolbox/tools&#39;, &#39;/home/ubuntu/anaconda3/envs/depth/lib/python37.zip&#39;, &#39;/home/ubuntu/anaconda3/envs/depth/lib/python3.7&#39;, &#39;/home/ubuntu/anaconda3/envs/depth/lib/python3.7/lib-dynload&#39;, &#39;/home/ubuntu/anaconda3/envs/depth/lib/python3.7/site-packages&#39;]Monocular-Depth-Estimation-Toolbox 경로가 등록되어 있지 않으면, 등록해준다.sys.path.append(&quot;/home/ubuntu/Monocular-Depth-Estimation-Toolbox/&quot;)  Bug report TypeError: multi_scale_deformable_attn_pytorch() takes 4 positional arguments but 6 were given output = multi_scale_deformable_attn_pytorch(value, spatial_shapes, level_start_index, sampling_locations, attention_weights, self.im2col_step)change tooutput = multi_scale_deformable_attn_pytorch(value, spatial_shapes, sampling_locations, attention_weights)https://github.com/open-mmlab/mmdetection/issues/6261   please usr “init_cfg” instead https://github.com/open-mmlab/mmdetection/issues/5177   CUDA out of memory CUDA_VISIBLE_DEVICES=-1 python tools/test.py configs/depthformer/depthformer_swinl_22k_w7_nyu.py path/to/your/model --eval x로 실행https://discuss.pytorch.kr/t/cuda-out-of-memory/216/6   cuDNN error: CUDNN_STATUS_NOT_INITIALIZED https://github.com/werner-duvaud/muzero-general/issues/139torch==1.8.0+cu112 하지 않고, torch==1.8.0 만 설치하고, 실행했더니 해당 에러가 뜬다.https://github.com/pytorch/pytorch/issues/53336  cuda버전에 맞는 pytorch 설치 torch==1.8.0+cu 11.2에 맞는 torch버전이 없어서 11.3을 설치한다.  pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113그러나 이 에러가 1.8이상 버전에서 발생하는 에러일 수도 있음. 그래서 11.3으로 설치해도 에러가 난다면 11.0으로 설치한다.pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html 둘 다 동일하게 에러 발생ImportError: /usr/local/lib/python3.7/dist-packages/mmcv/_ext.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at5sliceERKNS_6TensorElN3c108optionalIlEES5_l이는 pytorch버전이랑 mmcv 버전이 안맞아서 발생하는 것!  11.0버전!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html 동일하게 1.5.3 버전이 설치된다.  11.3버전!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html 결론 : mmcv-full 1.5.3 버전을 설치하니 버전이 너무 안맞다고 함… 결국 깃허브에 나와있는 버전대로 11.1버전에 맞게 설치하고자 한다.  원래 설명서에 나와있는 버전은 cuda 11.1, torch 1.8.0 mmcv 1.3.13pip uninstall torch torchvision torchaudio mmcv-full -ypip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.htmlpip install mmcv-full==1.3.13 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html   import 상대경로 설정 import syssys.path.append(path)   qt.qpa.xcb: could not connect to display QT라는 GUI 라이브러리가 있는데, 이를 실행할 수 없다는 것. colab에서는 실행이 불가능하다고 한다.qt.qpa.xcb: could not connect to display :1.0 qt.qpa.plugin: Could not load the Qt platform plugin “xcb” in “” even though it was found.  colab에서 cv2 함수 사용하는 방법import cv2 → from google.colab.patches import cv2_imshowCOLAB에서 OpenCV 함수 사용하기 cv2가 아닌 mmcv를 사용하고 있어서 어디를 고쳐야 할지 모름. 실패  cv2를 pillow로 변경mmcv.imfrombytes(backend=’cv2’) → mmcv.imfrombytes(backend=’pillow’)fileio - mmcv 1.5.3 documentation 실패  결론 : —show를 —eval x로 변경하여 GUI 사용하지 않기   추가 : Xming 설치해서 WSL2에서 GUI 실행하기[WSL] Windows Subsystem for Linux - 디스플레이 서버 설정 및 GUI 사용하기 - ECE - TUWLAB xming 설치 : https://sourceforge.net/projects/xming/ 실행 → 단축 아이콘 부분에 실행된 것을 확인할 수 있음 wsl 쉘에 /etc/machine-id 생성 sudo systemd-machine-id-setup sudo dbus-uuidgen --ensure 생성 확인 $ cat /etc/machine-id 7d7873cb6d4a46f1adcf99fba8b07d5a X-window 구성 요소 설치 sudo apt install x11-apps xfonts-base xfonts-100dpi xfonts-75dpi xfonts-cyrillic ~/.bashrc에 추가 export DISPLAY=:0 적용 source ~/.bashrc 동작확인 → wsl쉘에 타이핑 xeyes   TypeError: multi_scale_deformable_attn_pytorch() takes 4 positional arguments but 6 were given 4개 args만 들어가야 하는데 6개 들어간다는 의미이다. 코드를 보니 실제로 6개가 들어가고 있었다.https://github.com/open-mmlab/mmcv/pull/1223# usr/local/lib/python3.7/dist-packages/mmcv/ops/multi_scale_deform_attn.py line.351output = multi_scale_deformable_attn_pytorch( value, spatial_shapes, sampling_locations, attention_weights)  tcmalloc: large alloc python이 파일이 너무 큰 파일을 다룰 때, 나오는 경고 메시지라고 한다. 일반적으로는 warning으로만 출력되지만, 실제로 할당 메모리보다 자원이 부족한 경우에는 에러가 난다. 해결하는 방법은 환경 변수 export TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD를 더 크게 변경하면 된다. 약 10GB로 지정해준다.!sudo sh -c &#39;echo &quot;export TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=107374182400&quot; &amp;gt;&amp;gt; ~/.bashrc&#39;!source ~/.bashrc[Python / Linux] tcmalloc: large alloc 2219589632 bytes == 0x8fa7a000 @ 0x7fb7e4433680 해결법  AttributeError: ‘ConfigDict’ object has no attribute ‘eval_pipeline’ ./configs/_base_/datasets/kitti.py에./configs/_base_/datasets/nyu.py에 있는 eval_pipeline을 복사하여 추가한다.https://github.com/open-mmlab/mmdetection/issues/5739  KeyError: ‘cam_intrinsic’ ./depthdatasets/kitti.py에 있는 cam_instrinsic_dict를 cam_intrinsic으로 수정한다.  subprocess.CalledProcessError: Command ‘[‘/usr/bin/miniconda3/envs/venv/bin/python’, ‘-u’, ‘./tools/train.py’, ‘–local_rank=0’, ‘configs/depthformer/depthformer_swinl_22k_w7_kitti.py’, ‘–launcher’, ‘pytorch’, ‘–work-dir’, ‘work_dirs/saves/depthformer/depthformer_swinl_22k_w7_kitti’]’ returned non-zero exit status 1. " }, { "title": "[linux] wsl2 nvidia-driver, CUDA toolkit, CuDNN 설치 및 삭제 ", "url": "/posts/cuda/", "categories": "Review, linux", "tags": "linux, cuda", "date": "2022-07-02 23:00:00 +0900", "snippet": "nvidia driver 설치sudo apt updateubuntu-drivers devicessudo apt install -y ubuntu-drivers-commonCUDA Toolkit 설치설치 사이트 : https://developer.nvidia.com/cuda-11-6-0-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=WSL-Ubuntu&amp;amp;target_version=2.0&amp;amp;target_type=deb_local나의 경우 11.6 버전을 설치하려고 한다. wsl2이므로 아래와 같이 선택한다. $ wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin$ sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600$ wget https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda-repo-wsl-ubuntu-11-6-local_11.6.0-1_amd64.deb$ sudo dpkg -i cuda-repo-wsl-ubuntu-11-6-local_11.6.0-1_amd64.deb$ sudo apt-key add /var/cuda-repo-wsl-ubuntu-11-6-local/7fa2af80.pub$ sudo apt-get update$ sudo apt-get -y install cuda다 설치하고 나면 환경 설정을 해줘야 한다.sudo vim ~/.bashrc~/.bashrc파일을 열어 경로를 추가해준다.export PATH=/usr/local/cuda-11.6/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-11.6/lib64:$LD_LIBRARY_PATH  설치 확인$ nvcc -Vnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2022 NVIDIA CorporationBuilt on Tue_Mar__8_18:18:20_PST_2022Cuda compilation tools, release 11.6, V11.6.124Build cuda_11.6.r11.6/compiler.31057947_0 $ sudo sh -c &quot;echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2/lib64&#39; &amp;gt;&amp;gt; /etc/profile&quot;$ sudo sh -c &quot;echo &#39;export CUDADIR=/usr/local/cuda-11.2&#39; &amp;gt;&amp;gt; /etc/profile&quot;$ source /etc/profile  CuDNN 설치Toolkit 11.6에 맞는 cuDNN v8.4.0 설치설치 사이트 : https://developer.nvidia.com/rdp/cudnn-archive방금 설치한 CUDA toolkit 버전에 맞게 설치해주어야 한다. 나의 경우 11.6버전을 설치했기에 cuDNN v8.4.0을 설치하고자 한다. ubuntu18.04를 사용하고 있으므로 18.04에 대한 버전으로 선택한다.회원 전용 페이지이므로 wget 또는 curl이 먹히지 않을 수 있으니, 직접 다운로드한다.다운을 받고 나면, deb파일을 압축 풀어준다.ar vx cudnn-local-repo-ubuntu1804-8.4.0.27_1.0-1_amd64.debtar xvf control.tar.gztar xvf data.tar.xzdata.tar.xz파일 압축을 풀면, etc,usr,var 3개의 폴더가 생성된다. 각각 자신의 디렉토리에 복사해 넣는다.(여기에 include폴더가 있어야 환경 설정이 가능한것 같은데,, 11.6버전 실패해서 11.1로 재설치함)  Toolkit 11.1에 맞는 cuDNN v8.0.5 설치cudatoolkit v11.0 설치 사이트 : https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=WSLUbuntu&amp;amp;target_version=20&amp;amp;target_type=deblocal11.1에 맞는 버전은 8.0.5이므로 이에 대해 설치해보려고 한다.linux를 설치하면 tgz압축 파일이 다운받아진다.$ tar zxvf cudnn-11.1-linux-x64.v8.0.5.39.tgzcuda/include/cudnn.hcuda/include/cudnn_adv_infer.hcuda/include/cudnn_adv_train.hcuda/include/cudnn_backend.hcuda/include/cudnn_cnn_infer.hcuda/include/cudnn_cnn_train.hcuda/include/cudnn_ops_infer.hcuda/include/cudnn_ops_train.hcuda/include/cudnn_version.hcuda/NVIDIA_SLA_cuDNN_Support.txtcuda/lib64/libcudnn.socuda/lib64/libcudnn.so.8cuda/lib64/libcudnn.so.8.0.5cuda/lib64/libcudnn_adv_infer.socuda/lib64/libcudnn_adv_infer.so.8cuda/lib64/libcudnn_adv_infer.so.8.0.5...  파일 옮기기$ sudo cp cuda/include/cudnn* /usr/local/cuda-11.1/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda-11.1/lib64  권한 설정sudo chmod a+r /usr/local/cuda-11.1/include/cudnn.h /usr/local/cuda-11.1/lib64/libcudnn*  버전 확인cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2 symbolic link$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8$ sudo ln -sf /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8  설정 확인$ ldconfig -N -v $(sed &#39;s/:/ /&#39; &amp;lt;&amp;lt;&amp;lt; $LD_LIBRARY_PATH) 2&amp;gt;/dev/null | grep libcudnnlibcudnn_cnn_train.so.8 -&amp;gt; libcudnn_cnn_train.so.8.0.5libcudnn_cnn_infer.so.8 -&amp;gt; libcudnn_cnn_infer.so.8.0.5libcudnn_ops_infer.so.8 -&amp;gt; libcudnn_ops_infer.so.8.0.5libcudnn_ops_train.so.8 -&amp;gt; libcudnn_ops_train.so.8.0.5libcudnn_adv_infer.so.8 -&amp;gt; libcudnn_adv_infer.so.8.0.5libcudnn.so.8 -&amp;gt; libcudnn.so.8.0.5libcudnn_adv_train.so.8 -&amp;gt; libcudnn_adv_train.so.8.0.5  설정 적용$ sudo ldconfig/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link여기서 에러가 뜨고 있는데, 해당 파일을 링크걸어도 에러가 계속 뜬다. 이는 현재 내가 wsl2를 사용하고 있는 상황에서 발생하는 문제이다. 따라서 windows에서 cmd를 관리자 권한으로 실행하여 진행한다.$ cd lxss/lib$ del libcuda.so$ del libcuda.so.1$ mklink libcuda.so libcuda.so.1.1$ mklink libcuda.so.1 libcuda.so.1.1참고 자료 : https://github.com/microsoft/WSL/issues/5663#issuecomment-1068499676 $ sudo apt upgrade$ sudo ldconfig  Anaconda를 활용하여 cudatoolkit 및 cudnn 설치참고 사이트 : https://eehoeskrap.tistory.com/293conda install -c conda-forge cudatoolkit==11.1 cudnnconda list  패키지 삭제conda remove cudatoolkitNvidia driver 삭제sudo apt remove --purge &#39;^nvidia-.*&#39; CUDA 삭제 cuda 패키지 확인apt list | grep cuda cuda 패키지 삭제sudo apt --purge remove &#39;cuda*&#39;sudo apt autoremove --purge &#39;cuda*&#39; 패키지를 삭제하면 폴더가 삭제되지만, 삭제되지 않았다면, 직접 삭제해준다. cuda 파일 삭제sudo rm -rf /usr/local/cuda-11.6  reference cuda : https://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&amp;amp;logNo=222354396765 cuda : https://rdmkyg.blogspot.com/2021/12/ubuntu-2004-nvidia-gpu-cnn-2021-12-3.html cuda : https://blog.naver.com/PostView.naver?blogId=phoenixqq&amp;amp;logNo=222540035363 cudatoolkit,cudnn : https://webnautes.tistory.com/1479 anaconda : https://cceeddcc.tistory.com/4 gpu 최적화 : https://byeongjo-kim.tistory.com/34" }, { "title": "[linux] ubuntu18.04에 python3.8 버전 설치", "url": "/posts/Ubuntu1804_python3_8/", "categories": "Review, linux", "tags": "linux, python", "date": "2022-07-02 21:00:00 +0900", "snippet": "Ubuntu18.04에 python3.8 설치Ubuntu 18.04를 사용하거나, wsl2로 Ubuntu 18.04를 사용한다면, python 2.7이 기본 인터프리터일 가능성이 크다. python version 확인하는 방법$ python -VPython 2.7.17$ python2 -VPython 2.7.17$ python3 -VPython 3.6.9\\usr\\local\\lib 경로에 확인해보면 자신이 설치한 python 버전을 확인할 수 있다.3.6을 가지고 있긴 하나 기본으로 사용하고 있지 않다. 3.6을 기본으로 설정하는 방법은 추후 3.8 설치 설명 후에 설명하겠다.apt로 python3.8 설치 필요 패키지 설치$ sudo apt update$ sudo apt install software-properties-common ppa레포지토리 추가sudo add-apt-repository ppa:deadsnakes/ppa메시지가 표시되고, 계속하기 위해 ENTER 을 누른다. python 3.8 설치sudo apt install python3.8 python 3.8 확인$ python3.8 -VPython 3.8.13이렇게 설치가 완료되었다.Command &#39;python3.8&#39; not found, did you mean: command &#39;python3.6&#39; from deb python3.6-minimal command &#39;python3.7&#39; from deb python3.7-minimal이 메시지가 뜨면 설치가 되지 않은 것이므로 재설치한다.  Python 3.8 기본으로 설정 현재 기본 python 확인$ python -VPython 2.7.17  python 3.8로 변경$ sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python (python) in auto mode위의 메시지가 뜬다면 3.8로 변경이 성공되었다. update-alternatives: error: alternative path /usr/bin/python3.8 doesn&#39;t exist이 메시지가 뜬다면 3.8이 설치되지 않았다는 것이다.  기본 python 확인$ python -VPython 3.8.13  reference https://jjeongil.tistory.com/1806 https://sanghaklee.tistory.com/73" }, { "title": "[논문 리뷰] ORB-SLAM: a Versatile and Accurate Monocular SLAM System ", "url": "/posts/orb_slam/", "categories": "Review, SLAM", "tags": "SLAM, Visual-SLAM", "date": "2022-06-14 13:20:00 +0900", "snippet": "ORB-SLAM: a Versatile and Accurate Monocular SLAM System 논문에 대한 리뷰한 글입니다. 이 논문은 2015에 투고된 논문입니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract#" }, { "title": "[데브코스] 17주차 - Visual-SLAM Eigen tutorial ", "url": "/posts/eigen_tutorial/", "categories": "Classlog, devcourse", "tags": "devcourse, Visual-SLAM", "date": "2022-06-11 03:20:00 +0900", "snippet": "Eigen tutorial참고 자료 : Eigen 사이트 공식 문서1. Simply ClassEigen 사이트 공식 문서 reference guide Matrix2f, Matrix2d, Matrix3f, Matrix3d, MatrixXd … Vector2f, Vector2d, Vector3f, Vector3d, VectorXd …Vector와 Matrix의 사용 형태는 거의 동일하니 Matrix에 대해서만 다루려고 한다. 기본적인 변수 선언은 다음과 같다. Zero를 통해 0으로 구성된 Matrix를 구성할 수 있고, Random을 통해 무작위의 값으로 구성할 수 있다.#include &amp;lt;iostream&amp;gt;#include &amp;lt;Eigen/Dense&amp;gt;int main(){ Eigen::Matrix3d m1 = Eigen::Matrix3d::Zero(); Eigen::MatrixXd m2 = Eigen::MatrixXd::Zero(3,3); Eigen::Matrix2f m3 = Eigen::Matrix2f::Random(); Eigen::MatrixXf m4 = Eigen::Matrix2f::Random(2,2);}이런 Matrix에서의 element를 참조하는 방법은 다음과 같다.int main(){ m1(0, 1) = 3; m1(2, 2) = 9; std::cout &amp;lt;&amp;lt; m1 &amp;lt;&amp;lt; std::endl; std::cout &amp;lt;&amp;lt; &quot;\\n m4 before : \\n&quot; &amp;lt;&amp;lt; m4 &amp;lt;&amp;lt; std::endl; m4(1,1) = m4(0,0) + m4(0,1); std::cout &amp;lt;&amp;lt; &quot;\\n m4 after : \\n&quot; &amp;lt;&amp;lt; m4 &amp;lt;&amp;lt; std::endl;}/*0 3 00 0 00 0 9 m4 before : 0.823295 -0.329554-0.604897 0.536459 m4 after : 0.823295 -0.329554-0.604897 0.49374*/추가적인 사용방법은 아래 표와 같다.matrix나 vector에 값을 입력할 때는 comma initializer(&amp;lt;&amp;lt;)를 사용하면 편하다.int main(){ Eigen::Matrix3d intrinsic_matrix = Eigen::Matrix3d::Zero(); intrinsic_matrix &amp;lt;&amp;lt; 422.037858, 0.0, 245.895397, 0.0, 435.589734, 163.625535, 0.0, 0.0, 1.0; std::cout &amp;lt;&amp;lt; intrinsic_matrix &amp;lt;&amp;lt; std::endl;}/* 422.037858 0.0 245.8953970.0 435.589734 163.6255350.0 0.0 1.0*/추가적인 것들은 공식 문서를 참고하길 바란다.Eigen - Linear Algebra 참고 자료 : Eigen 공식 문서 linear algebra 참고 자료 : Eigen 공식 문서 Solving linear least squares systems linear algebra - linear solver - [https://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html](https://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html)- SVD - [https://eigen.tuxfamily.org/dox/group__LeastSquares.html](https://eigen.tuxfamily.org/dox/group__LeastSquares.html)- sparse matrix - [https://eigen.tuxfamily.org/dox/group__TutorialSparse.html](https://eigen.tuxfamily.org/dox/group__TutorialSparse.html) - [https://eigen.tuxfamily.org/dox/group__TopicSparseSystems.html](https://eigen.tuxfamily.org/dox/group__TopicSparseSystems.html) - [https://eigen.tuxfamily.org/dox/group__SparseQuickRefPage.html](https://eigen.tuxfamily.org/dox/group__SparseQuickRefPage.html)//// Created by dkssu on 2022-06-12.//// reference : https://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html#include &amp;lt;iostream&amp;gt;#include &amp;lt;Eigen/Dense&amp;gt;void linear_solve(){ // 1. Ax = b ( ColPivHouseholderQr solver ) // refence : https://eigen.tuxfamily.org/dox/classEigen_1_1ColPivHouseholderQR.html // performs rank-revealing QR decomposition of matrix A, permutation matrix P, unitary matrix Q, upper triangular matrix R // A P = Q R // this decomposition performs column pivoting in order to be rank-revealing Eigen::Matrix3f A = Eigen::Matrix3f::Zero(); Eigen::Vector3f b = Eigen::Vector3f::Zero(); A &amp;lt;&amp;lt; 1,2,3, 4,5,6, 7,8,10; b &amp;lt;&amp;lt; 3, 3, 4; Eigen::Vector3f x = A.colPivHouseholderQr().solve(b); std::cout &amp;lt;&amp;lt; &quot;colpivhouseholderQr solution is : \\n&quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl; // we can predefine decomposition constructor size Eigen::HouseholderQR&amp;lt;Eigen::MatrixXf&amp;gt; qr(50,50); Eigen::MatrixXf p = Eigen::MatrixXf::Random(50, 50); qr.compute(p); // no dynamic memory allocation // 2. Ax = b ( LDLT ) Eigen::Matrix2f A2, b2; A2 &amp;lt;&amp;lt; 2, -1, -1, 3; b2 &amp;lt;&amp;lt; 1, 2, 3, 1; Eigen::Matrix2f x2 = A2.ldlt().solve(b2); std::cout &amp;lt;&amp;lt; &quot;LDLT solution is : \\n&quot; &amp;lt;&amp;lt; x2 &amp;lt;&amp;lt; std::endl;};/* this is describes how to solve linear least squares systems using Eigen. define Ax = b, has no solution. it makes to search for the vector x which is closest to being a solution in the difference Ax - b is as small as possible. if Euclidean norm is used, x is called the least square solution. */void least_squares(){ // Ax = b ( normal equations - A_T * A * x = A^T * b ) Eigen::MatrixXf A = Eigen::MatrixXf::Random(3, 2); Eigen::VectorXf b = Eigen::VectorXf::Random(3); Eigen::VectorXf x = (A.transpose() * A).ldlt().solve(A.transpose() * b); std::cout &amp;lt;&amp;lt; &quot;The solution using normal equations is : \\n&quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; std::endl; /* ldlt perform a robust &#39;Cholesky decomposition&#39; of positive or negative matrix A such that A = P^T * L * D * L * P where, permutation matrix P, lower triangular matrix(하삼각행렬) with unit diagonal L, diagonal matrix D cholesky decomposition are not rank-revealing. because using ldlt() instead of LU is lower computation. L^T is upper triangular matrix(상삼각행렬), so U convert to DL^T in &#39;A = LU&#39; therefore, exists lower triangular matrix L, symmetric matrix D for symmetric matrix A reference : https://freshrimpsushi.github.io/posts/ldu-decomposition-of-symmetric-matrix/ */ // Ax = b ( SVD decomposition - BDCSVD ) Eigen::MatrixXf svd_A = Eigen::MatrixXf::Random(3, 2); Eigen::VectorXf svd_b = Eigen::VectorXf::Random(3); std::cout &amp;lt;&amp;lt; &quot;Here is the matrix A : \\n&quot; &amp;lt;&amp;lt; svd_A &amp;lt;&amp;lt; &quot;\\n&quot; &amp;lt;&amp;lt; &quot;Here is the right side vector b : \\n&quot; &amp;lt;&amp;lt; svd_b &amp;lt;&amp;lt; std::endl; std::cout &amp;lt;&amp;lt; &quot;The least squares solution is : \\n&quot; &amp;lt;&amp;lt; svd_A.template bdcSvd&amp;lt;Eigen::ComputeThinU | Eigen::ComputeThinV&amp;gt;().solve(svd_b) &amp;lt;&amp;lt; std::endl; // Ax = b ( QR decomposition ) Eigen::MatrixXf qr_A = Eigen::MatrixXf::Random(3, 2); Eigen::MatrixXf qr_b = Eigen::VectorXf::Random(3); Eigen::MatrixXf qr_x = qr_A.colPivHouseholderQr().solve(qr_b); std::cout &amp;lt;&amp;lt; &quot;The solution using the QR decomposition is : \\n&quot; &amp;lt;&amp;lt; qr_x &amp;lt;&amp;lt; std::endl; // Ax = b ( fullPivLu ) -&amp;gt; check if matrix is singular Eigen::MatrixXd fullLu_A = Eigen::MatrixXd::Random(100, 100); Eigen::MatrixXd fullLu_b = Eigen::MatrixXd::Random(100, 50); Eigen::MatrixXd fullLu_x = fullLu_A.fullPivLu().solve(fullLu_b); double relative_error = ( fullLu_A * fullLu_x - fullLu_b ).norm() / fullLu_b.norm(); // norm is L2 norm std::cout &amp;lt;&amp;lt; &quot;the relative error is : \\n&quot; &amp;lt;&amp;lt; relative_error &amp;lt;&amp;lt; std::endl; // Ax = b ( LLT ) -&amp;gt; separate computation , all decompositions have a compute(matrix) method that does the computation, and that can called again on an already-computed decomposition Eigen::Matrix2f llt_A, llt_b; Eigen::LLT&amp;lt;Eigen::Matrix2f&amp;gt; llt; // predefine size to the decomposition constructor llt_A &amp;lt;&amp;lt; 2, -1, -1, 3; llt_b &amp;lt;&amp;lt; 1, 2, 3, 1; // computing LLT decomposition llt.compute(llt_A); std::cout &amp;lt;&amp;lt; &quot;The solution is:\\n&quot; &amp;lt;&amp;lt; llt.solve(llt_b) &amp;lt;&amp;lt; std::endl; // +1 at (1,1) in A3 matrix llt_A(1,1)++; llt.compute(llt_A); std::cout &amp;lt;&amp;lt; &quot;The solution is:\\n&quot; &amp;lt;&amp;lt; llt.solve(llt_b) &amp;lt;&amp;lt; std::endl;};" }, { "title": "[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install ", "url": "/posts/cmake/", "categories": "Classlog, devcourse", "tags": "devcourse, CMake", "date": "2022-06-11 03:20:00 +0900", "snippet": "CMakeCMake란 cross platform으로 window, ubuntu, MacOS 등 다양한 운영 체제에서 사용이 가능하다. CMake를 활용하여 C++ 작업 공간을 구축할 수 있다. 내용 추가 예정 CMake install IDE : Clion Project name : hello_cmake build tools : WSL2 ubuntu 18.04 terminal : WSL2 ubuntu 18.04Package installDirectoryproject ⊢ CMakeLists.txt ⊢ examples ⊢ example1.cpp ⊢ example2.cpp ⊢ ... ⊢ modules ⊢ module1 ⊢ module1 ∟ include ∟ Class.hpp ⊢ src ∟ Class.cpp ∟ CMakeLists.txt ⊢ module2 ⊢ module2 ∟ include ∟ Class.hpp ⊢ src ∟ Class.cpp ∟ CMakeLists.txt ⊢ ... ⊢ main.cpp ∟ thirdparty ⊢ OpenCV ⊢ build ⊢ install ∟ opencv ∟ Eigen ⊢ build ⊢ install ∟ eigen OpenCV installOpenCV를 사용하기 위해서는 CMake 작업 공간에 OpenCV 라이브러리를 구성해야 한다.hello_cmake$ mkdir thirdParty &amp;amp;&amp;amp; cd thirdPartythirdParty$ mkdir -P OpenCV/build &amp;amp;&amp;amp; mkdir -P OpenCV/installthirdParty$ cd OpenCVOpenCV$ git clone https://github.com/opencv/opencv.gitOpenCV$ cd buildbuild$ cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=../install/ ../opencv/build$ make -j4build$ sudo make install이렇게 OpenCV 라이브러리를 설치할 깃허브 레포지토리를 클론하고, 디렉토리를 구축한다. 이 때, cuda와 같은 contrib 를 함께 사용하고자 한다면, 다음과 같이 설정해야 한다.build$ export &quot;pathToPython=&amp;lt;python Path&amp;gt;&quot; &amp;amp;&amp;amp; export &quot;pyVer=39&quot;build$ cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=../install/ \\-DOPENCV_EXTRA_MODULES_PATH=&quot;C:/opencv/opencv_contrib/modules&quot; \\-DBUILD_opencv_world=ON -DWITH_CUDA=ON \\-DCUDA_TOOLKIT_ROOT_DIR=&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.6&quot; \\-DCUDA_FAST_MATH=ON -DWITH_CUBLAS=ON -DCUDA_ARCH_BIN=8.6 -DWITH_NVCUVID=ON \\-DWITH_OPENGL=ON -DWITH_MFX=ON -DBUILD_opencv_python3=ON \\-DPYTHON3_INCLUDE_DIR=%pathToPython%/include \\-DPYTHON3_LIBRARY=%pathToPython%/libs/python%pyVer%.lib \\-DPYTHON3_EXECUTABLE=%pathToPython%/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=%pathToPython%/lib/site-packages/numpy/core/include \\-DPYTHON3_PACKAGES_PATH=%pathToPython%/Lib/site-packages/ -DOPENCV_SKIP_PYTHON_LOADER=ON \\-DOPENCV_CONFIG_PATH=C:/opencv/opencv/install/x64/vc17/bin ../opencv/pathToPython에는 자신의 python.exe 경로를 넣으면 된다. 그리고, pyVer도 본인의 python 버전에 맞게 넣는다.Eigen installthirdParty$ mkdir Eigen3thirdParty$ cd Eigen3Eigen3$ mkdir buildEigen3$ mkdir installEigen3$ git clone https://gitlab.com/libeigen/eigen.gitEigen3$ cd buildbuild$ cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=../install ../eigen/build$ make -j4build$ sudo make installPangolin install 참고 자료thirdParty$ mkdir pangolin &amp;amp;&amp;amp; cd pangolinpangolin$ git clone https://github.com/stevenlovegrove/Pangolin.gitpangolin$ sudo apt install build-essentialpangolin$ sudo apt install libgl1-mesa-devpangolin$ sudo apt install libglew-devpangolin$ mkdir build &amp;amp;&amp;amp; mkdir installpangolin$ cd build 이렇게 작업공간을 다 생성한 후에는 cmake build를 진행한다. 동일하게 make -j에서 4는 코어 수이므로 자신의 컴퓨터에 맞게 설정해주면 된다.build$ cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=../install/ ../Pangolin/build$ make -j4build$ sudo make install  CMakeLists.txt project/CMakeLists.txtcmake_minimum_required(VERSION 3.22)project(hello_cmake LANGUAGES CXX)set(CMAKE_CXX_STANDARD 14) # set은 어떤 변수에 값을 넣어주는 것, CMAKE_CXX_STANDARD에 14를 넣는다.set(CMAKE_CXX_STANDARD_REQUIRED ON) # CMAKE_ 는 종복 변수, 14를 넘어가는 기능들은 빌드가 되지 않도록 함.add_subdirectory(modules)add_executable(hello_cmake main.cpp) # 실행파일을 의미한다. 이런 실행파일을 만들 instructure를 추가하겠다. # add_executable(실행파일의 이름, 프로그램을 만드는데 필요한 소스코드)# examples/exec_module1.cpp 를 실행하는 코드add_executable(exec_module1 examples/exec_module1.cpp )# exec_module1에 library를 링크target_link_libraries(exec_module1 PUBLIC module1 )add_executable(exec_module1_module2 examples/exec_module1_module2.cpp)target_link_libraries(exec_module1_module2 PUBLIC module1 module2 )  project/modules/CMakeLists.txtadd_subdirectory(module1)add_subdirectory(module2)  project/modules/module1/CMakeLists.txtcmake_minimum_required(VERSION 3.22)cmake_policy(SET CMP0111 OLD)project(module1 LANGUAGES CXX)set(CMAKE_CXX_STANDARD 14)set(CMAKE_CXX_STANDARD_REQUIRED ON)set(MODULE1_SOURCE_FILES # name src/ClassMat.cpp )add_library(module1 # library name ${MODULE1_SOURCE_FILES} # add all files in MODULE1_SOURCE_FILES )# OpenCV packagefind_package(OpenCV REQUIRED HINTS ${CMAKE_SOURCE_DIR}/thirdParty/OpenCV/install/lib/cmake/opencv4)# if find, print directoryif (OpenCV_FOUND) message(STATUS &quot;OpenCV Found! - ${OpenCV_DIR}&quot;)endif()# header file directorytarget_include_directories(module1 PUBLIC include # directory ${OpenCV_INCLUDE_DIRS} )# library link - library link in module1target_link_libraries(module1 PUBLIC ${OpenCV_LIBS} )  project/modules/module2/CMakeLists.txtcmake_minimum_required(VERSION 3.22)project(module2 LANGUAGES CXX)set(CMAKE_CXX_STANDARD 14)set(CMAKE_CXX_STANDARD_REQUIRED ON)set(MODULE2_SOURCE_FILES # name src/ClassEigenMat.cpp )add_library(module2 # library name ${MODULE2_SOURCE_FILES} # add all files in MODULE1_SOURCE_FILES )find_package(Eigen3 REQUIRED HINTS ${CMAKE_SOURCE_DIR}/thirdParty/Eigen3/install/share/eigen3/cmake) # .cmake 파일 위치if (Eigen3_FOUND) message(STATUS &quot;Eigen3 Found! - ${Eigen3_DIR}&quot;) set(Eigen3_LIBS Eigen3::Eigen)endif()target_include_directories(module2 PUBLIC # header file directory include # directory ${Eigen3_INCLUDE_DIRS} )target_link_libraries(module2 PUBLIC # library link ${Eigen3_LIBS} )  Pangolin 에러 GLEW 오류cmake 시 Could not find GLEW 라는 에러가 발생할 수 있다. 이에 대해 libglew-dev 를 설치해서 해결해준다.sudo apt install libglew-dev  so파일 오류 LD_LIBRARY_PATH 추가.bash_profile에 LD_LIBRARY_PATH를 추가해서 경로를 추가한다.$ sudo gedit ~/.bash_profileLD_LIBRARY_PATH=/mnt/c/Users/dkssu/CLionProjects/Trajectory/thirdparty/Pangolin/install/lib$ source ~/.bash_profile echo $LD_LIBRARY_PATH/mnt/c/Users/dkssu/CLionProjects/Trajectory/thirdparty/Pangolin/install/lib 이렇게 환경변수가 등록되었다면, 이 경로를 실제 시스템에 등록한다.sudo ldconfig 나의 경우는 이렇게 해도 실행이 되지 않아서 상대경로를 실제 경로로 수정했다.std::string trajectory_file = &quot;/mnt/c/Users/dkssu/CLionProjects/Trajectory/examples/trajectory.txt&quot;;  PlotTrajectory 예시github   ld.so.conf 수정1번 방법이 안된다면 ld.so.conf 파일을 수정한다.$ more /etc/ld.so.confinclude /etc/ld.so.conf.d/*.conf이 값은 /etc/ld.so.conf 파일에 설정되어 있는 값이다. .conf 에 해당하는 모든 파일의 내용이 포함되어 있다. 이에 대해 새로운 라이브러리의 경로를 추가해줘야 한다. 일단 프로젝트에 필요한 *.so 파일을 확인한다.$ ls thirdparty/Pangolin/install/libcmake libpango_glgeometry.so libpango_plot.so libpango_vars.solibpango_core.so libpango_image.so libpango_python.so libpango_video.solibpango_display.so libpango_opengl.so libpango_scene.so libpango_windowing.solibpango_geometry.so libpango_packetstream.so libpango_tools.so libtinyobj.so이 경로를 /etc/ld.so.conf에 추가해준다.$ cd /etc/ld.so.conf &amp;amp;&amp;amp; sudo gedit pangolin.conf/mnt/c/Users/dkssu/CLionProjects/Trajectory/thirdparty/Pangolin/install/lib그 후, ldconfig 명령어로 시스템에 등록한다.$ sudo ldconfig  reference so 파일 오류 https://m.blog.naver.com/younguk0907/222084800809 https://the-masked-developer.github.io/wiki/no-so-file/ " }, { "title": "[데브코스] 16주차 - Visual-SLAM MAP and Non-linear Optimizer, Loop Closure ", "url": "/posts/optimizer/", "categories": "Classlog, devcourse", "tags": "devcourse, Visual-SLAM", "date": "2022-06-01 20:20:00 +0900", "snippet": "이때까지는 Projection에 대해 공부했다. 이제 SLAM으로 다시 돌아와보자.Least SquaresSLAM이란 Simultaneous Localization And Mapping으로 동시적 위치 추정 및 지도 작성이다. 최소자승법(Least Squares)은 SLAM 문제를 풀 때 사용하는 최적화 방법론이다. 최적화란 어떤 방정식에서 완벽하게 떨어지는 정답값을 찾지 못할 때 에러가 가장 작은 값을 찾아내는 기법이다. least squares는 에러에 제곱 연산을 통해 에러가 크면 클수록 기하급수적으로 커지게 된다.least square는 Over-determined-system에 해당하는데, 즉 미지수의 개수보다 방정식의 개수가 더 많은 시스템을 의미한다. SLAM에서는 항상 over-determined-system에 해당한다. 그 이유는 뽑고자 하는 feature의 수보다 센서 데이터가 항상 많기 때문이다.원래의 GT값과 비교해서 우리가 추정한 값과의 차이를 error라 하고, 이 error들의 합이 최소가 되도록 하는 값들을 찾는 것이 최적화라 할 수 있다.Maximum-a-posteriori estimation is SLAMsensor데이터에는 2가지 종류가 있다. proprioceptive sensing, exteroceptive sensing이고, 각각에 대한 수식은 motion model, obsevation model이 있다. 먼저 전자의 수식에 대해 알아보자. motion modelmotion model은 현재 위치에 대한 state를 추정하는 수식이고, 현재 state, $ x_k^r $에 대한 값은 다음과 같다.$ x_k^r = \\begin{bmatrix} x^r(k) \\ y^r(k) \\ \\theta^r(k) \\end{bmatrix} $$ x_k^r $ 는 지난 프레임에서의 state, $ x_{k-1}^r $와 관측한 odometry 또는 control 값, motion noise값 3개를 입력으로 받아 구한다. observation modelobservation model은 landmark의 위치에 관한 state를 추정하는 수식이고, landmark와의 거리에 대한 값 $ d_{z_k} $, 각도에 대한 값 $ \\theta_{z_k} $의 수식은 다음과 같다.$ d_{z_k} = \\sqrt{(x^r(k) - l_i^x)^2 - (y^r(k) - l_i^y)^2} + \\epsilon_z $$ \\theta_{z_k} = atan2(l_i^y - y^r(k), l_i^x - x^r(k)) - \\theta^r(k) + \\epsilon_z $이 때, observation model에 대한 값은 motion model의 값 $ x_k^r $과 $ l_i, \\epsilon_z $을 입력으로 하여 구한다. 이 때, $ l_i $는 차량에 대한 상대적인 위치를 나타내고, $ \\epsilon_z $는 노이즈에 해당한다.거리는 간단한 거리를 구하는 수식에 노이즈를 더한 값이고, 각도의 경우도 x방향, y방향으로의 삼각함수를 사용하여 구한다.SLAM state의 관점으로 본다면, SLAM에서의 현재 State는 1개의 robot pose와 다수의 관측 가능한 landmark의 위치를 담고 있을 것이다. 다음 프레임으로 가면 1개의 SLAM state가 하나 더 생길 것이고, 여러 개의 SLAM state를 통해 노이즈를 제거해줄 수 있다. 그러나 여러 개의 state가 생기면 노이즈도 그만큼 증가한다. 따라서 시간이 지날수록 노이즈가 증가해서 Uncertainty가 증가한다.증가하는 노이즈의 패턴 분석을 통해, motion model과 observation model을 기반으로 노이즈 누적에도 안정적인 최적의 robot pose와 최적의 landmark위치를 추정하고자 한다.지금까지 쌓아온 데이터의 확률 분포와 현재 설정한 최적의 파라미터에 대해 실제 데이터가 믿음직한지에 대한 판단을 통계학적으로 belief라고 한다.센서 데이터는 관측 후에 나타나는 사후 데이터이므로 이 사후 데이터를 가장 잘 표현하는 확률을 찾는 것, 즉 최대 사후 확률(Maximum a posteriori, MAP)를 찾는 것이 최종적인 목표이다. 다르게 표현하면 지금까지의 motion model과, observation model에 대한 확률 분포를 찾고, 그 확률 분포를 정확하게 표현하는 state 값을 찾는 것이 목표이다. 그를 위해 노이즈를 가장 최소가 되는 확률 분포를 찾아야 한다.MAP estimation을 푸는 방법을 알아보자. 최적의 state, x를 구해야 한다. motion modelproprioceptive 데이터(odometry)를 u = {u_1,u_2,…,u_k}라 하고, motion model을 f라 했을 때, 현재 state, x_k는 다음과 같다.$ x_k = f(x_{k-1}, u_k, \\epsilon_u) $이 때, $\\epsilon_u$는 노이즈인데, 우리가 하고자 하는 것은 노이즈가 가장 작은 확률 분포를 찾아야 하므로 0이라 가정하고 식을 세우면 다음과 같다.$ x \\approx f(x_{k-1, u_k}) =&amp;gt; x_k - f(x_{k-1}, u_k) \\approx 0 $ (1) observation modelexteroceptive 데이터(landmark의 상대적 위치)를 z = {z_1,z_2,…,z_k}라 하고, observation model을 h라 했을 때, landmark와의 거리와 방향, z_{k,j}는 다음과 같다.$ z_{k,j} = h(x_k, l_j, \\epsilon_z) \\; =&amp;gt; \\; z_{x_k,l_j} \\approx h(x_k, k_j) \\; =&amp;gt; z_{k,j} - h(x_k,l_j) \\approx 0 $ (2)이 둘의 식은 결국 error값을 의미하므로, 이 두 식을 활용하여 최적의 state를 찾는다.\\[argmin_{x_0,...,x_k} \\sum_{i=1}^k \\| x_i - f(x_{i-1}, u_i) \\|^2 + \\sum_{(i,j) \\in \\beta } \\| z_{i,j} - h(x_i, l_j) \\|^2\\]least square 기법을 사용하기 때문에 각 error에 제곱했다. 위의 식을 통해 error의 최소값이 아닌 최소가 되는 state를 구한다.Graph-based SLAMSLAM에는 두가지 방법론이 있다. Incremental SLAM (i.e. Online SLAM) Filter이 적용된 SLAM Particle filter, Kalman filter Batch SLAM (i.e. Offline SLAM) Graph-based SLAM Incremental SLAMIncremental SLAM의 가장 큰 특징은 가장 최근 state만 추정한다는 것이다. Markov chain assumption을 기본 전제로 하여 바로 이전의 state와 observation 정보만 있으면 새로운 state를 계산한다. 즉 새로운 state는 이전 state에 의존한다.따라서 계산이 간단하므로 굉장히 빠르게 연산이 된다. 가장 최신 정보를 출력하기 때문에 실시간으로 사용할 수 있다 하여 online SLAM이라고도 불렸다.Batch SLAMIncremental SLAM과는 달리 Batch SLAM은 여러 시점에서의 state들을 한번에 추론한다. 그래서 전체적인 연산량이 증가하여 과거에 offline SLAM이라고 불렸다. 하지만 현대에는 다양한 트릭을 사용해서 Batch SLAM을 실시간으로 사용이 가능하다.Incremental SLAM이 Batch SLAM보다 빠르게 동작하지만 정확도가 다소 떨어진다. 그러나 최근들어 한 논문에서 Batch SLAM이 Incremental SLAM보다 빠르다 하여 현재는 거의 대부분이 Batch SLAM을 사용한다. Factor graphgraph를 다루는 여러 가지 방식이 연구되다가 Factor graph방식이 생겨났다. Factor graph는 노드와 edge로 이루어져 있는 graph로, 노드에는 robot state나 landmark state가 저장되고, edge에는 motion model 정보나 observation model 정보가 저장된다. 이 edge에 저장되는 형태를 Factor라 부른다.Factor graph는 특정 노드에 연결되는 Factor들의 error를 통해 최적화한다. Graph-based SLAMFactor graph의 사용보다 least squares 를 쉽게 적용할 수 있는 방식으로 Graph-based SLAM 방식이 생겨났다. graph로 표현함으로써 얻게 되는 장점은 robot pose와 observation 정보를 쉽게 파악할 수 있다는 점이고, 그로 인해 graph안에 loop가 생겨난다. 이 loop를 최적화함으로써 loop속에 누적되는 uncertainty를 제거하고, loop안에 있는 모든 node에 대한 최적의 값을 찾을 수 있게 되었다. loop가 발생하게 되면 error를 해소시켜줌으로써 루프를 완성시켜줄 수 있다. 이를 Loop closure이라 한다.그리고, SLAM에서의 파이프라인이 생겨났다. Front-end에서는 node와 edges를 생성했고, back-end에서는 graph를 최적화했다.Bundle Adjustment이미지 출처 - 장형기님 블로그마찬가지로 지난 글에서의 Triangulation을 배웠는데, 이는 2view geometry에 대한 내용이었다. Bundle Adjustment는 한 단계 더 나아가 N-view geometry에 대한 내용이다. N개의 프레임 또는 N개의 카메라가 존재하고, 그에 따른 각각의 Rotation, translation이 존재한다. 이 때, 서로의 2D-2D correspondence를 공유하며, 3d point인 landmark에 대한 거리도 공유하고 있다고 가정한다. 그리고 1개의 landmark마다 2개 이상의 2D-3D correspondence를 가지고 있다.모든 값들이 다 계산이 되어 있지만, 매 프레임마다 motion model 정보와 observation model 정보에 노이즈가 계속 누적되므로 이를 처리하기 위해 Batch SLAM기법을 적용한다. 2D-2D correspondence, 2D-3D correspondence 등의 값들을 활용해서 camera pose, 3D landmark position를 보정해주는 작업을 Bundle Adjustment라 한다. 보정을 한다는 것은 graph 최적화를 통해 uncertainty를 해소해준다는 것을 의미한다. graph 최적화를 위해서는 위에서 배웠듯이 motion model에서의 error, observation model에서의 error이 필요하다. VSLAM에서는 이 두가지의 error를 ReProjection Error 하나로 간편하게 표현이 가능하다. landmark를 image plane에 재투영했을 때 생기는 error는 pixel 단위로 표현된다.3D landmark position과 camera pose가 완벽한 값이라고 가정하면 3D landmark를 image plane에 투영했을 때는 정확한 keypoint위치로 맞아떨어지겠지만, 모든 센서는 노이즈를 가지고 있기 때문에 조금의 오차가 발생한다. 이 때 reprojection error에 대한 function이 $ \\pi $이고, landmark를 image plane으로 투영한 위치와 원래 keypoint와의 오차를 $ \\triangle z_{ij} $에 해당할 때 오차가 제일 작아지는 keypoint는 다음과 같이 표현할 수 있다. landmark에서의 좌표를 P로, image plane에서의 좌표를 C로 표현되어 있다. $ argmin_x \\sum_i \\sum_j   x_{ij} - \\pi(P_j,C_i)   {w{ij}}^2 $ landmark재투영에 대한 error도 있지만, camera pose나 motion model, observation model에 대한 error도 구해줄 수 있다.최적화한다는 것은 결국 total reprojection error를 최소화되는 최적의 camera pose와 landmark를 찾는 것이다. 그러나 image projection 과정이 linear하지 않기 때문에 단순 미분을 통한 최적화가 불가능하다. 그래서 비선형 공간에서 선형 공간으로 근사화시켜서 최적화를 수행하려고 한다. 그 방법으로 Gauss-Newton method, Levenberg-Marquardt method가 있다.Nonlinear optimizationNon-linear 최적화에 사용되는 대표적인 방법들 중 Gauss-Newton method를 사용해보고자 한다.Gauss-Newton method우선 BA를 푸는데 필요한 파라미터의 개수는 총 3D landmark position (X,Y,Z = 3) + Extrinsic parameter (Rx,Ry,Rz, tx,ty,tz = 6) + intrinsic parameter (fx,fy, cx,cy,s = 5) + scale factor (1)로 1개의 3D landmark당 15개의 파라미터를 가진다.이것들을 State 벡터로 표현하면 다음과 같다.landmark state $ x_l = \\begin{bmatrix} X \\ Y \\ Z \\end{bmatrix} $camera state $ x_{cam} = \\begin{bmatrix} t_x ,\\ t_y, \\ y_z, \\ R_x, \\ R_y, \\ R_z, \\ \\lambda, \\ f_x, \\ f_y, \\ c_{x}, \\ c_y ,\\ s \\end{bmatrix}^T $x_l은 3D landmark에 대한 값, x_cam은 extrinsic matrix와 scale값, intrinsic matrix로 이루어져 있다.$ State \\; Vector \\;\\; x = \\begin{bmatrix} x_{cam}, \\ x_{l_1}, \\ x_{l_2}, \\cdots \\ ,x_{l_n} \\end{bmatrix}^T $그렇다면 reprojection error를 최소화하는 State vector는 어떤 값을 가지는가에 대한 것이 BA 최적화이고, 이 문제를 풀기 위해 노이즈가 가우시안 분포를 가지는 least squares optimization을 수행할 것이다. 가우시안으로 가정하는 이유는 가우시안 노이즈 데이터를 사용할 경우 간편한 Maximum Likelihood Estimation로 바뀌게 되고, MLE는 최적의 값을 가지는 것이 가능하기 때문이다.아까 사용하던 least square 식에 가우시안 노이즈 분포를 추가해준다.$ argmin_x \\sum_i \\sum_i | e_i(x) | \\; =&amp;gt; \\; argmin_x \\sum_i \\sum_j | e_i(x)^T \\Omega_i e_i(x) | $이 때, 오메가는 covariance matrix 또는 information matrix라고 부른다.최적의 값을 x*라고 했을 때, $ x^{*} = argmin_x E(x) $라 표현하고, 위치 x 에서 x*로의 변화량을 $ \\triangle x $라 했을 때,$ \\triangle x = x^{*} - x $ 이고, $ argmin_x $ 대신 $ argmin_{\\triangle x}$ 에 대한 값으로 정리하면 다음과 같다.$ x^{*} = argmin_{\\triangle x} E(x + \\triangle x) $그런데, $ E(x) = \\sum_i | e_i{x}^T \\Omega_i e_i(x) | $이므로 최종적인 x*는 다음과 같다.$ x^{*} = argmin_{\\triangle x} \\sum_i | e_i(x_i + \\triangle x)^T \\Omega_i e_i(x_i+\\triangle x) | $$ e_i(x_i + \\triangle x) $ 가 non-linear하기 때문에 linear하게 근사시키기 위해 미분을 해준다. 미분을 할 때는 테일러 급수를 사용한다.$ \\cfrac{\\partial (e_i(x_i + \\triangle x))}{\\partial x} \\approx e_i(x) + J_i \\triangle x =&amp;gt; e_i + J_i \\triangle x $J는 Jacobian을 뜻하고, 간편하게 표현하였다. 그래서 다시 x*에 대한 식으로 넘어와 대입해주면 다음과 같다.$ x^{*} = argmin_{\\triangle x} \\sum_i [e_i + J_i \\triangle x]^T \\Omega_i [e_i + J_i \\triangle x]$$ x^{*} = argmin_{\\triangle x} (\\sum_i e_i^T\\Omega_i e_i) + 2(\\sum_i e_i\\Omega_i J_i)\\triangle x + \\triangle x^T(\\sum_i J_i^T \\Omega_i J_i)\\triangle x $이 때, $ (\\sum_i e_i^T\\Omega_i e_i) = C \\;,\\; \\sum_i e_i\\Omega_i J_i = b^T \\;,\\; \\sum_i J_i^T \\Omega_i J_i = H $라고 가정한다면,$ x^{*} = argmin_{\\triangle x} C + 2b^T\\triangle x + \\triangle x^T H \\triangle x $이므로, 이는 $\\triangle x$에 대한 2차 방정식으로 간주할 수 있다.2차 방정식으로 생각하면 최적화가 간단해진다. 2차방정식은 미분을 해서 0이 되는 값은 최대 또는 최소이다. 그러나 위의 식에서는 항상 최소가 나오게 되어있다.$ \\triangle x$ 에 대해 미분을 하면$ \\cfrac{\\partial}{\\partial \\triangle x} [C + 2b^T \\triangle x + \\triangle x^T H \\triangle x] = 2b + 2H\\triangle x = 0 $따라서 $ \\triangle x = - H^{-1}b $가 되고, $ \\triangle x $의 의미는 현재 위치 x_0에서 최적의 x로 가기 위한 변화량을 의미한다.이 때의 최적의 x는 global한 최적값이 아니라 local minia에 해당한다. 위의 식이 의미하는 바는 현재의 값보다 낮은 값으로 찾아갈 것이고, 이를 통해 시간이 지속될수록 점차 minumum에 다가가는 것을 확인할 수 있다.그러나 현실적으로는 H matrix가 굉장히 크기 때문에 계산하는 것은 매우 복잡하다.이미지 출처 : cyrill stachniss 교수님 lectures2만장의 이미지가 있을 때, 각각의 이미지마다 18개의 feature를 뽑고, 각각의 landmark는 3번 정도 관찰된다고 가정해보자. 이런 경우 Jacobian matrix 1개가 3.5 x 10^11 개의 값이 존재한다. 이를 inverse하는 것은 불가능하다.이미지 출처 : cv-learn.com 블로그 글그래서 matrix의 특성을 이용해보고자 한다. Jacobian matrix를 살펴보면 대부분의 element가 비어있다. 그 이유는 factor들마다의 연관성을 미분한 matrix인데, node들은 인접한 것과는 연결이 되겠지만, 멀리 떨어져 있는 것과는 거의 연결이 되어 있지 않다. 거의 비어있는 matrix를 sparsity matrix라고 부른다.b matrix의 경우도 Jacobian이 거의 비어있으면 b도 거의 비어있게 출력된다. H matrix도 동일하게 Jacobian matrix가 거의 비어있어서 비어있는 matrix가 형성된다.그러나 식을 보면 전체의 합으로 구성되어 있으므로 b는 sparsity matrix가 모여 dense한 matrix가 만들어지지만, H는 대칭적인 형태가 구성되어 여전히 sparsity한 특성을 띈다.H의 sparsity한 특성을 활용하여 inverse matrix를 빠르게 구할 수 있을 것이다. 그러나 좋은 방법은 아니다.Schur ComplementH matrix에 대해 sparsity한 특성을 활용하는 것이 아닌 H matrix의 형태를 분석해서 연산하는 방법이 있다.H matrix의 형태는 4가지 구조로 나뉘어져 있다. 그래서 H를 4가지로 나누어서 표현할 수 있다. A는 Camera에 대한 정보로 H_S, C는 3D structure에 대한 정보로 H_C로 표현된다. B는 H_SC를 의미한다.$ \\begin{bmatrix} H_S \\ H_{SC} \\ H_{SC}^T \\ H_C \\end{bmatrix} \\begin{bmatrix} \\delta_S \\ \\delta_C \\end{bmatrix} = \\begin{bmatrix} \\varepsilon_S \\ \\varepsilon_C \\end{bmatrix}$1행의 값들이 3D structure에 대한 정보, 2행의 값들이 Camera Parameter에 대한 정보이다. 그 후 양변에 특정 행렬을 곱해 H_SC^T를 제거한다.$ \\begin{bmatrix} H_S \\ H_{SC} \\ H_{SC}^T \\ H_C \\end{bmatrix} \\begin{bmatrix} \\delta_S \\ \\delta_C \\end{bmatrix} \\begin{bmatrix} I \\ 0 \\ -H_{SC}^TH_S^{-1} \\ I \\end{bmatrix} = \\begin{bmatrix} \\varepsilon_S \\ \\varepsilon_C \\end{bmatrix}\\begin{bmatrix} I \\ 0 \\ -H_{SC}^TH_S^{-1} \\ I \\end{bmatrix} $$ \\begin{bmatrix} H_S \\ H_{SC} \\ 0 \\ H_C - H_{SC}^TH_S^{-1}H_{SC} \\end{bmatrix} \\begin{bmatrix} \\delta_S \\ \\delta_C \\end{bmatrix} = \\begin{bmatrix} \\varepsilon_S \\ \\varepsilon_C - \\varepsilon_S H_{SC}^TH_S{-1} \\end{bmatrix}$이로써 2행의 값들로만 방정식을 풀 수 있게 되었다.$ (H_C - H_{SC}^T H_S^{-1}H_{SC})\\delta_C = \\varepsilon_S H_{SC}^T H_S{-1}$이 때, 좌항의 괄호 안에 있는 식을 Schur Complement 식이라 한다.H_C와 H_SC는 우리가 가지고 있는 값이다. $ H_S^{-1} $는 안의 블록들이 3x3 diagonal matrix로 이루어져 있어 구하기 쉽다. 그러면 또 다시 Ax = b 의 구조를 가지게 된다.그러나 좌항의 것들을 그대로 우항을 넘기기는 쉽지 않다. 그래서 Schur complement를 1개의 큰 matrix로 만든 후 inverse 하기 편한 matrix로 분해한다. 분해하는 방법으로는 LU decomposition 또는 Cholesky decomposition을 많이 사용한다.그 후 이렇게 구한 $ \\delta_C $를 통해 $ \\delta_S $를 구하면 된다.$ \\delta_S = H_S^{-1}(\\varepsilon_C - H_{SC}^T \\delta_C)$여기서 $ \\delta_C $의 의미는 우리가 가지고 있는 H matrix의 카메라 파라미터들이 어떻게 바뀌어야 $ \\delta_S $와 같은지에 대한 값이고, $ \\delta_S $는 3D landmark position이 얼마나 바뀌어야 $ \\delta_C $와 같아지는지에 대한 값이다.Outlier rejectionleast squares를 할 때 조심해야 할 부분이 있다. least squares 알고리즘이 outlier에 매우 취약한 알고리즘이다. 따라서 least squares를 사용할 때는 M-estimator 커널과 같은 기법을 사용하여 데이터 분포속에서 outlier를 optimization 식에서 제외시켜야 한다.Nonliear optimization LibrariesSLAM에서 optimization을 수행하기 위해 이때까지 배운 복잡한 과정들을 library에 잘 정리되어 있다. Google Ceres-solver G2o GTSAM" }, { "title": "[데브코스] 16주차 - Visual-SLAM triangulation and perspective ", "url": "/posts/triangulation/", "categories": "Classlog, devcourse", "tags": "devcourse, Visual-SLAM", "date": "2022-06-01 15:20:00 +0900", "snippet": "Triangulation지난 글에서 F-matrix와 E-matrix를 구하는 방법을 배웠고, 이 matrix를 분해하면 두 이미지 간의 translation과 rotation matrix를 구할 수 있었다. 이 후의 작업으로 3D 구조를 복원해주는 mapping 단계가 수행되어야 한다. mapping의 첫단계가 triangulation 이다.Triangulation이란 두 개의 이미지 사이에서 translation 값과 두 이미지들간의 correspendence를 알고 있을 때, feature들이 의미하는 실제 x,y,z값을 복원하는 과정이다. 이 과정이 결국 2D에서 3D point로의 변환이다.위의 그림은 2개의 3D point를 각각의 ray로 나타낸 것으로 두 쌍의 correspendence를 triangulation을 통해 3d Point를 구한 후, 2개의 point 간의 거리 값도 추정을 할 수 있다.현실에서는 2개의 카메라가 1개의 point를 본다고 해도 노이즈가 껴있기 떄문에 두 이미지 간의 오차가 존재할 것이다. 그러면 두 카메라에서의 point 두 개를 평균내서 가상의 점으로 추정하도록 만들어야 한다.이 때, F와 G를 구하는 식은 다음과 같다.$ F = p + \\lambda r $$ G = q + \\mu s$$ \\lambda $와 $ \\mu $ 는 image point, P와 Q에 대한 3D point로 가는 방향 벡터이고, r과 s는 방향 벡터에 대한 스케일이다. 그리고 p와 q는 camera center를 의미한다.즉, 시작 위치에서 방향벡터와 스케일을 곱한 것을 더하면 3d point를 구할 수 있다. 이 때, p와 q는 우리가 아는 값이고, r과 s는 다음과 같이 구할 수 있다.$ r = R’^T x’^k \\; ,\\color{gray} with \\;\\; x’^k = (x’,y’,c)^T $$ s = R’‘^T x’‘^k \\; ,\\color{gray} with \\;\\; x’‘^k = (x’‘,y’‘,c)^T $$ x’^k $ 는 calibration된 P 카메라에 대한 camera coordinate에서의 좌표를 의미하고, $ x’‘^k $ 는 calibration된 Q 카메라에서의 2d point를 의미한다. 여기서 x’,y’는 정확한 pixel 위치, c는 focal length이다. R&#39;은 P에 대한 world coordinate에서 camera coordinate로의 옮기는 matrix를 의미한다. 그에 대한 T는 camera coordinate에서 world coordinate로 변환하는 matrix이다.그 후, 현재 2개의 3D ray는 교차하지 않는다고 가정했기 때문에, H를 구해줘야 한다. 기하학적으로 봤을 때, 점 F와 G를 잇는 선은 P ray와 Q ray에 대해 가장 가깝게 그은 선이므로 수직을 이룬다.따라서, 수직을 이루는 두 직선에 대한 수식을 활용하여 H를 구할 수 있다.$ (f - g) \\cdot r = 0 \\;\\; (f - g) \\cdot s = 0 $$ (p + \\lambda r - q - \\mu s) \\cdot r = 0 $$ (p + \\lambda r - q - \\mu s) \\cdot s = 0 $이 때, 우리는 p,q,r,s 를 알고 있기 때문에 미지수는 $\\lambda,\\mu$ 2개 뿐이다. 따라서 2개의 식을 활용하여 미지수를 모두 구해낼 수 있다. 그러면 결국 F,G를 구해낼 수 있고, 이 두 점을 잇는 line을 그리고 그 중간값을 계산하면 H를 구할 수 있다. 그러나 이 때 굳이 중간점을 사용하지 않아도 F와 G의 불확실성을 구하여 weight를 지정해줄 수 있다. 예를 들어 확률적으로 GT값이 P와 더 가깝다면 F의 가중치를 0.5대신 0.7로 하여 H를 구할 수도 있다.위의 식을 조금 더 풀어보도록 하자. p와 q는 각각 camera center 즉, 카메라 원점을 의미한다. 이를 X_O로 표현하고자 한다.$ (X_{O’} + \\lambda r - X_{O’’} - \\mu s)^T r = 0 $$ (X_{O’} + \\lambda r - X_{O’’} - \\mu s)^T s = 0 $이고, 이를 정리하면 간단한 식이 만들어진다.$ \\begin{bmatrix} (r^Ts - s^Tr) \\ (r^Ts - s^Ts) \\end{bmatrix} \\begin{bmatrix} \\lambda \\ \\mu \\end{bmatrix} = \\begin{bmatrix} (X_{O’’} - X_{O’})^T \\ (X_{O’’} - X_{O’})^T \\end{bmatrix} \\begin{bmatrix} r \\ s \\end{bmatrix} =&amp;gt; Ax = b =&amp;gt; x = A^{-1}b $Stereo Triangulation이는 두 카메라가 함께 전방을 바라보는 상황을 의미한다.이 둘 간의 baseline을 B, 3D world에서의 좌표 P가 각각의 이미지에 x&#39;, x&#39;&#39;으로 매핑된다. 각각의 feature x&#39;,x&#39;&#39;의 거리를 Parallax 또는 Disparity라 한다.위의 그림을 위에서 바라보면 다음과 같은 그림이 만들어진다.이 때, 3D point, P와 왼쪽 카메라의 카메라 원점 O’에 대한 ray를 O’‘으로 옮겨서 삼각형 O’‘PA을 만든다. c : focal length B : baseline (x’ - x’’) : disparity삼각비를 활용하여 Z값을 구해줄 수 있다.$ Z : c = B : (x’ - x’’) \\; =&amp;gt; \\; \\cfrac{Z}{c} = \\cfrac{B}{x’ - x’’} \\; =&amp;gt; \\; Z = c \\cfrac{B}{-(x’’ - x’)}$이 때, c와 B는 상수이므로 x’‘와 x만 알아도 Z값을 구할 수 있다. 동일하게 X방향도 추정이 가능하다.$ Z : c = X : x’ \\; =&amp;gt; \\; X = x’ \\cfrac{B}{-(x’‘-x’)} $이전에 Z를 구했으면 값을 그대로 집어넣으면 되고, 구하지 않았더라도 이전의 식을 그대로 사용하여 X를 구할 수 있다.Y방향도 동일하게 구해줄 수 있을 수도 있지만, 이는 틀린 가정이다. 원래 두 카메라의 feature point가 F와 G로 완벽하게 동일하지 않았다. 그런 상황에서 X를 동일하게 맞추고 계산을 했으므로 Y는 당연히 동일하지 않는다. 반대로 Y를 먼저 구하기 위해 동일시해서 계산을 한다면 X방향의 값이 동일하지 않을 것이다.그렇기에 Y방향으로의 계산은 조금 다른 방식으로 구해야 한다. 왼쪽 카메라에서의 Y값과 오른쪽 카메라에서의 Y값을 평균내서 Y를 추정한다.Perspective n PointsPnP(perspective n Points)는 n개의 데이터를 통해 world to camera coordinate transformation을 수행하는 것을 목표로 한다. PnP는 이미지에서 feature를 뽑고, world coordinate와 매칭함으로써 world에서의 나의 위치를 찾을 수 있도록 해준다.나의 위치를 찾게 해준다는 것은 localization을 수행해준다는 말이고, 실제로 visual localization 태스크에서 PnP는 많이 사용되는 기법이다. PnP에서는 3d point가 담고 있는 descriptor와 현재 이미지에서 보이는 descriptor를 매칭해서 2D-3D correspondence를 구하고, 충분한 양의 correspondence pair가 모이면 카메라 위치를 구할 수 있게 된다. 카메라 위치를 구하는 것이므로 6DoF를 가지고 있다. 즉 world coordinate system에서 camera coordinate system으로의 각각 Rotation matrix 3, translation 3을 가진다. 그래서 총 6개의 데이터이자 3개의 데이터 쌍을 사용하는 minimum solver에 해당한다.그러나 p3p의 경우는 노이즈를 전혀 고려하지 않는 알고리즘이므로, outlier를 제거해줄 RANSAC 알고리즘이 필요하다.PnP solver의 경우 OpenCV에서는 solvePnP()라는 함수로 구현되어 있다. OpenCV docsPnP의 경우 픽셀값으로 데이터를 계산하기 때문에, Calibration matrix, 즉 intrinsic matrix를 가지고 있어야 연산이 가능하다.P3P - GrunertP3P를 푸는 방법은 다양하다. 1842년도에 나온 Grunert가 있고, 그 이후에 나오는 E-P3P와 같이 다양한 알고리즘들이 있지만, 대부분의 경우 Grunert 기법을 기반으로 만들어졌다. 그래서 Grunert method를 배워보고자 한다.기본적으로 P3P 문제를 풀기 위한 전제 조건으로는 실제 world coordinate에서의 세 점 좌표(A,B,C)를 모두 알고 있고, 이미지 위에서의 세 점 좌표(a,b,c)도 알고 있어야 한다.그러면 optical center로부터의 픽셀 위치(focal length)를 알고 있기 때문에, 각 ray들마다의 각도($\\theta$)를 계산할 수 있다. 그러나 optical center에서 각 점으로의 거리는 알지 못한다. 그래서 P3P에서는 2가지 단계로 진행된다. projection ray들의 길이를 추정 길이와 각 ray들의 각도를 통한 방향 추정triangulation에서 배웠듯이 $ s_i x_i^k = R(X_i - E) $ 식을 사용하여 ray의 길이를 추정할 수 있다. i : i번째 ray s : ray의 길이 x^k : object point를 향하고 있는 ray의 방향 벡터 R : world to camera coordinate transformation의 rotation matrix X_i : A,B,C의 좌표 E : optical centerR이 world to camera transformation이므로 X_i - E, 즉 ray의 길이가 camera coordinate에서의 표현으로 만든다. 그렇게 만들어진 길이와 방향 벡터, x_i와 거리, s를 곱한 값과 같다는 것을 알 수 있다.ray의 방향벡터 x를 조금 더 자세히 보면$ x_i^K = -sin(c)N(K^{-1}x_i) $ c : focal length, -를 붙여준 이유는 대부분의 focal length의 방향은 image point에서 optical center로의 방향이다. 그래서 -를 붙여 center에서 image point로 가도록 만들어준다. N : x_i^k는 방향 벡터이므로 유닛 벡터로 만들기 위한 normalize K^-1 : x는 현재 픽셀 단위로 구성되어 있다. 그리고 K는 intrinsic matrix를 의미하는데, intrinsic matrix와 normalized image plane에 있는 값을 곱해주면 픽셀값으로 변환이 된다. 그래서 이를 역으로 normalized image plane에서 pixel값으로 변환해준다.1. Length of projection rays먼저 ray들간의 각도를 구한다. 각도를 구할 때는 내적을 사용하여 구한다. 두 벡터간의 내적은 길이를 서로 곱하고, 두 벡터가 이루는 각도의 cos값을 통해 구한다. $ \\overrightarrow{a} \\cdot \\overrightarrow{b} = | \\overrightarrow{a} | | \\overrightarrow{b} | cos\\theta$따라서 ray간의 각도를 구할 때 내적 공식을 사용한다. optical center를 X_0, 각 포인트를 각각 X_1,X_2,X_3 라 할 때의 각각의 각도 alpha, beta, gamma를 구한다.$ cos\\gamma = \\cfrac{(X_1 - X_0) \\cdot (X_2 - X_0)}{| X_1 - X_0 | |X_2 - X_0 |} $어차피 우리는 방향벡터, x_i^k를 알고 있다면, 간단한 식으로 표현이 가능하다.$ \\alpha = arccos(x_2^k, x_3^k), \\; \\beta = arccos(x_3^k, x_1^k), \\; \\gamma = arccos(x_1^k, x_2^k) $그 후 우리는 각각의 3D point 위치를 알고 있으므로 3D point들간의 거리도 알 수 있다.$ a = | X_3 - X_2 |, \\; b = | X_1 - X_3 | ,\\; c = | X_2 - X_1 | $이렇게 각 ray들간의 각도와 a,b,c를 알고 있다면 코사인 제2 법칙을 활용하여 각 ray의 길이 s1,s2,s3에 대해 정리해줄 수 있을 것이다. 그러나 이 3개의 식만으로는 s를 구할 수가 없다.그래서 나온 방법으로는 다른 외부 센서를 활용하여 orientation을 판단해서 방정식을 풀어내거나 다른 1개의 점을 하나 더 가져와서 풀어낼 수 있을 것이다. 그러나 전자의 경우 다른 센서의 값을 요구하기 때문에 카메라만 사용하는 visual SLAM에는 맞지 않다. 후자의 경우도 1개의 점을 더 요구하기 때문에 P3P solution에는 맞지 않다.최근 P3P 논문을 보면 위의 방법을 해결하기 위한 다른 기하학적 방식을 사용하기도 한다." }, { "title": "[데브코스] 16주차 - Visual-SLAM motion estimation ", "url": "/posts/motion_estimation/", "categories": "Classlog, devcourse", "tags": "devcourse, Visual-SLAM", "date": "2022-05-30 17:20:00 +0900", "snippet": "Epipolor Geometry3D point estimation을 수행할 때, 왜 2장의 이미지가 필요할까?카메라 투영 시 3D에서 2D로의 mapping 관계는 이전 글에서 많이 다뤘다. 그러나 2D에서 3D로의 mapping 관계를 풀기 위해 intrinsic과 extrinsic matrix를 활용할 수 있지만, extrinsic을 알지 못하는 경우가 너무 많다. 사실 이 extrinsic matrix를 안다는 것은 실제 3d point를 안다는 것과 같다. SLAM이나 mapping 기술들 대부분이 결국 extrinsic를 구하는 자체가 목적이 된다.그렇다면 Depth값을 이미 알고 있다면, SLAM을 굳이 사용하지 않아도 되는 거라 생각할 수 있다. 그러나 Depth카메라의 경우 노이즈가 많이 생겨서 오히려 더 부정확하게 3D point를 추정될 수 있다. 그래서 여러 장의 이미지를 사용하여 노이즈를 제거한다.또 그렇다면 Depth값을 정확하게 알고 있다면, SLAM을 사용하지 않아도 되는가? 그건 아니다. SLAM의 목적은 mapping도 있지만, localization도 필요하고, 맵을 확장시키는 기능도 있다.위의 이미지는 2장의 이미지를 사용하고 있는 모습이다. 바라보는 시점은 다르지만, 동일한 3D point를 바라보는 2개의 이미지가 있는 경우, 이를 2-view geometry라고 한다. monocular camera의 경우 1개는 previous image, 나머지 1개는 현재 image라 볼 수 있다. stereo camera인 경우에는 동시간대 왼쪽 오른쪽 카메라에 대한 이미지라 생각할 수 있다.이런 2-view geometry를 다른 말로 epipolar geometry라고도 한다. image plane 1에서의 3D point-&amp;gt; 2D point로의 mapping을 할 때의 projection line이 있고, 이는 위의 그림에서 초록색 선에 해당한다. 이제부터는 초록색 선을 ray라 부르고자 한다. Visual SLAM을 수행할 때, 3D point가 2D image point로 projection될 때의 직선을 ray라고 한다. 이 ray를 image plane2에서도 바라볼 수 있을 것이다. ray를 image plane2에 투영되는 것을 재투영이라 한다. image plane2안에 그려진 ray를 투영한 직선을 epipolar line이라 한다. image plane1과 image plane2가 함께 바라보는 3D point는 무조건 epipolar line위에 존재한다.image plane1과 image plane2의 rotation과 translatino을 정확하게 구하기 위해서는 image plane1에 존재하는 x와 image plane2에 존재하는 x’의 연관성이 반드시 필요하다.이번에는 2-view에서 image plane1이 3개의 3D point를 바라보고 있다고 생각해보자. 각각의 point들마다의 ray1,2,3이 존재할 것이고, image plane2에 재투영하면 3개의 epipolar line이 존재한다. 이 3개의 line이 한 점에서 만날 때, 이 점을 epipole이라 한다. 이 점은 모든 ray가 시작되는 지점인 optical center이다. 간단하게 보면 점들이 3차원으로 뻗어나가고 있을 때, 이 ray들은 한 점에서 만나고, 이를 image plane2에서 바라보고 있으니 epipolar line이 만나는 점은 당연히 ray들이 교차하는 지점인 optical center이다.즉, epipole은 반대편 이미지의 optical center가 투영된 것이다. 지난 카메라의 위치가 보이는 것이거나 stereo 일 경우에는 옆의 카메라를 바라보고 있다고 생각할 수 있다.만약 카메라가 같은 방향을 바라보고 있다면, 위와 같은 상황이 발생할 것이고, 이 때는 epipole이 존재하지 않고, 따라서 ray1,2,3은 모두 평행하다. stereo 카메라의 경우 calibration과정을 거치고 나서 rectification이라는 과정을 수행한다. 이는 카메라의 위치들을 조정해서 모두 같은 방향을 바라보도록 만드는 과정이다.이러한 과정을 수행하는 이유는 그림에서 힌트가 있다. 이러한 경우 모든 epipolar line들이 가로로 선이 그어지게 되고, 그렇게 되면 두 이미지의 연관성을 찾고 싶으면 x1,x2,x3 각각의 row를 찾으면 된다. x1에 대한 correspondence를 찾고 싶으면 l1을 탐색하고, x2에 대한 correspondence를 찾고 싶으면 l2를 탐색하면 된다.대각선의 선에서부터 correspondence를 찾는 것은 오래걸리고 메모리도 많이 차지할 것이다. 그러나 가로선의 경우에는 픽셀의 정보도 붙어있어서 쉽게 매칭을 해줄 수 있다.monocular 카메라에 대해 로봇이나 자동차가 직진을 하고 있다면, 위의 그림과 같은 상황이 발생한다. 이 경우에는 epipole1과 epipole2가 직선에 놓이게 된다. 즉 이미지 상에서의 동일한 위치에 형성될 것이고, epipolar line은 방사 형태가 될 것이다. 이 경우는 굳이 어렵게 correspondence를 구하지 않아도 rotation이 없다는 것을 알 수 있고, translation값만 알면 곧바로 이동거리를 판단할 수 있다.다시 epipolar geometry로 돌아와서 좌측 이미지와 우측 이미지의 optical center들과 3D point를 연결하면 삼각형이 만들어진다. 이 삼각형을 epipolar plane이라 한다. 이 plane은 epipolar line과 epipole의 정보를 모두 가지고 있다. 그리고 좌측 이미지의 optical center와 우측 이미지의 optical center를 연결하는 직선을 baseline이라 한다. 이 직선은 두 카메라 간의 거리값을 표현한다. baseline이 길면 길수록 삼각측량의 정확도가 커지기도 한다.baseline이 이미지 plane을 통과하면 통과하는 점이 epipole을 이룬다. 즉 epipole들은 baseline 직선 위에 올라가 있다는 것을 알 수 있다. 이미지를 통해 수많은 3D point를 관찰할 수 있다. 3D point, x를 위아래로 움직여보면 그에 맞게 epipolar plane이 생기는데, 이들은 baseline을 중점으로 회전을 한다는 것을 확인할 수 있다. 존재할 수 있는 epipolar plane은 굉장히 많을텐데, 이 모든 가능성을 epipolar pencil이라 한다.이 모든 특징들을 조합해서 correspondence가 존재하기 위한 조건들을 3가지로 요약할 수 있다. 3D point, x는 epipolar plane위에 있어야 한다. 모든 epipolar line들은 epipole과 교차해야 한다. 모든 baseline은 epipole과 교차해야 하며 epipolar plane은 baseline을 반드시 포함하고 있어야 한다.이러한 조건들을 기하학적 표현으로 geometry constraints라 한다. 어떤 현상이 나타나기 위해서 존재해야 하는 조건들을 의미한다.Essential / Fundamental matrixessential matrix와 fundamental matrix는 VSLAM에서 epipolar geometry가 사용되는 개념이다.e-matrix(essential matrix)는 epipolar constraint에 대한 정보를 담고 있는 3x3 matrix이다. 카메라 간의 모션을 추정하기 위해서는 correspondence를 정확하게 아는 것이 중요하다. 이 correspondence를 정확하게 얻어낼 수 있도록 하는 것이 epipolar constraint이기 때문에, 이 e-matrix를 얻어내는 것이 굉장히 중요하다. epipolar constraint가 뜻하는 것은 결국 두 카메라간의 회전과 이동을 담고 있다.$ E = t * R $$ \\widetilde{x}^T E \\widetilde{x} = 0 $E는 essential matrix이고, 이는 2개의 2d point를 이어주는 역할을 한다. t는 3x1 matrix translation, R는 3x3 matrix의 rotation을 뜻하고, 이 둘은 벡터곱 연산을 수행한다. 벡터곱 연산을 수행하기 위해서는 차원이 같아야 하므로 3x1 translation matrix를 대칭행렬(https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;amp;blogId=sw4r&amp;amp;logNo=221357931060) 형태로 바꿔주어 3x3 matrix로 변환시킨다. 결국 essential matrix는 3x3 matrix로 구성될 것이고, 이를 SVD를 통해 t와 R로 분해한다.그러나 이 essential matrix를 그대로 사용하려면 한가지 오류가 있다. 이 x가 어떻게 표현되어 있는지 모른다는 것이다. essential matrix를 표현할 때는 image point, x는 normalized image plane에서의 점으로 표현한다. 그러나 실제 이미지를 다루고자 할 때는 픽셀로서 좌표를 표현해야 하기에 normalized된 좌표를 픽셀 좌표로 변환해줘야 한다. 이 과정에 필요한 matrix가 instrinsic matrix인거고, instrinsic matrix까지 결합하여 matrix를 표현한 것을 Fundamental matrix라 한다. Fundamental matrix, F는 intrinsic matrix, K에 대해 다음과 같이 표현된다.$ F = (k’^{-1})^T E K^{-1} $$ F = (K’^{-1})^T [t_x] R K^{-1} $$ \\widetilde{x}’^T F \\widetilde{x} = 0 $이 때, $ [t_x] $ 는 translation matrix를 대칭행렬한 형태를 의미하고, k’,k는 stereo camera의 경우 두 이미지 plane은 각각의 intrinsic matrix를 가지기 때문에, 분리하여 표현해주었다. -1는 inverse를 의미한다.이 fundamental matrix를 사용하여 좌측 이미지의 점 $ \\widetilde{x} $와 F-matrix를 곱하면 우측 이미지에서의 epipolar line, l&#39;이 된다.$ l’ = F\\widetilde{x} $그리고, 아까 봤던 식을 통해 새로운관계식을 구상할 수 있다.$ \\widetilde{x}’^T F \\widetilde{x} = 0 $$ \\widetilde{x}’^T l’ = 0 $즉, 우측 이미지의 좌표와 우측 이미지에서의 epipolar line과 곱해주면 0이 된다. line과 point가 교차한다면 0이 된다는 정의가 그대로 적용된다. 이를 통해 correspondence를 굉장히 빠르게 구할 수 있다. 우측 이미지에서의 픽셀이 왼쪽이미지에서는 어떤 픽셀과 correspondence를 이루는가에 대한 문제를 풀기 위해 좌측 이미지에서 모든 픽셀에 가능성을 둘 필요없이 우측 이미지에서의 좌표와 F-matrix를 곱해줘서 나오는 line과 벡터 연산을 해서 0이 나오는 픽셀만 사용하면 되는 것이다.Essential matrix와 Fundamental matrix를 구하는 방법은 각각 5-point algorithm, 8-point algorithm이 있다. 최소 5개, 8개의 correspondence가 존재할 때 Essential matrix, Fundamental matrix를 구할 수 있다.Fundamental matrix의 경우 7DoF를 가지고 있다. translation(tx,ty,tz), totation(rx,ry,rz), focal length(f)와 c(principal point)가 있는데, 여기서 scale 값을 구할 수 없기 때문에 1개를 빼서 7이 된다. essential matrix는 5DoF, translation 3, rotation 3에서 1이 빠져서 5가 된다.DoF(degree of freedom)를 알아야 이 값을 추론하기 위해서 가져야 하는 최소한의 데이터의 수가 어떤지 알 수 있고, 아무런 prior없이 값을 추론하기 위해서는 데이터가 요구하는 DoF의 수만큼의 데이터가 필요하기 때문에 DoF를 알고 있어야 한다. 이처럼 최소한의 데이터의 수로 문제를 해결하는 알고리즘을 minimum-solver라 한다. 5DoF의 essensial matrix를 추론하기 위해서는 5개의 데이터가 필요하므로 이는 minimum-solver에 해당하지만, 7DoF의 fundamental matrix를 추론하기 위해서는 8개의 데이터가 필요하므로 이는 non-minimum-solver에 해당한다.fundamental matrix를 추론하기 위해 7개의 데이터를 사용해도 되지만, 오류를 잡기도 어렵고, 내부 구조가 어려워져서 대체로 8개로 추론하도록 한다.essential matrix나 fundamental matrix를 구하는 방법은 OpenCV에 잘 구현되어 있다. essential matrix의 경우는 recoverPose(), fundamental matrix의 경우는 findFundamentalMat()이다. recoverPose() OpenCV docs findFundamentalMat() OpenCV docs구현 과정 2장의 다른 이미지에 대해 feature matching을 한다. 이를 기반으로 fundamental matrix를 구해서 카메라들 간의 모션 값을 추정한다. 한 장의 이미지에 F-matrix를 곱하면 다른 한장의 이미지에서의 epipolar line이 나온다. 원래 이미지에서의 픽셀값과 line을 벡터 연산하여 0이 되는 지점을 예측한다.그러나 이 때, 문제는 f-matrix를 구하고 나서 점을 예측하고 나서야 feature matching이 잘 되었는지 아닌지를 판단할 수 있다.Outlier rejection(removal)SLAM에서 inlier와 outlier가 있다. inlier는 관측 시 예상했던 데이터 분포에 포함되어 있는 데이터이고, outlier는 데이터 분포에서 벗어나 있는 데이터를 말한다. 데이터 분포에서부터 얼마나 떨어져있는지에 대한 에러로 판단하는데, 모든 데이터는 노이즈를 가지고 있기 때문에 에러값으로만 판단하기는 어렵다. 그래서 inlier를 기대하면서 맞는 값, outlier를 기대하지 않았으면서 틀린 값이라 정의하고자 한다.outlier가 나타나는 이유는 밝기 정보가 급격하거나, 회전, 블러, 가림 등의 깔끔한 이미지를 얻지 못하거나 기존의 기하학적 정보가 깨지기 때문일 것이다. outlier를 filtering해야 더 정확하고 좋은 알고리즘이 된다.컴퓨터 비전에서는 2가지의 outlier filtering 알고리즘이 있다. closed-form algorithm 여기에는 수많은 minimum solver 알고리즘이 있다. fundamental matrix를 구하는 8-point는 정확하게 매칭이 된 correspondence를 전제로 한다. 이중에 하나라도 정확하지 않으면 잘못된 결과가 나온다. iterative optimization 훨씬 더 많은 데이터로 동작하지만, 이 데이터들이 좋은 데이터라고 판단이 될 때까지 지속적으로 찾아가는 알고리즘이다. 데이터 분포 속 inlier 사이에서 패턴을 찾아서 필터링을 한다. 그 데이터를 따르지 않는 데이터가 들어올 경우 잘못된 예측이 발생할 수 있다. outlier를 제거하여 깔끔한 inlier데이터들만 남겨놓고 계산을 하게 만드는 것이 중요하다. 그에 대한 예시를 몇가지 살펴보자.Linear regression2D plot에서 여러 개의 데이터 포인트들을 가로지르는 가장 정확한 직선을 추정하는 알고리즘이다. 이 때, 직선에 대해 각각의 데이터 포인트들마다의 거리값(error)를 가지므로 이에 대해 절대값을 취하는 SAE, 또는 제곱을 취하는 SSE가 있다. 이 error를 줄이는 것을 목표로 한다.이는 너무 한계가 뚜렷해서 최근에는 다양한 알고리즘들이 개발되었다. RANSAC M-Estimator MAXCON이 중 가장 유명한 RANSAC에 대해 알아보고자 한다.RANSACRANSAC(RAndom SAmple Consensus)은 1981년에 개발된 기법이다. 무작위로 데이터 샘플을 뽑아 모델을 만들고, 모델에 대해 데이터의 적합성을 판단한다. RANSAC은 template으로 안에서 돌아가는 알고리즘은 따로 지정해야 한다.RANSAC 기법 과정 무작위로 최소한의 데이터를 뽑는다. 데이터를 기반으로 모델을 추정한다. 추론한 모델을 기반으로 score를 측정한다. - 현재까지의 가장 최고의 score보다 현재의 score가 들어오면 score를 업데이트한다. 다시 1번으로 돌아간다.쉽게 보기 위해 homography에 RANSAC알고리즘을 추가한 과정을 설명하면 다음과 같다. homography를 구하기 위해 필요한 최소한의 데이터는 4개이므로 무작위로 4개의 데이터를 뽑는다. 그 데이터를 기반으로 homography matrix를 추정한다. reprojection(재투영) error를 계산한다. 한 이미지 픽셀값들에 homography matrix를 곱하면, 다른 하나의 이미지에 대한 픽셀값들이 나오게 된다. 이 때, GT와 계산값과의 error를 전부 더한다. 이 때, 특정 error threshold를 설정해서 그 error보다 낮은 값만 추가되도록 할 수 있다. 최고 score(lowest error)보다 현재의 score(error)가 낮으면 score를 업데이트한다. 1번으로 되돌아간다.최적의 모델을 얻기 위해 반복해야 할 횟수(T)를 논문에서 함께 제시했다.$ T = \\cfrac{log(1-p}{log(1-(1-e)^s)} $ T : 최적의 모델을 얻기 위해 반복해야 할 횟수 P : 뽑은 모델이 전부 inlier로 이루어지길 바라는 확률 (want P% accuracy) e : 전체 데이터셋 중 inlier와 outlier의 비율 (the dataset in made of e% inliers and (1-e)% outliers) s : 매 loop마다 뽑아야 하는 샘플의 수 (minimal sample of data to be a minimal set)이 때, 구한 T를 통해 전체 프로세스의 시간을 대략적으로 짐작할 수 있다. T에 homography solver 프로세스 한 번 돌리는데 걸리는 시간을 곱해서 전체 프로세스 시간을 구할 수 있다.RANSAC의 장점 outlier를 제거할 수 있다. T를 통해 총 걸리는 시간을 구할 수 있다. 운좋게 더 빠르게 찾는다면 더 빨리 끝낼 수 있다. 굉장히 쉬워서 이해하기 쉽다.RANSAC의 단점 알고리즘 자체가 랜덤성을 가지고 있으므로 돌릴 때마다 결과값이 다르게 나온다. 이로 인해 모델의 성능이 좋아지고 있는지 랜덤성으로 좋아진건지 판단하기 어렵다. 전체 데이터셋에 inlier보다 outlier의 수가 더 많아질 경우 실행 시간이 급격하게 늘어난다. 만약 RANSAC 알고리즘이 실패하면 모든 가능성을 순회하도록 속도로 수렴하게 된다. 하나의 데이터셋에서 여러 모델을 돌릴 수 없다. 예를 들어 RANSAC을 통해 동시에 3개의 Fundamental matrix를 구할 수가 없다.Modern RANSACs기존의 RANSAC을 보안한 Modern RANSAC도 있다. 기존의 RANSAC에는 여러가지 단점이 존재했다. 동시에 다중 모델을 추론하는 것이 불가능했다. 또한, 무작위로 뽑기 때문에, prior 정보를 활용하지 못했고, 센서에는 노이즈가 많은 편인데 기존의 RANSAC은 노이즈가 거의 없는 알고리즘으로 구현되어 있었다.개량 RANSAC에는 종류가 엄청 많다. 그래서 어떤 것을 써야할지 고민이 되기도 하는데, 가장 좋은 방법은 모든 방식을 공부해서 자신의 태스크에 맞게 사용하는 것이다. 또는 여러 방식을 조합할수도 있다. 여러 가지의 방식들을 간략하게 소개하려고 한다.Early-stop method좋은 모델을 빨리 찾는 경우 RANSAC 사이클을 끝내는 방식이다. 원래의 사이클보다 빨리 끝나면 해당 시간이 비게 되므로 뒤에 프로세스를 땡겨서 처리할수도 있다.이 방식을 구현하기 위해서는 3가지 변수를 미리 정의해야 한다. minimum iteration : 최소한의 퀄리티를 보장하기 위한 최소한의 반복 수 maximum iteration : 좋은 모델이 안찾아지면 계속 반복하는 것이 아닌 반드시 최대 특정 반복 수만큼 돌고 끝내도록 함 success score : 특정 score보다 높게되면 반복을 멈추도록 하기 위함PROSACPROSAC은 이미지 매칭에 특화된 RANSAC기법으로 데이터의 prior를 잘 활용하는 기법이다. PROSAC은 descriptor matching을 할 때, L2 norm이나 Hamming distance로 측정하게 되는데, descriptor들 간의 distance가 작을수록 모델 추론을 할 때 더욱 정확하게 추론할 가능성이 높다. 그래서 PROSAC은 낮은 distance를 가진 descriptor match를 샘플링하도록 만들었다.PROSAC의 장점은 운이 나빠서 완전 실패하더라도, 기존의 RANSAC으로 수렴하기에 반드시 기존의 RANSAC보다 성능이 좋다는 것이다.PROSAC 동작 방식 2개의 이미지에 대해 descriptor matching을 수행한다. 이 과정에서 match마다의 descriptor간의 distance를 기록한다. distance를 오름차순으로 정렬한다. 몇개씩 탐색할지에 대한 size(n)을 지정해준다. distance 리스트에서 n개의 top data를 샘플링한다. 샘플링한 데이터들로 모델을 추론한다. 좋은 결과가 나오면 score값을 업데이트하고, 원래의 score보다 낮으면 n을 증가시킨다. 다시 4번으로 돌아간다.PROSAC은 5~10개의 loop만으로 최적의 모델을 찾는 경우가 많다.Lo-RANSACLo-RANSAC은 데이터셋의 데이터 패턴을 활용한 방법이다. 보통 inlier는 뭉쳐있고, outlier는 산개한 특징이 있어서 그것을 활용했다. 즉, inlier데이터를 활용해서 분포를 찾고나면 정답은 그 근처에 있으므로 랜덤한 위치로 이동할 필요가 없다.Lo-RANSAC에서는 RANSAC 사이클 안에 추가로 RANSAC을 넣어 더 정확한 값을 찾는다.Lo-RANSAC 동작 방식 최소한의 데이터를 샘플링 모델을 추론한다. 기존의 score보다 현재의 score가 안좋다면 다시 1번으로 돌아간다. best score보다 좋은 score가 들어왔다면, inner RANSAC을 실행하여 방금 전 모델 score를 평가할 때 inlier라고 판단했던 데이터들을 새로운 데이터로 사용하여 데이터를 샘플링한다. 샘플링된 데이터로 모델을 추론한다. 만약 best score보다 현재 score가 높다면 score를 업데이트한다. 만약 best score보다 낮다면 2-1로 돌아간다. maximum loop를 초과하면 inner RANSAC loop를 빠져나간다. Gauss-Newton method나 Levenberg-Marquardt method 와 같은 최적화 기법을 수행한다. 이 때도, best score를 갱신한다. 1번으로 다시 돌아간다.descriptor match score(distance)를 가지고 있다면 PROSAC을 사용하는 것이 좋지만, 없다면 Lo-RANSAC을 사용하는 것이 적합하다. PROSAC을 사용하더라도inner-RANSAC을 함께 조합해서 사용하면 더욱 좋은 성능을 낼 수 있다.참고하면 좋은 논문 BRIEF : Binary Robust Independent Elementary Features ORB : an efficient alternative to SIFT or SURF" }, { "title": "[데브코스] 15주차 - Visual-SLAM Introduction ", "url": "/posts/slam/", "categories": "Classlog, devcourse", "tags": "devcourse, Visual-SLAM", "date": "2022-05-23 15:20:00 +0900", "snippet": "IntroductionSLAM을 수행하게 되면 지금까지 스캔했었던 공간을 기억할 수 있어서 공간에 대한 지도를 생성할 수 있게 된다. 지도를 가지게 된다는 것은 기존의 딥러닝을 통해 즉각적인 추론만으로 판단하는 것보다 훨씬 더 높은 수준의 제어가 가능해진다. 또한 현재의 위치를 파악할 수 있게 되고, 그렇게 되면 주변의 벽이나 객체의 위치도 파악이 가능해진다. 하지만 현재의 위치가 틀리게 추론된다면 주변의 벽이나 객체의 위치도 틀리게 된다는 단점이 존재한다.Visual SLAM을 사용하게 되면 주변 정보와 위치정보를 동시에 추론하게 되면서 서로 상호 보완이 가능해진다. 그로 인해 더 정확한 추론이 가능하다.SLAM 이란?SLAM이란 Simultaneous Localization and Mapping 을 줄인 말로 동시적(simultaneous) 위치 추정(localization) 및 지도 작성(mapping)이다. SLAM은 원래는 로보틱스에서 출발했는데, 로봇 제어를 하기 위해서는 위치를 추정하고 지도를 작성할 줄 알아야 한다.지도 작성을 할 때는 센서를 통해 들어온 데이터를 perception(딥러닝)을 통해 주변 환경을 파악한다.센서는 로봇의 다양한 감각을 담당한다. 카메라, 라이다, 레이다, 초음파, IMU 등등이 존재하고, 이를 통해 다양한 환경을 파악한다.사전 정보가 존재한다면 사전 정보를 통해 현재의 위치를 추정하고, 그를 통해 더 정확한 추정이 가능할 것이다. 그러나 SLAM은 사전 정보가 없이 위치를 추정을 하고, 지도를 제작하는 것이 특징이다. SLAM을 수행한다는 것 자체가 사전 정보를 제작한다고 할 수 있다.로보틱스 기술의 진화SLAM의 뿌리는 로보틱스에 있다. 특히 고정되어 있지 않고 움직일 수 있는 moblie robotics에서부터 시작되었는데, 이는 위험하거나 사람이 가기 어려운 곳과 같이 사람이 가지 못하는 곳을 대신 탐색하거나 사람 대신 기계가 대신하도록 만들기 위한 기술이다. 가장 기초에는 고정되어 있는 로봇을 바퀴나 레일을 달아 움직일 수 있도록 했는데, 움직이는 로봇으로 만들기 위해서, 즉 자율이동체로 만들기 위한 조건으로는 스스로 인지하고 결정하고 행동해야 한다. 그러나 단순하게 바퀴나 레일을 통해 한정적인 움직임으로는 mobile robotics라 할 수 없다.스스로 공간을 인지하는 것을 연구한 것이 SLAM이다. 공간을 인지한다는 것은 이동을 할 수 있는 공간과 없는 공간을 인지할 줄 알아야 하고, 벽이나 장애물도 인지해야 한다. 벽과 같이 외부 환경을 인지하기 위해 사용되는 센서를 exteroceptive sensing(외부 감각)이라 한다.센서는 항상 노이즈를 가지고 있는데, 센서를 통해 정확하게 추정하려면 노이즈를 제거해주어야 한다. 수많은 센서로부터 들어오는 데이터의 노이즈를 잘 처리하는 과정이 복잡하고, 어렵다. 노이즈로 인해 아주 정확한 SLAM은 불가능하기에 SLAM은 확률적인 프로세스라고 할 수 있다.mobile robotics에서 주변 환경을 파악하는 것도 중요하지만, 자신의 움직임도 잘 인식해야 한다. 자신의 움직임을 인식하는 센서를 Proprioceptive sensing이라 하고, IMU, GPS 등이 있다.움직이면서 주변을 파악하기 위해서는 이 두 sensing을 반복하면 된다. 주변 환경을 탐지하고 벽이 없는 곳으로 이동하고, 제대로 이동했는지 위치를 추정하고, 다시 주변 환경을 탐지하고 , 이동을 반복한다. 이를 perception &amp;amp; control feedback loop이라 하는데, 극 초기 단계의 mobile robotics는 이러한 형태를 띄었다. 현재는 사용하지 않는 이유는 여러 가지가 있다. proprioceptive sensing은 확률적인 분포를 가지기에 안정성 확보가 어렵다. exteroceptive sensing과 proprioceptive sensing은 항상 노이즈를 가지므로 신뢰도가 떨어질 수 있다. 노이즈 분석을 하는 동안에는 이동이 불가능하다.이들을 보완하기 위해서는 여러 개의 센서값들을 확보해서 중간값을 추정하면 되지만, 이는 여러 개의 센서를 확보해야 하므로 가격이 올라가고, 각각의 센서마다 정확도가 다를 수 있어서 어떤 것이 맞고, 어떤것이 틀린지 알수가 없다.또 다른 방법으로는 exteroceptive sensing을 통해 proprioceptive sensing이 맞는지 틀린지 확인해볼 수 있다. 로봇이 1m 전진했다면 벽과의 간격이 1m가 줄어야 할 것이다. 그러나 평소에는 오차가 0.2m의 오차를 가지지만, 갑자기 특정 환경에 의해 값이 뻥튀기 된다면 어떤 센서를 믿어야 할지 알 수 없을 것이다. exteroceptive sensing을 하기 위해서는 움직이지 않고 멈춰야 한다. 그러면 움직이다가 샘플링을 위해 멈추고, 다시 움직이고, 샘플링을 위해 다시 멈추고를 반복한다면 평균 이동속도가 엄청나게 감소될 것이다.Localization, Mapping,, and SLAM우리가 생각하는 완벽한 mobile robotics는 지속적으로 움직이면서 모션과 주변 환경을 인지할 줄 알아야 하고, 두 sensing이 안정적으로 취득되어야 한다. 이 완벽한 시스템을 만들기 위해 proprioceptive sensing을 집중적으로 보는 localization과 exteroceptive sensing을 집중적으로 보는 mapping, 두 가지 기술이 개발되었다. 이 두 기술은 각각 개발되다가 추후에 혼합되면서 SLAM으로 개발되었다.모든 센서는 확률 분포를 가지고 있다. 그렇다면 exteroceptive sensing과 proprioceptive sensing을 조합할 수는 없을까?두 확률 분포를 조합할 때는 조심해야 하는 부분이 있다. proprioceptive sensing이 상대적으로 부정확하므로 exteroceptive sensing을 통해 약간의 보정이 가능했다. 그러나 반대로 exteroceptive sensing도 부정확하다면 proprioceptive sensing을 보정해줄 수 없다. exteroceptive을 보정하기 위해 proprioceptive을 활용해줄 수도 있다.이처럼 두 확률 분포 중 하나만 정확하다면 다른 하나를 보정해줄 수 있다. 높은 정확도를 가진 proprioceptive sensing을 통해 exteroceptive sensing을 보정해주는 과정을 mapping, 반대로 높은 정확도의 exteroceptive sensing을 통해 proprioceptive sensing을 보정해주는 과정을 localization이라 한다.Mapping대동여지도는 사람이 직접 움직이면서 주변을 살펴보고 작은 지도를 계속 그려나가 전체 지도를 만들었을 것이다. 이처럼 주변 환경을 다양한 시점에서 바라보면서 나 자신의 위치를 정확하게 알고 있다는 가정하에(정확한 proprioceptive sensing 값을 알고 있을 때) 불안정한 주변 환경을 보정해 나가며 주변 환경을 그려나가는 것을 Mapping이라 할 수 있다.Localization롯데월드를 갔다고 생각했을 때, 정확한 지도를 보며 자신의 위치를 파악할 수 있을 것이다. 즉, 정확한 exteroceptive sensing의 결과인 지도를 통해 불안정한 proprioceptive sensing의 결과인 나의 위치와 움직임을 보정하여 나의 위치와 움직임을 추론하는 것을 localization이라 할 수 있다.이 두 기술을 자율주행에 빗대어 말하면, 지도를 정확하게 그려내야 자동차가 어떤 차선에서 달리고, 적절한 시기에 차선을 바꿀 수 있고, 경로를 미리 계획할 수 있다. 또한 지도를 통해 내가 갈 수 있는 곳과 가면 안되는 곳을 파악할 수도 있다. 주행에 필요한 신호등이나 표지판도 잘 파악할 수 있다.지도를 정확하게 그리는 것 뿐만 아니라 위치 정보를 정확하게 파악하는 것도 중요하다. 인도와의 경계, 신호등 위치 등을 정확하게 알아도 내 위치가 어디에 있는지 모르면 사고가 난다.그런데 만약 prior 정보가 주어지지 않거나 이 사전 정보가 정확하지 않다면 localization과 mapping이 불가능할까?Monte Carlo Localizationlocalization에서 가장 유명한 기법인 monte carlo localization의 작동 방법은 지도가 사전적으로 주어졌을 때, particle filter를 통해 위치를 추정한다. particle filter는 exteroceptive sensing의 확률 분포와 proprioceptive sensing의 확률 분포를 융합하여 최적의 값을 찾는 기법 중 하나이다.monte carlo localization의 단계는 다음과 같다. initialization : 파티클(사전 정보)를 분포시킨다. 사전 정보란 로봇이 존재할 수 잇는 모든 위치를 의미한다. motion update : 뿌려진 파티클마다 사전 센서로부터 들어온 모션 정보를 추가해서 위치 정보를 업데이트시킨다. 이 때 파티클이 벽으로 들어간다던가, 존재할 수 없는 위치에 있는 파티클은 전부 삭제한다. measurement : 뿌려진 파티클마다 exteroceptive 센서로부터 들어온 정보를 덧씌운다. weight update : 해당 위치에 실제로 이 관측값, exteroceptive센서로부터 들어온 정보가 나올 수 있는지, 즉 현재 위치와 주변 환경의 정보가 맞아 떨어지는지 계산한다. resampling : 잘 맞아 떨어지는 파티클만 남기고, 그 주변에서 다시 파티클을 새로 뿌려서 resampling을 수행한다.조금 복잡하지만, 로봇이 어디에 존재해야 proprioceptive센서의 값과 exteroceptive센서의 값이 어디에 위치해야 잘 맞아떨어지는지를 찾는 것이 이 알고리즘의 핵심이다.로봇이 존재할 수 있는 위치는 단 한 곳 뿐일 것이고, 이 알고리즘을 통해 지도 정보와 여러 센서를 조합하면서 점점 더 정답에 가까워질 것이다.그러나 위의 그림에서 resampling을 보면 여러 곳에 점이 찍혀있다. 이는 측정값들이 나올 수 잇을 만한 위치를 모두 찍은 것으로 꽤 다양한 위치로 예측이 되는 것을 볼 수 있다. 이렇게 되면 위치를 헷갈려할 수 있게 된다.이 알고리즘의 문제점은 지도를 전적으로 믿고 파악을 하기 때문에 지도가 틀리면 monte carlo localization은 다른 위치의 파티클을 믿게 될 수도 있고, 이곳저곳 우왕좌왕 할수도 있을 것이다.또는 지도가 주어지지 않으면 이 알고리즘을 수행할 수 없다. 정확한 위치를 알기 위해 알고리즘을 수행하는데, 이 알고리즘에서 정확한 지도가 필요하다는 역설적인 상황이 발생된다. 예전에는 매우 비싼 센서를 통해 이를 해결했다. 좋은 센서를 통해 정확한 위치 정보를 얻고, 이 정보를 기반으로 정확한 매핑을 해서 지도를 만든다. 그를 통해 정확한 localization을 수행했다.이는 매우매우 비싼 센서를 요하기도 하고, 실내에서만 사용이 가능하다던지, 실외에서만 사용 가능하다고 하는 여러 제약 조건이 존재했다. 또는 엄청 비싼 센서로 작업을 했지만, 주변 환경이 달라지면 다시 매핑을 해야 했다.주변 환경이 급격하게 자주 바뀌는 상황 속에서는 이 방법을 사용할 수 없었다. 그래서 SLAM이라는 기술이 개발되게 된 것이고, SLAM은 사전 정보없이 최적의 지도와 최적의 위치 정보를 동시에 추정한다.SLAM을 통해 사전 정보없이 exteroceptive센서와 proprioceptive센서만으로 최적의 지도외 위치 정보를 추론할 수 있다. 고품질의 사전 정보를 가지고 있다면, SLAM을 굳이 사용하지 않고 Localization을 하거나 Mapping을 하면 되고, 사전 정보가 없을 경우에만 SLAM을 사용한다고 볼 수 있다.SLAM에서 사용할 수 있는 센서proprioceptive Sensors자율주행에 사용되는 대표적인 센서로는 IMU, Wheel encoder가 있다.Wheel encoderwheel encoder는 자동차 바퀴에 탑재되어 바퀴의 회전량을 측정하는 센서이다. 회전량을 측정하는 방법에는 brush encoder, 위 그림과 같이 빛을 이용하는 optical sensor, 자기장이나 전기장을 사용하는 magnetic sensor 등이 있다.측정량에 바퀴의 둘레를 곱하면 이동량도 계산할 수 있다. wheel encoder를 통해 거리를 계산하는 알고리즘으로는 dead recoding 기법이 있는데, 모션값을 누적해 나가서 차체의 위치를 추정하는 방법이다. 그러나 이 방법은 시간이 오래 지나면 지날수록 에러가 점차 누적된다. GPS 등을 통해 이 에러값을 보정해줄 수 있지만, 루프를 빠르게 돌아야 하는 제어 시스템에서는 dead recoding 기법은 위험할 수 있다.wheel encoder는 다양한 이유로 에러가 생길 수 있다. 기본적으로 나타나는 센서의 노이즈도 있고, 비나 눈이 오는 날에 바퀴와 바닥의 접지가 좋지 않아 바퀴가 헛돌아서 오차가 발생할 수 있다. 또는 타이어가 눌려서 둘레가 바뀌는 경우에도 바뀔 수 있고, 코너를 돌 때 왼쪽과 오른쪽의 타이어 둘레가 달라질수도 있고, 고속도로 주행중 타이어 마찰에 의해 팽창되어 오차가 생길수도 있다. 최근에는 CAN 통신을 통해 타이어의 변화를 보정해주기도 한다.IMU선형 가속도를 측정하는 Linear accelerator와 각속도를 측정하는 Angular gyroscope 센서를 혼합한 센서다. 간단하게 IMU 센서는 관성을 측정하는 센서로, 관성을 측정하는 기술로 기계공학에서는 Spring-damper system이라고 부른다. 이 시스템을 칩으로 만든 것을 IMU라고 할 수 있다. 미세한 진동에도 변화를 감지할 수 있고, 자동차에 장착되는 IMU는 빛을 이용하는 optical system을 사용하여 온도나 자기장 변화에 더 강인하다.최근 slam에서는 카메라와 IMU를 결합하는 visual inertial geometry 기법이 유행하고 있고, 라이다와 IMU를 결합하는 Lidar inertial geometry 기법도 유행하고 있다. 이와 같이 IMU를 여러 센서를 결합하는 이유는 imu로 얻은 prior 정보를 카메라나 라이다에 적용해서 정확한 계산 결과를 더 빠르게 얻기 위함이다.IMU 센서는 자율주행 이외 제품에서는 저렴한 편이고, 높은 민감도를 가지고 있으며 높은 FPS(100Hz ~ 4000Hz)를 가진다. 자동차 안에서의 IMU센서는 안전 규제를 위한 수많은 검증 프로세스를 거치게 되면서 가격이 올라가게 되었다.단점으로는 에러에 대한 오차가 굉장히 커서 보정을 꼭 거쳐야 한다는 것이다.exteroceptive Sensors자율주행에서 사용할 수 있는 exteroceptive sensor로는 GNSS, LiDAR, Camera, Ultrasound, RADAR, Microphone 등이 있다.GNSS(GPS)GPS는 사실 미국에서 사용하는 GNSS 시스템을 지칭하는 말이다. GNSS(Global Navigation Satellite System)은 인공위성과의 통신을 통해 삼각측량을 하여 localization을 수행한다.GNSS는 싸고 사용하기 쉽다는 편이지만, 부정확하고(10~20m 오차), 인공위성과 일직선으로 신호가 도달해야 하는데, 고층빌딩 사이에서는 빌딩 벽에 반사되어 위치를 잘못 추정할 수 있는데 이를 multi-path라 하고, 실내나 지하에서는 사용할 수 없다.LiDARLiDAR(Light detection and ranging sensors)는 적외선 레이저를 쏘고 반사 시간을 측정하여 거리를 추정하는 센서다. 주변 환경을 3D point cloud 형태로 바로 알 수가 있다.장점으로는 exteroceptive센서중에서는 가장 정확한 편이고, 자율주행용 라이다는 ~100m 의 유효거리를 가진다. 빛의 파장이 이렁나지 않기 때문에 낮/밤 둘다 사용이 가능하다.단점으로는 비싸고, 카메라에 비해 해상도가 낮다. 그리고 눈이나 비/안개에 영향을 받기도 한다. 주변을 전부 파악하는 lidar도 있지만, 한 방향만 바라보는 solid-state LiDAR도 있는데, 이를 사용하기 위해서는 모든 방향을 보기 위해 여러 방향으로 여러 개를 탑재해야 한다. 또한, 레이저를 쏘기 때문에 여러 대를 사용할 경우 서로 간섭이 일어날 수도 있다. 또는 반사가 잘 이루어지는 물질에 의해서 물체의 위치를 잘못 추정할수도 있다.Camera카메라는 광센서를 이용해서 빛 신호를 받고, 아날로그 신호를 0~255값의 디지털 신호로 바꾸어 RGB 색으로 재구성시킨다. 3채널의 값을 가지게 하기 위해 debayering 프로세스를 사용한다.장점으로는 저렴하고, 모든 픽셀들이 값을 가지기 때문에 밀도있는 데이터를 생성하고 컬러를 가지며 높은 FPS를 가지기 때문에 좋은 성능을 가진다고 할 수 있다. 렌즈를 교환함으로써 시야각 변경이 가능하다. 사람이 보는 것과 거의 유사하기 때문에 시각화하기 가장 좋다.단점으로는 Depth 정보가 손실된다. 깊이 정보를 추정하기 위해 depth camera나 rgb-d 카메라가 개발되기도 했고, 최근에는 카메라와 라이다를 퓨전하기도 하고, monocular depth estimation을 통해 픽셀이 얼마나 거리값을 가지고 있는 지 추정하는 방법도 개발되고 있다. 또는 조명에 영향을 많이 받기 때문에 조명이 변함에 따라 다른 값들이 들어오기 된다.Ultrasound초음파(Ultrasound) 센서는 레이다와 작동 방식이 동일하다. 장점으로는 저렴하고, 가까운 범위에서 잘 동작한다.단점으로는 물체의 형태를 잘 추정하지 못하고, 그래서 거리 센서로만 사용한다. 그리고 노이즈가 많다.RADAR전파를 쏘고 반사되어 돌아오는 전파를 통해 거리를 재는 센서이다. doppler 효과를 이용해서 이동중인 물체의 속도를 추정 가능하다. 테슬라에서도 예전에는 레이다를 사용하기도 했다. 테슬라에서 레이다를 사용하지 않는 이유는 테슬라 보드를 통해 딥러닝 네트워크의 성능이 엄청 올라가서 레이다를 사용하는 것보다 비전을 사용하는 것이 훨씬 더 높은 정확도를 가지기 때문이다.장점으로는 빛을 사용하지 않기 때문에 날씨의 영향을 받지 않고, 타 센서에서는 얻지 못하는 속도 값을 추정할 수 있다. 속도를 추정할 수 있다는 것은 카메라나 라이다로는 추정이 불가능하기 때문에, 차량이 주차되어 있는 것인지 움직이지만 같은 속도로 달려서 정지되어 보이는 것인지를 확인할 수 없다. 그래서 레이다를 통해 이를 추정할 수 있다.단점으로는 작은 물체들은 검출이 불가능하고, lidar보다 더 낮은 해상도를 가지고 있다.Microphone마이크는 유일하게 소리를 측정할 수 잇는 센서로, 공기의 진동을 transducer 센서를 통해 전기 신호로 변환하는 센서다. 마이크를 하나만 사용하는 것이 아닌 여러 개의 마이ㅡ를 통해 소리의 근원에 대한 위치를 계산할 수 있다.장점으로는 저렴하다는 것이고, 단점으로는 노이즈가 너무 심하다.SLAM의 종류사용하는 센서에 따라 종류가 달라진다. Visual SLAM/VSLAM카메라를 사용하는 SLAM 방법이다. LiDAR SLAM RADAR SLAM추가적으로 마이크를 사용하면 microphone slam 이라고 할 수 있을 것이다. 또, 여러 개의 센서를 사용할수도 있는데, 이 때는 여러 개의 센서 타입을 나열해서 이름을 붙여준다. e.g. Visual-LiDAR SLAM(camera / LiDAR), Visual-inertial odometry(camera / IMU)… 여러 가지가 있지만, 나의 경우는 Visual SLAM을 주로 다룰 예정이다. Visual SLAMvisual slam은 카메라를 사용하는 slam이다.장점 저렴한 센서 센서의 성능을 조절하기 쉽다. (e.g. 렌즈 교체 -&amp;gt; 시야각, 초점, 노출 시간) 빠른 FPS 이미지 기반 딥러닝 적용 가능 ( object detection / segmentation ) 이미지로 사람이 이해하기 쉬운 시각화 가능단점 갑작스러운 빛 변화에 대응 불가능 시야가 가려지거나 어두운 곳에서 사용 불가능카메라는 3D 공간을 2D로 담는 것인데, 이를 통해 깊이 정보가 소실된다. VSLAM은 3D 공간을 재구축하기 위해 깊이 정보를 추정한다.SLAM을 잘 하기 위해서는 알고리즘 뿐만 아니라 센서에 대해서도 이해도가 높아야 한다. 모든 센서는 노이즈를 가지고 있고, SLAM은 확률적인 프로세스이므로 센서가 어떤 노이즈를 가지고 있는지 파악해서 노이즈를 제거해야 한다.아까 말했듯이 카메라는 3D 공간을 2D로 담아내는 것이므로, 카메라의 노이즈는 이 과정에서 생기는 한계점에 의해 나타나는 노이즈일 것이다. 이는 카메라의 칩에서 발생하는 노이즈일수도 있고, 렌즈에서 나타나는 노이즈일수도 있다. 칩에서 발생하는 노이즈는 아날로그를 디지털로 변환하는 과정에서 발생하는 것과 같이 전자기학적인 문제가 발생하는 것이다. 렌즈에서 발생하는 노이즈는 3D에서 2D로 차원 변화를 하면서 나타나는 노이즈다. 차원을 축소할 때 렌즈에 대한 수학적인 모델링이 완벽해야 하지만, 그렇지 않은 경우 노이즈가 발생한다.이를 파악하기 위해서는 칩의 종류도 파악해야 하고, 렌즈의 종류도 파악해야 한다. 이 종류에 따라 기하학적 모델링이 다 달라지기 때문이다.칩의 종류에 따른 카메라의 종류로는 RGB Camera Grayscale Camera Multi-spectral Camera Polarized camera (편광 카메라) Event Camera렌즈의 종류에 따른 카메라 종류로는 perspective Camera Wide FOV camera (광각 렌즈) Telecentric Camera (망원 렌즈) Fisheye Camera (어안 렌즈) 360 degree CameraVisual SLAM에서도 카메라 1대만 사용할수도 있지만, 다양한 카메라 종류를 사용할 수 있다. 카메라 1대 : Monocular visual SLAM 카메라 2대 / 카메라 여러 대 : Stereo visual SLAM / Multi visual SLAM depth 카메라 : Depth visual SLAM카메라 2대를 사용하는 것과 여러 대를 사용하는 것은 거의 동일하므로 저 두개를 묶어서 말하기도 한다. ORB-SLAM pipelineMonocular VSLAM1대의 카메라에서만 이미지를 받는 것이 특징이다. 장점다른 VSLAM에 비해 가격, 전력 소비량, 이미지 데이터 송수신 대역폭 등의 측면에서 저렴하다. 단점 1장의 이미지로는 depth를 추정할 수가 없다. 그래서 monocular vslam의 경우는 depth가 없는 2D 이미지로부터 실제 세상에 대한 스케일정보를 받을 수 없기 때문에, 재구축되는 3D 환경은 임의의 스케일로 생성되게 된다. 그렇게 되면 구축한 환경 내에서는 동일한 스케일이 유지되지만 실제 세상되는 얼마나 다른지는 확인할 수 없다. 이를 Scale ambiguity problem라 한다. 실제 세상에 대한 스케일을 Metric scale이라 하는데, 이는 실제 미터 단위를 의미한다. 이처럼 실제 스케일이 아닌 임의의 스케일을 up-to-scale이라 부르기도 한다.이 문제를 해결하기 위해 카메라와 IMU 센서를 융합해서 3d 세상을 구축할 수 있도록 해주기도 한다. 또한 융합을 통해 모션 사전 정보를 통해 더 정확한 정보를 더 빠르게 사용할 수 있다. 그러나 IMU에 사용되는 wheel odometry는 가벼운 알고리즘을 사용하는데 이 경우 모션 정보가 앞/뒤/좌/우, yaw인 3축만 나오게 된다. 6축을 요구하는 VSLAM에 대해서는 충분한 정보를 제공하지 못한다는 단점이 존재한다.또는 최근 딥러닝 기반에 monocular depth estimation을 통해 이 문제를 해결하려는 시도도 있다. 카메라만 사용하는 VSLAM은 구현하기도 어렵고, 시스템적으로 더 어려운 시스템을 구축해야 한다.이와 같이 많은 단점이 존재하기 때문에 아직은 잘 사용하지 않지만, 많은 상용화된 시스템들이 stereo 시스템(카메라 2대)를 사용하고 있고, 1대만을 사용하는 monocular VSLAM으로 가려는 움직임이 많다.Monocular VSLAM의 가장 유명한 논문으로는 DSO가 있다. 이 논문에서는 2016년에 나온 논문이고, direct tracking 기법을 통해 모션을 추정하고 있다. 그리고 픽셀값을 통해 밝기값이 많이 차이나는 edge들만 tracking하고 있고, 과거의 값들과 현재의 값들을 조합해서 3D 공간을 추론한다. 실제의 색상을 볼 수는 없지만, 대략적인 지도를 표현할 수 있다.이 3d 공간 특징들을 point cloud 형태로 표현을 하고 있으며, 3D point cloud를 띄엄띄엄 표현을 한 방식을 sparse SLAM이라 한다. monocular vslam은 대부분 sparse slam을 한다. dense slam을 못하는 것은 아니지만, monocular vslam의 특징을 봤을 때, 카메라가 싸고, 이미지가 단 1개를 통해 연산을 수행하므로 데이터가 적고, 컴퓨팅 보드가 가벼워도 충분히 돌 수 있다는 장점을 활용하기 위해 sparse slam을 수행한다.Stereo / Multi camera VSLAM2대~ 여러 대를 사용하는 VSLAM 방법을 말하는데, 인접한 카메라들간의 거리 값(baseline 거리)을 metric scale로 알고 있어야 거리와 깊이 추정이 가능하다. 이 baseline거리가 정확하게 구해져야 한다. 조금이라도 오차가 있다면 slam의 모든 과정에 에러가 포함된다.문제는 완벽한 baseline은 구할수가 없다. 그럼에도 정확한 측량을 위해 섬세한 calibration 과정을 자동차 하나하나마다 모든 카메라에 수행해야 한다. 그래서 자율주행 회사들은 3D 룸을 만들어서 여러 개의 사진을 찍어서 calibration을 수행한다.그렇다고 해서 calibration을 대충하게 되면 모든 slam 정보가 잘못 추정될 수 있기에 정확하게 수행되어야 한다. 장점두 이미지간의 disparity 정보를 이용해서 픽셀마다 depth를 추정할 수 있다. 픽셀마다 depth를 추정하는 것은 상당히 많은 연산을 필요로 하는데, 예를 들어 1280 x 720 의 해상도를 가지는 이미지라고 하면 2장이므로 최소 1280 x 720 x 2 의 연산을 필요로 하게 된다. cpu에서 이를 수행할 수가 없으므로 gpu에서 이를 수행한다.Stereo VSLAM 의 예시로는 stereo VIO 라고 하는 알고리즘이 있다. 이 알고리즘은 모션 추정에 특화되어 있는 가벼운 알고리즘이다.Multi VSLAM 의 예시로는 OmniSLAM이 있다. 이는 dense mapping에 초점을 둔 vslam 알고리즘이다. 주변 환경을 완벽하게 매핑하기 위해서 초광각 카메라 4개를 사용했고, 생성되는 맵이 매우 dense한 특징이 있다.이처럼 목적에 따라 파이프라인이 다 다르므로 알고리즘을 살펴볼 때는 어떤 목적의 알고리즘인지 파악하는 것이 중요하다.RGB-D VSLAM구조광(structured light) 또는 ToF(time-of-Flight) 센서를 이용한 카메라를 사용한다. 센서가 depth를 직접 얻어주기 때문에 계산이 필요없다. 두 센서 모두 대략 10m의 유효거리를 가진다.depth 데이터를 통해 3D 공간을 metric scale로 실시간으로 복원이 가능하다.단점으로는 10m안에서만 dpeth 데이터가 정확하고, FOV가 작다. 그리고 적외선 파장이 햇빛과 간섭이 발생하므로 실외에서 사용이 불가능하다.RGB-D 카메라에서의 이미지와 depth 정보는 동일한 스케일이 아니라서 동일하게 맞춰주는 과정이 필요하다.SLAM 기술의 적용자율주행 로보틱스 로봇 청소기가장 상용화가 잘 된 제품 중에 하나가 로봇 청소기다. 로봇은 지도를 가지고 있지 않기 때문에 집을 직접 돌아다니면서 탐색작업을 해야 한다. 이 과정에서도 청소가 가능할 것이고, 처음 보는 맵에 대해 최소 거리를 움직여서 맵을 완성해야 하는 최적화 문제를 수행해야 한다. 이를 active SLAM이라 하고, 이 SLAM은 SLAM과 path planning이 융합되어 있다고 볼 수 있다.예전에는 카메라, 2D 라이다 등을 사용했는데, 최근에는 전방 카메라와 2D 라이다, 천장을 바라보는 카메라 등이 존재한다. 산업 현장 측량 로봇로봇이 돌아다니면서 매핑을 하고 측량을 수행한다. 이를 통해 바닥이 기울어져 있는지, 벽이 잘 세워졌는지를 측량하기도 한다. 여기에는 라이다가 탑재되어 있고, RGB-D 카메라가 탑재되어 있으나 내비게이션을 위해 탑재되어 있다. 배달 / 물류 로봇병원에서 약품을 배달하기도 하고, 실내 뿐만 아니라 실외를 배달하기도 한다.자율주행 자동차자율비행 드론메타버스 - VR/AR각각의 분야에서 SLAM의 특성이 존재한다. 자율주행 로보틱스 제품화를 위한 규제 e.g. 배터리 폭발로 인한 위험성으로 인도에 다닐 수 없다 - 자유로운 알고리즘 개발 개발 언어, 개발 프로세스, 개발 알고리즘 등 다양하개 개발이 가능하다. - 전세계적으로 경쟁 자율주행 자동차 높은 수준의 딥러닝 솔루션 존재 딥러닝 + SLAM 안전한 소프트웨어 요구 오픈소스 알고리즘의 신뢰도 믿을 수 없다면 알고리즘을 직접 짜야함 다양한 환경에 의해 센서가 고장났을 때의 유연성 자율비행 드론 제품화를 위한 규제 가장 빠른 SLAM 요구 -&amp;gt; 가장 어려움 전력이 많이 소요됨 -&amp;gt; 컴퓨팅 전력이 작아짐 -&amp;gt; 가벼운 알고리즘 요구 배터리 폭발과 같은 위험성에 대해 안정성 메타버스 - VR/AR 가장 자유로운 개발 규제 익명 사고가 존재하지 않기 때문에 매우 자유로움 - 디바이스가 제한될 수 있음 - 빠르고, 정확하고, 가벼운 SLAM -&amp;gt; GPU를 사용해야 빠른 속도를 이룰 수 있지만, VR/VR과 같이 그래픽 렌더링을 위해 CPU를 사용해야 함 SLAM 이론 공부 팁SLAM 분야는 국내 자료가 다소 부족하기 때문에 대부분 해외 자료를 찾아봐야 한다. SLAM을 이해하기 위해 공부해야 하는 것 중 대표적으로 3가지가 있다. 그리고 그에 대해 참고 자료를 같이 소개하려고 한다. Mathematics Linear algebra (선형 대수) [youtube] 이상엽Math - 선형대수학 SLAM에서 사용하는 다양한 방법들을 설명하지는 않지만 개념을 익히기에 좋다. [교재] gilbert strang - Introduction to Linear algebra 5th Ed. SLAM에서 사용하는 다양한 방법들을 설명하지는 않으나 적절한 깊이까지 내용을 소개한다. [교재] Deisenroth et al - Mathematics for Machine Learning part1 수학과를 타겟으로 하기에 난이도가 상당히 높다. Part2는 머신러닝 기법이다. - probability &amp;amp; statistics (확률과 통계) [교재] Evans and Rosenthal - Probability and Statistics 기본적인 통계를 배우고, SLAM에서 사용하는 다양한 방법들을 소개해준다. 가까운 서점에서 책을 시작해도 좋다. Computer Vision Imaging theory 영상 처리와는 다르게 카메라가 이미지를 어떻게 얻어내는지에 대한 이론 [youtube] Marc Levoy - Lectures on Digital Photography 카메라의 컨트롤 기법과 깔끔한 이미지를 얻는 방법을 소개한다. 깔끔한 이미지를 얻어야 노이즈가 적어진다. - Image Processing [교재] 황선규 박사님 - OpenCV 4로 배우는 컴퓨터 비전과 머신 러닝 OpenCV 공식 document Optimization Optimization Theory SLAM은 최적의 지도를 얻어야 함 e.g. gradient descent [교재] Wright et al - Numerical Optimization ch8, ch10 전체적인 수치 최적화가 소개되고, SLAM에서 사용되는 이론은 ch10에서 소개된다. ch8은 수학적인 내부 구조를 보는 이론 [교재] Press et al - Numerical Recipes 2007년에 나온 책이라 공부는 말고, 최적화 이론과 이론을 코드로 옮기는 방법을 배우는 것 - eigen library 추후 실제 자율주행을 개발할 때는 라이브러리를 사용하지 않고 구현해야 하므로 1번 정도는 참고해보는 것도 좋다. - Numerical Optimization 전과 동 SLAM [youtube] Cyrill Stachniss 대학강의를 온라인으로 진행하게 되면서 공개했다. visual slam 뿐만 아니라 lidar slam도 어느정도 포함되어 있어서 가장 추천 - [교재] Gao - Introduction to Visual SLAM visual slam에 대해 깊게 파고 드는 책으로, 국내에서 커뮤니티에 번역판이 있어서 참고하면 좋다. - [교재] Ma - An Invitation to 3D vision 옛날 책이긴 하나 예전에 어떻게 풀었는지 확인할 수 있고, 현재까지 이어져오는 부분도 꽤 있다. SLAM 공부는 이 자료들로 시작하는 것을 매우 추천한다. 위의 1~3번도 필요하지만 최종 목표는 각각의 이론들을 통달하는 것이 목표가 아니라 SLAM이 목표이기 때문에 이 3가지를 먼저 공부하는 것을 추천한다. slam이란 굉장히 어려운 분야다. 시작하기는 어렵지만 공부하다보면 메리트가 있을 것이다. 논문을 봐도 이해가 되지 않거나, 방향성을 확정짓기 어려울수도 있다. C++ / 소프트웨어 공부 꿀팁 C++python에 비해 더 어려운 언어이고, 에러 메시지도 꽤나 어렵게 되어 있다. 대학교 강의에서 배우는 C++도 C언어와 융합된 부분을 가르치기도 하면서 더 헷갈리기도 한다. 모던 C++는 2011년에 만들어진 C++로, 기업에서 사용하는 C++는 대부분이 모던 C++이다. 모던 C++는 C++ 11,14,17,20,22 등이 있고, 대체로 자율주행에서는 14,17을 사용하고, 드론은 17, AR 분야에서는 17,20 등을 사용한다.C++을 사용하는 이유는 포인터나 동적할당에 큰 장점이 있기 때문이다. 그러나 이러한 포인터나 동적할당에서 에러가 굉장히 많이 발생하여 오히려 더 안좋은 효율을 보이기도 해서 이를 사용하려면 매우 깊이 알아야 했었다. 모던 C++에서 언어적으로 방지하는 기능을 추가해서 더 안전하게 프로그래밍하고, 더 간결하게 표현할 수 있게 되었다.모던 C++ 참고 자료 [교재] Marc Gregory - 전문가를 위한 C++ 굉장히 체계적이고, 디테일하게 설명해주므로 필수라고 할 수 있다. [교재] Jason Turner - C++ Best Practices 어떻게 하면 C++ 코드를 짤 수 있는지에 대한 책으로, 코드를 개선할 수 있는 방법을 소개한다. [youtube] CppCon - Back To Basics 시리즈 cpp conference 언어에 있는 다양한 기능을 소개하고, 목적이 무엇인지에 대한 초보자의 눈높이에 맞게 소개해준다. 소프트웨어파이썬과 C++의 가장 큰 차이점은 python은 개발자의 시간을 아끼는 프로그래밍이고, c++는 컴퓨터의 시간을 아끼는 프로그래밍이라 할 수 있다. C++는 cpu사이클이 덜 돌고 메모리를 덜 차지하는 것이 목표라고 할 수 있다.자료 구조 / 알고리즘 참고 자료 [블로그] 장형기 강사님 - https://www.cv-learn.com/20210214-data-structures-and-algorithms/ 자료구조에 대한 overview로 어떤 자료구조가 있고, 어떻게 작동하는지 이해할 수 있다. [교재] 코딩 테스트를 위한 자료 구조와 알고리즘 with C++ 여러 가지 자료구조를 직접 구현해보고, 예시문제를 따라가면 좋다. 이를 통해 c++로 여러 가지 자료구조를 사용하는데 익숙해질 수 있다. [youtube] 나동빈 - 이것이 취업을 위한 코딩 테스트다. with Python (C++도 있음) 기업 코테 수준의 문제들을 풀어보고 따라가며 익히면 좋다. 개발자 역량 강화 참고 자료 알고리즘 개발자는 효율성이 좋고, 성능이 좋은 알고리즘을 만드는 것이 목표일 것이다. 내가 짠 코드가 지속적으로 사용되게 하기 위해서는 좋은 소프트웨어를 짜야 한다. 버그가 없어야 하고, 쉽게 유지보수가 되어야 하며, 어느 곳에서든 쉽게 쓸 수 있는 코드여야 한다. [교재] 모던 C++ 디자인 패턴 우리가 짜는 소프트웨어 모듈이 어떤 디자인을 가져야 사용자가 쉽게 사용할 수 있고, 쉽게 유지보수가 될 수 잇는지에 대한 책이다. 디자인 패턴은 주니어 단계에서 시니어 단계로 넘어갈 때 숙지해야 할 부분이다. solid 법칙과 같이 깔끔한 코드를 적기 위한 룰이 몇가지 존재한다. 어느 정도 연차가 쌓이면 미팅을 할 때, 어떻게 디자인할지 어떻게 코드를 사용할지 서로 논의를 해야 한다. 이를 위해 디자인 패턴을 잘 익혀놓아야 한다. [교재] Robert C. Martin - the Clean Coder, The clean code the clean coder 코드를 잘 작성하는 개발자가 되기 위한 방법에 대해 소개 the clean code 코드를 잘 작성하는 방법에 대해 소개 사회 초년생에게 추천되는 책 알고리즘이나 프레임워크를 만들 때는 여러 모듈들이 함께 돌아가는 큰 스케일이 구성될 것이다. 작은 단위의 모듈들부터 큰 단위의 프레임워크를 다뤄야 하므로 이들을 다루는 방법과 마음가짐을 소개한다. " }, { "title": "[detection] DeepLearning Ping-Pong Ball Detection", "url": "/posts/pingpong/", "categories": "Projects, detection", "tags": "detection, distance-estimation", "date": "2022-05-16 21:20:00 +0900", "snippet": "프로젝트 개요목표 RGB 카메라로부터 입력된 이미지에 존재하는 탁구공을 검출 데이터 수집, 학습 데이터 생성 학습 (+ augmentation) deep learning inference 탁구공의 실제 위치를 추정 camera calibration camera + lidar sensor fusion 위치 추정 (bbox의 위치와 크기를 통해 or 바닥과 평면의 관계를 사용) 2D 지도에 탁구공의 위치를 표시 occupancy grid map 파이프라인 실시간으로 카메라로부터 이미지를 입력받음 (opencv , gstreamer) 딥러닝 데이터 준비 / 처리 데이터 왜곡 : 모델에 넣는 이미지는 왜곡을 삭제하는 것이 좋다. 그 이유는 탁구공을 검출할 때 원이 아닌 타원으로 입력되면 검출이 이상해질 수 있다. 또는 왜곡이 있어도 잘 되었다고 하더라도, 실제 위치를 추정하기 위해서는 왜곡을 보정해야 한다. 왜곡을 수정할 때는 레이블링 이전에 해도 되고, 모델 학습 이후 bbox에 대해서만 왜곡을 보정해줄 수도 있다. 분명 전체 이미지에 대해 보정할 필요가 없을 수 있지만, bbox에 대해서만 왜곡을 보정하면 디버깅이 다소 난해할 수 있다. - 데이터 레이블링 - 데이터 증강 - 모델 학습 yolov3 - 인식 결과 bbox (2D) - 탁구공 2D 위치 추정 카메라의 기하학적 투영 방법 활용 사전 조건이 필요 extrinsic 필요 탁구공의 평면과 지면 사이의 평면 변환식을 사용 데이터를 변환하는 식에 대한 추가 노력이 필요 -&amp;gt; 두 방법 모두 bbox 하단부분을 통해 탁구공의 종/횡방향 위치를 찾을 수 있다. sensor fusion object data가 image data로 어떻게 투영되어 있는지에 대한 RT를 계산하는 것이 중요 3D 위치 추정 (OGM) 실제 차량의 물리적 셀을 지정하고, 위치에 대한 grid 포인트도 계산이 가능 이 사진은 테슬라 AI day에서 보여준 실제 위치를 추정한 결과를 보여준 것이다.프로젝트 가이드data collection 차량에 장착된 카메라를 사용하여 탁구공 이미지 또는 비디오를 촬영 이미지 : 일정 시간 간격을 가지고 데이터를 촬영해야 함 비디오 : 이미지 학습 데이터를 만들 때 원하는 부분만 잘라서 사용할 수 있음 일정 시간 간격에 대한 이미지를 추출 데이터 레이블링을 위해 별도 디렉토리에 저장data labeling 적절한 레이블링 툴 선택 : CVAT, LabelImg, Labelme [Data collection]에서 저장한 데이터에 대해 레이블링 수행 레이블링 데이터의 형식을 지정 : 어떤 모델을 사용할지, 어떤 데이터셋을 기준으로 할지 지정 kitti2yolo, bdd2yolo model training 적절한 OD(Object detection) 모델을 선택 학습 데이터 형식 지정 레이블링 데이터를 모델 학습 데이터 형식에 맞게 변환 모델 학습 파라미터 튜닝 hyperparameter 조정 dataset size up or data augmentation model inference 학습 그래프의 추이를 확인, 적절한 모델 파일을 선택 (best epoch model pth) 실제 차량에서 동작 가능한 inference 코드를 작성 더 작은 모델을 사용 pruning , quantization GPU 가속기 : tensorRT - nvidia에서 제공하는 모델 가속기 방법 pytorch -&amp;gt; onnx -&amp;gt; tensorRT 성능 지표 model inference FPS (한 장의 이미지를 처리하는데 걸리는 시간) 15 fps 정도면 좋은 성능이다. 일반적인 8 fps를 목표로 하기 model prediction Accuracy (실제 환경에서 검출이 잘 되는지) F1 score, mAP object position estimation 사전 지식 탁구공의 실제 지름을 측정 카메라 높이, 카메라의 extrinsic 정보 모델의 예측 결과(bbox의 크기와 위치, 클래스) 중에서 활용 가능한 정보를 선택 object 거리 정확도 (종/횡 방향 정확도) 비교 카메라와 탁구공 사이의 실제 거리를 측정해서 비교 프로젝트 결과프로젝트를 통해 어떤 것들을 얻어가야 할지와 파이프라인에서 기본적인 방법들을 소개했는데, 추후 정보들을 찾아보기 위한 키워드를 소개한다. 모델 학습 개선 방향 fine tunning for hyper parameter more training dataset another OD model 차량에서의 모델 결과 model quantization model acceleration 위치 추정 정확도 또는 장애물의 속도와 가속도 추정 vision geometry LiDAR, RADAR sensor fusion 이때까지는 간단한 파이프라인을 소개했지만, top down 방식으로 더 많은 분야를 연구해볼 수 있을 것이다. 데이터 레이블링 data labeling/processing exploratory Data analysis data augmentation Object detection 2d/3d object detection segmentation model release model release / deploy model quantization position estimation distance/position estimation modo-depth estimation sensor fusion ( multi-model sensor ) real world position estimation occupancy grid map semantic map local dynamic map devops에서 사용되는 자동화 방법을 차용하여 Mechine learning 에서의 자동화 방법을 MLOps라 한다. 이는 ML-flow라는 패키지가 있으므로 참고하면 좋다.프로젝트 진행" }, { "title": "[데브코스] 14주차 - ImageProcessing geometrical distance estimation", "url": "/posts/distestim/", "categories": "Classlog, devcourse", "tags": "devcourse, ImageProcessing", "date": "2022-05-16 15:20:00 +0900", "snippet": "거리 추정을 위해 여러 가지 과정을 작성할 것이다. 먼저 거리를 추정하는 방법에는 크게 2가지가 있다. homography geometric method using FOVhomography의 경우 치명적인 단점이 있다. 지면이 평면이어야만 하는데, 실제 상황에서는 평면일 경우가 거의 드물다. 오르막길, 내리막길은 물론이고, 급출발, 급정거를 할 때도 pitch가 생겨서 평면이 아니게 된다. 그래서 homography를 사용한 방법은 간단하게 설명하고 2번으로 넘어가겠다.distance estimation using homography method&quot;&quot;&quot;순서1. 실측을 통한 obj point , img point 설정2. intrinsic, distort coefficients 정보 불러오기3. undistort4. obj point -&amp;gt; homogeneous 좌표계로 변환5. solvePnP, drawFrameAxes로 좌표축 확인, 0,0,0 좌표로 그려져야 맞는것6. findHomography 로 perspective matrix 추출7. image point -&amp;gt; homogeneous 좌표계로 변환8. homography와 image point를 내적하면 x,y,z 가 추출됨.&quot;&quot;&quot;import os, jsonimport cv2import mathimport numpy as npimport matplotlib.pyplot as pltvisualize = Falsetesting = Falsedef estimation_distance(): ######## step 2. intrinsic, distort coefficients information calibration_jsonfile_path = os.path.join(&quot;./calibration.json&quot;) with open(calibration_jsonfile_path, &#39;r&#39;) as calibration_file: calibration_info = json.load(calibration_file) intrinsic = calibration_info[&#39;intrinsic&#39;] fx, fy, cx, cy = intrinsic[&#39;fx&#39;], intrinsic[&#39;fy&#39;], intrinsic[&#39;cx&#39;], intrinsic[&#39;cy&#39;] camera_matrix = np.array([ [fx, 0.00000, cx], [0.00000, fy, cy], [0.00000, 0.00000, 1.00000], ], dtype=np.float64) dist_coeff = np.array([intrinsic[&#39;distortion_coff&#39;]], dtype=np.float64) img_file_path = &quot;./source/images/img_1.jpg&quot; img = cv2.imread(img_file_path) ######## step 3. undistort mapx, mapy = cv2.initUndistortRectifyMap(camera_matrix, dist_coeff, None, None, (img.shape[1], img.shape[0]), 5) undistort_img = cv2.remap(img, mapx, mapy, cv2.INTER_LINEAR) &quot;&quot;&quot; 높이 16.5cm, 가로 25cm, 세로 30cm &quot;&quot;&quot; img_points = np.array([ [170, 426], # bottom left [459, 426], # bottom right [193, 409], # second left [433, 409], # second right [206, 380], # third left [416, 380], # third right [216, 366], # top left [403, 366], # top right ], dtype=np.float32) obj_points = np.array([ [1, 00.0, 00.0], # bottom left [1, 25.0, 00.0], # bottom right [1, 00.0, 10.0], # second left [1, 25.0, 10.0], # second right [1, 00.0, 20.0], # third left [1, 25.0, 20.0], # third right [1, 00.0, 30.0], # top left [1, 25.0, 30.0], # top right ], dtype=np.float32) data_size = len(img_points) obj_points = obj_points / obj_points[0,0] homo_obj_points = cv2.hconcat([obj_points[:,1], obj_points[:,2], obj_points[:,0]]) homo_obj_points[:,1] = 0 - homo_obj_points[:,1] if visualize: # PLT SHOW for search pixel value # both_img = cv2.hconcat([undistort_img, img]) plt_img = cv2.cvtColor(undistort_img, cv2.COLOR_BGR2RGB) plt.imshow(plt_img) plt.show() ######## get rotation , translation vector using obj points and img points _, rvec, tvec = cv2.solvePnP(obj_points, img_points, camera_matrix, distCoeffs=None, useExtrinsicGuess=True, flags=cv2.SOLVEPNP_EPNP) undistort_img = cv2.drawFrameAxes(undistort_img, camera_matrix, distCoeffs=dist_coeff, rvec=rvec, tvec=tvec, length=2, thickness=5) ######## image points, object points 의 pair 쌍이 맞는지 재투영을 통해 확인 proj_image_points, _ = cv2.projectPoints(obj_points, rvec, tvec, camera_matrix, None) for proj_image_point, img_point in zip(proj_image_points, img_points): print(img_point, proj_image_point, &quot;\\n&quot;) ######## 카메라 축 확인 img = cv2.cvtColor(undistort_img, cv2.COLOR_BGR2RGB) plt.imshow(img) plt.show() homography, _ = cv2.findHomography(img_points, homo_obj_points) print(f&quot;data_size {data_size}&quot;) return homographyif __name__ ==&quot;__main__&quot;: # get homography img = cv2.imread(&quot;img_1.jpg&quot;, cv2.IMREAD_ANYCOLOR) ### bbox example top_left = (160,255) #(416,248) top_right = (168,255) #(426,248) bottom_left = (160,247) #(417,258) bottom_right = (168,247) #(427,258) center_point = (bottom_right[0] - bottom_left[0], bottom_left[1] - top_left[1]) homography = estimation_distance() img_point = np.array([center_point[0],center_point[1], 1], dtype=np.float32) ######## inference estimation = np.dot(homography, img_point) x,y,z = estimation[0] ,estimation[1],estimation[2] distance = math.sqrt((x/z)**2 + (y/z)**2 + (z/z)**2) ######## visualize cv2.rectangle(img, (top_left), (bottom_right), (255,255,0), 2) cv2.putText(img, &quot;distance : &quot;+str(round(distance,2)) + &quot;cm&quot;, top_left, 2, 0.5, (10,250,10),1) cv2.imshow(&quot;img&quot;, img) cv2.waitKey() print(f&quot;homography matrix \\n{homography}\\n&quot;) print(f&quot;distance {round(distance,2)}cm&quot;) 순서 좌표축 원점으로 정할 객체를 기준으로 실측을 통한 obj point , img point 설정 intrinsic, distortion coefficients 정보 불러오기 undistort obj point -&amp;gt; homogeneous 좌표계로 변환 solvePnP, drawFrameAxes로 좌표축 확인, 0,0,0 좌표로 그려져야 맞는것 findHomography 로 perspective matrix 추출 image point -&amp;gt; homogeneous 좌표계로 변환 homography와 image point를 내적하면 x,y,z 가 추출됨. 이렇게하면 실제 객체와의 거리를 구할 수 있다. 이 때 주의해야 할 점은 homography랑 내적을 하면 x,y,z에서 z가 1이어야 하지만, 1이 아니다. 따라서 z를 나눠주어야 한다.homography matrix [[-7.97942714e-02 -4.84927735e-02 3.48389453e+01] [ 3.63617110e-07 -2.79608421e-01 1.20439194e+02] [-5.94557243e-07 -4.39631252e-03 1.00000000e+00]]distance 127.46cmhomography matrix라는 것 자체가 2차원에서 3차원으로 투영하기 위한 행렬이다. 그래서 image points에서 object points로 가기 위한 3x3 행렬이고, object points를 잡을 때 후륜축을 잡을지 카메라 위치를 잡을지에 따라 중심 좌표와 객체와의 거리를 추정할 수 있다. homography를 하면 카메라의 높이를 고려하지 않아도 되지만 지면 자체가 평면이라고 생각하고 사용하는 알고리즘이므로 지면이 평면이 아니라면 사용할 수 없다고 한다." }, { "title": "[데브코스] 14주차 - DeepLearning Geometrical Distance Estimation", "url": "/posts/distanceestim/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-05-16 15:20:00 +0900", "snippet": "이전 글에서 computer vision에서 활용하는 기하학적 방법으로 객체의 위치를 추정하는 방법을 배웠다.Geometrical Distance Estimation이처럼 기하학적 방법으로 객체의 위치를 추정하는 방법들은 많이 존재한다. 그 첫번째 방법이 카메라의 투영(calibration)을 활용한 방법이다.Geometrical Method카메라와 대상 객체의 기하학적 조건을 활용한 방법으로 카메라의 설치 노이와 대상 객체가 3차원 공간에 존재할 때 Extrinsic calibration 정보를 활용한 방법이다.이는 camera와 차량의 위치 관계에 대한 그림이다. 카메라를 A에 장착하고, B차량을 촬영하면 B차량의 타이어 지점은 이미지에서 y1에 표현된다.그러나 이 방법은 바닥이 평면을 이루어야 하고, 바닥과 광학축이 평행해야 한다는 조건이 존재했다.삼각 비례식을 활용하여 객체의 실제 거리를 추정할 수 있다.$ object:distance = \\cfrac{focal:length * real:world’s:height:of:object}{image’s:height:of:object} $이를 3차원 좌표로 표현하면 다음과 같다.전방을 z, 오른쪽을 y, 아래를 x인 카메라 좌표계로 되어 있고, 3차원 공간에 존재하는 객체 P에 대해 이미지에 투영한다. 식은 위의 식과 거의 동일하다.$ y : f = h : Z , Z = \\cfrac{f * h}{y} $이를 구하기 위해서는 ZX plane 뿐만이 아닌, ZY plane을 사용해야 한다. 또한 객체가 지면에 붙어 있지 않는다면 추정을 하기 어렵다. 예를 들어 신호등을 인식했지만, 기둥은 추론이 되지 않았고, 신호등만 인식이 되어 있다면 지면에서 일정 높이 떨어져 있는 객체의 bbox만을 사용하여 거리를 추정해야 한다.Field of View카메라의 대상 객체의 특성을 활용한 방법으로 카메라의 고유한 특성(intrinsic , sensor, lens의 속성)과 대상 객체의 실제 크기 정보를 활용하는 방법이다. 이는 extrinsic calibration 정보에 대해 상대적으로 의존도가 낮다.FOV(Field of View)를 사용하면 ZY plane에 대한 거리 정보와 지면에서 일정 높이 떨어져 있는 객체에 대한 거리를 추정할 수 있다.카메라가 좌표계 원점에 존재하고, 사각형이 이미지일 때, 가로를 width, 세로를 height라 할 수 있다. 이 때, width와 height에 대해 $ FOV_v $ 와 $ FOV_H $ 가 존재한다. v는 vertical, h는 horizontal이고, FOV는 $ ^{\\circ} $ 단위로 표기한다. 이미지와 원점간의 거리를 f라 할 수 있다.FOV가 커질수록 실제 공간을 더 넓게 투영하여 이미지로 표현할 수 있을 것이고, FOV가 작을수록 실제 공간을 더 좁게 투영하여 이미지를 표현할 수 있다.간단하게 2차원으로 보자.실제 객체는 H의 높이를 가지고, 객체와 렌즈 사이의 거리를 $ D_o $, 이미지 안에서의 객체 높이를 h, 이미지와 렌즈 사이의 거리를 $ D_f $ 라 한다. F는 focal length이다. 그리고 $ \\alpha $는 이미지 안에서 객체의 bbox에 대한 FOV_H값이다. 즉 bbox 위와 아래가 이루는 각도이다.이 때, 수식 $ \\cfrac{1}{D_o} + \\cfrac{1}{D_f} = \\cfrac{1}{F} $ 에 의해 $ D_o = \\cfrac{D_f * F}{D_f - F} $ 로 표현된다. 그리고, $ tan(\\cfrac{\\alpha}{2}) = \\cfrac{h}{2 D_f} $ 이므로, D_o 는 다음과 같다.\\[D_o = \\cfrac{F * \\cfrac{h}{2 tan\\cfrac{\\alpha}{2}}}{\\cfrac{h}{2 tan\\cfrac{\\alpha}{2}} - F}\\]이 때, a는 FOV로 계산이 가능하고, 다른 변수도 모두 알고 있는 값이므로 이미지 내에서의 object의 크기와 위치를 알면 D_o를 구할 수 있다.더 자세한 설명을 위해 가상의 카메라의 intrinsic, FOV를 다음과 같이 정의하자. image size : 1280px, 720 px fx = 1000, fy = 1000 cx = 640, cy = 360 $ FOV_H = 80 ^{\\circ} $, $ FOV_V = 40 ^{\\circ} $이미지 내 표지판의 위치에 대한 값 bbox size : 40px, 40px center about bbox bottom to bbox bottom : 940px image center to bbox left bottom : 640px바운딩 박스의 좌하단과 height 방향으로의 직선 사이의 각도를 $ \\theta $ 라 한다. 그리고 실제 표지판의 크기는 반지름이 0.5m이다. 어차피 대부분의 경우 사각형을 기준으로 측정하므로 w = 1m, h = 1m 로 할 수 있다.$ FOV_{H(640)} = 0 ^{\\circ} $ 인 이유는 이미지를 기준으로 중앙이 0, 왼쪽 방향이 (-), 오른쪽 방향이 (+)이다. 따라서 $ FOV_{H} $ 의 범위는 -40 ~ 40 이다.이미지의 중점을 기준으로 객체의 중점과의 이루는 방위각이 $ \\theta $ 이고, 비례식 $ 640 : 300 = 40 : \\theta $ 을 세워서 계산한다.$ \\theta = \\cfrac{\\triangle x}{640} * 40.0 ^{\\circ} = \\cfrac{300}{640} * 40.0 = 18.75 ^{\\circ} $$ \\theta $ 를 구했다면, 표지판과의 거리를 구할 수 있다.$ D_o = \\cfrac{F * \\cfrac{h}{2 tan\\cfrac{\\alpha}{2}}}{\\cfrac{h}{2 tan\\cfrac{\\alpha}{2}} - F} = \\cfrac{1000 * \\cfrac{40}{2 tan(18.75 ^{\\circ})}}{\\cfrac{40}{2 tan(18.75 ^{\\circ} )} - 1000} $그리고 종방향과 횡방향의 분리를 위해 dx, dy로 분리한다.\\[d_x = d * cos(\\theta) , d_y = d * sin(\\theta)\\]이때까지는 FOV_H 를 사용하여 구했지만, FOV_V를 사용하여 구할 수도 있다.FOV_H에 대한 좌표와 값들은 빨간색에 해당되고, FOV_V는 파란색에 해당한다. 평면의 특성을 활용한 방법 여기서 bbox 각각의 좌표를 a,b,c,d 라 표현했을 때, 각각의 좌표는 다른 값을 가지고 있겠지만, 다른 평면으로 바라봤을 때는 직선으로 표현될 것이고, 그것이 첫번째 이미지가 된다. 그렇다면 bbox 각각의 좌표는 다 동일한 distance, d과 높이 R을 가지고 있으므로 다음과 같은 비례식을 사용할 수 있다.\\[r_a : f = d : R_a\\]\\[r_b : f = d : R_b\\] 이를 정리하여 d에 대해 정리하면 $ d = \\cfrac{(r_b - r_a) * (R_b - R_a) }{f} = \\cfrac{40px * 1m}{1000px} $ 로 정리될 수 있다. 그러나 이 방법은 extrinsic에 의존적이라 잘 설정해줘야 한다.위의 두 방법(geometric method, FOV) 는 복잡한 기하학적 변환이나 수식이 들어가 이해하기 어려운 단점이 있다.Perspective Projection Method또 다른 방법으로 평면 변환(plane transform)이 있다. 이 방법에는 객체가 지정하고자 하는 평면과 관련이 있어야 하는 단점이 존재한다.Homographyperspective projection method의 대표적인 방법으로 homography가 있다.이를 위해서는 평면과 평면 변환에 대해 알고 있어야 한다. 투영(projection)이란, 3차원 공간에 존재하는 어떤 대상을 2차원 이미지 공간(평면)에 투영하는 과정이다. 이미지를 다룬다면 언제나 1개 이상의 평면을 사용하고 있다. 이 평면을 image coordinate에서 정의되는 image plane이라고 한다.3차원 공간에 존재하는 어떤 점이 2차원 평면에 투영한다고 할 때, 3차원 공간은 무한 개의 평면을 가지고 있다. 그래서 간단하게 보기 위해 하나의 평면으로 표시하게 되면, 2차원 투영 좌표는 (x,y)에서 (x,y,1)로 , 3차원 평면의 좌표는 (X,Y,Z)에서 (X,Y,1)이 된다.이처럼 3차원 공간 상에 놓여진 하나의 2차원 평면을 표현할 때 homogeneous 좌표계를 사용한다. homogeneous 좌표계에서는 2차원 점이 3차원으로 투영이 된다.그렇다면 카메라 좌표계를 기준으로 normalized image coordinate 좌표계에서의 점과 image coordinate에서의 점을 3차원으로 변환할 수 있다.카메라 좌표계에서 Z는 전방을 가리킨다. 이 때 normalized image coordinate는 원점으로부터 1만큼 떨어져 있고, image coordinate는 f, 실제 좌표계에서의 점은 Z만큼 떨어져 있다. 그래서 각각의 점들을 변환한다. normalized image coordinate : $ (u_n, v_n, 1) $ image coordinate : $ (f_x, f_y, f) $ world coordinate : $ (X_{w_o}, Y_{w_o}, Z_{w_o}) $homogeneous 좌표계에서는 같은 투영선(projection ray) 상에 있는 좌표들은 다 동일한 점이라고 판단한다. 결과적으로 homogeneous 좌표계는 3차원 공간을 2차원 공간으로 투영하면 무한개의 점으로 표현이 가능해진다. 그러나 어떤 이미지 공간에 투영하는가에 따라 달라지는 것이지만, 그 대상은 동일하다. 2차원을 3차원으로 변환하는 것을 Inverse-Projection이라 부른다.이미지 평면에서 정규 이미지 평면으로 변환하는 것을 평면 변환(projective transform)의 한 종류로 볼 수 있다. 평면의 좌표계 기준은 달라지지 않았지만, 차선과 같이 실제 공간에서는 평행하는 두 선이 이미지 상에서는 만나지도록 변환이 된다.projective geometry의 성질을 이용한 이미지 평면을 다른 평면으로 변환하는 것도 가능하다. 이를 활용한 대표적인 예가 BEV(bird eye view) 변환이다. 같은 높이에서 바라보는 방향과 새처럼 위에서 바라보는 방향은 다르다. 위에서 아래로 바라보는 시점을 BEV라 한다.원래의 이미지 상에서는 차선들이 소실점에서 만나게 된다. 그러나 이를 BEV로 변환하게 되면 차선들이 다시 평행하게 보인다. 원래 카메라 이미지 상에서 객체의 bbox, class를 추출하고 이를 bird`s eye view에 투영시켜서 회피를 할지 말지를 결정한다.여기서 ground plane은 실제 지면을 의미하고 이는 m단위를 사용하고 있다. 카메라로 보는 이미지 a를 c로 바꾸는 작업을 수행하는 것을 perspective transformation이라 하고, 원래의 이미지 a에서는 pixel단위이고, BEV인 c에서는 m단위를 사용한다. BEV도 이미지이므로 pixel 단위라고 생각할 수 있지만 이는 틀렸다.2차원 좌표계에서 데이터를 부르는 단위를 Pixel이라 하고, 3차원 좌표계에서 데이터를 부르는 단위를 Voxel이라 하는데, 픽셀이 이미지 해상도를 의미하지는 않는다. 즉 이미지를 부르는 방법이 픽셀이지 모든 픽셀이 이미지를 의미하는 것은 아니다.Geometrical Distance Estimation code본질적으로 원근 변환(perspective transform)과 투영 변환(projective transform, homography)은 동일하다. 원근 변환은 투영 변환이지만, 투영 변환은 원근 변환일 필요는 없다. 개념과 변환의 결과는 동일하지만 투영 변환이 조금 더 상위 개념이다. 대체로 같은 것으로 판단한다.openCV에서 제공하는 homography와 perspective transformation에 대해 공부하고자 한다. 이는 geometrical distance estimation을 수행할 때 반드시 필요한 intrinsic matrix와 extrinsic matrix에 대한 개념과 공부를 하기에 적절하다.Homographytutorial : https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html먼저 homography를 공부하고자 한다. openCV에서는 평면 변환을 위한 함수를 제공한다. getPerspectiveTransform() findHomography()이 두 함수는 동일하지만, 입력 인자가 조금 다르다. 전자의 경우 4개의 점들을 입력으로 받지만, 후자의 경우 4개 이상의 점들을 입력으로 받는다. 4개의 점을 가지고 있다면 전자를 사용하고, 4개 이상의 점들을 가지고 있어서 좀 더 정확한 변환을 하고자 하거나, 점들 사이에 존재하는 오차를 알아서 제거해주길 원하는 경우에는 후자를 사용하는 것이 좋다. 그리고 findhomography의 경우 입력 포인트들을 통해 변환 행렬을 계산할 때 오차를 발생시키는 값을 제거하는 방법(RANSAC)이 포함되어 있다.Geometrical Method에서는 카메라와 지면이 서로 평행하다는 것을 가정했지만, 실제로는 완벽하게 평행할 수 없다. 그래서 카메라의 자세를 추정하는 별도의 알고리즘을 또 사용해야 한다. 즉 이미지를 취득한 후 소실점/소실선을 이용한 카메라 자세를 추정한 후 객체와의 거리를 추정한다. 그러나 이 평면 변환의 경우 이미지와 그 타겟이 되는 가상의 평면에 대한 변환 과정을 다루기 때문에 카메라와 지면이 서로 평행하지 않아도 된다. 즉 어떤 카메라의 자세든 변환이 가능하다.이전에 배웠던 camera calibration 코드를 먼저 보자.이 코드에서는 캘리브래이션 패턴에 존재하는 특징(코너)를 검출한다. 가장 중요한 것은 imgpoints와 objpoints의 pair 데이터를 만드는 것이 중요하다. 그 이유는 추후에 distance estimation을 할 때는 image가 아닌 pair 데이터를 사용하기 때문이다.import cv2import globimport numpy as npimport timeimport sysDISPLAY_IMAGE = False# Get Image Path Listimage_path_list = glob.glob(&quot;images/*.jpg&quot;)# Chessboard ConfigBOARD_WIDTH = 9BOARD_HEIGHT = 6SQUARE_SIZE = 0.025# Calibration Configflags = ( cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + cv2.CALIB_CB_FAST_CHECK)pattern_size = (BOARD_WIDTH, BOARD_HEIGHT)counter = 0image_points = list()&#39;&#39;&#39; =========== calibration pattern&#39;s corner detection ==========&#39;&#39;&#39;for image_path in image_path_list: image = cv2.imread(image_path, cv2.IMREAD_COLOR) # OpneCV Color Space -&amp;gt; BGR image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ret, corners = cv2.findChessboardCorners(image_gray, pattern_size, flags) if ret == True: if DISPLAY_IMAGE: image_draw = cv2.drawChessboardCorners(image, pattern_size, corners, ret) for corner in corners: counter_text = str(counter) point = (int(corner[0][0]), int(corner[0][1])) cv2.putText(image_draw, counter_text, point, 2, 0.5, (0, 0, 255), 1) counter += 1 counter = 0 cv2.imshow(&quot;img&quot;, image_draw) cv2.waitKey(0) image_points.append(corners)object_points = list()object_points = np.asarray(object_points, dtype=np.float32) # change numpy array camera calibration1개의 이미지를 통해 intrinsic, extrinsic 정보를 얻어오는 코드이다.&#39;&#39;&#39; =========== camera calibration ==========&#39;&#39;&#39;tmp_image = cv2.imread(&quot;images/left01.jpg&quot;, cv2.IMREAD_ANYCOLOR)image_shape = np.shape(tmp_image)image_height = image_shape[0]image_width = image_shape[1]image_size = (image_width, image_height)ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(object_points, image_points, image_size, None, None) intrinsic ret : RMS(ROOT Mean Square) 오차 =&amp;gt; extrinsic 정보와 objpoints를 가지고 이미지에 재투영하여 원래의 점과 비교해서 오차를 계산한다. 계산했을 때 많이 튀는 노이즈 값을 필터링하는 것도 중요 mtx : intrinsic matrix dist : distortion cofficients extrinsic 각 이미지에서 설정한 objpoints의 원점에 대한 실제 카메라의 extrinsic calibration 정보이다. rvecs : 각 이미지에 대한 camera coordinate에서의 rotation tvecs : 각 이미지에 대한 camera coordinate에서의 translation get homography matrix&#39;&#39;&#39; =========== homography matrix ==========&#39;&#39;&#39;# Part 2. Find homography matrix# Step A. Select imagesimg1 = cv.imread(os.path.join(&quot;images&quot;, &quot;left01.jpg&quot;))ret, corners1 = cv.findChessboardCorners(img1, (CHESSBOARD_WIDTH, CHESSBOARD_HEIGHT))corners1 = corners1.reshape(-1, 2)img2 = cv.imread(os.path.join(&quot;images&quot;, &quot;right13.jpg&quot;))ret, corners2 = cv.findChessboardCorners(img2, (CHESSBOARD_WIDTH, CHESSBOARD_HEIGHT))corners2 = corners2.reshape(-1, 2)## Homography# src -&amp;gt; dst point transformation -&amp;gt; find matrix# Step B. Find homographyhomography, status = cv.findHomography(corners1, corners2, cv.RANSAC)# Step C. Display Resultimg_draw_matches = cv.hconcat([img1, img2])for i in range(len(corners1)): pt1 = np.array([corners1[i][0], corners1[i][1], 1]) pt1 = pt1.reshape(3, 1) pt2 = np.dot(homography, pt1) pt2 = pt2 / pt2[2] end = (int(img1.shape[1] + pt2[0]), int(pt2[1])) start = (int(pt1[0][0]), int(pt1[1][0])) color = list(np.random.choice(range(256), size=3)) color = (int(color[0]), int(color[1]), int(color[2])) cv.line(img_draw_matches, start, end, tuple(color), 5)cv.namedWindow(&quot;Draw Matches&quot;, cv.WINDOW_NORMAL)cv.imshow(&quot;Draw Matches&quot;, img_draw_matches)cv.imwrite(&quot;draw.png&quot;, img_draw_matches)cv.waitKey(0) step A img1은 원본 이미지, img2는 대상이 되는 이미지 findChessboardCorners를 사용하여 transform의 pair 데이터를 결정 추후 변환에 사용되는 데이터는 이미지가 아니라 pair 데이터이다. pair 포인트들을 자기 마음대로 찍어도 상관은 없으나 편리함을 위해 함수를 사용한 것이다. step B 이미지가 아닌 N개의 corners1, corners2 를 사용 cv.RANSAC은 여러 개의 pair 데이터를 사용하면서 오류가 있는 데이터는 제외하는 알고리즘 step C 결과 확인 cv.line를 사용하여 pair 데이터가 어떻게 변환이 되었는지 확인 cv.warpPerspective 를 사용하여 homography matrix로 이미지 자체를 변환 convert img plane to ground plane이때까지는 ground plane를 image plane으로 변환했다. 이제는 반대로 image plane을 ground plane으로 변환하는 과정을 수행한다. homography matrix로 카메라의 extrinsic 정보를 추출할 수 있다. 카메라의 POSE를 그리는 것이 중요 : drawFrameAxes() 이 좌표가 obj point의 0index와 동일한 좌표를 가져야 한다. homography distance estimationimage point와 homography를 통해 거리를 추정하는 코드# Step A. homography distance estimationimport cv2import jsonimport osimport numpy as npimport matplotlib.pyplot as pltimport calibration_parserif __name__ == &quot;__main__&quot;: calibration_json_filepath = os.path.join(&quot;image&quot;, &quot;cologne_000065_000019_camera.json&quot;) camera_matrix = calibration_parser.read_json_file(calibration_json_filepath) image = cv2.imread(os.path.join(&quot;image&quot;, &quot;cologne_000065_000019_leftImg8bit.png&quot;), cv2.IMREAD_ANYCOLOR) # extrinsic -&amp;gt; homography src, dst # prior dst -&amp;gt; image coordinate # present dst -&amp;gt; vehicle coordinate (=camera coordinate) # world&#39;s lane value # lane (inner) width -&amp;gt; 2.5m, lane width -&amp;gt; 0.25m # lane length -&amp;gt; 2.0m # lane interval -&amp;gt; 2.0m &quot;&quot;&quot; Extrinsic Calibration for Ground Plane z가 전방 , x,y,z [0, 1] 464, 833 -&amp;gt; 0.0, 0.0, 0.0 1639, 833 -&amp;gt; 0.0, 3.0, 0.0 lane width + 2 * lane inner width [2, 3] 638, 709 -&amp;gt; 0.0, 0.0, 2.0 lane length 1467, 709 -&amp;gt; 0.0, 3.0, 2.0 [4, 5] 742, 643 -&amp;gt; 0.0, 0.0, 4.0 1361, 643 -&amp;gt; 0.0, 3.0, 4.0 [6, 7] 797, 605 -&amp;gt; 0.0, 0.0, 6.0 1310, 605 -&amp;gt; 0.0, 3.0, 6.0 &quot;&quot;&quot; image_points = np.array([ [464, 833], [1639, 833], [638, 709], [1467, 709], [742, 643], [1361, 643], [797, 605], [1310, 605] ], dtype=np.float32) # X Y Z, X -&amp;gt; down, Z -&amp;gt; forward, Y -&amp;gt; Right # 실측이 중요하다. object_points = np.array([ [0.0, 0.0, 0.0], [0.0, 3.7, 0.0], [0.0, 0.0, 6.0], [0.0, 3.7, 6.0], [0.0, 0.0, 12.0], [0.0, 3.6, 12.0], [0.0, 0.0, 18.0], [0.0, 3.7, 18.0] ], dtype=np.float32) # z,y,x 이고, 각각의 0.0 자리에 z는 extrinsic에 있는 z를 넣으면 실제 카메라의 위치에 대한 축을 생성할 수 있음 DATA_SIZE = 8 # object point # X: forward, Y: left, Z: 1 # homography를 하기 위해서는 0index와 (1,2)index를 떼어내서 카메라 좌표계로 변환해줘야 한다. (2,-1,0) # homo에서 계산을 편하게 하기 위해 z를 사용하지 않으려고 1로 지정 homo_object_point = np.append(object_points[:,2:3], -object_points[:,1:2], axis=1) homo_object_point = np.append(homo_object_point, np.ones([1, DATA_SIZE]).T, axis=1) print(homo_object_point) # 지면에 대해서 위치와 자세 추정이 가능하다면, # 임의의 포인트를 생성하여 이미지에 투영할수있다. # extrinsic은 차량의 중심축에 대한 값들이므로 , 객체와 차량의 중심축으로부터의 거리를 다 계산할 수 있고, 이를 통해 범퍼까지의 거리를 계산할 수 있다. retval, rvec, tvec = cv2.solvePnP(object_points, image_points, camera_matrix, distCoeffs=None, useExtrinsicGuess=True, flags=cv2.SOLVEPNP_EPNP) # 잘 맞지 않는다. # 왜냐하면, 이미지 좌표와 실제 오브젝트와의 관계가 부정확하기 때문 # 실제 측정을 통해 개선이 가능하다. # TODO: 축과 차선의 위치가 맞지 않는 이유는 실제 이미지 좌표에 대한 거리정보를 모르기 때문에 발생됨. 실측을 통해 정확한 거리값으로 맵핑하면 잘 맞게 된다. image = cv2.drawFrameAxes(image, camera_matrix, None, rvec, tvec, 1, 5) # proj_image_points, _ = cv2.projectPoints(object_points, rvec, tvec, camera_matrix, None) # print(proj_image_points) # TODO: 정확한 obj point를 한다면 출력값이 image points와 같은 값을 가질 것 homography, _ = cv2.findHomography(image_points, homo_object_point) # None 이 나온다면 두 shape이 맞지 않기 때문이다. # print(proj_image_points.shape) # (u, v) -&amp;gt; (u, v, 1) appned_image_points = np.append(image_points.reshape(8, 2), np.ones([1, DATA_SIZE]).T, axis=1) # print(homography.shape) for image_point in appned_image_points: # estimation point(object_point) -&amp;gt; homography * src(image_point) estimation_distance = np.dot(homography, image_point) x = estimation_distance[0] y = estimation_distance[1] z = estimation_distance[2] print(x/z, y/z, z/z) # homogeneous 좌표게에서는 마지막이 항상 1이어야 하므로 나눔 # homo object point와 비교하여 얼마나 차이나는지 확인 순서 실제 측량을 통해 obj point와 image point를 생성 random의 intrinsic 정보를 설정 opj point는 world 좌표계이므로 이를 카메라 좌표계로 변환 solvePnp를 통해 카메라 위치 추정 rotation, translation vector를 얻고 그를 통해 drawFrameAxes 이미지 좌표와 실제 오브젝트의 관계를 수정 projectPoints를 통해 iamge point와 object point가 거의 동일한지 확인 findHomography를 통해 iamge point와 object point에 대한 변환행렬 생성 iamge point의 좌표계를 homogeneous 좌표계로 변환 각 image point에 대해 homography를 곱하여 거리를 추정 여기서 중요한 점 : 호모그래피는 평면과 평면 사이의 변환을 의미한다. 따라서 바닥면이 평면이 아니라면 사용하기 어렵다. geometrical distance estimationintrinsic을 통해 이미지 왜곡 보정 및 bbox에 대한 거리 추정한 값을 실제와 비교하는 코드# Step B. geometrical distance estimationimport jsonimport cv2from cv2 import undistortimport matplotlib.pyplot as pltimport osimport numpy as np# file parsingjson_file_path = os.path.join(&quot;data&quot;, &quot;000076.json&quot;)image_file_path = os.path.join(&quot;data&quot;, &quot;1616343619200.jpg&quot;)window_name = &quot;Perception&quot;with open(json_file_path, &quot;r&quot;) as json_file: labeling_info = json.load(json_file)image = cv2.imread(image_file_path, cv2.IMREAD_ANYCOLOR)camera_matrix = np.asarray(labeling_info[&quot;calib&quot;][&quot;cam01&quot;][&quot;cam_intrinsic&quot;], dtype=np.float32)dist_coeff = np.asarray(labeling_info[&quot;calib&quot;][&quot;cam01&quot;][&quot;distortion&quot;], dtype=np.float32)undist_image = cv2.undistort(image, camera_matrix, dist_coeff, None, None)labeling = labeling_info[&quot;frames&quot;][0][&quot;annos&quot;]class_names = labeling[&quot;names&quot;]boxes_2d = labeling[&quot;boxes_2d&quot;][&quot;cam01&quot;]CAMERA_HEIGHT = 1.3 # TODO: 이 값에 따라 거리가 크게 바뀜. 지면과 카메라가 바라보는 방향이 이루는 각이 특정한 각을 이룬다면 pitch에 대한 보정이 필요하다.# distance = f * height / img(y)# 종/횡 방향으로 분리된 거리가 아닌, 직선거리# FOV 정보를 알면 -&amp;gt; 종/횡 분리가 가능하다.index = 0for class_name, bbox in zip(class_names, boxes_2d): xmin, ymin, xmax, ymax = bbox xmin = int(xmin) ymin = int(ymin) xmax = int(xmax) ymax = int(ymax) if xmin &amp;lt; 0 or ymin &amp;lt; 0 or xmax &amp;lt; 0 or ymax &amp;lt; 0: # if -1, pass continue width = xmax - xmin height = ymax - ymin # Normalized Image Plane y_norm = (ymax - camera_matrix[1][2]) / camera_matrix[1][1] distance = 1 * CAMERA_HEIGHT / y_norm print(int(distance)) cv2.rectangle(undist_image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 3) cv2.putText(undist_image, f&quot;{index}-{class_name}-{int(distance)}&quot;, (xmin, ymin+25), 1, 2, (255, 255, 0), 2) index += 1display_image = cv2.cvtColor(undist_image, cv2.COLOR_BGR2RGB)plt.imshow(display_image)plt.show() 순서 실제 객체와 카메라 사이의 거리를 측정 카메라 intrinsic, distortion 획득 undistorting image bbox 정보 저장 bbox 정보에 대한 fy, cy를 가져와서 normalized image plane 구함 distance 저장 실제 거리와 distance가 맞는지 확인하고, 실제 camera 높이도 측정하여 camera height를 조정 만약 높이가 맞다면 intrinsic이 틀렸거나 bbox가 틀리게 나온 것 bbox가 맞게 들어오고 있는지 확인을 위해 visualize 맞다면 intrinsic 조정 calibration_parser.pyimport jsonimport numpy as npdef read_json_file(path): print(&quot;=&quot; * 50) print(&quot;Read JSON File: &quot;, path) print(&quot;=&quot; * 50) with open(path, &quot;r&quot;,) as f: calibration_json = json.load(f) intrinsic = calibration_json[&quot;intrinsic&quot;] print(&quot;Intrinsic Calibration\\n&quot;, intrinsic) extrinsic = calibration_json[&quot;extrinsic&quot;] print(&quot;Extrinsic Calibration\\n&quot;, extrinsic) camera_matrix = parse_intrinsic_calibration(intrinsic) return camera_matrix&quot;&quot;&quot; [ [fx, 0, cx = u0], [0, fy, cy = v0], [0, 0, 1] ]&quot;&quot;&quot;def parse_intrinsic_calibration(intrinsic): fx = intrinsic[&quot;fx&quot;] fy = intrinsic[&quot;fy&quot;] cx = intrinsic[&quot;u0&quot;] cy = intrinsic[&quot;v0&quot;] camera_matrix = np.zeros([3, 3], dtype=np.float32) camera_matrix[0][0] = fx camera_matrix[0][2] = cx camera_matrix[1][1] = fy camera_matrix[1][2] = cy camera_matrix[2][2] = 0.0 return camera_matrixSummary3차원 정보를 복원하는 방법 카메라의 extrinsic을 이용하는 방법 지면과 카메라 광학축을 활용한 삼각비 카메라의 intrinsic과 대상 객체의 사전 정보를 이용하는 방법 FOV를 활용하여 이미지 bbox에 대한 dx, dy를 구하는 방법 geometrical distance estimation 카메라의 image plane과 대상 plane의 변환 matrix를 사용하는 방법 warpPerspectiveTransform, findHomography homography distance estimation 카메라는 본질적으로 3차원 공간에 대한 정보가 이미지를 취득하는 동시에 소실되기 때문에 3차원 공간에 대한 이해가 굉장히 어렵다.이때까지는 1개의 카메라(Monocular camera)를 사용한 3D vision 방법을 배웠다. 그러나 카메라는 1개만 사용하는 것은 불가능하기에 multiple camera를 사용해야 하는데 이는 매우 복잡한 분야다. multiple camera에 대한 바이블인 눈깔책으로 불리는 Multiple View Geometry 책을 보길 추천한다." }, { "title": "[lane detection] traffic sign and traffic light detection utilizing yolov3 and hough transform", "url": "/posts/tstl/", "categories": "Projects, detection", "tags": "detection, lane-detection", "date": "2022-05-15 06:02:00 +0900", "snippet": "term : 2022.05.10 ~ 2022.05.13flow 객체 인식 data labeling labelImg yolov3 training augmentation use pretrained darknet weights choose optimizer and loss method (SGD/Adam , bcelogloss/mseloss/cross entropy loss) optimize hyperparamter (lr, loss weight) convert yolov3 → darknet’s weights file convert darknet’s weights → onnx file onnx file → tensorRT file 차선 인식 utilize detected object houghline transform path planning cte ( between detected lane mid position and mid position of image (320 pixel) ) 아쉬웠던 점들path planning Pure Pursuit + PID Rear-Wheel Feedback + PID Front-Wheel Feedback / Stanley + PID LQR + PID Linear MPC https://github.com/zhm-real/MotionPlanning" }, { "title": "[ROS] ROS Ultrasonic Data processing ", "url": "/posts/utralsonic/", "categories": "Review, ROS", "tags": "ROS, ultrasonic", "date": "2022-05-15 04:00:00 +0900", "snippet": "초음파 센서를 가지고, 직접 데이터를 받아보는 방법에 대해 설명하고자 한다. 먼저 초음파 센서 장비는 가지고 있다는 가정 하에 진행한다. 기술 스택 Arduino ROS python  1. 초음파 센서 데이터 받아오기 (Arduino)아두이노를 실행하려면, IDE를 설치해야 한다.다운로드 링크다운로드를 한 후, 압축을 풀면, 안에 install.sh 라느 파일이 있다. 이를 실행한다. 압축 풀기tar xvf arduino-1.8.19-linux64.tar.xz &amp;amp;&amp;amp; rm -rf arduino-1.8.19-linux64.tar.xz  install.sh 실행$ cd arduino-1.8.19 &amp;amp;&amp;amp; sudo sh install.shAdding desktop shortcut and menu item for Arduino IDE... done!  아두이노 실행sudo arduino  초음파센서와 pc 연결 확인$ lsusbBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub 여기서 wsl을 사용한다면 lsusb로 나오지 않을 수 있다. 따라서 usb연결을 진행해야 한다. 만약 네이티브 우분투를 사용할 경우는 그냥 넘어가도 좋다. 참고 자료 usbipd 설치usb 디바이스 연결에 대한 지원은 wsl에서 기본적으로 사용할 수 없으므로 usbipd-win 을 설치해야 한다.powershell을 켜서 명령어를 실행한다.&amp;gt; winget install --interactive --exact dorssel.usbipd-win&#39;msstore&#39; 원본을 사용하려면 다음 계약을 확인해야 합니다.Terms of Transaction: https://aka.ms/microsoft-store-terms-of-transaction원본이 제대로 작동하려면 현재 컴퓨터의 두 글자 지리적 지역을 백 엔드 서비스로 보내야 합니다(예: &quot;미국&quot;).모든 원본 사용 약관에 동의하십니까?[Y] 예 [N] 아니요: y찾음 usbipd-win [dorssel.usbipd-win] 버전 2.3.0이 응용 프로그램의 라이선스는 그 소유자가 사용자에게 부여했습니다.Microsoft는 타사 패키지에 대한 책임을 지지 않고 라이선스를 부여하지도 않습니다.Downloading https://github.com/dorssel/usbipd-win/releases/download/v2.3.0/usbipd-win_2.3.0.msi ██████████████████████████████ 10.4 MB / 10.4 MB설치 관리자 해시를 확인했습니다.패키지 설치를 시작하는 중...설치 성공 설치가 다 되면, 설치 창이 하나 뜬다. 설치하면 된다.  usbip 도구 및 하드웨어 데이터베이스 설치Linux에서 아래 명령어를 사용하여 USB 하드웨어 식별자 데이터베이스를 설치해야 한다.$ sudo apt install linux-tools-5.4.0-77-generic hwdata$ sudo update-alternatives --install /usr/local/bin/usbip usbip /usr/lib/linux-tools/5.4.0-77-generic/usbip 20update-alternatives: using /usr/lib/linux-tools/5.4.0-77-generic/usbip to provide /usr/local/bin/usbip (usbip) in auto mode이를 다 진행하고 나면, Windows와 Linux와의 디바이스 공유 도구가 설치되어 있을 것이다.  usb 디바이스 연결&amp;gt; usbipd wsl listBUSID VID:PID DEVICE STATE2-5 04e8:730b CanvasBio Fingerprint Driver Not attached2-6 2b7e:0134 720p HD Camera Not attached2-7 1a86:7523 USB-SERIAL CH340(COM5) Not attached2-10 8087:0026 인텔(R) 무선 Bluetooth(R) Not attached2-13 0bc2:231a UAS(USB Attached SCSI) 대용량 저장 장치 Not attached&amp;gt; usbipd wsl attach --busid 2-7usbipd: info: Using default distribution &#39;Ubuntu-18.04&#39;. 이 후, ubuntu로 들어와, 다음 명령어를 사용한다.$ lsusbBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 001 Device 002: ID 1a86:7523 QinHeng Electronics HL-340 USB-Serial adapterBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub   arduino 소스코드/*HL-340 초음파 센서 아두이노 펌웨어*/#define trig 2 // 트리거 핀 선언#define echo 3 // 에코 핀 선언void setup(){ Serial.begin(9600); // 통신속도 9600bps로 시리얼 통신 시작 pinMode(trig, OUTPUT); // 트리거 핀을 출력으로 선언 pinMode(echo, INPUT); // 에코핀을 입력으로 선선}void loop() { long duration, distance; // 거리 측정을 위한 변수 선언 // 트리거 핀으로 10us 동안 펄스 출력 digitalWrite(trig, LOW); // Trig 핀 Low delayMicroseconds(2); // 2us 딜레이 digitalWrite(trig, HIGH); // Trig 핀 High delayMicroseconds(10); // 2us 딜레이 digitalWrite(trig, LOW); // Trig 핀 Low // pulseln() 함수는 핀에서 펄스 신호를 읽어서 마이크로초 단위로 반환 duration = pulseIn(echo, HIGH); distance = duration * 170 / 1000; // 왕복시간이므로 340/2=170 곱하는걸로 계산, 마이크로초를 mm로 변환하기 위해 1000나눔 Serial.print(&quot;Distance(mm): &quot;); Serial.println(distance); // 거리정보를 시리얼 모니터에 출력 delay(100);}  2. 초음파 센서 데이터 받아오기 (ROS)아두이노에서 보내주는 물체까지의 거리 정보를 받아와 토픽으로 Publishing하는 코드를 만든다. ultrasonic_pub.py#!/usr/bin/env pythonimport serial, time, rospyfrom std_msgs.msg import Int32ser_front = serial.Serial( port=&#39;/dev/ttyUSB0&#39;, # 아두이노가 연결된 포트 baudrate=9600, # 아두이노에서 선언한 통신 속도)def read_sensor(): serial_data = ser_front.readline() # 시리얼 포트로 들어온 데이터를 받아옴 ser_front.flushInput() # 중간에 버퍼들이 있어서 그를 삭제해주는 flush ser_front.flushOutput() ultrasonic_data = int(filter(str.isdigit, serial_data)) # string을 숫자로 변환 msg.data = ultrasonic_dataif __name__ == &#39;__main__&#39;: rospy.init_node(&#39;ultrasonic_pub&#39;, anonymous=False) pub = rospy.Publisher(&#39;ultrasonic&#39;, Int32, queue_size= 1) msg = Int32() while not rospy.is_shutdown(): read_sensor() # 시리얼포트에서 센서가 보내준 문자열 읽엇거 거리 정보 추출 pub.publish(msg) time.sleep(0.2) # 토픽에 담아서 publish ser_front.close() # 끝나면 시리얼포트 닫기 초음파 센서 데이터 viewer초음파 데이터가 정확하게 잘 도착하는지, 데이터가 ROS로 잘 전달되고 있는지 확인하기 위한 검증 코드이다. ultrasonic_sub.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32def callback(msg): print(msg.data)rospy.init_node(&#39;ultrasonic_sub&#39;)sub = rospy.Subscriber(&#39;ultrasonic&#39;, Int32, callback)rospy.spin()  실행실행을 위한 launch파일을 제작한다. ultra.launch&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;ultrasonic&quot; type=&quot;ultrasonic_pub.py&quot; name=&quot;ultrasonic_pub&quot;/&amp;gt; &amp;lt;node pkg=&quot;ultrasonic&quot; type=&quot;ultrasonic_sub.py&quot; name=&quot;ultrasonic_sub&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt;  실행roslaunch ultra ultra.launch " }, { "title": "[ETC] wsl2 필수 명령어 모음 ", "url": "/posts/wsl2/", "categories": "Review, ETC", "tags": "ETC, wsl", "date": "2022-05-15 04:00:00 +0900", "snippet": "사용자 추가 명령어wsl 설치시 root계정으로 시작할 경우가 존재한다. 이 때 사용자를 추가하는 방법은 다음과 같다. 만약 추가되어 있더라도, 처음 접속하는 곳이 root로 되어 있으면 변경해줄 수 있다.사용자를 추가하는 방법은 ubuntu내에서 사용자를 추가하는 것이다. ubuntu 터미널을 연다. sudo adduser &amp;lt;username&amp;gt; 을 친다.sudo adduser jhyoon마지막으로 Y를 눌러 유저를 생성한다. 3.user계정을 sudo 그룹에 추가한다.sudo usermod -aG sudo jhyoon wsl bash가 실행되지 않을 때이 때는 다양한 문제가 있을 수 있다.먼저 윈도우에서 wsl이 사용 가능하도록 되어 있는지 확인해야 한다. powershell에서 아래 명령어를 쳐본다.Get-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux사용이 가능하다고 하면, 다음 단계로 넘어간다. 가상 메모리가 할당되어 있는지 확인참고 : https://qkrm.tistory.com/3사용 가능하다고 되어 있어도 오류가 난다면, 가상 메모리가 할당되어 있는지 확인해본다. 먼저 windows + R 을 눌러 실행창을 띄우고, SystemPropertiesAdvanced를 친다. 그리고, 성능 부분에 설정을 클릭한다. 그 후 고급에 들어가 가상 메모리 편집을 클릭한다. 그러면 본인이 사용하고자 했던 디스크에 가상메모리가 할당되어 있는지 확인할 수 있다. 나의 경우 D:에 wsl을 설치했는데, 페이징 파일이 없었다. 그래서 시스템이 관리하는 크기 를 선택해주어 시스템에서 자동으로 메모리를 할당하도록 했다.이를 다 하고 나면, 재부팅하라고 뜨는데, 재부팅하고, wsl을 실행하면 된다.  다른 디렉토리에 wsl 경로 설정다른 드라이브에 wsl 경로를 설정하고자 한다면, 먼저 기본으로 설정되어 있는 것을 해제해야 한다.powershell을 연다.&amp;gt; wsl -l -v NAME STATE VERSION* Ubuntu-18.04 Running 2 Ubuntu-20.04 Stopped 2 docker-desktop-data Stopped 2 docker-desktop Stopped 2 다른 배포판을 기본으로 설정하고자 하면 -s 또는 --set-default를 사용한다.wsl -s Ubuntu-20.04 하나의 배포판을 다른 디렉토리로 옮기기 위해서는 배포판을 압축파일로 만든 후 옮겨야 한다. wsl --export {배포판 이름} {내보낼 파일 이름}wsl --export Ubuntu-18.04 Ubuntu-18.04.tar이렇게 하면 현재 디렉토리에 Ubuntu-18.04.tar 파일이 생성된다.이를 다른 폴더로 import 시켜준다. wsl --import {배포판 이름} {배포판 위치 지정} {배포판 파일 이름}wsl --import Ubuntu-18.04 D:\\wsl\\ Ubuntu-18.04.tar그 후, 배포한다.wsl -d Ubuntu-18.04여기서 에러가 나지 않고, 우분투 계정으로 로그인이 되면, 잘 수행된 것이다. 이렇게 하고 나면, 기본 계정이 root로 되어 있다. root에서 작업을 하는 것은 위험하므로 일반 계정을 기본 계정으로 지정해주어야 한다.  참고 블로그ubuntu 18.04 를 기준으로 ubuntu1804 config --default-user {계정 이름} 만약 ubuntu 20.04라면 ubuntu2004 config --default-user {계정 이름} wsl 진입이 불가능할 때powershell에서도 wsl을 진입할 수 없을 때 사용하는 방법이 있다.wsl -u rootroot 계정으로 진입하는 것으로, 이것을 사용해도 진입이 안된다면…. sudoers 파일 오류sudoers 파일은 살짝만 오류나도 시스템이 전부 고장날 수 있다. 그래서 조심스럽게 수정해야 하지만, 피치못할 사정으로 수정을 했는데 오류가 나는 경우 복구하는 방법을 설명한다. https://meoru-tech.tistory.com/m/70중요한 것은 sudoers 파일은 쓰기 권한을 주면 안된다. 쓰기 권한을 주면 바로 오류가 난다고 한다.만약 wsl을 사용하다가 오류가 난 것이라면, 위에 wsl -u root를 사용하여 wsl 시스템에 진입해야 한다.진입을 해서 nano 나 vim 등을 사용하여 /etc/sudoers 파일을 수정한다.nano /etc/sudoers또는pkexec visudopkexec를 사용했는데, 아래와 같은 오류가 난다면 nano를 통해 수정해야 한다.Error getting authority: Error initializing authority: Could not connect: No such file or directory" }, { "title": "[ETC] colab을 local에 연결하기 ", "url": "/posts/colab/", "categories": "Review, ETC", "tags": "ETC, colab", "date": "2022-05-15 04:00:00 +0900", "snippet": "document : https://research.google.com/colaboratory/local-runtimes.html jupyter 설치document : https://jupyter.org/installpip install notebook또는pip install jupyterlabnotebook은 다소 예전꺼라, jupyterlab을 사용해보려고 한다. 설치 확인jupyter-lab 여기서 notebook에 있는 python을 누르게 되면, 기존의 jupyter 방식의 ipynb파일이 열린다. 그리고 console을 선택하면 터미널 창 형태로 python이 열린다. other에 터미널을 열면 평범한 cmd창이 열린다. 실행시켰던 터미널에서 명령을 끄면, 서버가 닫힌다.  jupyter_http_over_ws 설치pip install jupyter_http_over_ws서버 시작 및 인증새로운 노트 서버를 생성한다. 서버가 시작되면 인증에 사용될 초기 백엔드 URL과 함께 메세지가 출력된다.$ jupyter-lab --allow-root --NotebookApp.allow_origin=&#39;https://colab.research.google.com&#39; --port=8888 --NotebookApp.port_retries=0Enabling: jupyter_http_over_ws- Writing config: /root/.jupyter - Validating... jupyter_http_over_ws 0.0.7 OK코랩과 서버 연결하기연결하고자 하는 코랩 ipynb로 가서 연결 -&amp;gt; 로컬 런타임에 연결 하면 다음과 같은 화면이 나온다. 연결이 되면 연결됨(로컬)이라는 표시를 볼 수 있고, 연결이 되면 자기 컴퓨터에 있는 GPU 및 공간이 코랩에 사용되고 있는 것을 확인할 수 있다.  서버 제거jupyter serverextension disable --py jupyter_http_over_ws   패키지 삭제pip uninstall jupyter_http_over_ws" }, { "title": "[ETC] colab GPU vscode로 연결하기 ", "url": "/posts/colabgpu/", "categories": "Review, ETC", "tags": "ETC, gpu", "date": "2022-05-15 04:00:00 +0900", "snippet": "colab gpu를 vscode로 연결해서 사용하는 방법을 리뷰하고자 한다.remote ssh extension 설치먼저 vscode에서 remote ssh extension을 설치한다.cloudflare 설치cloudflare이라는 프로그램을 설치한다.설치 사이트colab cloudflared 설치colab에서 아래 코드를 실행하여 cloudflared를 설치해준다.!pip install colab-ssh --upgradefrom colab_ssh import launch_ssh_cloudflared, init_git_cloudflaredlaunch_ssh_cloudflared(password=&#39;0000&#39;)이를 실행하면 아래 화면이 뜰 것이다.여기서 아래 하단에 ssh terminal에서의 명령 코드와 vscode에서의 remote ssh 코드가 있다. 이는 colab과 vscode를 연결해주는 방법 두 가지를 설명해주는 것인데, 두 가지 방법 다 가능하다.ssh terminalssh terminal을 linux에서 실행하여 연결시킬 수 있지만, 우리는 vscode로 연결할 것이다. 먼저 vscode에서 원격 탐색기를 들어간다.그런 다음 ssh targets를 선택해준다.그리고 아래 +인 Add New를 선택하면 입력하는 창이 하나 뜬다. 거기에 코랩에 있던 좌하단 부분의 코드를 기입해준다.그리고 enter를 치면 select ssh configuration 이라고 뜬다. 여기서 .ssh\\config 를 선택하고 enter를 다시 치면 ssh targets 부분에 추가된 것을 볼 수 있다.config 파일을 보면 host가 추가된 것을 확인할 수 있다. 추가되었으면 해당 ssh를 새 창에서 열면 사용이 가능하다. config에서 해당 host를 삭제하면 ssh targets에서 지워진다.vscode remote ssh 먼저 config 파일을 수정한다.ctrl+shift+p 단축키를 누르고, open SSH configuration file을 선택한다. 그러면 config파일이 나오게 되고, 여기에 다음 코드를 추가한다.Host *.trycloudflare.com HostName %h User root Port 22 ProxyCommand &amp;lt;PUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&amp;gt; access ssh --hostname %h이 부분을 추가하고, &amp;lt;&amp;gt;를 수정해줘야 한다. 위에서 cloudflare를 설치했다. 설치한 cloudflare 파일을 원하는 곳에 이동시키고, 실행시키면 config 파일이 생성된다. 그리고 그 파일의 경로를 &amp;lt;&amp;gt;에 넣어주면 된다.Host *.trycloudflare.com HostName %h User root Port 22 ProxyCommand C:\\Users\\dkssu\\cloudflare\\cloudflared-windows-amd64.exe access ssh --hostname %hdirectory 가 아닌 file에 대해 경로를 넣어줘야 오류가 나지 않는다. connect to host위의 colab 애서 우하단의 코드를 복사한 후에 vscode에서 ctrl+shift+P 단축키를 눌러 remote SSH: connect to host 를 선택한다. 그리고 복사한 것을 넣어주고, enter를 누른다. 그러면 새 창이 나오고, 운영체제를 선택하라고 할 것이다. 그러면 linux를 선택하고, continue를 선택 후 아까 설정했던 password를 기입한다. 그러면 연결이 완료되고, 우측 탭에서 맨 위의 부분을 클릭해서 open folder를 하여 사용할 부분을 선택한다.최종 화면여기서 최종 확인을 위해 터미널에 nvidia-smi를 쳐본다." }, { "title": "[데브코스] 13주차 - DeepLearning Model Compression ", "url": "/posts/compress/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-05-09 20:20:00 +0900", "snippet": "perception DL process간단하게 딥러닝 프로세스를 살펴보자.딥러닝이 perception에서 어떻게 진행되는지 data labeling 데이터 레이블링의 퀄리티가 중요하다. bbox가 객체에 fit하게 잘 되어 있는지, segmentation이라면 영역이 잘 설정되어 있는지가 중요하다. 또는 클래스를 잘못 지정할 경우 성능에 영향을 끼친다. design DL model 러프하게 모델을 선택해야 한다. OD이면 yolo인지, SSD, RCNN 등등의 모델 기준을 삼고, 성능을 높일만한 레이어 추가, 활성 함수 변경 등의 기법을 적용해서 성능을 높인다. 중요한 것은 자율주행에는 작은 칩을 사용하므로 모델이 무거우면 안된다. loss function, find optimal hyperparameters loss : celoss, mseloss, iouloss, focal loss hyperparameters : lr, optimizer, loss weight(cls loss, bbox loss, objness loss) evaluate models to satisfy KPI(평가 지표)객체를 잘 찾는지에 대한 평가 지표를 적용한다. recall, precision, PR curve model compressionmodel compressionSOC(system on chip) in self driving car, 즉 자율주행 안에 들어가는 칩을 말하는데, 데스크탑에 들어가는 gpu가 아닌 chip이다.light architectureMAC(Multiply Add Calculation), y = wx + b 의 연산 하나를 1MAC이라 한다. 즉 MAC : OC * OH * OW * KH * KW * IC가벼운 아키텍쳐를 만들기 위해서는 MAC을 줄여야 한다. 그를 위해 kernel size를 줄일 수 있다.또는 resolution scaling, depth scaling, width scaling을 감소시키는 방법이 있다. resolution scaling : 1920 x 1080 → 1280 x 720 depth scaling : layer의 수 width scaling : 각 layer의 채널 수pruningpruning이란 결과에 영향을 미치지 않는 가지들을 삭제시키는 것을 말한다. layer단위로 지울수도 있고, channels 단위로 지울수도 있다. gpu에서만 돌리면 가능한데, chip마다 pruning이 지원을 하는 칩도 있고, 지원이 안되는 칩도 있다.Quantizationprecision 조정을 통해 lower bit 연산을 하는 것을 말한다. 예를 들어 float32를 float16이나 float8로 변환하면 연산이 당연히 빨라진다. float를 int로 변환할 때 생기는 rounding error를 최대한 줄이는 것이 중요하다. 그렇다면, weight들이 어떻게 표현되어 있는지에 대한 히스토그램을 봐야 하는데, 전체를 -1~1로 범위를 잡아서 하는데, 대부분의 weight들이 -0.25~0.25 사이에서만 존재한다면 더 정교한 값으로 변환이 어려울 수 있다. 그래서 최대 최소 값을 적절하게 맞춰서 quantization을 수행하여 정교한 변환을 수행한다.fuse modules를 수행하여 연산량을 줄인다. 이는 아래 batchNorm folding에서 더 자세히 설명하겠다.batchNorm folding기존의 conv layer는 conv + batchNorm + ReLU 순으로 진행되는데, 이를 Conv 와 BN layer를 합쳐버리면 연산이 줄어들고, 속도가 증가할 것이다.conv 연산의 수식은 $ y = wx + b $이고, BN 연산의 수식은 $ y = gamma * (x - mean) / var + beta $이다.이를 합치면, $ y = ((gamma * w) / var) * x + gamma(b - mean) / var + beta $ 이 된다.Deploy modelspytorch model을 deploy할 때는 ONNX , TensorRT, SDK, naive C/C++ 등을 사용하여 target device에 맞게 내보낸다. 여기서는 ONNX와 TensorRT를 다뤄볼 것이다.ONNXONNX(Open Neural Network Exchange)로 다른 프레임워크의 형태로 변환하거나 deploy를 할 때 사용할 수 있는 프레임워크이다TensorRTTensorRT를 사용하면 모델을 손해보지 않고도, 딥러닝 모델을 low latency로 이동이 가능하고, 추론에 적용이 가능하다." }, { "title": "[데브코스] 12주차 - DeepLearning Perception Applications", "url": "/posts/perception/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-05-06 15:20:00 +0900", "snippet": "자율주행에서 Perception, 인지란 인식 + 이해하는 과정으로, 유의미한 정보를 생성하는 단계가 필요하다. 유의미한 정보라 함은 객체를 인식하는 것도 중요하지만, 객체를 인식하는 것만으로는 거리 정보를 얻을 수 없다. 그래서 그 객체와의 거리를 추정하는 3D POSE estimation도 함께 수행해야 한다.3D POSE estimation을 적용하기 위해서는 vision geometry의 기본적인 지식이 수반되어야 한다. 그를 위해 카메라의 투영 과정을 이해해야 한다.3차원 위치를 가진 객체가 (Xw, Yw, Zw) 좌표를 가진 P로 표현되었을 때, 이 점이 카메라에 투영되면 (u,v) 좌표 지점에 상이 존재하는 이미지 평면을 얻게 된다. 그렇다면 이 이미지 평면을 통해 객체 위치(X,Y,Z)를 추정하는 과정도 수행이 가능해진다.사람은 2D 이미지만 봐도 입체감이 느껴지는데, 이를 공간감(illusion of space)라 한다. 사람은 이 공간감을 가지고 있지만, 컴퓨터 비전(기계)는 이 공간감을 가지고 있지 않기 때문에 다양한 수학적 모델을 통해 공간감을 대체해야만 한다.실제에서는 직선이지만, 카메라에서 원근법으로 인해 기울어지게 그려진 직선들이 모이는 점을 소실점(vanishing point)이라 하고, 찍는 각도에 따라서 소실점의 위치는 달라진다. 소실점으로 모이고 있는 여러 직선들을 소실선(vanishing line)이라 한다.사람이 사물을 인식한다는 것은 물체에 반사된 빛들을 인식하는 것이고, 카메라는 그 빛을 기록하는 장치이다. 힌홀 카메라(pinhole camera)는 빛이 들어오는 구멍을 아주 작게 만들어서 맨 부터 맨 아래까지의 반사된 빛들이 매우 작은 구멍으로 들어와 뒤집힌 채로 상이 맺힌다.이 카메라는 동일한 크기의 물체여도 가까이서 촬영하면 크게 보이고, 멀리서 촬영하면 작게 보인다. 그 이유는 멀리 있으면 물체가 이루는 각도가 작아서 상이 작게 맺히고, 가까우면 물체가 이루는 각도가 커서 상이 크게 맺힌다.이를 수학적으로 판단해봤을 때, 투과되는 구멍과 맺힌 상과의 거리를 f라 하고 맺힌 상의 크기를 h, 물체와 구멍사이의 거리를 D, 실제 물체의 크기를 H라 할 때, h는 f와 D의 크기에 따라 달라진다.다시 이 그림으로 돌아와서, 이미지 평면에서의 점,s 와 실제 3차원에서의 점 좌표, P에 대해 projection matrix를 세워보면 다음과 같다.\\[s \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f_x \\ 0 \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix} \\begin{bmatrix} r_{11} \\ r_{12} \\ r_{13} \\ t_1 \\\\ r_{21} \\ r_{22} \\ r_{23} \\ t_2 \\\\ r_{31} \\ r_{32} \\ r_{33} \\ t_3 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\]u,v,1은 이미지 평면에서의 점 좌표를, X,Y,Z,1은 실제 좌표계에서의 점 좌표를 나타낸다. 그리고 우항에서 1번째 항을 Intrinsic항, 2번째 항을 extrinsic항이라 한다. intrinsic은 카메라 자체의 특성을 의미하고, extrinsic은 카메라와 대상과의 관계를 의미한다. extrinsic에서 r은 회전(rotation), t는 평행이동(translation)을 의미히므로, 따라서 기준 좌표계가 이미지 좌표계로 변환되기 위한 좌표 변환을 나타낸다.pinhole camera model이외에도 fisheye camera model(초광각), weak perspective model 등이 있다. fisheye 는 369도를 다 관찰할 수 있는 카메라이고, weak perspective는 약한 변환, 즉 가상의 depth plane을 생성해서 투과하여 변환이 덜 되도록 만드는 방법이다.사용하려는 카메라에 따라 matrix가 다 달라지므로 확실하게 알아보고 연산을 수행해야 한다.Camera Intrinsic calibrationcamera calibration이란 카메라가 가지고 있는 고유한 특성을 파악하는 과정이다. 카메라 렌즈와 이미지 센서와의 관계로부터 파생되는 초점거리가 intrinsic에 해당되고, 카메라의 위치와 자세에 대한 특성은 extrinsic에 해당한다.같은 거리에서 서로 다른 카메라로 동일한 피사체를 촬영하면 결과가 다르다. 이는 카메라의 intrinsic 특성이 달라서 생기는 문제이다. 같은 카메라로 다른 위치에서 동일한 피사체를 촬영해고 결과는 다르게 나온다. 이는 extrinsic 특성이 다르기 때문에 생기는 문제이다.projection matrix에서 intrinsic 특성을 보면 f(focus distance)와 c(center point)가 있다. 이는 각각 초점거리와 주점을 의미한다. 초점거리초점거리란 렌즈 또는 구멍으로부터 상이 맺힌 평면 사이의 거리를 말한다. 동일한 x에 대해 초점거리를 측정한다면 f_y라고 표기할 수 있고, 동일한 y에 대해 상이 맺힌 초점거리를 측정한다면 f_x라고도 표현할 수 있다. 이상적인 카메라의 특성으로는 f_x = f_y가 될 것이다. 초점거리에 따라 피사체의 크기가 달라진다.컴퓨터 비전에서 초점거리는 pixel 단위로 표현된다. 카메라에서는 초점거리를 mm단위로 표현하지만 픽셀 단위로 표현하게 되면, f = 500(pixel) 가 되고, 만약 cell 하나의 크기를 0.1mm이라 정의하고, f=500px, 해상도가 100x100, cell size도 100x100 라면 카메라에서의 초점거리는 f = 0.1mm x 500px = 50mm라 할 수 있다. 그러나 만약 동일한 조건에서 해상도가 50x50이라면 0.2mm x 500px = 100mm이 된다. 또는 f(mm)가 50으로 고정되어 있다면 f = 0.2 x 250px = 50mm이라는 식이 된다.이를 일반화하면 n x n cell size, p mm each pixel =&amp;gt; r x r resolution (n/r) x (n/r) cell size = p mm p mm * focal length(pixel) = focal length(mm) 주점주점이란 pinhole(구멍)의 중심이 이미지 센서에 직교하는 위치(Cx,Cy)를 의미한다. 대체로 이미지가 (height, width)의 크기를 가진다고 할 때, $ Cx = \\frac{width}{2}, Cy = \\frac{height}{2} $ 라 할 수 있으나 모든 경우가 이렇지는 않다. 이미지의 중심점과 주점(렌즈 중심점)은 다른 의미인데, 이상적인 경우에는 이 두개가 일치하지만, 카메라 제조 공정에서 발생하는 다양한 이슈로 인해 일치하지 않는 경우도 있다. 따라서 이 주점을 구하기 위해 calibration이 정의하는 것이 필요하다.원래의 instrinsic의 행렬은\\[\\begin{bmatrix} f_x \\ 0 \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix}\\]이지만, 공정에 의한 오류로 인해 발생하는 계수를 표기하기 위해 추가 파라미터가 존재한다.\\[\\begin{bmatrix} f_x \\ skew_{cf_x} \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix}\\]이 때, skew_cfx는 이미지의 비대칭 계수(skew coefficient)를 의미하는데, 이는 이미지 센서가 기울어진 정도를 의미한다. 그러나 최근에는 이러한 상황이 발생하는 경우가 거의 없기 때문에 0으로 계산을 한다.카메라 좌표계3차원 공간에 존재하는 한 물체를 2차원 이미지 공간에 투영되는 과정을 설명하기 위해서는 좌표계가 정의되어야 한다. computer vision에서는 4개의 좌표계를 사용한다. World(realworld) coordinate : Xw, Yw, Zw camera coordinate : Xc, Yc, Zc image coordinate : u, v Normalized image coordinate : u_n, v_nP =&amp;gt; Pn =&amp;gt; Pimg =&amp;gt; Pw world coordinate월드 좌표계는 우리가 살고 있는 3차원 공간에 존재하는 좌표계를 말한다. 어떤 물체의 위치를 3차원 좌표로 표현하면 (Xw, Yw, Zw) 이다. 이 좌표는 어떤 곳을 원점으로 고정하냐에 따라 값이 달라질 수 있는데, 컴퓨터 비전에서는 주로 카메라 좌표계를 원점으로 잡는다. camera coordinate카메라 좌표계는 카메라를 기준으로 표현한 좌표계이다. 카메라를 기준으로 설정하였기 때문에 각각의 요소들은 다음과 같이 정의할 수 있다. Zc : 카메라 렌즈가 바라보는 방향 Xc : 카메라 아래쪽 방향 Yc : 카메라 오른쪽 방향기본적인 월드 좌표계의 방향과는 조금 다르게 되어 있으므로 이를 하나로 통일하는 것이 중요하다. image coordinate이미지 좌표계는 실제 이미지에 대한 데이터를 표현하는 좌표계이다. 이미지 좌표계의 기준은 다음과 같다. 이미지 왼쪽 상단을 원점으로 한다 이미지의 오른쪽 방향을 x 또는 u로 표현 이미지 아래쪽 방향을 y 또는 v로 표현 이미지 크기는 (height, width)로 표현 normalized image coordinate정규 이미지 좌표계는 실제로 존재하지 않는 좌표계로 컴퓨터 비전에서 해석을 위해 정의한 가상의 좌표계이다. 렌즈로부터 이미지 평면까지의 거리를 초점거리라 정의하고, 같은 물체를 동일한 위치에서 서로 다른 카메라로 촬영할 경우 이미지는 다르게 표현된다. 따라서 초점거리를 1로 정규화한 가상의 이미지 좌표계를 사용하여 초점거리에 대한 영향을 제거시킨다. 이를 위한 좌표계이다.카메라의 intrinsic calibration 정보를 알면 이미지를 정규 이미지 좌표계로 변환이 가능해진다.\\[\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} f_x \\ 0 \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}\\]좌항의 x,y,1이 이미지 좌표계, u,v,1이 정규 이미지 좌표계이다. 이를 변환하게 위해서는 초점거리와 주점에 대한 정보를 알아야 할 것이고, 이를 통해 행렬을 풀면 아래와 같다.\\[u = \\frac{(x - c_x)}{f_x}\\]\\[v = \\frac{(y - c_y)}{f_y}\\]Distortion실제 카메라는 구멍이 아닌 볼록렌즈를 통해 빛을 모은다. 렌즈에 의해 빛이 굴절되는데, 빛의 굴절은 이미지 센서에 보이는 이미지를 왜곡한다. 직선이 곡선으로 보이는 상황이 왜곡이라 할 수 있다.볼록 렌즈의 형상이 곡률을 가지는 구면 형태이므로 이미지 중심부(주점)에서 멀어질수록 표현의 비율이 달라지기 때문에 왜곡이 발생한다. 어느 위치에 있을 것이라 예측한 값인 PD(Predicted Distance)와 실제 거리 값인 AD(Actual Distance)가 존재할 때 렌즈의 왜곡 정도 D(%)는 다음과 같은 식을 가진다.\\[D = \\frac{AD - PD}{PD} * 100%\\]왜곡에서도 다양한 종류의 왜곡이 존재한다. 방사 왜곡렌즈 왜곡의 대표적인 예로, 대표적으로 두 가지 형태로 표현된다. Barrel Distortion : 중심부가 외각부보다 원래의 크기에 비해 큰 형태로 발생 가운데가 볼록한 형태 Pincushion Distortion : 중심부가 외각부보다 원래의 크기에 비해 작은 형태로 발생 가운데가 오목한 형태 접선 왜곡(Tangential Distortion)접선 왜곡은 렌즈와 이미지 센서의 관계에 의해 생겨나는 왜곡이다. 즉 이미지의 평면과 렌즈의 평면이 평행하지 않는다는 의미이다.접선 왜곡의 경우 가운데가 원형으로 볼록한 것이 아닌 타원으로 볼록/오목한 형태를 가진다. 원근 왜곡(Perspective Distortion)방사/접선 왜곡이 대표적인 왜곡이지만, 다른 형태의 왜곡도 존재한다. 원근 왜곡은 3차원 공간이 2차원 공간으로 투영되면서 발생하는 왜곡이다. 이미지는 공간의 깊이 정보가 하나의 평면으로 투영되는 데이터이므로 촬영하는 환경에 따라 원근감 손실과 같은 다양한 왜곡이 생길 수 있다.사람의 경우 이미지를 통해 원근감을 추정하기 위해서 세 가지 정보를 통해 추정한다. 사물의 실제 크기에 대한 정보 사물과 주변의 관계에 대한 정보 추정이 가능한 기하학적 구조를 가지는지이를 통해 원근감을 추정하는데, 이를 컴퓨터비전에서도 적용을 할 수는 있다. 그러나 이 원근 왜곡에 의해 손실된 객체에 대해서는 추정이 거의 불가능하다.이 원근감 손실을 해결하기 위해 해결하기 위한 다양한 방법이 존재한다. 다수의 카메라를 사용2개 이상의 카메라로 동일한 시점에 촬영한 각 1장의 이미지만으로 3차원 위치 정보를 추정한다. 각 카메라의 Extrinsic 파라미터를 알아야 정확한 정보를 추정할 수 있다. 2장 이상의 이미지를 사용같은 카메라로 카메라가 움직이는 환경에서 연속된 이미지 정보를 활용하여 3차원 위치 정보를 추정한다. 카메라의 움직임 정보를 정밀하게 측정/추정해야 정확한 정보를 추정할 수 있다.카메라 좌표계를 기준으로 3차원 공간상에 존재하는 객체를 투영하는 모델을 계산해본다. 이 때는 distortion이 있을 때와 없을 때 두 가지를 각각 계산하여 비교해볼 것이다.extrinsic calibration에서 translation이 없다고 가정을 하면 rotation만 남게 된다.\\[extrinsic = rotation \\| translation = \\begin{bmatrix} r_{11} \\ r_{12} \\ r_{13} \\ t_1 \\\\ r_{21} \\ r_{22} \\ r_{23} \\ t_2 \\\\ r_{31} \\ r_{32} \\ r_{33} t_3 \\end{bmatrix}\\]이 때, translation이 없다면 rotation matrix는 identity matrix가 된다.\\[extrinsic = \\begin{bmatrix} r_{11} \\ r_{12} \\ r_{13} \\\\ r_{21} \\ r_{22} \\ r_{23} \\\\ r_{31} \\ r_{32} \\ r_{33} \\end{bmatrix} = \\begin{bmatrix} 1 \\ 0 \\ 0 \\\\ 0 \\ 1 \\ 0 \\\\ 0 \\ 0 \\ 1 \\end{bmatrix} = I\\]그리고 normalized image plane에 투영을 한다면 instrinsic matrix도 생략이 가능하다. 따라서 투영하는 projection matrix의 식은 다음과 같이 간편화된다.\\[\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix}\\]이를 다시 정리하면\\[\\begin{bmatrix} u_{n_u} \\ v_{n_u} \\end{bmatrix} = \\begin{bmatrix} \\frac{X_c}{Z_c} \\\\ \\frac{Y_c}{Z_c} \\end{bmatrix}\\]여기서 u는 undistortion, 왜곡이 되지않음을, n은 normalize를 의미한다.왜곡항은 또 radial(방사)와 tangential(접선) 왜곡, 2가지로 나뉠 수 있다. 그에 대한 식으로는 다음과 같다. d는 distortion, 왜곡이 됨을 의미한다.\\[\\begin{bmatrix} u_{n_d} \\\\ v_{n_d} \\end{bmatrix} = (1 + k_1r_u^2 + k_2r_u^4 + k_3r_u^6) \\begin{bmatrix} u_{n_u} \\\\ v_{n_u} \\end{bmatrix} \\begin{bmatrix} 2p_1u_{n_u}v_{n_u} + p_2(r_u^2 + 2u_{n_u}^2) \\\\ p_1(r_u^2 + 2v_{n_u}^2) + 2p_2u_{n_u}v_{n_u} \\end{bmatrix}\\]그리고, r_u에 대해서는\\[r_u^2 = u_{n_u}^2 + v_{n_u}^2\\]의 식을 가진다.normalize 이미지 평면을 image plane으로 변환을 하려면 intrinsic matrix를 추가하면 된다.\\[\\begin{bmatrix} x_{p_d} \\\\ y_{p_d} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f_x \\ skew\\_cfx \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix} \\begin{bmatrix} u_{n_d} \\\\ v_{n_d} \\\\ 1 \\end{bmatrix}\\]p는 plane을 의미한다. 그래서 최종적인 좌표 x,y,1에 대한 왜곡이 있는 이미지 projection matrix를 완성한다. 위의 식을 정리하면 다음과 같이 간략해진다. skew_cfx 는 대체로 0으로 간주하므로 0을 넣어준다.\\[x_{p_d} = f_x(u_{n_d} + skew\\_cfxv_{n_d}) + c_x = f_xu_{n_d} + c_x , y_{p_d} = f_yv_{n_d} + c_y\\]지금까지는 왜곡이 없는 이미지에서 왜곡이 있는 이미지를 계산했다. 그러나 우리가 실제로 궁금한 것은 왜곡이 있는 이미지에서 왜곡이 없는 이미지 좌표를 알고 싶기에 이를 계산해보자.앞서 계산한 image plane을 역으로 계산하면 된다.\\[\\begin{bmatrix} x_{p_u} \\\\ y_{p_u} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f_x \\ skew\\_cfx \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix} \\begin{bmatrix} u_{n_d} \\\\ v_{n_d} \\\\ 1 \\end{bmatrix} -&amp;gt; \\begin{bmatrix} u_{n_d} \\\\ v_{n_d} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f_x \\ skew\\_cfx \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix}^{-1} \\begin{bmatrix} x_{p_u} \\\\ y_{p_u} \\\\ 1 \\end{bmatrix}\\]\\[u_{n_u} = \\frac{x_{p_u} - c_x}{f_x} - skew\\_cv_{n_u} , v_{n_u} = \\frac{(y_{p_u} - c_y)}{f_y}\\]여기서 왜곡 모델을 적용하면 다음과 같다. 왜곡x -&amp;gt; 왜곡o 변환 과정과 다른 점은 역행렬(-1)이 들어간 것뿐이다.\\[\\begin{bmatrix} u_{n_d} \\\\ v_{n_d} \\end{bmatrix} = (1 + k_1r_u^2 + k_2r_u^4 + k_3r_u^6) \\begin{bmatrix} u_{n_u} \\\\ v_{n_u} \\end{bmatrix} \\begin{bmatrix} 2p_1u_{n_u}v_{n_u} + p_2(r_u^2 + 2u_{n_u}^2) \\\\ p_1(r_u^2 + 2v_{n_u}^2) + 2p_2u_{n_u}v_{n_u} \\end{bmatrix}\\]\\[\\begin{bmatrix} x_{p_d} \\\\ y_{p_d} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f_x \\ skew\\_cfx \\ c_x \\\\ 0 \\ f_y \\ c_y \\\\ 0 \\ 0 \\ 1 \\end{bmatrix} \\begin{bmatrix} u_{n_d} \\\\ v_{n_d} \\\\ 1 \\end{bmatrix}\\]이 모든 과정은 코드로 직접 옮길 필요없이 openCV의 함수로 undistort나 initUndistortRectifyMap&amp;amp;remap 이다. 이 함수의 의미, 즉 왜곡 보정은 왜곡된 이미지로부터 왜곡이 제거된 이미지로의 mapping을 의미한다.Intrinsic Calibration codeintrinsic matrix와 왜곡 계수(distortion coefficients)를 계산하는 코드를 작성해보도록 하자. 참고: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html3차원 위치 과정 input distorted image undistort detection pose estimation여기서 전체 이미지가 아닌 detection 후 4개의 좌표에 대해서만 undistort를 사용하여 위치 추정을 하기도 한다.캘리브레이션 보드를 활용한 intrinsic matrix 출력위의 참고자료를 통해 툴박스를 사용하여 calibration을 할 수 있지만 직접 코드를 구현해서 사용할수도 있다.intrinsic calibration을 수행하기 위해서는 준비물이 필요하다. calibration pattern이라 부르는 특별한 보드인데, 체스보드와 같이 직각으로 이루어진 사진이 좋다. 이 체스보드 모양을 프린트해서 화면에 비춰볼 예정인데, calibration 보드를 통해 3차원 공간을 이미지라는 2차원 공간에 투영하는 과정에서 필요한 카메라의 고유 특성을 파악할 수 있다.현재 캘리브레이션 보드에서의 3차원 좌표 (x,y,z)를 (u,v)로 투영을 할 건데, 투영하는 식은\\[\\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} intrinsic \\end{bmatrix} \\begin{bmatrix} extrinsic \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\\]이고, 3차원 좌표 (x,y,z)를 알고, extrinsic은 원점을 기준으로 하고 있으므로 identity matrix가 되어서 intrinsic의 파라미터들을 최적화시킬 수 있게 되는 것이다. 이때, z는 일정하게 되어야 하므로 캘리브레이션 보드는 반드시 평면에 만들어져 있어야 한다. 또한, 각 grid cell의 정확한 크기와 grid 사이즈를 알고 있어야 한다. grid 사이즈란 맨 끝의 테두리를 제외하고 안쪽에 존재하는 직각좌표를 의미한다. 대체로 4x7과 같은 짝수 x 홀수로 이루어진 보드를 많이 사용한다.import numpy as npimport cv2 as cvimport glob# termination criteriacriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001) # constant # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)objp = np.zeros((6*7,3), np.float32) # chess board size , 3(x,y,z)objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2) * 0.025 # 0.025 == 1 grid size (m unit)# Arrays to store object points and image points from all the images.objpoints = [] # 3d point in real world spaceimgpoints = [] # 2d points in image plane.images = glob.glob(&#39;*.jpg&#39;) # image file listfor fname in images: img = cv.imread(fname) gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # Find the chess board corners ret, corners = cv.findChessboardCorners(gray, (7,6), None) # gray image, grid size -&amp;gt; bool value about finding corners, corner points of image # If found, add object points, image points (after refining them) if ret == True: objpoints.append(objp) # refines corner locations using gradient direction # image,corners, window size for search,zero zone(no size of -1,-1),end point after max count or value under epsilon to move corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria) imgpoints.append(corners) # Draw and display the corners cv.drawChessboardCorners(img, (7,6), corners2, ret) cv.imshow(&#39;img&#39;, img) cv.waitKey(500)cv.destroyAllWindows()3차원 object 위치 정보인 objpoints와 2차원 이미지 공간에 존재하는 object 이미지 픽셀 위치 정보인 imgpoints를 통해 intrinsic 파라미터를 조정한다.openCV의 calibrateCamera라는 함수를 사용하여 matrix를 구할 수 있다. 이 함수는 입력, 출력인자가 꽤 많다.ret, mat, dist, revecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)입력 object points for 3 dimension image points for 2 dimension in 2d image image size for gray scale intrinsic matrix (필수 인자이므로 가지고 있지 않으면 None) distortion coefficients (필수 인자이므로 가지고 있지 않으면 None)출력 True or False about calibration intrinsic calibration intrinsic matrix distortion coefficients extrinsic calibration rvecs, tvecs : extrinsic matrix 여기서 rvecs, tvecs는 카메라의 위치와 calibration을 수행할 때의 가상의 카메라 위치(캘리브레이션 보드의 좌상단 좌표를 (0,0,0)으로 가정)에 대한 값이다.objpoints를 카메라 좌표계인 (0,0,0)으로 시작하면 첫번째 인덱스를 기준으로 카메라가 해당 위치에 존재한다고 가정이 된다. 즉 원래의 위치에서 캘리브레이션 보드의 좌상단의 위치로 카메라 위치를 가정한다. 그렇다면 calibration은 가상의 카메라 위치를 기준으로 계산하지만, 실제로는 실제 카메라 위치와의 차이가 존재한다. 좌표계 변환을 위한 rotation, translation matrix를 반환한다. 이 extrinsic matrix들은 이미지를 어떻게 찍냐에 따라, 즉 캘리브레이션 보드의 위치에 따라 값이 다 달라진다.intrinsic matrix를 활용하여 OpenCV 왜곡 보정 함수를 통해 보정OpenCV에서 왜곡 보정(undistortion)을 하는 방법은 2가지가 있다. undistort initUndistortRectifyMap &amp;amp; remap# undistort 1.dst = cv.undistort(img, mtx, dist, None, mtx)# undistort 2.mapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, None, (w,h), 5)remap = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)uninstort는 내부적으로 initUndistortRectifyMap, remap을 호출하고 있다. 하지만 recitymap를 통해 camera matrix(intrinsic)과 distortion coefficients(왜곡 계수)를 얻는 것이므로 매 이미지마다 연산을 할 필요가 없다. 그래서 두 함수를 분리하여 적용하는 것이 효율적이다.추가적으로 getOptimalNewCameraMatrix()라는 함수를 사용하여 camera matrix, 즉 intrinsic matrix를 변환해줄 수 있다. 즉, 예를 들어 방사 왜곡이 0.5의 정도로 적용되는 카메라 특성이 있다고 생각했을 때, 이 함수를 사용하여 방사 왜곡이 0.8의 정도로 왜곡이 되는 matrix를 만들어낼 수 있다. 따라서 이 함수는 입력으로 camera matrix를 받아 출력으로 새로운 camera matrix를 츨력한다.원래의 camera matrix와 새로운 camera matrix를 비교해볼 수 있다.예제 코드 image point 저장chessboard 함수를 통해 image point를 저장한다. 추후 calibration에 사용될 점들이다.import cv2import globimport numpy as npimport timeimport sysDISPLAY_IMAGE = False# Get Image Path Listimage_path_list = glob.glob(&quot;images/*.jpg&quot;)# Chessboard ConfigBOARD_WIDTH = 9BOARD_HEIGHT = 6SQUARE_SIZE = 0.025# Calibration Configflags = ( cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + cv2.CALIB_CB_FAST_CHECK)pattern_size = (BOARD_WIDTH, BOARD_HEIGHT)counter = 0image_points = list()for image_path in image_path_list: image = cv2.imread(image_path, cv2.IMREAD_COLOR) # OpneCV Color Space -&amp;gt; BGR image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ret, corners = cv2.findChessboardCorners(image_gray, pattern_size, flags) if ret == True: if DISPLAY_IMAGE: image_draw = cv2.drawChessboardCorners(image, pattern_size, corners, ret) for corner in corners: counter_text = str(counter) point = (int(corner[0][0]), int(corner[0][1])) cv2.putText(image_draw, counter_text, point, 2, 0.5, (0, 0, 255), 1) counter += 1 counter = 0 cv2.imshow(&quot;img&quot;, image_draw) cv2.waitKey(0) image_points.append(corners)object_points = list()print(np.shape(image_points))# (13, 54, 1, 2)# (number of image, number of feature point, list, image_point(u, v))# ojbect_points# (13, 54, 1, 3)# (number of image, number of feature point, list, object_point(x, y, z))이미지는 opencv에서 제공하는 파일들로, chessboard에 대한 사진이다.이 사진들을 불러와 각 board에서의 코너 점들을 저장하는 과정으로, 필요한 상수들은 grid(pattern)개수와 scale size이다. 맨 마지막 모서리값은 제외하고, 중간에서의 코너들의 개수를 센다. 투영을 할 때 scale의 크기도 지정해준다. flags는 opencv에서 제공하는 것으로 지정되어 있는 값들이다.저장한 이미지 리스트를 하나씩 불러와 이미지를 열고, findChessboardCorners를 통해 코너점들을 찾는다. ret는 코너점들을 찾았는지에 대한 true or false 값으로, 코너점들을 찾았을 경우 display image라는 인자를 통해 시각화 여부를 결정하여 찾은 점들을 보고 싶다면 display image를 True로 설정한다.image points의 shape을 출력해보면 (이미지 개수, 각 이미지 마다의 점 개수, 1, object point의 크기)로 이루어져 있다. 제공된 이미지의 개수가 13개이므로 13, 코너점의 개수가 9x6=54이므로 54이고, iamge point의 경우 2차원 좌표(u,v)로 이루어지므로 2이다. calibration카메라가 바라보는 방향은 전방이 z, 오른쪽이 y, 아래쪽이 x이다. 위에서 출력해본 점들의 순서는 width방향으로 간 후 height방향으로 진행된다.&quot;&quot;&quot; forward: Z right: Y down: X&quot;&quot;&quot;for i in range(len(image_path_list)): object_point = list() height = 0 for _ in range(BOARD_HEIGHT): # Loop Height -&amp;gt; 9 width = 0 for _ in range(BOARD_WIDTH): # Loop Width -&amp;gt; 6 point = [[height, width, 0]] print(&quot;point&quot;,point) object_point.append(point) width += SQUARE_SIZE height += SQUARE_SIZE object_points.append(object_point)# ---------------------- #point [[0, 0, 0]]point [[0, 0.025, 0]]point [[0, 0.05, 0]]point [[0, 0.07500000000000001, 0]]point [[0, 0.1, 0]]point [[0, 0.125, 0]]point [[0, 0.15, 0]]point [[0, 0.175, 0]]point [[0, 0.19999999999999998, 0]]point [[0.025, 0, 0]]point [[0.025, 0.025, 0]]가상의 카메라 위치를 기반으로 하면 좌상단 기준 (x,y,z) = (0,0,0)이 원점이다. 그리고, 이미지 평면은 2차원이므로 항상 z는 일정이기에 0으로 지정한다. 출력값을 보면, width, x방향이 먼저 증가하고, 끝나면 height가 1단계 증가한다. 0.025씩 증가하는 이유는 grid의 크기가 0.025이기 때문이다. 캘리브레이션 보드는 가로 0.025, 세로 0.025의 grid로 이루어져 있다.object_points = np.asarray(object_points, dtype=np.float32) # change numpy arraytmp_image = cv2.imread(&quot;images/left01.jpg&quot;, cv2.IMREAD_ANYCOLOR)image_shape = np.shape(tmp_image)image_height = image_shape[0]image_width = image_shape[1]image_size = (image_width, image_height)ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(object_points, image_points, image_size, None, None)print(&quot;=&quot; * 20)print(f&quot;re-projection error\\n {ret}\\n&quot;)print(f&quot;camera matrix\\n {camera_matrix}\\n&quot;)print(f&quot;distortion coefficientes error\\n {dist_coeffs}\\n&quot;)print(f&quot;extrinsic for each image\\n {len(rvecs)} {len(tvecs)}&quot;)# ---------------------------- #re-projection error 0.3811715802258524camera matrix [[531.14747903 0. 341.82113196] [ 0. 531.43207499 235.03970371] [ 0. 0. 1. ]]distortion coefficientes error [[-2.74391037e-01 -3.07998458e-02 8.95674654e-04 -2.21527245e-04 2.76816016e-01]]extrinsic for each image 13 13opencv에 입력으로 넣을 때는 Mat이나 numpy로 입력이 되어야 하므로 numpy로 변환해준다.1장의 이미지를 미리 읽어오는 이유는 위에서 말한 것과 같이 intrinsic matrix를 미리 얻어놓고 반복문에서는 적용만 시키는 것이 효율적이기 때문이다. openCV에서 size를 입력할 때는 (width, height) 순으로 되어 있다. numpy의 shape를 사용해보면 반대로 (height, width)로 출력되기 때문에 위와 같이 입력해준다.출력을 보게 되면 camera matrix(instrinsic matrix)와 distortion coefficients(왜곡 계수)를 얻은 것을 볼 수 있고, extrinsic 벡터들은 각 이미지마다의 값을 가지므로 이미지 개수인 13개가 있는 것을 확인할 수 있다.for rvec, tvec, op, ip in zip(rvecs, tvecs, object_points, image_points): imagePoints, jacobian = cv2.projectPoints(op, rvec, tvec, camera_matrix, dist_coeffs) for det, proj in zip(ip, imagePoints): print(det, proj) sub += sum((det - proj)[0])print(sub)# ------------------- #[[244.45415 94.33141]] [[244.59253 94.102356]][[274.62177 92.24126]] [[274.3844 92.1637]][[305.49387 90.402885]] [[305.59717 90.43659]][[338.36407 88.836266]] [[338.12567 88.95949]][[371.59216 87.98364]] [[371.82565 87.773575]][[406.84354 86.916374]] [[406.50888 86.92322]][[441.63345 86.37207]] [[441.93878 86.45395]][[477.623 86.3797]] [[477.83344 86.40558]][[513.9866 86.79912]] [[513.8886 86.7965]]...-0.01483917236328125projectpoint 함수는 object points 들을 통해 다시 image_points 형태로 투영시켜주는 함수이다. 그렇게 출력된 imagePoints와 원래의 image_points를 비교해보면 거의 일치하는 것을 확인할 수 있다. 이 두개를 각각 뺀 값을 다 더한 후 전체적인 값들을 고려하여 출력한 값이 re-projection error, ret이다.start_time = time.process_time()for image_path in image_path_list: image = cv2.imread(image_path, cv2.IMREAD_COLOR) image_undist = cv2.undistort(image, camera_matrix, dist_coeffs, None)end_time = time.process_time()print(&quot;undistort time : &quot;,end_time - start_time)start_time = time.process_time()mapx, mapy = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, None, None, image_size, cv2.CV_32FC1)for image_path in image_path_list: image = cv2.imread(image_path, cv2.IMREAD_COLOR) image_undist = cv2.remap(image, mapx, mapy, cv2.INTER_LINEAR)end_time = time.process_time()print(&quot;remap time : &quot;,end_time - start_time)# -------------------- #undistort time : 0.53125remap time : 0.078125약 7배의 시간이 차이가 나고 있는 것을 확인할 수 있다. projection points가지고 있는 object point를 통해 image point로 투영한 후 xy,yz,zx 평면을 표시해본다.for rvec, tvec, image_path in zip(rvecs, tvecs, image_path_list): # read image image = cv2.imread(image_path, cv2.IMREAD_COLOR) # draw frame coordinate image = cv2.drawFrameAxes(image, camera_matrix, dist_coeffs, rvec, tvec, 0.2, 3) cv2.imshow(window_name, image) cv2.waitKey(0)이미지를 불러와 그에 맞는 좌표축을 그리는 코드이다.# B. projection frame points## object points -&amp;gt; [X, Y, Z]## camera coordinate# X -&amp;gt; down# Y -&amp;gt; right# Z -&amp;gt; forward# points for XY plane, YZ plane, XZ planexy_points, yz_points, zx_points = list(), list(), list()# each 1cm point, total size = 1meterpoint_size = 0.01for i in range(len(image_path_list)): # xy plane xy_point = list() x,y,z = 0,0,0 for _ in range(100): y = 0 for _ in range(100): point = [[x, y, z]] y += point_size xy_point.append(point) x += point_size xy_points.append(xy_point) # yz plane yz_point = list() x,y,z = 0,0,0 for _ in range(100): z = 0 for _ in range(100): point = [[x, y, z]] z += point_size yz_point.append(point) y += point_size yz_points.append(yz_point) # zx plane zx_point = list() x,y,z = 0,0,0 for _ in range(100): x = 0 for _ in range(100): point = [[x, y, z]] x += point_size zx_point.append(point) z += point_size zx_points.append(zx_point)object point를 image point로 투영하고자 하는데, object point는 단순히 3차원의 0.01단위로 구성된 각각의 평면을 의미한다.xy_points = np.asarray(xy_points, dtype=np.float32)yz_points = np.asarray(yz_points, dtype=np.float32)zx_points = np.asarray(zx_points, dtype=np.float32)# BGRblue_color = (255, 0, 0)green_color = (0, 255, 0)red_color = (0, 0, 255)opencv에 집어넣기 위해서는 numpy로 변환해야 하기 때문에 numpy로 변환해주고, plane을 표시할 색깔을 지정해둔다.def projection_points(image, object_points, rvec, tvec, camera_matrix, dist_coeffs, color): print(object_points.shape, object_points[:5]) # (10000, 1, 3) image_points, jacobians = cv2.projectPoints(object_points, rvec, tvec, camera_matrix, dist_coeffs) print(image_points.shape, image_points[:5]) # (10000, 1, 2) for image_point in image_points: image_point = image_point[0] x = image_point[0] y = image_point[1] if x &amp;gt; 0 and y &amp;gt; 0 and x &amp;lt; image_width and y &amp;lt; image_height: image = cv2.circle(image, (int(x), int(y)), 1, color) return image for rvec, tvec, image_path, xy, yz, zx in zip(rvecs, tvecs, image_path_list, xy_points, yz_points, zx_points): image = cv2.imread(image_path, cv2.IMREAD_COLOR) image = projection_points(image, xy, rvec, tvec, camera_matrix, dist_coeffs, blue_color) image = projection_points(image, yz, rvec, tvec, camera_matrix, dist_coeffs, red_color) image = projection_points(image, zx, rvec, tvec, camera_matrix, dist_coeffs, green_color) image = cv2.drawFrameAxes(image, camera_matrix, dist_coeffs, rvec, tvec, 0.03, 3) cv2.imshow(window_name, image) if cv2.waitKey(0) == 27: break# ------------------------ #[[[0. 0. 0. ]] [[0.01 0. 0. ]] [[0.02 0. 0. ]] [[0.03 0. 0. ]] [[0.04 0. 0. ]]][[[244.59253 94.102356]] [[244.73958 106.93609 ]] [[244.92354 119.808784]] [[245.14368 132.70506 ]] [[245.39908 145.6097 ]]]projectPoints 함수를 통해 3차원 좌표를 2차원 image point로 변환한다. 변환된 이미지의 shape은 (number of points, list, point size(u,v))로 구성되어 있다. 이를 출력해보면 3차원 좌표에 맞는 캘리브레이션 보드 위의 좌표를 의미한다. object point는 실제 world에 대한 3차원 좌표, image point는 가상의 카메라가 캘리브레이션 보드 좌상단의 위치를 기준으로 한 2차원 좌표이기 때문이다.2차원 좌표로 변환된 점들을 image에 표기하는데, 만약 0보다 작거나 이미지 크기보다 크다면 필터링을 걸어준다. 이에 대해 각각 xy, yz, zx 평면을 모두 표시해준 후 맞게 그려졌는지 확인을 위해 축도 함께 그려준다.첫번째 사진은 squared_size = 0.025로 설정한 것이고, 두번째 사진은 squared_size = 0.25로 준 사진이다. 즉 squared_size를 어떻게 주냐에 따라 각 point들의 scale이 달라진다.Camera Extrinsic Calibrationextrinsic이란 외부, 즉 위치와 자세를 의미한다. extrinsic calibration은 카메라가 실제로 존재하는 3차원 공간에 대한 정보를 다룬다. 월드 좌표계(3차원)에서 이미지 좌표계(2차원)으로 투영된 후 다시 월드 좌표계로 변환된다. 이 extrinsic calibration은 두 가지 방법으로 활용될 수 있다. sensor fusion을 위한 정보로 활용하거나 perception application을 위한 정보로 활용할 수 있다. extrinsic calibration은 환경과 조건에 따라 목적과 방법이 달라진다. 그래서 다양한 방법론을 소개하고 자신에게 주어진 환경에 맞는 방법을 찾는 것이 중요하다.일반적으로 자동차 좌표계라고 하는 정보를 통일하기 위한 좌표계가 존재한다. 이는 후륜 구동축의 중심 지면을 원점으로 하고, 회전 방향은 다음과 같다.차량에 장착되는 센서는 종류가 다양하다. 카메라, Lidar, gps, imu 등이 있다. 이들은 각각의 좌표계를 따른다. 카메라 : 카메라 좌표계를 따름 LIDAR : LIDAR 좌표계를 따름 GPS, IMU : 각 센서의 좌표계를 따름각 센서의 장착 위치도 다르고, 사용하는 좌표계도 다르기 때문에 이를 통합하는 과정이 필요하다. 다양한 좌표계를 통합하기 위해 기준 자동차 좌표계를 사용하는 것이고, 통합하는 과정을 sensor fusion이라 한다.각 센서의 장착 위치와 자세를 파악해야 하고, 카메라의 경우 extrinsic matrix의 추출값인 rvecs, tvecs를 사용한다. 이 때의 rvecs, tvecs는 카메라의 위치를 기준으로 한 값들인데, 이를 자동차 좌표계이 기준이 되도록 변경을 해야 한다. 이는 calibration 코드에 적용을 할 때 자동차 좌표계에 대한 정보를 입력으로 넣으면 자돋차 좌표계에 대한 rvec, tvec를 얻을 수 있다.자동차를 위에서 바라본 그림이다. 주황색 사각형이 카메라를 의미하고, 보라색은 캘리브레이션 보드, 초록색, 노란색이 후륜축과 객체와의 거리를 측정하기 위한 장치이다. 카메라의 높이가 h라고 가정하고, 캘리브레이션 보드의 패턴 시작이 A distance와의 직교하는 위치에서의 높이 H, 보드 중심에서 패턴 시작 위치까지의 거리가 n라 가정한다. 그렇다면 자동차 좌표계에서의 패턴 시작 좌표는 (A distance, n, H) 이 될 것이다. 그렇다면 패턴들의 object point들은 (A, n, H)를 기준으로 한 값들이 쭉 있을 것이고, 이 값을 카메라 calibration 연산에 넣으면 image point들과 자동차 좌표계 원점이 카메라 위치로 가는 rvecs, tvecs가 출력될 것이다.쉽게 말해 object points들의 좌표의 기준을 (0,0,0)이 아닌 (A,n,H)로 설정하는 것이다. 이 때는 intrinsic matrix는 변화없이 연산을 해야 한다.그래서 object point들을 통해 카메라의 위치와 자세를 계산하기 위해서는 opencv의 solvePnP 함수를 사용한다.retval, rvec, tvec = cv.solvePnP(objpoints, imgpoints, mtx, dist)이 때, 중요한 것은 mtx, dist는 intrinsic matrix로, 이것들을 필요로 한다.참고여러 개의 패턴을 사용한다고 하면 자동차 좌표계부터 패턴까지의 r,t를 미리 계산을 해놓는다면, 새로운 카메라를 사용하더라도, 또는 새로운 패턴을 사용하더라도 패턴들 사이의 관계만 알고 있다면 패턴을 찍지 않아도 r,t 추정이 가능해진다. 즉, 모든 패턴들을 자동차 좌표계에 대한 rotation, translation을 구하고, 각각의 패턴들 사이의 관계만 구해놓으면, 차량이 그 위치에 들어가서 패턴을 1장만 찍어도 모든 패턴의 r,t를 추정할 수 있다.camera - LIDAR calibrationextrinsic calibration의 활용 사례로는 camera - LIDAR fusion calibration이다.3차원 데이터를 출력하는 LIDAR 센서와 카메라를 data fusion하면 object의 depth 정보를 구할 수 있다. 카메라는 Z축, 깊이 정보가 없기 때문에 이 둘을 융합하기 위해서는 LIDAR의 정보를 카메라 좌표계로 이동시켜야 한다. 이동시키기 위한 rvec,tvec를 구하기 위해 3D object points와 이것에 대응되는 2D object points를 얻는 것이 중요하다. 얻는 방법은 매우 많지만 대표적으로 2가지가 있다. lidar의 데이터는 가로 줄 형태로 표시되기 때문에 객체의 높이 정보를 얻기가 어렵다. 그래서 구멍이 뚫려 있는 사각형이나, 마름모를 사용한다.원형을 뚫는 방법에서는 뚫린 곳은 라이다 data에서 구멍 형태로 차이가 있을 것이고, 이 형태를 통해 원형의 좌표를 얻을 수 있다. 이들을 평균내서 원의 중심점을 찾을 수 있다면 3d objpoint와 2d imgpoint를 대칭시킬 수 있다.라이다로 얻어진 objpoint와 imgpoint로 solvePnP 함수를 사용하면 lidar에서 camera로 이동시키기 위한 rvec,tvec를 계산할 수 있다. 그리고 projectpoints 함수를 사용하여 입력으로 lidar 데이터에 대한 objpoint와 lidar-&amp;gt;camera의 rvec,tvec를 넣게 되면 lidar 데이터와 함께 projection하면 이미지 위에 lidar 데이터를 올릴 수 있다. 중요한 점은 전방에 대한 정보를 투영할 수 있지만, 후방에 대한 정보도 투영이 될 수 있다. 그래서 라이다에 대한 정보를 알맞게 잘라야 정확한 정보를 얻을 수 있다. 또는 라이다는 카메라보다 위에 위치하므로 카메라에서는 가려지는 객체에 대한 정보가 라이다에는 같이 들어 있을 수 있다. 그래서 카메라가 인지하고 있는 객체인지 확인하여 제거하는 것이 중요하다.주의할 것은 rvec, tvec를 통해 이미지 데이터로 3차원 공간 정보를 복원할 수는 없다. 무수히 많은 해가 존재하기 때문이다.Geometrical Distance Estimation TheoryVision-based ACC with a Single Camera : Bounds on Range and Range Rate Accuracy, 28 July 20032D를 3D로 변환하는 접근 방법에 대해 설명하는 논문이다.Abstract단안 카메라를 통한 범위(깊이 정보, 거리 등) 추정 방법에 대한 논문이다. 오래된 논문이지만, 지금도 다양한 응용으로 활용되는 방법이다. 이 논문은 단안 카메라(single camera)를 입력으로 하는 Vision-based Adaptive Cruise Control(ACC) 시스템을 설명한다. 특히 범위(깊이 정보, 거리)등에 대한 계산을 설명하고, 이 범위가 이미지의 기하학적 요소가 범위에 미치는 영향에 대해 설명한다.Introduction기존 시스템의 기본 범위 측정 기술은 LIDAR 및 스테레오 이미지를 포함한 직접 범위 센서를 사용한다. 그러나 본 논문에서는 원근 법칙을 이용하여 간접 범위(범위 정보를 추정하는 방법)만을 이용한 단안 카메라를 사용하여 직렬 생산 ACC 제품을 만들어 거리 제어를 수행하고자 한다. 사람의 시각 시스템은 사람의 손이 닿는 범위 및 더 먼 거리에서 대락적으로 거리를 측정한다. 반면 ACC 어플리케이션은 사람이 시각적으로 측정할 수 없는 100m 정도의 거리에 대해서도 거리 측정이 필요하다. 이 때 RADAR나 LIDAR에 의해 제공되는 거리의 정확도는 거리 제어에 충분하지만, 사람의 시각 시스템인 원근 법칙(law of perspective)만으로도 만족스러운 성능을 보일 수 있음을 보여주고자 한다.단안 시각 시스템을 사용하기 위해서는 두 가지의 문제점이 존재한다. 첫번째는 대상에 대한 깊이 정보가 부족하다는 것이고, 두번째는 깊이 정보가 패턴 인식 기술(이미지의 특징을 인식)에 크게 의존적이라는 것이다. 이를 해결하기 위한 원근 법칙과 망막 발산(retinal divergence)만을 사용하여 ACC의 요구 정확도를 만족시킬 수 있을지가 중요하다.range원근법을 사용한다는 것은 차량의 크기와 차량 바닥의 위치를 활용한다는 것이다. 도로면의 기하학적 정보를 활용하면 더 정확한 결과를 얻을 수 있다. 카메라를 평평한 바닥에 평행하게 맞추고, 카메라로부터 Z만큼 떨어진 노면은 이미지에서 높이 y에 투영되고, y는 다음과 같이 구할 수 있다.\\[y = \\frac{fH}{Z}\\]이 때, f는 초점거리, H는 카메라의 높이를 말한다.A를 기준으로 하여 B와 C 차량을 탐지한다고 할 때, P는 카메라가 있는 위치를 말하고, f만큼의 거리를 가진 초점거리 뒤에 떨어져 있는 이미지 평면 I는 뒤집힌 상태이고, B의 하단 좌표를 통해 이미지 평면 상에서의 y2 높이를 구할 수 있다. C도 동일하게 y1을 구한다. 그림에서는 y1이 위에 위치하지만, 이는 뒤집혀있는 상태이므로 뒤집으면 y2가 위에 있는 상태로 출력될 것이다.이 때, 바닥이 평평하고, 광학축이 바닥과 평행하다는 가정하에 y:f = H:Z 의 비례식이 가능하므로 간편한 연산이 가능하다.실제 주행 중 거리를 추정한 결과를 보여준다. 객체의 위치가 멀수록 이미지 평면 상에서 원점과 가깝다. $ y = \\frac{fH}{Z} $ 식을 사용하여 \\(Z = \\frac{fH}{y}\\)에서 Z를 추정한다.그러나 다음과 같은 상황에서는 오차가 발생할 수 있다. 카메라의 광학 축이 노면과 평행할 수 없다 -&amp;gt; 수평선이 이미지 중앙에 위치할 수 없다. 카메라 장착 각도와 차량의 움직임(pitch)에 의해 변화가 생길 수 있다. 이러한 오차를 보정하더라도 차량이 노면과 닿는 지점의 이미지 좌표를 결정하는데 문제가 발생할 수 있다. 그 이유는 거리 정보는 원래는 실수지만 이미지 좌표는 정수이기 때문이다. $ Z_{err} $를 연산해보면 다음과 같다.\\[Z_{err} = Z_n - Z = \\frac{fH}{y + n} - \\frac{fH}{y}\\]\\[= \\frac{-nfH}{y^2 + yn} = \\frac{-nfH}{\\frac{f^2H^2}{Z^2} + n\\frac{fH}{Z}} = \\| \\frac{-nZ^2}{fH + nZ} \\|\\]이 때, $ Z_n $ 은 우리가 예측한 값을 의미하고, Z는 실제 값을 의미한다. n은 y값에서 조금의 오차가 발생한 픽셀값으로 2라면 실제값이 100px일 때, 추정값이 98px 또는 102px라고 예측한 값을 의미한다. 이 오차가 생기는 원인은 두 가지가 있다. 하나는 float 에서 int로 변환하면서 생기는 오차일 것이고, 두번째는 bbox를 잘못 추정했을 때 생기는 오차일 것이다. n은 대체로 1로 가정하여 사용한다.식을 보면, $Z^2$이므로 먼 거리의 값일수록 더 큰 오차가 발생한다는 것을 알 수 있다. f = 740pixel H = 1.2m n = 1 pixel error Z_err/Z = 5% error일 때, Z를 구해보면, 일단 fH에 비해 Z는 작은 값이므로 생략을 했을 때 $ Z_{err} = \\frac{Z^2}{fH} , Z = 0.05 * 740 * 1.2 = 44m $5%의 오차를 가지는 지점이 45m정도이고, 90m에서는 10%정도의 오차를 가진다. 얼핏보면 커보이는 수치이지만, ACC를 설계하는 과점에서 거리만을 따지는 것이 아닌 거리의 비율과 상대 속도로 판단하기 때문에 이정도의 오차는 괜찮다고 한다.range raterange rate에서는 scale 변경에 대한 계산 방법과 이산 시간 차이를 계산하는 방법을 소개한다.간단한 수식을 통해 속도를 구한다. $ v = \\frac{\\triangle Z}{\\triangle t} $ Scalescale이란 w의 너비를 가지는 차량 두개가 Z, Z’에 대해 w, w’로 표현된다면, 다음과 같은 식을 따른다. $ w = \\frac{fW}{Z} , w’ = \\frac{fW}{Z’} $이 때, v는\\[v = \\frac{\\triangle Z}{\\triangle t} = \\frac{Z&#39; - Z}{\\triangle t} = \\frac{Z \\frac{w - w&#39;}{w&#39;}}{\\triangle t}\\]이다. 이 때, s를 $ \\frac{w - w’}{w’} $ 라고 한다면 v는 $ v = \\frac{Zs}{\\triangle t} $ 이 된다. scale이 필요한 이유는 같은 너비의 객체가 가까울 때와 멀 때에 대한 좌표들이 선형적으로 비례되는 관계가 아니기 때문이다. 이들의 관계를 연산하기 위해서는 비선형적인 matrix를 구해야 하고 이를 위해 scale을 사용한다. Errorscale의 변화는 t, t’ 두 지점에서 촬영한 이미지에서 차량을 정렬하여 계산할 수 있다. 이 때 사용하는 다양한 이미지 정렬 기술이 있는데, 이미지 패치(객체에 대한 사각형)이 수 백 픽셀이라면 0.1 pixel의 정렬 오류가 발생할 수 있다. 예를 들어 이미지에서 75m 거리에 있는 소형 자동차는 15x15 픽셀로 표현되는 경우가 있다. 즉 매우 작은 객체로 표현되어 있다는 것이다. 이 때 차량을 정렬하는 오류가 발생할 수 있다. 오류는 이미지에서 대상의 크기에 따라 달라진다. 따라서 대상의 크기에 대한 값인 스케일 오류(S_acc)를 차량 이미지 너비로 나눈 정렬 오류(S_err)로 정의한다.\\[s_{acc} = \\frac{s_{err}}{w} = \\frac{s_{err}Z}{fW}\\]그렇다면 아까 구한 v를 v_err에 대한 값으로 생성할 수 있다. 범위(거리, Z)가 정확하다면 상대 속도 오류(v_err)는 다음과 같다.\\[v_{err} = \\frac{Zs_{acc}}{\\triangle t} = \\frac{Z^2 s_{err}}{fW \\triangle t}\\]이 때 중요한 점은 상대 속도 오류는 상대 속도와 무관하다. 상대 속도 오차는 거리 제곱에 따라 증가한다. 상대 속도 오류는 시간대에 반비례한다. 즉 시간 간격이 더 멀리 떨어져 있는 이미지를 사용하면 더 정확한 상대 속도를 얻는다. 시야가 좁은(초점 거리가 큰 카메라)를 사용하면 오류가 줄어들고, 정확도가 선형적으로 증가한다.상대 속도 오류가 작으면 상대 속도를 예측할 정확도가 높아진다는 것을 의미한다.이제 상대 속도가 아닌 속도 오류(v_zerr)를 계산하고자 한다.\\[Z_{err} \\approx \\frac{nZ^2}{fH} (where n \\approx 1) , v = \\frac{Zs}{\\triangle t}\\]\\[v_{zerr} = \\frac{Z_{err}s}{\\triangle t} = \\frac{nZ^2}{fH} \\frac{s}{\\triangle t} = \\frac{nZv}{fH}\\]이 때, 속도 오류를 고려한 거리 오류는 다음과 같다.\\[v_{err} = \\frac{Z^2s_{err}}{fH \\triangle t} + \\frac{nZv}{fH}\\]예를 들어 한 차량에 대해 Z = 30m f = 740pixels W = 1.2 h = 1.2 v = 0 $\\triangle t $ = 0.1s라고 한다면 v_err는\\[v_{err} = \\frac{Z^2s_{err}}{fH \\triangle t} + \\frac{nZv}{fH} = \\frac{30^2 * 0.1}{740 * 1.2 * 0.1} + \\frac{30 * 0}{740 * 1.2} = 1m/s\\]따라서 30m거리에 있는 차량에 대한 상대 속도 오류는 1m/s 정도 오류가 날 수 있다. 이 때, △t를 증가시키면 v_err가 감소된다. 비슷한 움직임을 하는 차량이라면 오랜 시간 동안 차량을 검출할 수 있다.그러나 △t가 무한정 늘어난다고 해서 지속적으로 결과가 좋아지는 것은 아니다. 일정한 가속도의 경우 위치와 속도는 다음과 같이 표현된다.\\[Z(\\triangle t) = \\frac{1}{2}a\\triangle t^2 + v\\triangle t + Z_0\\]\\[v(\\triangle t) = a\\triangle t + v_0\\]여기서 a는 상대 가속도를 의미하고, 두 시간대에서 거리 차이를 계산하고 두 시간대로 나눈다.\\[\\triangle Z = Z(\\triangle t) - Z_0 = \\frac{1}{2}a\\triangle t^2 + v_0\\triangle t\\]\\[\\frac{\\triangle Z}{\\triangle t} = \\frac{1}{2}a\\triangle t + v_0 -\\&amp;gt; v\\]마지막 식에 따르면 속도가 무한정 올라가면 속도도 무한정 증가되어야 한다. 그러나 속도가 충분히 커지면 오류가 커지게 된다. 따라서 v_err 식에 추가 항을 생성한다.\\[v_{err} = \\frac{Z^2s_{err}}{fH \\triangle t} + \\frac{nZv}{fH} + \\frac{1}{2}a\\triangle t\\]이렇게 하면 △t에 의존하는 항이 두 개가 되고, 첫번째 항은 △t에 반비례, 두번째 항은 비례한다. 그래서 위의 식을 미분하여 v_err가 최소가 되는 최적의 △t를 찾는다.\\[-\\frac{Z^2s_{err}}{fW\\triangle t^2} + \\frac{1}{2}a = 0\\]\\(\\triangle t = \\sqrt{\\frac{2Z^2 s_{err}}{fWa}}\\) 이므로, 이를 원래의 v_err에 대입한다.\\[v_{err} = Z\\sqrt{\\frac{2as_{err}}{fa}} + \\frac{nZv}{fH}\\]정리하면 최적의 △t는 거리 Z에 선형 속도 오류를 가진다. 가속도가 0인 경우 최적 △t는 무한대이다. 따라서 실제 시스템에서는 △t = 2s로 제한한다." }, { "title": "[데브코스] 12주차 - DeepLearning Data Labeling and Data Augmentation", "url": "/posts/labeling/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-05-04 15:20:00 +0900", "snippet": "Data Labelinglabeling이란 확보한 원시 데이터(raw data)를 학습에 사용하기 위해 데이터를 만드는 작업을 말한다. 레이블링(Labeling) 또는 어노테이션(Annotation), 태깅(Tagging)이라 부른다. Computer vision에서는 주로 이미지에 필요한 태스크(classification, detection,..)에 대해 결과를 미리 입력하는 작업이다. 분류의 경우는 이미지에 해당하는 클래스 정보를 입력하고, detection에 경우 이미지에 대상 객체의 bbox와 클래스 정보를 입력해야 한다.데이터를 잘못 만드는 경우 결과는 당연히 좋지 않을 것이다. 그래서 데이터 레이블링을 하는 툴을 하나 정도는 다룰 줄 아는 것이 중요하다. 데이터를 만들어야 하는 상황은 가지고 있는 데이터셋과는 다른 환경과 조건에 대한 데이터도 함께 학습을 시키고 싶다면, 자신이 얻은 데이터셋을 직접 레이블링해서 추가할수도 있다. 또는 가상환경에서의 데이터와 현실환경에서의 데이터를 융합해서 데이터 증강을 통해 학습을 시킬수도 있을 것이다.데이터를 다루는 작업이 필요한 이유는 데이터의 개수는 한계적인데, 모델의 성능은 데이터의 수가 많을수록 올라간다. 실제로 2~3TB의 데이터를 직접 제작하기도 하기 때문에 중요한 작업이다.Data labeling Tool이미지 데이터를 레이블링하는 툴은 CVAT, labelme, labelImg 등 많이 있다.CVAT설치 홈페이지CVAT는 Docker Image를 제공하여 편리하게 설치할 수 있다.WSL 설치window에 wsl2를 설치하면 윈도우 커널에서 linux 환경을 사용할 수 있다. 이를 설치하면 대체로 linux 커널을 자주 바꿔줄 필요가 없어진다. 실제 자율주행과 동일한 임베디드 환경을 사용하거나 멀티 카메라를 받아서 사용할 때는 리눅스를 써야할 수 있지만, 이를 제외하고는 wsl2가 편리하다. wsl2에서는 GPU Device를 linux 커널에서 사용할 수 있어서 window환경에서 linux/ubuntu 기반의 tensorflow, pytorch, cuda 등 다양한 환경을 구성할 수 있다.wsl1에서는 gpu를 사용하기 어려웠고, linux에서의 opencv에서 show를 하기 복잡했다. 그러나 wsl2로 넘어오면서 바로 사용이 가능해졌다.wsl 설치 홈페이지 : https://docs.microsoft.com/en-us/windows/wsl/install설치 전 윈도우 버전을 반드시 확인해야 한다.위의 홈페이지로 들어가면 WSL 설치 과정이 나와있고, 1과 2가 통합되면서 설치가 간단해졌다. wsl 패키지 설치관리자 권한으로 powershell 이나 cmd를 실행하여 wsl 기능을 설치한다.wsl --install ubuntu 배포판 설치microsoft store에서 ubuntu 20.04나 18.04를 설치한다. 여기서 LTS는 lone term support 라고 해서 장기간에 걸쳐 지원하도록 만든 소프트웨어 버전이다. 가장 최신버전인 22.04도 지원하고 있다.20.04 까지는 systemd를 사용할 수 없지만, 22.04부터는 systemd를 사용할 수 있다고 한다.설치가 끝나고 실행하면 계정을 설정하라고 나온다. 자신이 사용할 계정을 설정해주면 된다. 이 프로그램은 ubuntu와 동일하기 때문에 gcc나 ubuntu에서 사용 가능한 명령어들을 사용할 수 있다.설정한 계정이름은 매 줄마다 표시되는 이름이자 이 os의 계정 이름이다. 그리고 비밀번호는 재설정이 불가능하기 때문에 까먹는다면 ubuntu를 재설치해야 하므로 신중히 생성하도록 한다.docker최신 노트북들이 20.04 ubuntu버전이 아닌 18.04를 하면 네트워크가 잘 안잡히거나 소리가 안나는 문제들이 많다. 그래서 메인 os는 20.04로 하되 docker로 18.04 컨테이너를 만들면 따로 18.04 ubuntu 버전을 설치할 필요가 없다. 따라서 WSL2와 docker를 사용하는데 익숙해지면 환경을 구축하는데 드는 비용이 감소하고, 다른 사람들에게도 손쉽게 실행 가능한 환경을 구축할 수 있다.docker를 사용할 때 window에서는 docker desktop을 사용할 수 있지만, linux의 경우는 cli에서만 사용이 가능하다.docker desktop을 다 다운 받았다면 다음 화면이 나올 것이다.이는 튜토리얼이므로 스킵을 해도 되고 따라가도 된다. git clone을 할 경우는 오른쪽의 command를 통해 CVAT을 설치한다. 그러나 어차피 튜토리얼이 종료되면 다시 up을 해야하므로 스킵해서 하는 것이 좋다.스킵을 하거나 튜토리얼을 끝내고 나면, 아래와 같은 화면이 뜬다.여기서 cmd를 하나 켜서 작업할 폴더에 들어간 후 화면에 보이는 코드를 복사하여 그대로 붙여넣는다.docker run -d -p 80:80 docker/getting-started-p는 port를 뜻한다. 이를 치면 image가 없다고 뜨면서 설치하게 된다. 설치가 완료되면, 다음 코드를 친다.CVAT install in Window원래는 window에서 docker를 사용하면 docker가 window를 참조하게 되어 있지만, window에서 wsl을 사용하는 곳에 docker를 올리게 되면, docker가 linux(wsl)을 참조하게 된다. 그래서 window + wsl + docker를 사용할 때는 window → wsl → docker 순으로 올려진다.git clone https://github.com/opencv/cvatcd cvatdocker-compose up -d뒤의 -d는 daemon을 뜻하는 것으로 background process로 동작을 시키겠다는 의미이다. linux를 사용하다보면 daemon이 되게 많이 등장한다.설치가 완료되면 아래와 같이 CVAT에 대한 컨테이너가 생성된다.CVAT install in Ubuntu위의 과정은 window에서의 CVAT 설치 방법이었고, 이제는 ubuntu에서 CVAT을 설치하는 방법을 소개한다.설치에 대한 설명 사이트는 https://docs.docker.com/engine/install/ubuntu/ 이고, docker desktop과 달리 linux는 CLI 환경에서 설치해야 한다.docker를 사용할 때 권한 문제가 발생할 수 있다. 이 때는 sudo su-를 사용하는 것보다 유저 그룹에 docker를 추가하면 편리하게 사용이 가능하다.$ sudo groupadd docker$ sudo usermod -aG docker $USER그룹에 docker를 추가하고, user 그룹에 docker 그룹을 추가하는 코드이다.그 후 docker-compose 패키지를 설치하고, CVAT 레포지토리를 불러온다.$ sudo apt --no-install-recommands install -y python3-pip python3-setuptools$ sudo python3 -m pip install setuptools docker-compose$ sudo apt --no-install-recommands install -y git$ git clone https://github.com/opencv/cvat$ cd cvat그 후 CVAT을 공용 컴퓨터에 넣고, 다수의 사람들이 접근을 해서 사용할수 있는데, 이 때 HOST를 설정해줘야 하므로 아래의 코드를 설정해줘야 한다.$ ifconfig$ export CVAT_HOST=my-ip-address윈도우에서 ip주소를 확인할 때는 ipconfig이다.$ docker-compose up -dCVAT create accountwindow 또는 ubuntu에서 모든 설치가 끝나고 나서 만약 CVAT_HOST를 설정하지 않았다면 다음 주소를 입력해서 들어가야 한다. http://localhost:8080/auth/loginCVAT_HOST를 설정했다면, http://{IP address}:PORT/auth/login 를 통해 공용 컴퓨터로 접속이 가능하고, 위의 화면에서 각자의 계정이 존재해야 하므로 계정을 생성한다. 누가 어떤 작업을 하는지 다른 사람이 볼 수 있어야 겹치는 일이 없다.이 화면은 메인 페이지이고, CVAT의 작업 단위는 3단계로 구성되어 있다. 가장 큰 단위는 Projects 이고, 그 밑에 여러 개의 task가 존재한다. task에는 classification task, detection task, segmentation task 등이 존재할 것이고, 그 밑에 Job이라는 이미지 단위가 존재한다. 만약 한 task가 1만장을 10명에서 진행한다고 가정할 때, 하나의 task로 작업하다가 task가 날아가거나 삭제되면 모든 작업이 리셋되는 것이므로, 특정 N개의 이미지를 기준으로 job 단위로 나누어 작업을 수행한다.CVAT make Task메인 화면에서 프로젝트를 생성해도 되고, task를 새로 생성해도 되는데, 현재는 task만 생성해볼 것이다. +를 눌러 create a new task를 클릭한다.그러면 이렇게 화면이 하나 뜬다. Name : task의 이름 Project : 연결시킬 project 이름 labels : 원하는 클래스를 원하는 색상으로 추가하는 기능이고, 이는 포함되어 있는 job들이 이 label을 공유하기 때문에 잘 정의해줘야 한다. select files : labeling할 이미지를 직접 업로드하는 기능이다. advanced configuration : 상세옵션으로, 이미지 퀄리티나 태스크, job의 개수 등을 지정할 수 있는 기능이다.생성하고 나면 생성 시 설정한 이미지 개수로 자동 구성되고, 어느 정도 진행이 되고 있는지 확인할 수 있다.CVAT은 labeling 한 데이터를 배포할 때 kitti, bdd100k, cityscape, yolo 등 다양한 포맷의 데이터로 변환이 가능하다.CVAT make Project상단의 탭에서 project를 누른 후 + -&amp;gt; create new project 를 클릭한다.이름과 label을 지정해준 후 submit한 후 open projects를 누르게 되면 생성된 project를 확인할 수 있다.현재는 task가 없으므로 하단에 +를 눌러 task를 생성한다. 이름을 지정해주고, advanced configuration에서 image quality를 보면 현재는 70%이다. 그 이유는 원본 그대로를 올리게 되면 부담이 될 수 있기 때문이다. 그러나 우리는 이미지를 아주 작게 진행할 것이므로 100으로 지정해주고, job을 구분할 때 사용하는 segment size를 2로 한다. 2로 주게 되면 이미지 개수를 2로 하여 job을 구분한다.이미지를 업로드한 후 submit한다.동일한 방식으로 task를 하나 더 생성한다.object detection을 먼저 진행해보자. task에서 open을 누르고, 첫번째 job을 클릭한다.이와 같이 직접 레이블링이 가능하다. 좌측에 사각형 모양 위에 마우스 커서를 가져가면 class를 지정해줄 수 있고, share을 클릭하면 사각형을 그릴 수 있게 된다. 객체에 맞게 그려주고, N키를 누르면 다시 사각형 그리기가 가능하다. 그려진 박스에 우클릭하면 클래스를 변경해줄 수 있고, 우측 하단에 appearance에 보면 opacity, 즉 사각형의 투명도를 의미하고, 그 밑에 selected opacity는 마우스 커서를 박스 위로 가져갔을 때의 투명도를 의미한다. 상단에 &amp;gt;를 누르거나 F키를 누르면 다음 이미지로 가고, &amp;lt;를 누르거나 G키를 누르면 이전 이미지로 이동이 가능하다.완료한 후 save를 누르고, export task에서 label format을 지정해준다. 그 후 상단의 task로 이동하고, object detection task를 open해보면 방금 작업했던 job의 state가 in progress라고 되어 있다. 이는 아직 우리가 끝내고 나오지 않았기 때문이다.다시 job을 들어가서 finish the job을 클릭해준다.그러면 job이 completed되었다고 나온다.또는 finish the job 위에 change job state가 있다. 이를 통해 현재 상태를 변경해줄 수 있다.추가적으로 review모드를 통해 잘못된 레이블링은 잘못되었다고 설정해줄 수 있다.tasks 탭으로 이동하여 보게 되면 얼마나 진행되었는지 확인이 가능하다.이제 sementation을 진행해보자. segmentation의 경우 1픽셀에 대한 값들을 레이블링해야 하므로 매우 정교해야 한다. 이 때는 사각형 밑에 polygon을 사용하여 점을 일일이 다 찍어줘야 한다. free space 즉 이동 가능한 공간을 칠해준다.alt키를 누르면서 점을 클릭하면 삭제, shift키를 누르면서 점을 클릭하면 추가 점인데, 이것은 계속 누르고 있으면 연속적인 점을 찍게 되므로 조심히 클릭한 후 마우스 좌클릭으로 점을 찍어줘야 한다. 점을 드래그하면 점을 이동시킬 수 있다.cvat에서는 이미지 밖에 좌표를 찍어도 이미지 안으로 옮겨진다.cityscape format으로 추출한 뒤 colo.png를 보게 되면 다음과 같이 생성된 것을 볼 수 있다.Data Augmentation색상, 기하학적 형태 등을 변형하는 것을 data augmentation이라 하는데, 객체에 대해서는 색상을 변형해도 문제가 되지 않을 수 있으나, 신호등에 대해 색상을 변형한다면 정보가 틀려지게 되는 것이므로 문제가 발생할 수 있다.augmentation에 사용되는 라이브러리는 imgaug(https://github.com/aleju/imgaug), albumentations(https://albumentations.ai) 등이 있지만, 대표적으로 이 두가지를 많이 사용한다. albumentations가 조금 더 전문적인데, pytorch와 잘 연동이 되어 있어 pytorch를 사용한다면 후자를 사용하는 것을 추천한다.이 글에서는 albumentations를 사용할 것이고, imgaug에 대해서는 지난 블로그를 참고하는 것이 좋을 것 같다.Albumentations 설치github : https://github.com/albumentations-team/albumentationsdocumentation : https://albumentations.ai/docs/pip install -U albumentationsopenCV(opencv-python-headless, opencv-python) 등을 이미 설치했다면, 위의 코드 대신 아래 코드를 사용하는 것이 좋다.pip install -U albumentations --no-binary qudida,albumentations단순한 이미지 변환에 대해서는 documentation을 찾아보면 다 나오고, 가우시안블러, rotation, distortion 등등 다양하게 제공하고 있다. 따라서 여기서는 object detection, semgentation에 대해서만 다룰 것이다.augmentation for object detectionhttps://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/albumentations는 pascal_voc, coco, yolo format과 albumentation이라는 파이썬 패키지를 지원한다.pascal_voc의 경우 [min_x, min_y, max_x, max_y] 의 bbox를 가지고 있고, coco format의 경우 [min_x, y_min, width, height]의 bbox label을 가지고 있다. albumentations는 normalized[min_x, min_y, max_x, max_y] 의 형태이고, yolo는 normalized[center_x, center_y, width, height]를 가지고 있다.KITTI dataset format을 coco, yolo, pascal voc format으로 변환해보자. KITTI Dataset Format Values Name Description 1 type Describes the type of object: ‘Car’, ‘Van’, ‘Truck’, ‘Pedestrian’, ‘Person_sitting’, ‘Cyclist’, ‘Tram’, ‘Misc’ or ‘DontCare’ 1 truncated Float from 0 (non-truncated) to 1 (truncated), where truncated refers to the object leaving image boundaries 1 occluded Integer (0,1,2,3) indicating occlusion state: 0 = fully visible, 1 = partly occluded, 2 = largely occluded, 3 = unknown 1 alpha Observation angle of object, ranging [-pi..pi] 4 bbox 2D bounding box of object in the image (0-based index): contains left, top, right, bottom pixel coordinates 3 dimensions 3D object dimensions: height, width, length (in meters) 3 location 3D object location x,y,z in camera coordinates (in meters) 1 rotation_y Rotation ry around Y-axis in camera coordinates [-pi..pi] 1 score Only for results: Float, indicating confidence in detection, needed for p/r curves, higher is better. import random, sysimport cv2import numpy as npfrom matplotlib import pyplot as pltimport albumentations as Aimport albumentations.augmentations.bbox_utils as bbox_utilsBOX_COLOR = (255, 0, 0) # RedTEXT_COLOR = (255, 255, 255) # Whitedef my_transform(format : str): transform = A.Compose([ A.RandomCrop(width=image_width * 3 // 5, height=image_height * 3 // 5), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2), A.CLAHE(), # Contrast Limited Adaptive Histogram Equalization, # clip_limit : upper threshold for contrast, tile_grid_size : size of grid for histogram equalization A.RandomRotate90(), A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75), A.Blur(blur_limit=3), A.HueSaturationValue(), # randomly change hue, saturation A.GaussNoise(p=0.5), ], bbox_params=A.BboxParams(format=format, label_fields=[&#39;category_ids&#39;])) # label_fields : class numbers e.g. 0,1,2,3,...,8 # class_labels : class name list = category_id_to_name return transformdef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2, coco : bool = False, yolo : bool = False, pascal : bool = False): &quot;&quot;&quot;Visualizes a single bounding box on the image&quot;&quot;&quot; if coco: x_min, y_min, w, h = bbox x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h) elif yolo: cx, cy, w, h = bbox print(&quot;img shape&quot;,img.shape, &quot;\\nbbox shape&quot;, bbox) cx, w = cx * np.shape(img)[1], w * np.shape(img)[1] # unpack normalize cy, h = cy * np.shape(img)[0], h * np.shape(img)[0] x_min, y_min, x_max, y_max = int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2) elif pascal: x_min, y_min, x_max, y_max = list(map(int,bbox)) print(x_min, y_min, x_max, y_max) cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness) ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1) cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1) cv2.putText( img, text=class_name, org=(x_min, y_min - int(0.3 * text_height)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.35, color=TEXT_COLOR, lineType=cv2.LINE_AA, ) return imgdef visualize(image, bboxes, category_ids, category_id_to_name): img = image.copy() for bbox, category_id in zip(bboxes, category_ids): class_name = category_id_to_name[category_id] img = visualize_bbox(img, bbox, class_name, yolo=True) plt.figure(figsize=(12, 12)) plt.axis(&#39;off&#39;) plt.imshow(img) plt.show()image = cv2.imread(&quot;./od/image.png&quot;)# cv2.imshow(&quot;TEST&quot;, image)# cv2.waitKey(0)image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)CLASS_TABLE = { &quot;Car&quot;: 0, &quot;Van&quot;: 1, &quot;Truck&quot;: 2, &quot;Pedestrian&quot;: 3, &quot;Person_sitting&quot;: 4, &quot;Cyclist&quot;: 5, &quot;Tram&quot;: 6, &quot;Misc&quot;: 7, &quot;DontCare&quot;: 8}image_size = np.shape(image)image_height = image_size[0]image_width = image_size[1]category_id_to_name = dict()for key, value in CLASS_TABLE.items(): category_id_to_name[value] = key# TODO YOLO Parsebboxes = list()category_ids = list()with open(&quot;od/label.txt&quot;, &quot;r&quot;, encoding=&quot;UTF-8&quot;) as od_label: lines = od_label.readlines()&quot;&quot;&quot; COCO Dataset Format[x_min y_min width height]&quot;&quot;&quot;for line in lines: line = line.split(&#39; &#39;) class_id = CLASS_TABLE[line[0]] left = float(line[4]) top = float(line[5]) right = float(line[6]) bottom = float(line[7]) left, top, right, bottom = bbox_utils.normalize_bbox((left, top, right, bottom), image_height, image_width) width = right - left height = bottom - top bboxes.append((left+1e-5, top, width, height)) category_ids.append(class_id)transform = my_transform(format=&quot;yolo&quot;)transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)transformed_image = transformed[&#39;image&#39;]transformed_bboxes = transformed[&#39;bboxes&#39;]visualize(transformed_image, transformed_bboxes, category_ids, category_id_to_name)albumentations에는 yolo, coco, pascal format을 지원하므로 이에 대해 my_transform에서의 format과 visualize에서의 visualize_bbox의 인자로 어떤 format을 사용할지만 지정해주면 변환이 가능하다. 그러나 yolo의 경우 transform을 할 때 normalize된 bbox를 가져야 하기 때문에 변형시켜줘야 한다. normalize를 해주고, yolo의 경우 (0, 1] 의 범위를 가져야 한다. 따라서 0.0에서 작은 값인 1e-5를 더해준다.augmentation for segmentationimport cv2import numpy as npfrom matplotlib import pyplot as pltimport albumentations as Adef visualize(image, mask, original_image=None, original_mask=None): fontsize = 18 if original_image is None and original_mask is None: f, ax = plt.subplots(2, 1, figsize=(8, 8)) ax[0].imshow(image) ax[1].imshow(mask) else: f, ax = plt.subplots(2, 2, figsize=(8, 8)) ax[0, 0].imshow(original_image) ax[0, 0].set_title(&#39;Original image&#39;, fontsize=fontsize) ax[1, 0].imshow(original_mask) ax[1, 0].set_title(&#39;Original mask&#39;, fontsize=fontsize) ax[0, 1].imshow(image) ax[0, 1].set_title(&#39;Transformed image&#39;, fontsize=fontsize) ax[1, 1].imshow(mask) ax[1, 1].set_title(&#39;Transformed mask&#39;, fontsize=fontsize) plt.show()# Read image, maskdef read_image(path): image = cv2.imread(path, cv2.IMREAD_ANYCOLOR) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) return imageimage = read_image(&quot;seg/image.png&quot;)mask = read_image(&quot;seg/mask.png&quot;)original_height, original_width = image.shape[:2]# Augmentationaug = A.Compose([ A.OneOf([ # choose only one randomly A.RandomSizedCrop(min_max_height=(50, 101), height=original_height, width=original_width, p=0.5), A.PadIfNeeded(min_height=original_height, min_width=original_width, p=0.5) ], p=1), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5), A.OneOf([ A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5), A.GridDistortion(p=0.5), A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1) ], p=0.8), A.CLAHE(p=0.8), A.RandomBrightnessContrast(p=0.8), A.RandomGamma(p=0.8)]) augmented = aug(image=image, mask=mask)image_heavy = augmented[&#39;image&#39;]mask_heavy = augmented[&#39;mask&#39;]visualize(image_heavy, mask_heavy, original_image=image, original_mask=mask)oneof 는 저 리스트 중 랜덤으로 1개만을 사용하겠다는 의미이다." }, { "title": "[데브코스] 12주차 - DeepLearning Open Dataset and Dataset label format Convert to Yolo format ", "url": "/posts/dataset/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-05-03 17:20:00 +0900", "snippet": "딥러닝을 하기 위해서는 데이터셋에 대한 이해가 필요하다. 자신이 진행하고자 하는 문제에 대해 정의하고, 데이터를 가져오는데, 데이터의 클래스가 어떤지, 그에 대한 비율이 어떤지, 어떤 환경을 가지고 있는지 살펴봐야 한다.데이터를 가져오는 과정은 다음과 같다. 데이터 탐색(EDA) 데이터의 클래스 종류와 비율, 어떤 환경을 가지는지 확인 데이터 가공(전처리) 데이터를 가져온 후에는 데이터 전처리를 진행해야 한다. (data augmentation, removing noise) 데이터 나누기 train/valid/test 로 나누는데, 비율을 잘 나눠야 한다. 예를 들어 특정 dataset에서 환경/클래스 비율 등이 완벽하게 나눠지지 않는다. 그러므로 k-cross validation을 사용하되 StratifiedKFold와 같이 클래스 비율에 맞게 분류하는 것도 중요하다. 모델 생성 모델 검증 대규모 학습 대규모를 학습하는데 모델 생성과 검증에 비해 더 많은 시간이 걸리기 때문이다. 자율주행과 관련된 다양한 데이터를 전체적으로 확인할 수 있는 사이트가 있다.https://scale.com/open-datasetsDataSetsKITTI datasetKITTI데이터셋은 2012년에 공개된 데이터로 다소 오래된 데이터셋이다. 자율주행을 위한 데이터는 아니지만, 좋은 gps와 lidar 데이터를 가지고 있다.Kitti - Setup카메라와 라이다의 위치들이 각각 다르므로, 이 위치를 일치시켜서 코드를 구현해야 한다. 좌표계는 각자의 디바이스를 기준으로 짜여져 있기 때문이다.KITTI - Object Detection2d, 3d object detection에 사용되는 데이터셋을 지원한다. 2D OD의 경우 7481장의 training 데이터를 지원하고, 3D OD의 경우 point cloud 를 함께 지원한다.또한, bird’s eye view에 대한 데이터도 구성되어 있다.2D object detection여기 left,right는 KITTI가 카메라 2대를 사용했으므로 왼쪽 카메라, 오른쪽 카메라를 나누어 제공한다. 또한, lidar데이터, 카메라 calibration에 대한 정보도 제공한다.데이터를 다운 받은 후 label 데이터를 확인한다. kitti의 label format은 다음과 같다.Truck 0.00 0 -1.57 599.41 156.40 629.75 189.25 2.85 2.63 12.34 0.47 1.49 69.44 -1.56 type: 총 9개의 클래스에 대한 정보이다. tram은 길거리에 있는 기차를 말하고, misc는 구별하기 어려운 애매한 차량들을 지칭하는 것이고, doncares는 너무 멀리 있어서 점으로 보이거나, 잘려서 보이는 차량들에 대한 클래스로 지정해놓은 것이다. truncated : 차량이 이미지에서 벗어난 정도를 나타낸다. 0~1사이의 값으로 표현된다. occluded : 다른 오브젝트에 의해 가려진 정도를 나타낸다. 0은 전체가 다 보이는 것, 1은 일부만, 2는 많이 가려진 정도, 3은 아예 안보이는 것으로 나타낸다. alpha : 객체의 각도를 나타낸다. bbox : left, top, right, bottom에 대한 bounding box 좌표를 나타낸다.KITTI to Yolo labelkitti데이터를 yolo로 변환하기 위해서는 min_x,min_y,max_x,max_y를 center_x, center_y, width, height 로 변환해야 한다. center_x : (max_x + min_x) / 2 center_y : (max_y + min_y) / 2 width : max_x - min_x height : max_y - min_yimport torchfrom torch.utils.data.dataset import Datasetfrom torch.utils.data.dataloader import DataLoaderfrom torchvision import transformsimport numpy as npfrom glob import globimport os, json, sysfrom PIL import Imageimport matplotlib.pyplot as pltimport cv2# Visualization 1 imagedef eda(img : str, labels : list = None) -&amp;gt; None: img = cv2.imread(img, cv2.IMREAD_ANYCOLOR) if labels is not None: for label in labels: cx, cy, w, h = label[&#39;bbox&#39;] min_x, min_y, max_x, max_y = \\ int(cx-w//2), int(cy-h//2), int(cx+w//2), int(cy+h//2) cv2.rectangle(img, (min_x, min_y), (max_x, max_y), (255,255,0), 2) cv2.putText(img, label[&#39;name&#39;], org=(min_x,min_y), fontFace=cv2.FONT_ITALIC, fontScale=1, color=(0,255,0), thickness=2) cv2.imshow(&quot;img&quot;, img) cv2.waitKey(0)# Visualization 20 imagedef eda_subplot(label_list : list) -&amp;gt; None: plt.figure(figsize=(10,8)) # width, height for i,file in enumerate(label_list): plt.subplot(4,5,i+1) img_path = file.split(&#39;\\\\&#39;)[-1].split(&#39;.&#39;)[0] img = &quot;./kitti/images/train/&quot; + img_path + &quot;.png&quot; img = Image.open(img) img = np.array(img, np.uint8) plt.imshow(img) plt.show()# yes/no visualizationvisualization = False# convert min_x, min_y, max_x, max_y =&amp;gt; center_x, center_y, width, heightdef xyxy2xywh_np(bbox): min_x, min_y, max_x, max_y = np.array(bbox, dtype=np.float32) center_x = round((max_x + min_x) / 2,2) center_y = round((max_y + min_y) / 2,2) bbox_width = round(max_x - min_x,2) bbox_height = round(max_y - min_y,2) bbox = (center_x, center_y, bbox_width, bbox_height) return bbox# initial channels about grayscalechannels = 1# convert kitti label to yolo label formatclass convert2yolo(): def __init__(self): self.label_dir = &quot;./kitti/labels/&quot; self.img_dir = &quot;./kitti/images/&quot; self.img_train_dir = self.img_dir + &quot;train/&quot; self.img_valid_dir = self.img_dir + &quot;valid/&quot; self.output_dir = &quot;./yolo/labels/&quot; self.class_names = { &#39;Car&#39; : 0, &#39;Van&#39; : 1, &#39;Truck&#39; : 2, &#39;Pedestrian&#39; : 3, &#39;Person_sitting&#39; : 4, &#39;Cyclist&#39; : 5, &#39;Tram&#39; : 6, &#39;Misc&#39; : 7, &#39;DontCare&#39; : 8 } self.label_dir_list = glob(self.label_dir + &quot;/*&quot;) os.makedirs(self.output_dir, exist_ok=True) # save the txt file for yolo label format to convert from kitti label format def save(self): for file in self.label_dir_list: labels = [] with open(file, &#39;r&#39;, encoding=&#39;UTF-8&#39;) as f: lines = f.readlines() for line in lines: line = line.split(&#39; &#39;) class_id = self.class_names[line[0]] cx,cy,w,h = xyxy2xywh_np(line[4:8]) f.close() with open(self.output_dir + file.split(&quot;\\\\&quot;)[-1],&quot;w+&quot;) as yolo_file: yolo_file.write(f&quot;{class_id} {cx} {cy} {w} {h}\\n&quot;) # visualization img_path = file.split(&#39;\\\\&#39;)[-1].split(&#39;.&#39;)[0] img = self.img_train_dir + img_path + &quot;.png&quot; if visualization: eda(img, labels) yolo_file.close() def __len__(self): return len(self.label_dir_list)if __name__ == &quot;__main__&quot;: convert = convert2yolo() convert.save()BDD100K버클리 인공지능 실험 연구실에서 제공한 데이터셋으로 다양한 종류의 데이터들이 구축되어 있다. BDD 데이터셋은 다양한 도시, 다양한 환경에서 생성된 데이터들이다. yolov3에 적용할 때는 100K images와 Detection 2020 labels을 다운받는다.image는 위와 같이 train/val/test로 되어 있고, label에서는 train/val에 대한 json파일만이 존재한다.json파일은 JavaScript 언어의 자료형을 텍스트로 표현한 포맷으로 데이터의 이름을 지정해줄 수 있어서 다른 언어에서도 많이 사용되는 포맷이다.형태는 {}를 통해 블록을 분할하고, 이름 : 값, 이름 : 값 ... 의 형태를 가진다. []는 배열을 의미한다.위의 형태는 BDD100K label에 대한 포맷으로 파일명이 있어서 label의 name을 읽어오면 그에 대한 image를 불러와 읽는다. labels안에는 Bounding box에 대한 정보들이다.CityscapeCityscape는 segmentation을 위한 데이터셋이다.홈페이지cityscape의 데이터 포맷은 gtFine이 labels를 의미하고, train안에는 도시 이름으로 파일들을 구분하고 있다. leftImg8bit가 이미지 파일 경로를 의미한다.그리고 1개의 이미지 파일에 대해 4개의 label 데이터가 존재한다. polygons.json은 segmentation을 수행하기 위한 polygone 데이터들을 담고 있다. 나머지 label파일이 png인 이유는 segmentation을 할 때는 label이 image파일로 구성되어야 하기 때문이다. 이 label파일이 맞는지 확인하기 위해서는 이미지 파일에 덮어서 맞는지 확인해봐야 한다. color.pngcolor.png파일은 객체를 고유한 색상으로 표현한 참고용 이미지이다. 이미지마다 본넷이 존재하고 이를 마스킹하고 있는 것을 확인할 수 있다. instancelds.pnginstance를 수행할 때 사용하는 파일로 동일한 클래스에 대해 다른 아이디를 가지는 것을 표현하고 있다. 확인해보면 색이 거의 비슷하지만, 자세히보면 다른 값들을 가지고 있다. labellds.pngsemantic segmentation을 수행할 때 사용하는 파일로, 클래스 id에 따라 동일한 색상을 가진다.segmentation을 조금 쉽게 수행할 수 있게 만들어놓은 모델이 있다. 이는 pytorch를 사용하고 있는 간단한 모델이어서 segmentation에 대해 확인해보기 위해서 https://github.com/chenjun2hao/DDRNet.pytorch 의 깃허브를 사용하면 좋다.이는 json파일을 사용하고 있는데, 이미지의 height,width를 가지고있고, object에 대한 polygon 값을 가지고 있다. 이 polygon은 객체의 shape에 따라 개수가 다르므로 주의해서 사용해야 한다." }, { "title": "[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection", "url": "/posts/tusimple/", "categories": "Review, Object Detection", "tags": "Object Detection, tusimple", "date": "2022-04-29 03:20:00 +0900", "snippet": "Lane Detection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요.Abstract최근 Lane detection 방법들은 픽셀 단위의 segmentation 기반의 방법으로 다양한 환경과 처리 속도 문제를 해결하기 위해 노력해왔다. 사람의 인지 시스템을 참고하여, 심각한 가려짐이나 매우 힘든 밝기 조건을 전 후 상황이나 전체적인 정보(global information)를 활용하여 차선을 인식한다. 본 논문에서는 전체적인 정보를 활용하기 위한 row-based selecting을 통해 차선을 인식하여 계산 비용을 줄이고, 처리 속도를 올렸다. 확장된 receptive field는 더 많은 global feature를 받아옴으로써 모델을 robust하게 만들었다. 또한, 차선의 구조를 명시적으로 모델링하기 위한 구조적 손실(structural loss)도 제안했다.코드는 다음 github를 참고하면 된다. https://github.com/cfzd/Ultra-Fast-Lane-Detection1. Introductionlane detection은 광범위하게 적용할 수 있는 메커니즘이다. lane detection은 크게 2가지의 방법으로 분류되는데, openCV를 통한 차선 인식, deep segmentation을 활용한 차선 인식이 있다. 최근에는 딥러닝의 성능이 향상됨에 따라 deep segmentation 방법이 성능이 좋아졌지만, 아직 풀지 못한 문제가 존재한다. 자율 주행에 탑재하기 위한 차선 인식 알고리즘은 매우 낮은 계산 비용을 필요로 한다. 게다가 여러 개의 카메라를 사용하기 때문에 낮은 연산량은 필수적이다.self-distilling(모델 압축)을 통해 이를 해결하려 했지만, segmentation의 특성상 픽셀 단위의 예측이 이루어지기 때문에 여전히 문제가 발생했다. 또 다른 문제점으로, no-visual-clue, 즉 다양한 환경에서 가려짐이나 빛 조건에으로 인한 차선 탐지의 문제가 발생한다.이러한 경우 차선에 대한 높은 수준의 의미론적 정보(semantic analysis)가 필요하다. 그래서 인접한 픽셀을 활용한 메커니즘이 제안되기도 했지만, 픽셀 단위의 연산으로 인해 더 큰 계산 비용이 요구되었다.또, 차선이 선이나 곡선이 아닌 이진화된 특징으로 표현되고 있어서 차선에 대한 매끄러움이나 강직도 등의 차선의 특성을 잘 활용하지 못하고 있다.그래서 차선 탐지의 속도를 높이고, no-visual-clue문제를 해결하기 위해, 본 논문에서는 차선의 사전 정보(매그러움이나 강직도)를 잘 활용할 수 있도록 구조적 손실을 제안한다. 특히 우리는 부분적 receptive field를 기반한 픽셀 단위의 차선 탐지 대신, global feature을 활용한 미리 정의된 행에 존재하는 차선의 위치를 선택하는 방법을 사용한다.전체적인 특징을 기반으로 한 행-선택 메커니즘을 통해 no-visual-clue의 문제를 해결할 수 있다. global features를 통해 전체 이미지에 대한 receptive field를 가지기 때문에, 제한된 receptive field에 비해 가려지는 정보나 다른 위치에서의 추가적인 정보를 학습하고 활용할 수 있다. 위의 그림은 row anchor(row based selecting)에 의해 차선을 인식하는 모습을 보여주고 있다.이러한 메커니즘은 segmentation map 대신, 각기 다른 row에서의 차선의 위치를 나타낸다. 이를 통해 선택된 위치간의 관계, 즉 구조적 손실(structural loss)을 최적화함으로써 prior 정보(강직도(rigidity)나 매끄러움(smoothness))를 직접적으로 활용할 수 있다.2. Related Work2.1 Traditional method전통적인 방법들은 시각적 정보를 통해 차선 인식을 했다. 시각적 정보가 부족할 경우 tracking을 통해 정보를 얻는다.2.2 Depp learning models딥러닝을 사용한 모델들은 semantic segmentation task로서 문제를 해결했다. 예를 틀어, VPGNet은 차선이나 road marking 인식에 대해 사라지는 점들을 탐지할 수 있는 multi-task network를 제안했고, SCNN은 특별한 convolution 연산을 활용했다. 이는 찾아낸 얇은 feature들을 하나씩 덧대어 다른 차원의 정보를 얻는데, 이는 반복적인 신경망과 비슷한 메커니즘이다. SAD의 경우 real time 적용을 위해 가벼운 가중치들을 사용하려 했다. 이는 모델 압축 메커니즘을 적용한 것인데, 이는 high layer와 low layer가 선생과 제자의 관계처럼 연결되어 있다.LSTM과 같은 연속적인 예측과 클러스터링을 하는 방법도 있다.3. Method새로운 알고리즘과 차선 구조적 손실(lane structural loss)에 대해 설명하고자 한다. 또한, high level의 sementics와 low level의 시각적 정보에 대한 특징 추출도 설명한다.3.1 New formulation for lane detection이번 장에서는 전체 이미지의 특징을 기반으로 한 row-based selecting 방법을 설명한다. 즉, 각각의 미리 정의된 row들에 대해 global feature을 활용한 차선의 위치를 선택한다.그림을 조금 더 자세히 설명하자면, 원래의 segmentation방법은 특정 pixel_w, pixel_h을 가진 픽셀에 대해 [pixel_w, pixel_h]의 픽셀 값에서 연산을 했다. 한 픽셀 안에서도 각각의 채널 수만큼 각각을 연산해야 했다. 이렇게 하면 가려짐이나 시각적 정보가 없는 픽셀에 대해서는 예측이 불가능하다.그에 반해 본 논문에서의 방법은 한 픽셀에 대해서가 아닌 grid로 잘린 셀들마다의 특정 grid_w, grid_h를 가진 cell에 대해 [grid_w, grid_h]각각을 연산하는 것이 아닌 특정 grid_h를 가진 w 전체를 한번에 연산하여 각각의 채널마다 차선 위치를 예측하게 되는 것이고, grid_h마다의 출력되는 개수는 C, 채널의 수와 같다.a에서의 차선의 수의 최대값은 C, row anchor의 수는 h, gridding cells의 수는 w이다. 그리고 global image feature을 나타내는 X와 j번째 row anchor의 i번쨰 lane에 대한 분류기 $ f^{i,j} $ 라는 변수도 선언했다. 따라서 차선의 예측값은 다음과 같이 정의된다.\\[P_{i,j} = f^{i,j}(X), i \\in [1,C], j \\in [1,h] (1)\\]이 때, $ P_{i,j} $ 는 j번째 row anchor의 i번째 lane의 (w+1)개의 gridding cell의 확률값에 대한 (w+1)차원의 벡터이다. 차원이 (w+1)인 이유는 차선이 없는 경우가 포함되어야 하기 때문이다. 그리고, $ T_{i,j} $ 는 gt값에 대한 one-hot label일 때, 최적화 수식은 다음과 같다.\\[L_{cls} = \\sum_{i=1}^C \\sum_{j=1}^h L_{CE}(P_{i,j,:},T_{i,j,:}) (2)\\]L(CE)는 교차 엔트로피 손실을 나타내고, 식 (1)을 통해 global feature를 기반으로 한 각 row anchor에 대한 모든 위치의 확률 분포를 예측한 후, 식 (2)를 통해 확률 분포를 기반으로 차선에 대한 올바른 위치를 선택한다. 이 때, cls(classification)이라 표현하는 이유는 차선이 해당 grid에 있는지 없는지를 분류한다는 의미인 듯하다.본 논문의 알고리즘이 빠른 속도를 가질 수 있는 이유는 원래의 segmentation 방법들보다 계산이 간단하기 때문이다. 이미지 크기를 H x W 라고 가정을 해보자. 그러면 gridding cells의 수는 w &amp;lt;&amp;lt; W, row anchor의 수도 h &amp;lt;&amp;lt; H 로서 생성될 것이다. 원래의 segmentation은 H x W x (C+1)개의 예측을 만들어야 한다. 이 때, c+1인 이유도 차선이 없는 경우를 포함시키기 때문이다. 반면 본 논문의 방법의 경우 C x h x (w + 1)개의 예측을 만든다. 그러면 당연히 H x W x (C + 1)보다 C x h x (w + 1)이 계산 비용이 적다.예를 들어, CULane dataset에서의 기존의 segmentation 방법의 계산 비용은 1.15 x 10^6 이지만, 본 논문의 방법으로는 1.7 x 10^4 로 계산이 된다.no-visual-clue 문제를 해결하기 위해 다른 위치에서의 정보를 활용한다. 예를 들어 차선이 차에 의해 가려진다해도 본 논문의 방법은 다른 차선, 도로 형태, 차량 방향으로부터 정보를 얻어 차선 예측이 가능하다.receptive field를 전체 이미지로 확장시킴으로써, 맥락적 정보와 이미지 내 다른 위치에서의 정보를 통해 no-visual-clue를 해결할 수 있다.학습의 관점에서 봤을 때도, 도로 형태나 방향과 같은 사전 정보를 structural loss를 활용해서 학습함으로서 no-visual-clue를 해결한다.또한, row-based selecting을 통해 얻은 차선 위치는 다른 행의 위치와의 관계를 만들 수 있다.3.2 Lane structural loss차선 분류 loss외에도 차선 위치들의 관계를 모델링하는 것 또한 목표로 하기 때문에, 2가지의 loss를 추가로 제안한다.첫번째는 차선의 연속성에 대한 loss이다. 인접한 row anchor에 대해서는 차선의 위치가 가까워야 한다. 차선 분류에 대한 값은 벡터로 만들어져 있으므로, 연속성은 인접한 row anchor에 대한 분류 벡터의 분포를 제한하여 만들어진다. 즉, 얼마까지 가까워야 인접하다고 판단할 수 있는지에 대한 제한값이 필요하다는 말인데, 이를 위해 유사성에 대한 손실 함수는 다음과 같이 정의될 수 있다.\\[Lsim = \\sum_{ㅑ=1}^C \\sum_{j=1}^{h-1} || P_{i,j,:} - P_{i,j+1,:} ||_1\\]이 때, $ P_{i,j} $는 j번째 row anchor, i번째 lane에 대한 예측값을 의미한다. 그리고 $ || . ||_1 $ 는 L1 norm을 나타낸다.두번째는 차선의 모양에 대한 loss이다. 차선의 모양은 대부분 직선이고, 곡선이라 해도 미소 단위로 보면 직선이라 할 수 있다. 그래서 직선에서는 0을 만들기 위해 2차 미분 방정식을 사용하여 차선의 모양을 추정한다.j번째 row anchor, i번째 lane index에 대해 차선의 위치에 대한 수식은 다음과 같이 표현된다.\\[Loc_{i,j} = argmax_k P_{i,j,k} , k \\in [1,w]\\]이 때, k는 차선의 위치 index에 대한 상수이다. 즉, fig3 (a)에서 lane #1, lane #2, 에 대해 각각의 차선 위치를 나타내고 있다. 이 나타내는 위치가 어디인지에 대한 index가 k이다. k는 배경 gridding cell을 카운트하지 않기 때문에 1~w의 값을 가진다.여기서 문제는 2차 미분 방정식을 사용해야 하는데, argmax는 미분이 불가능한 함수이므로 다른 row anchor들과의 관계를 설명하기 어렵다. 이 문제를 해결하기 위해 softmax를 사용한다.\\[Prob_{i,j,:} = softmax(P_P{i,j,1:w})\\]\\[Loc_{i,j} = \\sum_{k=1}^w k \\cdot Prob_{i,j,k}\\]이 때도 마찬가지로 배경 gridding cell을 카운트하지 않기 때문에 1~w값을 가진다. 그리고, $ P_{i,j,1:w} $는 w 차원의 벡터를 가지며, $ Prob_{i,j,:} $ 는 각 위치에서의 확률값을 나타낸다.그리고 $ Prob_{i,j,k} $ 는 j번째 row anchor, i번째 lane에서의 k번째 위치에 대한 확률값이다.softmax를 통해 미분이 가능해졌으므로 2차 미분을 적용한 차선의 모양(shp == shape)에 대한 loss는 다음과 같이 정의할 수 있다.\\[L_{shp} = \\sum_{i=1}^C \\sum_{j=1}^{h-2} ||(Loc_{i,j} - Loc_{i,j+1}) - (Loc_{i,j+1} - Loc_{i,j+2}) ||_1\\]$ Loc_{i,j} $ 는 j번째 row anchor, i번째 lane, 에 대한 위치값이고, 1차 미분은 거의 대부분의 경우가 0이 되지 않으므로, 1차 미분 대신 2차 미분을 사용했다. 그래서 차선 위치에 대한 1차 미분의 분포를 학습하기 위해 추가적인 파라미터가 필요하다. 또한, 2차 미분의 제약은 1차 미분보다 상대적으로 약하기에 차선이 직선이 아닐 때 영향이 줄어든다.최종적으로 structural loss, Lstr는 다음과 같다.\\[L_{str} = L_{sim} + \\lambda L_{shp}\\]이 때, $ \\lambda $ 는 상수이다. 유사성에 대한 loss와 shape에 대한 loss를 더해서 최종적인 구조적 손실을 구한다. 2차 미분 방정식을 사용한다는 의미는 차선이 있을 때, 위치를 x,y로 나타내면, 1차 미분 방정식을 사용하여 x,y의 변화율을 판단하면 $ \\frac{dy}{dx} $ 이므로 직선일 때조차 0이 아니다. 그러나 2차 미분 방정식을 사용한다면 x,y의 변화율이 아닌 x,y의 변화율의 변화율 즉, dx,dy의 변화율을 판단하게 된다. 그러면 $ dx_1 = x_{1,2} - x_{1,1}, dy_1 = y_{1,2} - y_{1,1} $ 이라 하고, $ dx_2 = x_{2,2} - x_{2,1}, dy_2 = y_{2,2} - y_{2,1} $ 이라 할 때 2차 미분 방정식의 수식은 $ \\frac{d^2 y}{dx^2} = \\frac{dy_2 - dy_1}{dx_2 - dx_1} = \\frac{(y_{2,2} - y_{2,1}) - (y_{1,2} - y_{1,1})}{(x_{2,2} - x_{2,1}) - (x_{1,2} - x_{1,1})} $ 가 된다. 이 때, 직선이면 2차 미분 방정식은 0이 된다.3.3 Feature aggregation3.2에서는 global feature에 대한 loss를 설명했다. 이번 세션에서는 auxiliary feature 추정 방법에 대해 설명하고자 한다.multi scale feature들을 활용하는 auxiliary segmentation task는 local feature을 모델링한다. auxiliary segmentation loss에 대해 cross entropy를 사용했고, 전체 total loss는 다음과 같다. segmentation task란 원래 객체마다의 이진 분류가 아닌 색상별로 분류하는 task이다.\\[L_{total} = L_{cls} + \\alpha L_{str} + \\beta L_{seg}\\]Lseg는 segmentation loss이고, Lstr은 방금 전 설명했던 structural loss, cls는 classification loss이다. 이 때, $ \\alpha, \\beta $ 는 손실 계수이다.auxiliary sengmentation task는 훈련 시에만 사용할 수 있고, test 시에는 삭제된다.전체적인 구조는 아래 그림과 같다.res block(backbone) 을 통해 여러 개의 feature map을 만들고, classification, 차선에 대한 이진 분류만 할지, instance segmentation을 수행하여 차선끼리도 색상을 분리할지를 결정할 수 있다.원래의 모델은 classification만 수행하지만, segmenation을 수행하려면 auxiliary기능을 따로 생성해줘야 한다.4. Experiments4.1 Experimental settingDatasetTuSimple과 CULane, 2가지의 데이터셋을 사용했다. TuSimple dataset은 고속도로 위의 안정적인 빛 조건에서 수집된 데이터셋이고, CULane dataset은 도시 지역에서의 normal, crowd, curve, dazzle, light, night, no line, shadow, arrow, 9가지의 다른 시나리오로 구성되어 있다.Evaluation metrics두 데이터셋의 평가 방법은 다르다. TuSimple의 경우 정확도를 기준으로 평가하고, CULane은 각 차선을 30픽셀 너비에 대해 IoU를 계산한다.\\[TuSimple&#39;s accuracy = \\frac{\\sum_{clip} C_{clip}}{\\sum_{clip} S_{clip}}\\]\\[CULane&#39;s F1-measure = \\frac{2 * Precision * Recall}{Precision + Recall}\\]Cclip은 올바르게 예측된 차선의 점들의 개수이고, Sclip은 각 이미지마다 ground truth의 총 개수이다.CULane에서는 IoU가 0.5보다 큰 값을 positive로 예측하고, 그에 대한 F1-measure에서 precision과 recall은 다음과 같다.\\[Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP + FN}\\]TP는 true positive, FP는 false positive, FN은 false negative 이다.Implementation detailsTuSimple image height : 720 row anchor : range(160,710,10) number of gridding cell : 100CULane image height : 540 row anchor : range(260,530,10) number of gridding cell : 150optimizing resize : 288 x 800 optimizer : Adam learning rate : 4e-4, cosine decay learning rate strategy loss coefficients $ \\lambda, \\alpha, \\beta $ : 1 batch size : 32 epoch TuSimple : 100 CULane : 50 Data Augmentationoverfitting을 방지하기 위해 rotation, vertical/horizontal shift augmentation을 수행했고, 차선 구조를 보존하기 위해 이미지가 변형되어도 이미지 경계 끝까지 선을 그려주었다.4.2 Ablation studyEffects of number of gridding cellsTusimple dataset에서 gridding cell의 개수에 따라 성능이 달라지는지 판단하기 위해 gridding cells의 개수를 25,50,100,200을 각각 적용해보았다. 그 결과는 다음과 같다.gridding cells이 증가함에 따라 성능이 떨어진다. 이는 gridding cell이 많아질수록 더 세분화되고 분류가 어려워지기 때문이다. 그러나 evaluation에서는 다른 그래프를 볼 수 있다. 100일 때 evaluation 정확도가 가장 높았다.Effectiveness of localization methods본 논문에서는 각각의 픽셀을 판단하는 것이 아닌 group으로 묶어서 분류를 하기 때문에 분류와 비슷한 메커니즘일 수 있다. 그래서 비슷한 본 논문의 구조에서 classification head를 비슷한 구조의 regression head로 바꿔보았다. REG, REG norm, CLS, CLS exp 총 4가지의 경우에 대해 실험했는데, CLS는 classification based 방법이고, REG는 regression based 방법을 의미한다. Norm은 normalization을 의미한다.classlfication head가 앞도적으로 높은 성능을 보였다. 그 이유는 regression은 argmax를 사용하는데 반해 이 논문에서는 softmax를 사용했기 때문이라고 한다.Resultsbackbone을 Resnet18, Resnet34사용했고, Resnet18-reg, Resnet34-reg을 포함한 LaneNet, EL-GAN, SCANN, SAD 에 대해 본 논문의 모델과 비교했다.정확도 측면에서는 거의 비슷하나 RunTime의 관점에서 엄청난 차이를 볼 수 있었다.reference Ultra Fast Structure-aware Deep Lane Detection https://go-hard.tistory.com/57" }, { "title": "[데브코스] 11주차 - DeepLearning Many CNN models structure", "url": "/posts/models/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-26 15:20:00 +0900", "snippet": "AlexNet pytorchclass AlexNet(nn.Module): def __init__(self, n_classes : int) -&amp;gt; None: super(AlexNet, self).__init__() self.n_classes = n_classes self.layer = nn.Sequential( # conv1 nn.Conv2d(3, 64, 11, 4, 2) nn.ReLU(inplace=True) nn.MaxPool2d(3, 2) # conv2 nn.Conv2d(64,192,5,padding=2) nn.ReLU(inplace=True) nn.MaxPool2d(3,2) # conv3 nn.Conv2d(192,384,3,padding=1) nn.ReLU(inplace=True) # conv4 nn.Conv2d(384,256,3,padding=1) nn.ReLU(inplace=True) # conv5 nn.Conv2d(256,256,3,padding=1) nn.ReLU(inplace=True) nn.MaxPool2d(3,2) ) self.avgpool = nn.AdaptiveavgPool2d((6,6)) self.classifier = nn.Sequential( # dropout nn.Dropout(), nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096,4096), nn.ReLU(inplace=True), nn.Linear(4096, n_classes), ) def forward(self, x : torch.Tensor) -&amp;gt; torch.Tensor: x = self.layer(x) x = self.avgpool(x) # output shape : (batch size * 256, 6, 6) x = torch.flatten(x,1) # output shape : (batch_size, 256 * 6 * 6) x = self.classifier(x) return xmodel1 = AlexNet(10)import torchsummarytorchsummary.summary(model1, (3,256,256))# --------------------- #---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 63, 63] 23,296 ReLU-2 [-1, 64, 63, 63] 0 MaxPool2d-3 [-1, 64, 31, 31] 0 Conv2d-4 [-1, 192, 31, 31] 307,392 ReLU-5 [-1, 192, 31, 31] 0 MaxPool2d-6 [-1, 192, 15, 15] 0 Conv2d-7 [-1, 384, 15, 15] 663,936 ReLU-8 [-1, 384, 15, 15] 0 Conv2d-9 [-1, 256, 15, 15] 884,992 ReLU-10 [-1, 256, 15, 15] 0 Conv2d-11 [-1, 256, 15, 15] 590,080 ReLU-12 [-1, 256, 15, 15] 0 MaxPool2d-13 [-1, 256, 7, 7] 0AdaptiveAvgPool2d-14 [-1, 256, 6, 6] 0 Dropout-15 [-1, 9216] 0 Linear-16 [-1, 4096] 37,752,832 ReLU-17 [-1, 4096] 0 Dropout-18 [-1, 4096] 0 Linear-19 [-1, 4096] 16,781,312 ReLU-20 [-1, 4096] 0 Linear-21 [-1, 10] 40,970================================================================Total params: 57,044,810Trainable params: 57,044,810Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.75Forward/backward pass size (MB): 10.96Params size (MB): 217.61Estimated Total Size (MB): 229.32----------------------------------------------------------------파라미터의 크기를 보면, linear layer에서 가장 크다. 그럼에도 불구하고, linear(4096, 4096)를 쓰는 이유는??convolution 연산의 출력의 크기 out_w = (in_w - k_w + 2 * pad) / stride + 1 out_h = (in_h - k_h + 2 * pad) / stride + 1 out_channels = num_kernels매개 변수의 수 커널마다 (kernel size x kernel size x in_channels)개의 가중치와 1개의 bias를 가진다. 따라서 전체 매개변수의 수는 (kernel size x kernel size x in_channels) x num_kernels + 1 x num_kernels tensorflowdef AlexNet(input_shape=None, weight=None, classes=1000, classifier_activation=&#39;softmax&#39;) : model = tf.keras.Sequential([ # conv1 tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), stride=4, padding=&quot;valid&quot;, # no zero padding activation=tf.keras.activations.relu, input_shape=input_shape), tf.keras.layers.MaxPool2D(pool_size=(3,3), stride=2, padding=&quot;valid&quot;), tf.keras.layers.BatchNormalization(), # conv2 tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=1, padding=&quot;same&quot;, # zero padding activation=tf.keras.activations.relu), tf.keras.layers.MaxPool2D(pool_size=(3,3), stride=2, padding=&quot;valid&quot;), tf.keras.layers.BatchNormalization(), # conv3 tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=1, padding=&quot;same&quot;, activation=tf.keras.activations.relu), # conv4 tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=1, padding=&quot;same&quot;, activation=tf.keras.activations.relu), # conv5 tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=1, padding=&quot;same&quot;, activation=tf.keras.activations.relu), tf.keras.layers.MaxPool2D(pool_size=(3,3), stride=2, padding=&quot;same&quot;), tf.keras.layers.BatchNormalization(), tf.keras.layers.Flatten(), # classifier tf.keras.layers.Dense(units=4096, # linear activation=tf.keras.activations.relu), tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dense(units=4096, activation=tf.keras.activations.relu), tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dense(units=classes, activation=tf.keras.activations.softmax) ]) return modelVGGNet16 pytorchhttps://github.com/pytorch/vision/blob/main/torchvision/models/vgg.pyimport torchimport torch.nn as nnclass VGG(nn.Module): def __init__(self, features : nn.Module, num_classes: int = 1000, init_weights : bool = True) -&amp;gt; None: super(VGG,self).__init__() self.features = features # make layers self.avgpool = nn.AdaptiveAvgPool2d((7,7)) # output shape : [in_channels, 7, 7] self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes) ) if init_weights: self._initialize_weights() def forward(self, x: torch.Tensor) -&amp;gt; torch.Tensor: x = self.features(x) x = self.avgpool(x) x = torch.flatten(x,1) x = self.classifier(x) return xdef vgg16(pretrained : bool = False, progress : bool = True, **kwargs) -&amp;gt; VGG: &#39;&#39;&#39; VGG 16 layer model very deep convolutional networks for large scale image recognition args: pretrained (bool) : if True, returns a model pretrained on imageNet progress (bool) : if True, displays a progress bar of the download to stderr &#39;&#39;&#39; return _vgg(&#39;vgg16&#39;, &#39;D&#39;, False, pretrained, progress, **kwargs)def vgg16_bn(pretrained : bool = False, progress : bool = True, **kwargs) -&amp;gt; VGG: return _vgg(&#39;vgg16&#39;, &#39;D&#39;, True, pretrained, progress, **kwargs)def make_layers(cfg : dict, batch_norm : bool = False) -&amp;gt; nn.Sequential: layer : list[nn.Module] = [] in_channels = 3 for v in cfg: # max pooling if v == &quot;M&quot;: layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: v = cast(int, v) conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) # conv # batch_norm # activation if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] # conv # activation else: layers += [conv2d, nn.ReLU(inplace=True)] # 다음 conv input channel in_channels = v return nn.Sequential(*layers)cfgs : dict = { &#39;A&#39; : [64, &#39;M&#39;, 128, &#39;M&#39;, 256, 256, &#39;M&#39;, 512, 512, &#39;M&#39;, 512, 512, &#39;M&#39;], &#39;B&#39; : [64, 64, &#39;M&#39;, 128, 128, &#39;M&#39;, 256, 256, &#39;M&#39;, 512, 512, &#39;M&#39;, 512, 512, &#39;M&#39;], &#39;D&#39; : [64, 64, &#39;M&#39;, 128, 128, &#39;M&#39;, 256, 256, 256, &#39;M&#39;, 512, 512, 512, &#39;M&#39;, 512, 512, 512, &#39;M&#39;], &#39;E&#39; : [64, 64, &#39;M&#39;, 128, 128, &#39;M&#39;, 256, 256, 256, 256, &#39;M&#39;, 512, 512, 512, 512, &#39;M&#39;, 512, 512, 512, 512, &#39;M&#39;]}def _vgg(arch : str, cfg : str, batch_norm : bool, pretrained : bool, progress : bool, **kwargs : Any) -&amp;gt; VGG: if pretrained: kwargs[&#39;init_weights&#39;] = False model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs) ... tensorflowdef VGG16(input_shape=None, weights=&#39;imagenet&#39;, input_tensor=None, pooling=None, classes=1000, classifier_activation=&#39;softmax&#39;) : &#39;&#39;&#39; args: input_shape : classifier를 포함할지 말지에 대한 인자 &#39;&#39;&#39; layer = tf.keras.Sequential([ # Block1 tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, input_shape=input_shape name=&#39;block1_conv1&#39;), tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block1_conv2&#39;), tf.keras.layers.MaxPool2D(pool_size=(2,2), stride=2, name=&#39;block1_pool&#39;), # Block2 tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block2_conv1&#39;), tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block2_conv2&#39;), tf.keras.layers.MaxPool2D(pool_size=(2,2), stride=2, name=&#39;block2_pool&#39;), # Block3 tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block3_conv1&#39;), tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block3_conv2&#39;), tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block3_conv3&#39;), tf.keras.layers.MaxPool2D(pool_size=(2,2), stride=2, name=&#39;block3_pool&#39;), # Block4 tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block4_conv1&#39;), tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block4_conv2&#39;), tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block4_conv3&#39;), tf.keras.layers.MaxPool2D(pool_size=(2,2), stride=2, name=&#39;block4_pool&#39;), # Block5 tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block5_conv1&#39;), tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block5_conv2&#39;), tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=&quot;same&quot;, activation=&#39;relu&#39;, name=&#39;block5_conv3&#39;), tf.keras.layers.MaxPool2D(pool_size=(2,2), stride=2, name=&#39;block5_pool&#39;), if include_top: tf.keras.layers.Flatten(name=&#39;flatten&#39;), # classifier tf.keras.layers.Dense(units=4096, # linear activation=&#39;relu&#39;, name=&#39;fc1&#39;), tf.keras.layers.Dense(units=4096, # linear activation=&#39;relu&#39;, name=&#39;fc2&#39;), tf.keras.layers.Dense(units=classes, # linear activation=classifier_activation, name=&#39;prediction&#39;), ]) if weights == &#39;imagenet&#39;: if include_top: weight_path = data_utils.get_file( &#39;vgg16_weights_tf_dim_ordering_tf_kernels.h5&#39;, WEIGHTS_PATH, cache_subdir=&#39;models&#39;, file_hash = &#39;&#39;) else: weight_path = data_utils.get_file( &#39;vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;, WEIGHTS_PATH_NO_TOP, cache_subdir=&#39;models&#39;, file_hash = &#39;&#39;) model.load_weights(weights_path) else: model.load_weights(weights) return modelmodel = tf.keras.Sequential()for l in layer: model.add(l)model.add(keras.layers.Dense(num_classes, activation=&#39;softmax&#39;, name=&quot;prediction&quot;))GoogLeNet 1x1 커널이 때, 사용했던 1x1 커널을 설명하고 넘어가자.1x1 커널이란 차원의 통합 및 축소를 할 수 있는 효과를 가지고 있다.예를 들어 input이 [in_b, in_c, in_h, in_w] 를 가지고 있고, filter를 [1, in_c, 1, 1] 을 사용하게 되면 output의 shape은 [in_b, 1, (in_h - 1 + 2p) // stride + 1, (in_h - 1 + 2*pad) // stride + 1] 을 가진다. 만약 pad=0, stride=1이라면 (in_h - 1) + 1 = in_h, in_w가 된다. 따라서 1x1 커널을 사용하게 되면 크기는 그대로지만, 차원을 축소시킬 수 있는 효과를 가지고 있다.def __init__(self): self.fc0 = nn.Linear(1024 * 8 * 8, self.n_classes, bias = False)def forward(self, x): ... x = x.view(-1,1024 * 8 * 8) x = self.fc0(x)# --------------- #Linear-33 [-1, 10] 655,360def __init__(self): self.conv6 = nn.Conv2d(1024, 10, kernel_size=1, stride=1, padding=0)def forward(self,x): ... x = self.conv6(x)# ---------------- #Conv2d-33 [-1, 10, 8, 8] 10,250이 두 경우의 weight를 비교해보았더니 fc layer를 사용할 경우 약 65.5만개의 weight가 존재했지만, 1x1 conv를 통해 차원을 축소했더니 1만개에 불과했다.(kernel size x kernel size x in_channels) x num_kernels + 1 x num_kernels 를 통해 직접 파라미터의 수를 구해보면, (1 x 1 x 1024 x 10 + 1 x 10) = 10,250개에 불과하므로 약 65배가 차이나는 것을 확인할 수 있다. inception module또 하나 googLeNet의 핵심은 인셉션 모듈이라는 다양한 특징을 추출하기 위해 NIN 구조를 확장한 복수의 병렬적인 컨볼루션 층을 사용했다. NIN 구조에서는 기존의 컨볼루션 연산을 MLPConv 연산으로 대체하여 커널 대신 비선형 함수를 활성함수로 포함하는 MLP를 사용하여 특징을 추출한다.a가 기존의 방법으로, 하나의 receptive field를 커널과 연산하여 output을 내는 방식이었다. 그러나 MLPconv 층의 경우에는 한 receptive field를 MLP의 입력으로 넣어 연산을 한다. 이렇게 하면 해당하는 위치에 대해 완전 연결, FC layer를 사용하므로 특징을 조금 더 잘 추출할 수도 있고, receptive field안의 또 다른 다양한 receptive field 크기를 만들 수 있게 된다.NIN의 또다른 특징으로는 전역 평균 풀링을 사용했다. MLPconv layer에서의 특징맵 채널을 분류하고자 하는 클래스의 수만큼으로 만들고, 각각의 클래스가 1개의 채널만을 가리킬 수 있도록, 즉 1개의 채널마다의 평균을 구해서 확률값으로 넣어서 모든 채널을 평균내면 클래스의 수만큼의 길이로 output이 생성된다.예를 들어, 이와 같이 3채널일 때, 이를 각각의 채널을 평균 내서 각각의 클래스에 해당되도록 만든다. 그러면 최종 output은 [batch, n_classes, 1, 1]의 shape을 가지게 된다. 이를 fc layer에 넣은 후 softmax를 적용하여 확률값으로 만들어 출력한다. 이와 같은 방법의 가장 큰 장점은 전역 평균 풀링으로 만들어진 feature map에서 평균내는 것에는 매개변수가 존재하지 않는다는 것이다. 이전의 방법인 VGGNet에서는 3개의 fc layer를 사용했고, 이에 대한 매개변수는 1억2천2백만 개의 매개변수를 가지는데, 그에 반해 전역 평균 풀링과 1개의 fc layer를 사용하면 1억개의 매개변수가 사라지게 된다.이러한 MLPconv 개념을 googLeNet에 활용한 방법이 인셉션 모듈이다. MLP 대신 네 종류의 컨볼루션 연산을 사용하여 다양한 특징을 추출한다.또한, 그림을 보면, a처럼 1x1,3x3,5x5,7x7… 을 하게되면 계속 연산량이 증가하게 된다. 그래서 b처럼 1x1 conv를 사용해서 차원을 줄여 연산량을 줄인 후 연산을 한다. 마지막에는 3x3 maxpooling을 추가했다. 이렇게 구해진 4개 각각의 feature map을 concat하기 위해서는 차원을 맞춰줘야 한다. 그를 위해 maxpooling에도 1x1 convolution을 하여 차원을 변경한다.그래서 googLeNet은 inception 모듈을 9개 결합했다. 그리고 예전에는 학습이 잘 진행되지 않아서 보조 분류기를 사용해서 역전파의 결과를 결합해서 경사 소멸 문제를 완화했다. 학습할 때 도움을 주고, 추론할 때는 제거한다.ResNetResNet은 residual block을 사용했다. 층이 깊을수록 좋은 것은 확인했으나 실제 20개의 layer와 50개의 layer를 사용했을 때 학습을 진행해보면, 20layer가 오히려 loss가 더 낮게 나오는 경향이 발생했다. 이러한 이유는 최적화가 잘 되지 않았다는 것이라 생각했고, 이를 해결하는 방법으로 residual learning을 생각했다.residual block에 사용되는 residual learning의 특징으로는 원래의 학습이 입력 x를 넣으면 conv-relu-conv 를 거쳐 출력 H(x)가 된다고 할 때, 입력 x에서 H(x)가 되기 위한 변화량을 F(x)라 하여 H(x) = F(x) + x로 식을 새로 만들었다. 이 때 F(x)를 잔류(잔차)(residual)이라 한다.https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py" }, { "title": "[데브코스] 10주차 - DeepLearning Yolo v3 coding (2)", "url": "/posts/yolo2/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-25 22:30:00 +0900", "snippet": "Yolo v3Lossclass Darknet53(nn.Module): def __init__(self, cfg, param, training): ... self.module_cfg = parse_model_config(cfg) self.module_list = self.set_layer(self.module_cfg) self.yolo_layers = [layer[0] for layer in self.module_list if isinstance(layer[0], Yololayer)]진행하기 전에 Darknet53에서 yolo_layers를 추가한다. 이는 darknet53 클래스 내에서 module_list, 즉 우리가 만들어준 layer들에서 Yololayer에 해당하는 것만 저장되어 있는 멤버 변수이다. 추후에 yololayer에서의 anchor과 stride를 사용하기 위해 선언해주었다.class Yololoss(nn.Module): def __init__(self, device, n_class): super(Yololoss, self).__init__() self.device = device self.n_class = n_class self.mseloss = nn.MSELoss().to(device) # mean squared entropy self.bceloss = nn.BCELoss().to(device) # binary cross entropy self.bcelogloss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], device = device)).to(device) # take log for BCE 추후 loss를 구할 때 사용할 mseloss와 bcelogloss를 선언해놓는다. mseloss의 경우 두 입력값의 제곱의 차를 통해 loss를 구하는 것이다. bcelogloss의 경우 bceloss에 log를 취해준 방법이다.loss를 구하기 위해 함수를 생성한다. def compute_loss(self, pred, targets, yololayer): # loss_class, loss_box, loss_objectness lcls, lbox, lobj = torch.zeros(1, device = self.device), torch.zeros(1, device = self.device), torch.zeros(1, device = self.device) # predict idx, bbox for 3 yolo layers for pidx, pout in enumerate(pred): # pout.shape : [batch, anchors, grid_h, grid_w, box_attrib] print(&quot;yolo {}, shape {}&quot;.format(pidx, pout.shape))학습을 통해 예측된 값이 pred로 들어오고, targets가 GT값이다. 이를 실행해보면 출력은 다음과 같은 형태로 된다. 각 yololayer에서의 bbox의 개수를 구해보면, anchor * grid_h * grid_w 이다. 따라서 0번째 layer는 3 * 19 * 19이고, 3개의 layer에서의 총 box 개수는 22743개가 된다. 이 많은 box들에 대해 loss를 구하게 되면 연산량이 너무 많아진다.yolo 0, shape torch.Size([2, 3, 19, 19, 13])yolo 1, shape torch.Size([2, 3, 38, 38, 13])yolo 2, shape torch.Size([2, 3, 76, 76, 13])box들에는 positive prediction과 negative prediction이 존재한다. positive, negative란 예측한 bbox가 gt bbox와 얼마나 겹치는지를 계산한 후 특정 threshold보다 높으면 positive, 낮으면 negative이다. 우리가 loss를 구하는 종류에는 objectness 에 대한 loss, class score에 대한 loss, bbox에 대한 loss가 있어야 한다. 보통은 bbox loss와 class loss는 positive predict로만 구하고, objectness loss는 negative predict로만 구하는데, 만약 pos : neg = 0.01 : 0.99 의 비율을 가진다면, loss가 노이즈가 너무 많이 생기는 현상이 발생할 수 있다.따라서 positive에 대한 값들만 추출해서 loss를 계산하고자 했다. def compute_loss(self, pred, targets, yololayer): # loss_class, loss_box, loss_objectness lcls, lbox, lobj = torch.zeros(1, device = self.device), torch.zeros(1, device = self.device), torch.zeros(1, device = self.device) tcls, tbox, tindices, tanchors = self.get_targets(pred, targets, yololayer)먼저 추후에 저장할 각 loss들을 0으로 된 tensor로 선언해둔다. 그리고 positive에 대한 값들만 추출하기 위한 함수, get_targets를 선언한다.들어가기 전 각 인자들의 shape를 살펴보자.pred : torch.Size([2, 3, 19, 19, 13]), targets : torch.Size([9, 6]) pred : [batch, num of anchor, grid_y, grid_x, box_attrib] targets : [num of targets in batch, (batch_id, class_id, box[4])] # for comparing prediction and gt conveniently, we transpose shape def get_targets(self, pred, targets, yololayer): num_anch = 3 num_targets = targets.shape[0] # num of targets in batch tcls, tboxes, tindices, anch = [], [], [], [] # output : target_class, target_box, index, anchor gain = torch.ones(7, device=self.device) # make targets to 7-dim, [batch_id, cls_id, cx, cy, w, h, anchor_id] # anchor index # ai.shape = (1x3) =&amp;gt; 3x1, and repeat targets&#39;s num ai = torch.arange(num_anch, device=targets.device).float().view(num_anch, 1).repeat(1, num_targets) # to make targets to be anchor&#39;s number, targets.shape multiple anchor&#39;s num(3) targets = torch.cat((targets.repeat(num_anch, 1, 1), ai[:,:,None]), dim=2) #print(&quot;targets : &quot;, targets.shape) # [batch_id, class_id, box_cx, box_cy, box_w, box_h, anchor_id] for yi, yl in enumerate(yololayer): # 각 yolo layer feature map에 맞게 설정 # cfg 파일에서의 anchors는 608에 대한 값, 19x19, 38x38에 대한 값으로 만들어줘야 함 anchors = yl.anchor / yl.stride gain[2:6] = torch.tensor(pred[yi].shape)[[3,2,3,2]] # [1,1,grid_w, grid_h, grid_w, grid_h,1] # multiple [box_cx, box_cy,box_w,box_y] * grid size, to unpack normalize # targets&#39;s[2:6] is to be some number dependent on grid size t = targets * gain if num_targets: # in figure2 of yolov3 paper, w, h of bounding box is anchor size * exp(prediction&#39;s w) or exp(prediction&#39;s h) # so, r = exp(prediction_w) = box_w / anchor_w r = t[:,:,4:6] / anchors[:, None] # r.shape : torch.Size([3, 15, 2]) t.shape : torch.Size([3, 15, 7]) anchors[:, None].shape : torch.Size([3, 1, 2]) # extract maximum exp(prediction_w) # select the ratios less than 4, remove the too large ratios # print(r) j = torch.max(r, 1. / r).max(dim = 2)[0] &amp;lt; 4 # print(&quot;max : &quot;, torch.max(r, 1. / r).max(dim = 2)[0]) # print(j) t = t[j] # extract value for true else: # num_targets == 0 t = targets[0] # batch_id, class_id with long and transpose using filtered data to had proper anchor shape batch, cls = t[:, :2].long().T gt_xy = t[:, 2:4] gt_wh = t[:, 4:6] # define the Cx, Cy in figure2. Cx Cy is index of grid # if in 19x19 gt_xy is 17.2,17.3, Cx Cy about object is 17,17 gt_ij = gt_xy.long() # make integer from float type gt_i, gt_j = gt_ij.T # make independent each value # anchor index a = t[:, 6].long() # add indices # clamp() : 19x19 이상의 값이 되지 않기 위해 # always 0 &amp;lt; gt_j &amp;lt; grid_h -1 tindices.append((batch, a, gt_j.clamp(0, gain[3]-1), gt_i.clamp(0, gain[2]-1))) # add target box # prediction_x, prediction_y normalized is box_x - Cx, or box_y - Cy in figure2 # shape : [p_x, p_y, gt_w, gt_h] tboxes.append(torch.cat(gt_xy-gt_ij, gt_wh), dim=1) # add anchor # a is index of anchor box to guess positive box, so insert anchor box for indices anch.append(anchors[a]) # add class tcls.append(cls) return tcls, tboxes, indices, anch필요한 상수들을 지정해주고, 출력값인 tcls, tboxes, indices, anch를 먼저 할당해둔다. gain : cfg 파일에서의 anchor 박스의 크기를 맞게 설정하고, box 좌표들의 정규화를 풀어주기 위한 변수이다. ai : anchor index, 우리는 targets에 anchor id도 추가할 것이므로 arange를 통해 0,1,2 index를 만들고, 동일한 dtype을 맞추고, 연산의 정확성을 위해 float로 생성했으며, 생성한 arange는 (1,3)의 shape을 가지므로 이를 (3,1)으로 변환한다. 그리고 타겟의 수만큼 생성한다. num_targets : 9 , ai.shape : torch.Size([3, 9]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1., 1., 1., 1.], [2., 2., 2., 2., 2., 2., 2., 2., 2.]]) targets : 각 target마다 anchor가 3개씩 있으므로 개수를 맞게 만들어주기 위해 repeat를 했다. 즉 각 anchor마다의 GT target 값을 가져야 하기 때문에 이를 맞춰주었다. ai는 2차원이므로 같은 차원으로 만들어줘야 concat이 되기에 마지막 차원을 None값으로 채워 같은 차원으로 맞추었다. targets[0].shape : [batch_id, class_id, box_cx, box_cy, box_w, box_h, anchor_id] ai.shape : torch.Size([3, 12]) targets before : torch.Size([12, 6]) targets after : torch.Size([3, 12, 7]) 각 targets마다 7가지의 속성을 가지고 있는데, 이것을 anchor 개수만큼 생성했기 때문에 3,12,7이 되었다. 이 때, 12는 객체 개수이다. yi, yl : yololayers index(0,1,2), yololayers’ layer anchor : cfg 파일의 anchor들은 608크기에서의 값들이다. 그러므로 이를 feature map에 맞게 재설정해야 한다. 그래서 Yololayer에서 계산한 anchor들을 stride로 나눈다. stride란 feature map의 1grid가 가지는 픽셀의 값이다. 즉 19x19이면 input_size 608 / feature map size 19로 나눈 값이다. anchors.shape : torch.Size([3, 2]) t : 앞서 채워준 gain[2:6]에는 grid_w, grid_h, grid_w, grid_h 가 들어있고, 나머지는 1로 되어 있다. box좌표들의 정규화를 풀어주기 위해 targets와 gain을 곱한다. r : 위의 그림을 보면 $ box_w = anchor_w * \\exp{(t_w)} $ 이다. 따라서 $ r = exp(t_w) = box_w / anchor_w $이 된다. t는 [batch_id, class_id, box_cx, box_cy, box_w, box_h, anchor_id]의 형태를 가지고 있으므로, 4,5번째 index값인 box_w, box_h만을 사용하여 $ exp{(t_w)} $ 를 계산한다. 연산을 위해 동일한 차원으로 만들어줘야 해서 anchors[:, None]을 통해 3차원으로 만든다. 이 때, t_w는 predict_w, 즉 예측한 박스의 w를 의미한다. j : w,h 중에서 r과 1/r 중 큰 값을 저장하고, w와 h와 비교해서 큰 값을 추출하고, 너무 큰 값들은 제거한다. 그렇게 되면 True, False의 bool로 이루어진 list가 되서 True인 것들에 대한 것들만 추출한다. 이렇게 추출된 t는 적절한 anchor 사이즈를 갖는 데이터들로만 필터링하게 된다. batch, cls : t는 7dim을 가지는 변수이고, 그 중 0 index는 batch, 1 index는 class이므로 [:, :2] 로 작성한다. 그 값들을 int값으로 변환하기 위해 long을 붙이고, 열별로 되어 있던 데이터를 1열로 만들기 위해 transpose했다. t[:,:2] : tensor([[0., 0.], [0., 0.], [1., 0.], [1., 0.], [1., 0.], [0., 0.], [0., 0.], [1., 0.], [1., 0.], [0., 0.], [0., 0.], [1., 0.], [1., 0.]]) batch, class : torch.Size([13]), torch.Size([13]) indices : [batch index, anchor index, Cy, Cx], Cy,Cx는 위의 그림에서 Cy,Cx를 의미하는데, 이는 몇번째 grid인지에 대한 상수이다. 만약 gt_xy가 17.2, 17.3를 가진다면 17x17의 index를 가지게 되고, Cx,Cy = 17이 된다. 이 때, clamp 메서드를 사용했는데, 이는 값의 범위를 지정해서 그 이하 또는 이상의 값이 되지 않도록 필터링해준다. tboxes : 필터링해서 유의미한 타겟에 대한 박스만을 넣어준다. 이 때, gt_xy - gt_ij를 하는 이유는 위의 그림에서 볼 수 있듯이 tx,ty는 feature map의 절대 위치가 아닌 한 그리드 안에서의 위치 좌표를 나타낸다. 따라서 gt_ij가 Cx,Cy를 나타내므로 이를 빼주는 방식으로 0~1 값을 가질 수 있게 된다. anch : 유의미한 타겟에 대한 박스라고 생각한 값들에 사용한 anchor값들을 anch에 삽입 tcls : 위에서 이미 positive라고 생각한 타겟에 대한 리스트로 t를 필터링했으므로 이에 대한 cls들만 집어넣어주면 된다.get_targets를 통해 한 yololayer당 num_targets의 개수만큼의, 즉 1개의 객체당 1개의 anchor박스만을 가지게 만들었다.def compute_loss(self, pred, targets, yololayer): # loss_class, loss_box, loss_objectness lcls, lbox, lobj = torch.zeros(1, device = self.device), torch.zeros(1, device = self.device), torch.zeros(1, device = self.device) tcls, tbox, tindices, tanchors = self.get_targets(pred, targets, yololayer) for pidx, pout in enumerate(pred): batch_id, anchor_id, gy, gx = tindices[pidx] # objectness information tobj = torch.zeros_like(pout[...,0], device=self.device) num_targets = batch_id.shape[0] # number of object in the batch size if num_targets: # pout shape : [batch, anchor, grid_h, grid_w, box_attrib] # get the only box_attrib information in grid, so then we can know batch index, anchor index ba = pout[batch_id, anchor_id, gy, gx] pred_xy = torch.sigmoid(ba[...,0:2]) pred_wh = torch.exp(ba[...,2:4]) * tanchors[pidx] pred_box = torch.cat((pred_xy, pred_wh),dim=1) # pred_x,pred_y,pred_w,pred_h # iou iou = bbox_iou(pred_box, tbox[pidx], xyxy=False) # can get iou about each box # box loss ## MSE(Mean Squared loss) # mse_loss_wh = self.mseloss(pred_box[...,2:4], tbox[pidx][...,2:4]]) # mse_loss_xy = self.mseloss(pred_box[...,0:2], tbox[pidx][...,0:2]]) # print(&quot;MSE loss_xy : {}, loss_wh : {}&quot;.format(mse_loss_wh, mse_loss_xy)) lbox += (1 - iou).mean() # iou의 평균값들, 3 layer가 다 더해지도록 # class loss if ba.size(1) - 5 &amp;gt; 1: # xywh, obj_info, cls_info t = torch.zeros_like(ba[...,5:], device = self.device) # one hot encoding for the corresponding class # if the information is for the 0th class, insert 1 into 0 index t[range[num_targets], tcls[pidx]] = 1 # compute probability(ba[:,5:]) about class and list for 0 of non correct or 1 of correct (t) and sum lcls += self.bcelogloss(ba[:,5:],t) # objectness loss # gt box and prediction box are coincide -&amp;gt; positive = 1, negative = 0 # instead of dividing into 0 or 1, insert as a value between 0 and 1 tobj[batch_id, anchor_id, gy, gx] = iou.detach().clamp(0).type(tobj.dtype) # we can get also objectness loss, even if num_target is 0 lobj += self.bcelogloss(pout[...,4], tobj) # assign loss weight, to set balence for each loss lcls *= 0.05 lobj *= 1.0 lbox *= 0.5 total_loss = lcls + lbox + lobj # define the loss graph visualization loss_list = [total_loss.item(), lcls.item(), lobj.item(), lbox.item()] return total_loss, loss_list lcls, lbox, lobj : 추후에 loss를 계산하는데 사용될 리스트들을 미리 메모리 할당 for문 : 예측한 모든 pred, yolo layer에서의 출력값들을 루프를 돌면서 19x19, 38x38, 76x76 feature map에서의 예측값들에 대해 모든 boxes인 22743개를 다 보는 것이 아닌 방금 구했던 positive라고 판단되는 box들만 본다. tcls, tbox 등에 들어있는 것들은 len() = 3을 가진 각각의 feature map에서의 positive box에 대한 정보가 담겨져 있다. tobj : objectness에 대한 값이므로 마지막 shape을 1로 고정시킨 상태로 생성했다. tobj의 shape을 보면 [2, 3, 19, 19]의 크기를 가진다. 0 대신 :5를 하면 [2, 3, 19, 19, 5]가 될 것이다. num_targets : batch_id는 num_targets만큼 길이를 가지고 있다. (왜 batch_id가 num_targets 길이를 가지고 있는가) ba : 해당 grid안에 특정 anchor index, batch index를 가진 box의 속성만을 가져온다. shape는 [num_targets, 13]이다. 즉 각 target에 대한 box_attrib를 가져온다. box attribution[13]에는 bbox[4] + objectness score[1] + classes score[8]로 구성되어 있다. pred_xy, pred_wh : figure2에서처럼 box_x, box_y는 sigmoid 처리를 하고, box_w, box_h는 exp처리를 하고, anchor box 크기를 곱해준다. (pred_xy에 Cx, Cy는 왜 안곱해주는거지)# box_a, box_b IOU# xyxy is value for whether or not boxes are [minx,miny,maxx,maxy]# when we divided by 0 eps use to prevent errordef bbox_iou(box1, box2, xyxy=False, eps = 1e-9): box1 = box1.T box2 = box2.T if xyxy: b1_x1, b1_y1, b1_x2, b1_y2 = box1[0],box1[1],box1[2],box1[3] b2_x1, b2_y1, b2_x2, b2_y2 = box2[0],box2[1],box2[2],box2[3] else: b1_x1, b1_y1 = box1[0] - box1[2] / 2, box1[1] - box1[3] / 2 b1_x2, b1_y2 = box1[0] + box1[2] / 2, box1[1] + box1[3] / 2 b2_x1, b2_y1 = box2[0] - box2[2] / 2, box2[1] - box2[3] / 2 b2_x2, b2_y2 = box2[0] + box2[2] / 2, box2[1] + box2[3] / 2 # intersection inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\ (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0) # union b1_w, b1_h = b1_x2 - b1_x1, b1_y2 - b1_y1 * eps b2_w, b2_h = b2_x2 - b2_x1, b2_y2 - b2_y1 * eps # get two area and subtract intersection once. union = b1_w * b1_h + b2_w * b2_h - inter * eps # IOU iou = inter / union return ioubbox의 iou를 구하는 함수를 선언했다. 여기서 xyxy는 bbox의 포맷이 [minx, miny, maxx, maxy]인지아닌지에 대한 값이다. 그리고 eps, epsilon은 나눗셈을 할 때, 0으로 나누게 되면 에러가 생기게 되므로 이를 방지하기 위해 만든 매우 작은 상수이다.이 때, 전치를 하지 않을 경우에는 [20, 4]의 shape을 가진다. 이 상태로 계산을 하려면 아래의 식들을 수정해줘야 한다. 전치의 의미는 단지 계산의 과정의 차이이다.center_x, center_y, w, h 에 대해 box의 min x, min y, max x, max y 를 구하고, 그것들을 통해 교집합을 구한다. 영역의 교집합이므로 특정 상수가 나오는 것이 아닌 공간의 크기가 교집합이 되고, 이를 합집합 영역의 크기와 나누게 되면 iou를 구할 수 있게 된다.다시 compute_loss로 돌아가서 살펴보도록 하자. iou : 두 박스를 통해 IOU를 구한다. pred_box는 한 yololayer당 객체 개수만큼의 box를 가지고 있고, tbox[pidx] 또한 동일한 크기의 shape을 가지고 있다. mseloss : bbox에 대한 loss를 구할 때, MSEloss를 사용하게 되면, 두 박스의 제곱의 차로 구하기 때문에 값이 많이 튈 수 있다. 해당 yololayer index에서의 필터링한 tbox에서의 x,y,w,h 각각과 predicted bbox의 x,y,w,h를 비교한다. lbox : iou는 num_targets와 같은 길이를 가진다. 즉, 각 object 1개마다의 box들의 iou를 구한다. iou의 값들은 항상 0~1사이의 값을 가진다. 그래서 1-iou의 평균으로 loss를 구했다. class loss : ba에는 bbox[4] + objectness score[1] + classes score[8]가 있으므로 예외 처리를 위해, 즉 classes score가 있을 경우에는 classes score에 대한 loss를 구한다. class에 대한 shape에 대해서만 연산할 것이므로 ba[…, 5:]에 대해서만 메모리 공간을 생성했다. t : 해당 target class에 맞는 위치의 index에만 1로 바꾸는 one hot encoding 방식이다. 즉, 총 num_targets의 개수가 있고, 1개의 targets 공간마다 target class에 대한 곳에만 1로 만든다. tensor([[1., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0.]]) 총 8개의 class가 있고, 방금 필터링한 tcls가 있는데, 이 class의 index에만 1을 부여하는 식이다. lcls : 구한 one-hot 벡터와 예측한 각 class에 대한 확률값을 통해 bcelogitloss를 사용해서 lcls를 구한다. lobj : 각 object에 대해 0 또는 1의 값으로 loss를 구할 수 있지만, 조금 더 정확한 연산을 위해 iou 값을 집어넣어준다. 그리고 이것과 pred의 4index, objectness score과 BCE loss를 사용하여 loss를 구한다. loss’s weight : 모든 loss들에 동등한 크기로 계산을 할 수 있지만, 밸런스를 맞게 조정하고, 더 정확한 계산을 위해서는 각 loss에 weight를 부여해야 한다.이렇게 구해진 최종 loss를 리턴하는데, 추후에 loss graph를 그릴 때 사용하기 위해 loss list에 total_loss, lcls, lobj, lbox를 넣어서 리턴한다.최종 train# yolov3.pyfrom tensorboardX import SummaryWriter... torchwriter = SummaryWriter(&quot;./output/tensorboard&quot;)시각화하기 위해 사용하는 함수로 tensorboard를 사용할 것이다. 이를 위해 미리 선언해주고, trainer의 인자로 넣어준다.import os, sysimport torchimport torch.optim as optimfrom util.tools import *from loss.loss import *class Trainer: def __init__(self, model, train_loader, eval_loader, hyparam, device, torchwriter): self.model = model self.train_loader = train_loader self.eval_loader = eval_loader self.max_batch = hyparam[&#39;max_batch&#39;] self.device = device self.epoch = 0 self.iter = 0 self.yololoss = Yololoss(self.device, self.model.n_classes) self.optimizer = optim.SGD(model.parameters(), lr=hyparam[&#39;lr&#39;], momentum=hyparam[&#39;momentum&#39;]) self.scheduler_multistep = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[20,40,60], gamma = 0.5) # 학습을 진행할 때마다 lr이 떨어져야 더 정교하게 학습이 가능하다. 떨어지는 빈도를 multisteplr로 설정 self.torchwriter = torchwriter def run_iter(self): for i, batch in enumerate(self.train_loader): # drop the batch when invalid values if batch is None: continue input_img, targets, anno_path = batch print(&quot;input {} {}&quot;.format(input_img.shape, targets.shape)) # [batch, C, H, W], [object number, (batchidx, cls_id, box_attirb)] input_img = input_img.to(self.device, non_blocking=True) # non_blocking output = self.model(input_img) # print(input_img.shape, targets.shape) # print(output[0].shape) # get loss between output and target(gt) loss, loss_list = self.yololoss.compute_loss(output, targets, self.model.yolo_layers) loss.backward() self.optimizer.step() # gradient가 weight에 반영해서 update self.optimizer.zero_grad() self.scheduler_multistep.step(self.iter) # step마다 lr을 줄임 self.iter += 1 # [total_loss.item(), lcls.item(), lobj.item(), lbox.item()] loss_name = [&#39;total_loss&#39;, &#39;cls_loss&#39;, &#39;obj_loss&#39;, &#39;box_loss&#39;] if i % 10 == 0 : print(&quot;epoch {} / iter {} lr {} loss {}&quot;.format(self.epoch, self.iter, get_lr(self.optimizer), loss.item())) self.torchwriter.add_scalar(&#39;lr&#39;, get_lr(self.optimizer), self.iter) self.torchwriter.add_scalar(&#39;total_loss&#39;, loss, self.iter) for ln, lv in zip(loss_name, loss_list): self.torchwriter.add_scalar(ln,lv,self.iter) return loss최종적으로 계산한 loss를 역전파를 수행하고, 최적화를 사용하여 weight에 반영하여 업데이트되도록 만든다. 그리고 scheduler를 통해 learning rate를 조정한다. 이를 조정하는 이유는 학습이 진행될수록 learning rate가 작아져야 학습이 더 잘 되기 때문에 이를 조정하기 위해 사용한다.graph를 그리기 위해 torchwriter.add_scalar를 사용하여 학습된 값들을 저장한다. 이때, learning rate를 저장하기 위해서는 optimizer에서 특정 코드를 통해 빼내야 한다. 이는 아래에 추가해놓았다. 단순하게 optimizer.param_groups에서 [‘lr’]을 추출하는 것이다.# get learning rate in optimizer def get_lr(optimizer): for param_group in optimizer.param_groups: return param_group[&#39;lr&#39;] def run(self): while True: # train self.model.train() loss = self.run_iter() # save model checkpoint_path = os.path.join(&quot;./output&quot;,&quot;model_epoch&quot;+str(self.epoch)+&quot;.pth&quot;) torch.save({&#39;epoch&#39;: self.epoch, &#39;iteration&#39;: self.iter, &#39;model_state_dict&#39;: self.model.state_dict(), &#39;optimizer_state_dict&#39;: self.optimizer.state_dict(), &#39;loss&#39;:loss}, checkpoint_path) self.epoch += 1 if self.epoch == self.max_batch: breakrun_iter는 1epoch안에서 train_loader 크기만큼 학습하는 과정이고, 1 epoch이 다 돌아가면 checkpoint, model parameter들을 저장한다. torch.save에 model로만 작성하면 model 그대로 저장되지만, 이처럼 딕셔너리로 저장하고 싶은 부분만 저장할 수가 있다.최종 코드yolov3.pyimport torchimport torchvision.transforms as transformsfrom torch.utils.data.dataloader import DataLoaderimport argparseimport os, sys, timefrom dataloader.yolo_data import Yolodatafrom util.tools import *from dataloader.data_transforms import *from model.yolov3 import * from train.train import * from tensorboardX import SummaryWriterdef parse_args(): parser = argparse.ArgumentParser(description=&quot;YOLOV3_PYTORCH arguments&quot;) parser.add_argument(&quot;--gpus&quot;, type=int, nargs=&#39;+&#39;, help=&quot;List of GPU device id&quot;, default=[]) parser.add_argument(&quot;--mode&quot;, type=str, help=&quot;train / eval / test&quot;, default=None) parser.add_argument(&quot;--cfg&quot;, type=str, help=&quot;model config path&quot;, default=None) parser.add_argument(&quot;--checkpoint&quot;, type=str, help=&quot;model checkpoint path&quot;, default=None) parser.add_argument(&quot;--download&quot;, type=bool, help=&quot;download the dataset&quot;, default=False) if len(sys.argv) == 1: parser.print_help() sys.exit() args = parser.parse_args() return argsdef collate_fn(batch): # only use valid data batch = [data for data in batch if data is not None] # skip invalid data if len(batch) == 0: return imgs, targets, anno_path = list(zip(*batch)) imgs = torch.stack([img for img in imgs]) # mk 3dim -&amp;gt; 4dim, 0index = batch for i, boxes in enumerate(targets): # insert 0 index of box of dataloader function, instead of zero boxes[:,0] = i #print(boxes.shape) targets = torch.cat(targets,0) return imgs, targets, anno_path&#39;&#39;&#39; train &#39;&#39;&#39;def train(cfg_param = None, using_gpus = None): print(&quot;train&quot;) my_transform = get_transformations(cfg_param=cfg_param, is_train=True) # dataloader train_dataset = Yolodata(is_train=True, transform=my_transform, cfg_param=cfg_param) train_loader = DataLoader(train_dataset, batch_size=cfg_param[&#39;batch&#39;], num_workers = 0, # num_worker : cpu와 gpu의 데이터 교류를 담당함. 0이면 default로 single process와 같이 진행, 0이상이면 multi thred pin_memory = True, # pin_memory : img나 데이터 array를 gpu로 올릴 때 memory의 위치를 고정시킨건지 할당할건지말지에 대한 것 drop_last = True, shuffle = True, collate_fn = collate_fn) # collate_fn : batch size로 getitem할 때 각각의 이미지에 대해서만 가져온다. 그러나 학습을 할 떄는 batch 단위로 만들어줘야 하기 때문에 이를 collate fn으로 진행 model = Darknet53(args.cfg, cfg_param, training=True) model.train() model.initialize_weights() #device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) device = torch.device(&quot;cpu&quot;) model = model.to(device) torchwriter = SummaryWriter(&quot;./output/tensorboard&quot;) train = Trainer(model = model, train_loader = train_loader, eval_loader=None, hyparam=cfg_param, device = device, torchwriter = torchwriter) train.run() &#39;&#39;&#39; eval &#39;&#39;&#39;def eval(cfg_param = None, using_gpus = None): print(&quot;eval&quot;)&#39;&#39;&#39; test &#39;&#39;&#39;def test(cfg_param = None, using_gpus = None): print(&quot;test&quot;)&#39;&#39;&#39; main &#39;&#39;&#39;if __name__ == &quot;__main__&quot;: args = parse_args() if args.download == True: os.system(&quot;./install_dataset.sh&quot;) # print config file net_data, conv_data = parse_hyperparam_config(args.cfg) cfg_param = get_hyperparam(net_data) usingf_gpus = [int(g) for g in args.gpus] if args.mode == &quot;train&quot;: train(cfg_param = cfg_param) elif args.mode == &quot;eval&quot;: eval(cfg_param = cfg_param) elif args.mode == &quot;test&quot;: test(cfg_param = cfg_param) else: print(&quot;unknown mode&quot;) models/yolov3.pyimport enumimport os, sysimport numpy as npimport torchimport torch.nn as nnfrom util.tools import *def make_conv_layer(layer_idx : int, modules : nn.Module, layer_info : dict, in_channels : int): filters = int(layer_info[&#39;filters&#39;]) # output channel size size = int(layer_info[&#39;size&#39;]) # kernel size stride = int(layer_info[&#39;stride&#39;]) pad = (size - 1) // 2 # layer_info[&#39;pad&#39;] modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_conv&#39;, nn.Conv2d(in_channels, filters, size, stride, pad)) if layer_info[&#39;batch_normalize&#39;] == &#39;1&#39;: modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_bn&#39;, nn.BatchNorm2d(filters)) if layer_info[&#39;activation&#39;] == &#39;leaky&#39;: modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_act&#39;, nn.LeakyReLU()) elif layer_info[&#39;activation&#39;] == &#39;relu&#39;: modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_act&#39;, nn.ReLU())def make_shortcut_layer(layer_idx : int, modules : nn.Module): modules.add_module(&#39;layer_&#39;+str(layer_idx)+&quot;_shortcut&quot;, nn.Identity()) # modulelist에서 info 타입이 맞지 않으면 복잡해지므로 빈 공간으로 initdef make_route_layer(layer_idx : int, modules : nn.Module): modules.add_module(&#39;layer_&#39;+str(layer_idx)+&quot;_route&quot;, nn.Identity())def make_upsample_layer(layer_idx : int, modules : nn.Module, layer_info : dict): stride = int(layer_info[&#39;stride&#39;]) modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_upsample&#39;, nn.Upsample(scale_factor=stride, mode=&#39;nearest&#39;))class Yololayer(nn.Module): def __init__(self, layer_info : dict, in_width : int, in_height : int, is_train : bool): super(Yololayer, self).__init__() self.n_classes = int(layer_info[&#39;classes&#39;]) self.ignore_thresh = float(layer_info[&#39;ignore_thresh&#39;]) # loss 계산시 해당 박스가 특정 값 이상일 때만 연산에 포함되도록 self.box_attr = self.n_classes + 5 # output channel = box[4] + objectness[1] + class_prob[n] mask_idxes = [int(x) for x in layer_info[&#39;mask&#39;].split(&#39;,&#39;)] # cfg파일에서 mask의 역할은 anchor가 총 9개 선언되어 잇는데, 각각의 yololayer에서 어떤 anchor를 사용할지에 대한 index이다. 0,1,2이면 0,1,2index의 anchor를 사용한다는 뜻 anchor_all = [int(x) for x in layer_info[&#39;anchors&#39;].split(&#39;,&#39;)] # w1,h1 , w2,h2 , w3,h3 , ... 로 되어 있으므로 이것을 다시 w,h 를 묶어줘야 한다. anchor_all = [(anchor_all[i], anchor_all[i+1]) for i in range(0, len(anchor_all), 2)] self.anchor = torch.tensor([anchor_all[x] for x in mask_idxes]) self.in_width = in_width self.in_height = in_height self.stride = None # feature map의 1 grid가 차지하는 픽셀의 값 == n x n self.lw = None self.lh = None self.is_train = is_train def forward(self, x): # bounding box를 뽑을 수 있게 sigmoid나 exponantional을 취해줌 # x is input. [N C H W] self.lw, self.lh = x.shape[3], x.shape[2] # feature map&#39;s width, height self.anchor = self.anchor.to(x.device) # 연산을 할 때 동일한 곳에 올라가 있어야함, cpu input이라면 cpu에, gpu input이라면 gpu에 self.stride = torch.tensor([torch.div(self.in_width, self.lw, rounding_mode = &#39;floor&#39;), torch.div(self.in_height, self.lh, rounding_mode = &#39;floor&#39;)]).to(x.device) # stride = input size / feature map size # if kitti data, n_classes = 8, C = (8 + 5) * 3 = 39, yolo layer 이전의 filters 즉 output channels을 보면 다 39인 것을 확인할 수 있다. # [batch, box_attrib * anchor, lh, lw] ex) [1,39,19,19] # 4dim -&amp;gt; 5dim [batch, anchor, lh, lw, box_attrib] x = x.view(-1, self.anchor.shape[0], self.box_attr, self.lh, self.lw).permute(0,1,3,4,2).contiguous() # permute를 통해 dimension 순서를 변경, configuouse를 해야 바뀐채로 진행됨 return xclass Darknet53(nn.Module): def __init__(self, cfg, param, training): super().__init__() self.batch = int(param[&#39;batch&#39;]) self.in_channels = int(param[&#39;in_channels&#39;]) self.in_width = int(param[&#39;in_width&#39;]) self.in_height = int(param[&#39;in_height&#39;]) self.n_classes = int(param[&#39;classes&#39;]) self.module_cfg = parse_model_config(cfg) self.module_list = self.set_layer(self.module_cfg) self.yolo_layers = [layer[0]for layer in self.module_list if isinstance(layer[0], Yololayer)] self.training = training def set_layer(self, layer_info): # init layer setting module_list = nn.ModuleList() in_channels = [self.in_channels] # first channels of input for layer_idx, info in enumerate(layer_info): modules = nn.Sequential() if info[&#39;type&#39;] == &#39;convolutional&#39;: make_conv_layer(layer_idx, modules, info, in_channels[-1]) in_channels.append(int(info[&#39;filters&#39;])) # store each module&#39;s input channels elif info[&#39;type&#39;] == &#39;shortcut&#39;: make_shortcut_layer(layer_idx, modules) in_channels.append(in_channels[-1]) elif info[&#39;type&#39;] == &#39;route&#39;: make_route_layer(layer_idx, modules) layers = [int(y) for y in info[&#39;layers&#39;].split(&#39;,&#39;)] if len(layers) == 1: in_channels.append(in_channels[layers[0]]) elif len(layers) == 2: in_channels.append(in_channels[layers[0]] + in_channels[layers[1]]) elif info[&#39;type&#39;] == &#39;upsample&#39;: make_upsample_layer(layer_idx, modules, info) in_channels.append(in_channels[-1]) # width, height만 커지므로 channel은 동일 elif info[&#39;type&#39;] == &#39;yolo&#39;: yololayer = Yololayer(info, self.in_width, self.in_height, self.training) modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_yolo&#39;, yololayer) in_channels.append(in_channels[-1]) module_list.append(modules) return module_list def initialize_weights(self): # track all layers for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_uniform_(m.weight) # weight initializing if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) # scale nn.init.constant_(m.bias, 0) # shift elif isinstance(m, nn.Linear): nn.init.kaiming_uniform_(m.weight) nn.init.constant_(m.bias, 0) def forward(self, x): yolo_result = [] # 최종 output, 마지막 layer가 yolo이므로 layer_result = [] # shortcut, route에서 사용하기 위해 저장 for idx, (name, layer) in enumerate(zip(self.module_cfg, self.module_list)): if name[&#39;type&#39;] == &#39;convolutional&#39;: x = layer(x) layer_result.append(x) elif name[&#39;type&#39;] == &#39;shortcut&#39;: x = x + layer_result[int(name[&#39;from&#39;])] layer_result.append(x) elif name[&#39;type&#39;] == &#39;yolo&#39;: yolo_x = layer(x) layer_result.append(yolo_x) yolo_result.append(yolo_x) elif name[&#39;type&#39;] == &#39;upsample&#39;: x = layer(x) layer_result.append(x) elif name[&#39;type&#39;] == &#39;route&#39;: layers = [int(y) for y in name[&#39;layers&#39;].split(&#39;,&#39;)] x = torch.cat([layer_result[l] for l in layers], dim=1) layer_result.append(x) #print(&quot;idx : {}, result : {}&quot;.format(idx, layer_result[-1].shape)) return yolo_resulttrain/train.pyimport os, sysimport torchimport torch.optim as optimfrom util.tools import *from loss.loss import *class Trainer: def __init__(self, model, train_loader, eval_loader, hyparam, device, torchwriter): self.model = model self.train_loader = train_loader self.eval_loader = eval_loader self.max_batch = hyparam[&#39;max_batch&#39;] self.device = device self.epoch = 0 self.iter = 0 self.yololoss = Yololoss(self.device, self.model.n_classes) self.optimizer = optim.SGD(model.parameters(), lr=hyparam[&#39;lr&#39;], momentum=hyparam[&#39;momentum&#39;]) self.scheduler_multistep = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[20,40,60], gamma = 0.5) # 학습을 진행할 때마다 lr이 떨어져야 더 정교하게 학습이 가능하다. 떨어지는 빈도를 multisteplr로 설정 self.torchwriter = torchwriter def run_iter(self): for i, batch in enumerate(self.train_loader): # drop the batch when invalid values if batch is None: continue input_img, targets, anno_path = batch #print(&quot;input {} {}&quot;.format(input_img.shape, targets.shape)) # [batch, C, H, W], [object number, (batchidx, cls_id, box_attirb)] input_img = input_img.to(self.device, non_blocking=True) # non_blocking output = self.model(input_img) # get loss between output and target(gt) loss, loss_list = self.yololoss.compute_loss(output, targets, self.model.yolo_layers) loss.backward() self.optimizer.step() # gradient가 weight에 반영해서 update self.optimizer.zero_grad() self.scheduler_multistep.step(self.iter) # step마다 lr을 줄임 self.iter += 1 # [total_loss.item(), lcls.item(), lobj.item(), lbox.item()] loss_name = [&#39;total_loss&#39;, &#39;cls_loss&#39;, &#39;obj_loss&#39;, &#39;box_loss&#39;] if i % 10 == 0 : print(&quot;epoch {} / iter {} lr {} loss {}&quot;.format(self.epoch, self.iter, get_lr(self.optimizer), loss.item())) self.torchwriter.add_scalar(&#39;lr&#39;, get_lr(self.optimizer), self.iter) self.torchwriter.add_scalar(&#39;total_loss&#39;, loss, self.iter) for ln, lv in zip(loss_name, loss_list): self.torchwriter.add_scalar(ln,lv,self.iter) return loss def run(self): while True: # train self.model.train() loss = self.run_iter() # save model checkpoint_path = os.path.join(&quot;./output&quot;,&quot;model_epoch&quot;+str(self.epoch)+&quot;.pth&quot;) torch.save({&#39;epoch&#39;: self.epoch, &#39;iteration&#39;: self.iter, &#39;model_state_dict&#39;: self.model.state_dict(), &#39;optimizer_state_dict&#39;: self.optimizer.state_dict(), &#39;loss&#39;:loss}, checkpoint_path) # evaluation self.epoch += 1 if self.epoch == self.max_batch: breakloss/loss.pyimport torchimport torch.nn as nnimport sysfrom util.tools import *class Yololoss(nn.Module): def __init__(self, device, n_class): super(Yololoss, self).__init__() self.device = device self.n_class = n_class self.mseloss = nn.MSELoss().to(device) # mean squared entropy self.bceloss = nn.BCELoss().to(device) # binary cross entropy self.bcelogloss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], device = device)).to(device) # take log for BCE def compute_loss(self, pred, targets, yololayer): lcls, lbox, lobj = torch.zeros(1, device = self.device), torch.zeros(1, device = self.device), torch.zeros(1, device = self.device) # loss_class, loss_box, loss_objectness tcls, tbox, tindices, tanchors = self.get_targets(pred, targets, yololayer) for pidx, pout in enumerate(pred): batch_id, anchor_id, gy, gx = tindices[pidx] # objectness information tobj = torch.zeros_like(pout[...,0], device=self.device) num_targets = batch_id.shape[0] # number of object in the batch size if num_targets: # pout shape : [batch, anchor, grid_h, grid_w, box_attrib] # get the only box_attrib information in grid, so then we can know batch index, anchor index ba = pout[batch_id, anchor_id, gy, gx] pred_xy = torch.sigmoid(ba[...,0:2]) pred_wh = torch.exp(ba[...,2:4]) * tanchors[pidx] pred_box = torch.cat((pred_xy, pred_wh),dim=1) # pred_x,pred_y,pred_w,pred_h # print(pred_box.shape, tbox[pidx].shape) # iou iou = bbox_iou(pred_box, tbox[pidx], xyxy=False) # can get iou about each box # box loss lbox += (1 - iou).mean() # iou의 평균값들, 3 layer가 다 더해지도록 # objectness loss # gt box and prediction box are coincide -&amp;gt; positive = 1, negative = 0 # instead of dividing into 0 or 1, insert as a value between 0 and 1 tobj[batch_id, anchor_id, gy, gx] = iou.detach().clamp(0).type(tobj.dtype) # class loss if ba.size(1) - 5 &amp;gt; 1: # xywh, obj_info, cls_info t = torch.zeros_like(ba[...,5:], device = self.device) # one hot encoding for the corresponding class # if the information is for the 0th class, insert 1 into 0 index t[range(num_targets), tcls[pidx]] = 1 print(t) # compute probability(ba[:,5:]) about class and list for 0 of non correct or 1 of correct (t) and sum lcls += self.bcelogloss(ba[:,5:],t) # we can get also objectness loss, even if num_target is 0 lobj += self.bcelogloss(pout[...,4], tobj) # assign loss weight, to set balence for each loss lcls *= 0.05 lobj *= 1.0 lbox *= 0.5 total_loss = lcls + lbox + lobj # define the loss graph visualization loss_list = [total_loss.item(), lcls.item(), lobj.item(), lbox.item()] return total_loss, loss_list # for comparing prediction and gt conveniently, we transpose shape def get_targets(self, pred, targets, yololayer): num_anch = 3 num_targets = targets.shape[0] # batch size tcls, tboxes, tindices, anch = [], [], [], [] # output, target_class, target_box, index, anchor gain = torch.ones(7, device=self.device) # targets is to be 7 dim, [b_id, c_id, cx,cy,w,h,a_id] # anchor index # ai.shape = (1x3) =&amp;gt; 3x1, and repeat targets&#39;s num ai = torch.arange(num_anch, device=targets.device).float().view(num_anch, 1).repeat(1, num_targets) # to make targets to be anchor&#39;s number, targets.shape multiple anchor&#39;s num(3) targets = torch.cat((targets.repeat(num_anch, 1, 1), ai[:,:,None]), dim=2) for yi, yl in enumerate(yololayer): # 각 yolo layer feature map에 맞게 설정 # cfg 파일에서의 anchors는 608에 대한 값, 19x19, 38x38에 대한 값으로 만들어줘야 함 anchors = yl.anchor / yl.stride gain[2:6] = torch.tensor(pred[yi].shape)[[3,2,3,2]] # [1,1,grid_w, grid_h, grid_w, grid_h,1] # multiple [box_cx, box_cy,box_w,box_y] * grid size, to unpack normalize t = targets * gain # print(t) # targets&#39;s[2:6] is to be some number dependent on grid size if num_targets: # in figure2 of yolov3 paper, w, h of bounding box is anchor size * exp(prediction&#39;s w) or exp(prediction&#39;s h) # so, r = exp(prediction_w) = box_w / anchor_w r = t[:,:,4:6] / anchors[:, None] # extract maximum exp(prediction_w) # select the ratios less than 4, remove the too large ratios # print(r) j = torch.max(r, 1. / r).max(dim = 2)[0] &amp;lt; 4 # print(&quot;max : &quot;, torch.max(r, 1. / r).max(dim = 2)[0]) # print(j) t = t[j] # extract value for true else: # num_targets == 0 t = targets[0] # batch_id, class_id with integer and transpose batch, cls = t[:,:2].long().T # print(&quot;batch, class&quot;, batch.shape, cls.shape, &quot;\\n&quot;, t[:, :2].shape, t.shape) gt_xy = t[:, 2:4] gt_wh = t[:, 4:6] # define the Cx, Cy in figure2. Cx Cy is index of grid # if in 19x19 gt_xy is 17.2,17.3, Cx Cy about object is 17,17 gt_ij = gt_xy.long() # make integer from float type gt_i, gt_j = gt_ij.T # make 1 row, many col # print(gt_ij.shape, gt_i.shape, gt_j.shape) # anchor index a = t[:, 6].long() # add indices # clamp() : 19x19 이상의 값이 되지 않기 위해 # always 0 &amp;lt; gt_j &amp;lt; grid_h -1 tindices.append((batch, a, gt_j.clamp(0, gain[3]-1), gt_i.clamp(0, gain[2]-1))) # [batch id, anchor id, Cy, Cx] # add target box # prediction_x, prediction_y normalized by sigmoid is box_x - Cx, or box_y - Cy in figure2 # shape : [p_x, p_y, gt_w, gt_h] tboxes.append(torch.cat((gt_xy-gt_ij, gt_wh), dim=1)) # add anchor # a is index of anchor box to guess positive box, so insert anchor box for indices anch.append(anchors[a]) # add class tcls.append(cls) return tcls, tboxes, tindices, anchdataloader/data_transforms.pyimport numpy as npimport cv2import torchimport torchvision.transforms as transformsimport imgaug as iafrom imgaug import augmenters as iaafrom imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImagefrom util.tools import xywh2xyxy_npdef get_transformations(cfg_param = None, is_train = None): if is_train: data_transform = transforms.Compose([AbsoluteLabels(), DefaultAug(), RelativeLabels(), ResizeImage(new_size = (cfg_param[&#39;in_width&#39;], cfg_param[&#39;in_height&#39;])), ToTensor(), ]) else: data_transform = transforms.Compose([AbsoluteLabels(), DefaultAug(), RelativeLabels(), ResizeImage(new_size = (cfg_param[&#39;in_width&#39;], cfg_param[&#39;in_height&#39;])), ToTensor(), ]) return data_transform# absolute bbox, 현재는 이미지 크기에 따른 0~1값을 가지지만, 이것을 절대값으로 가지고 있어야 transform을 해도 정보를 잃지 않는다.class AbsoluteLabels(object): def __init__(self,): pass def __call__(self, data): # yolodata코드에서 transform이 들어갈 때 (img, bbox)가 data로 들어옴 img, label = data h, w , _ = img.shape label[:,[1, 3]] *= w # cx, w *= w label[:,[2, 4]] *= h # cy, h *= h return img, label# relative bboxclass RelativeLabels(object): def __init__(self,): pass def __call__(self, data): img, label = data h, w, _ = img.shape label[:,[1,3]] /= w label[:,[2,4]] /= h return img, labelclass ResizeImage(object): def __init__(self, new_size, interpolation = cv2.INTER_LINEAR): # interpolation은 보간으로 이미지를 변환할 때 빈공간을 어떻게 처리할지 self.new_size = tuple(new_size) self.interpolation = interpolation def __call__(self, data): img, label = data img = cv2.resize(img, self.new_size, interpolation=self.interpolation) return img, label # label은 normalize된 값이므로 resize하지 않아도 된다. 나중에 width, height를 곱하면 resize된 label로 만들어질 것이다.class ToTensor(object): def __init__(self,): pass def __call__(self, data): img, label = data img = torch.tensor(np.transpose(np.array(img, dtype=float) / 255, (2,0,1)), dtype=torch.float32) # normalize, transpose HWC to CHW label = torch.FloatTensor(np.array(label)) return img, label# augmentation template, 앞으로 다른 augmentation을 사용할 때 이 template을 상속받아서 구현할 것이다. 공통적으로 augmentation을 할 때마다 bbox가 augmentation방식에 따라 값이 변해야 하므로class ImgAug(object): def __init__(self, augmentations=[]): self.augmentations = augmentations def __call__(self, data): # unpack data img, labels = data # convert xywh to minx,miny,maxx,maxy because convenient and the imgAug format boxes = np.array(labels) boxes[:,1:] = xywh2xyxy_np(boxes[:,1:]) #0번째는 cls 정보이므로 # convert bbox to imgaug format bounding_boxes = BoundingBoxesOnImage( [BoundingBox(*box[1:], label=box[0]) for box in boxes], shape=img.shape) #apply augmentation img, bounding_boxes = self.augmentations(image=img, bounding_boxes=bounding_boxes) # 예외 처리, 이미지 밖으로 나가는 bounding box를 제거 bounding_boxes = bounding_boxes.clip_out_of_image() # convert bounding boxes to np.array() boxes = np.zeros((len(bounding_boxes), 5)) # memory assignment for box_idx, box in enumerate(bounding_boxes): x1, y1, x2, y2 = box.x1, box.y1, box.x2, box.y2 # x1,y1,x2,y2 멤버 변수를 가지고 있음 # return [x, y, w, h], 원래의 포맷은 xywh이므로 다시 변환 boxes[box_idx, 0] = box.label boxes[box_idx, 1] = (x1 + x2) / 2 boxes[box_idx, 2] = (y1 + y2) / 2 boxes[box_idx, 3] = x2 - x1 boxes[box_idx, 4] = y2 - y1 return img, boxesclass DefaultAug(ImgAug): def __init__(self,): self.augmentations = iaa.Sequential([ iaa.Sharpen(0.0, 0.1), iaa.Affine(rotate=(-0,0), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)) ])dataloader/yolo_data.pyimport torchfrom torch.utils.data import Datasetimport torchvisionfrom PIL import Imageimport numpy as npimport os, sysclass Yolodata(Dataset): # torch utils data의 dataset을 상속받는다. # format path file_dir = &#39;&#39; anno_dir = &#39;&#39; file_txt = &#39;&#39; base_dir = &#39;C:\\\\Users\\\\dkssu\\\\dev\\\\datasets\\\\KITTI\\\\&#39; # train dataset path train_img = base_dir + &#39;train\\\\JPEGimages\\\\&#39; train_txt = base_dir + &#39;train\\\\Annotations\\\\&#39; # valud dataset path valid_img = base_dir + &#39;valid\\\\JPEGimages\\\\&#39; valid_txt = base_dir + &#39;valid\\\\Annotations\\\\&#39; class_names = [&#39;Car&#39;, &#39;Van&#39;, &#39;Truck&#39;, &#39;Pedestrian&#39;, &#39;Persion_sitting&#39;, &#39;Cyclist&#39;, &#39;Tram&#39;, &#39;Misc&#39;] # doncare는 x num_classes = None img_data = [] def __init__(self, is_train=True, transform=None, cfg_param=None): super(Yolodata, self).__init__() self.is_train = is_train self.transform = transform self.num_class = cfg_param[&#39;classes&#39;] if self.is_train: self.file_dir = self.train_img self.anno_dir = self.train_txt self.file_txt = self.base_dir + &#39;train\\\\ImageSets\\\\train.txt&#39; else: self.file_dir = self.valid_img self.anno_dir = self.valid_txt self.file_txt = self.base_dir + &#39;valid\\\\ImageSets\\\\valid.txt&#39; img_names = [] img_data = [] with open(self.file_txt, &#39;r&#39;, encoding=&#39;UTF-8&#39;, errors=&#39;ignore&#39;) as f: img_names = [i.replace(&quot;\\n&quot;, &quot;&quot;) for i in f.readlines()] for i in img_names: if os.path.exists(self.file_dir + i + &quot;.jpg&quot;): img_data.append(i + &quot;.jpg&quot;) elif os.path.exists(self.file_dir + i + &quot;.JPG&quot;): img_data.append(i + &quot;.JPG&quot;) elif os.path.exists(self.file_dir + i + &quot;.png&quot;): img_data.append(i + &quot;.png&quot;) elif os.path.exists(self.file_dir + i + &quot;.PNG&quot;): img_data.append(i + &quot;.PNG&quot;) self.img_data = img_data print(&quot;data length : {}&quot;.format(len(self.img_data))) def __getitem__(self, index): img_path = self.file_dir + self.img_data[index] with open(img_path, &#39;rb&#39;) as f: img = np.array(Image.open(img_path).convert(&#39;RGB&#39;), dtype=np.uint8) img_origin_h, img_origin_w = img.shape[:2] # img shape : [H,W,C] # annotation dir이 있는지 확인, txt파일 읽기 if os.path.isdir(self.anno_dir): txt_name = self.img_data[index] for ext in [&#39;.png&#39;, &#39;.PNG&#39;, &#39;.jpg&#39;, &#39;.JPG&#39;]: txt_name = txt_name.replace(ext, &#39;.txt&#39;) anno_path = self.anno_dir + txt_name if not os.path.exists(anno_path): return bbox = [] with open(anno_path, &#39;r&#39;) as f: # annotation about each image for line in f.readlines(): line = line.replace(&quot;\\n&quot;,&#39;&#39;) gt_data = [l for l in line.split(&#39; &#39;)] # [class, center_x, center_y, width, height] # skip when abnormal data if len(gt_data) &amp;lt; 5: continue cls, cx, cy, w, h = float(gt_data[0]), float(gt_data[1]), float(gt_data[2]), float(gt_data[3]), float(gt_data[4]) bbox.append([cls, cx, cy, w, h]) bbox = np.array(bbox) # skip empty target empty_target = False # even if target does not exist, we have to put bbox data if bbox.shape[0] == 0: empty_target = True # bbox의 형태가 객체가 2개일경우 [[a,b,c,d,e],[a,b,c,d,e]] 이므로 형태를 맞추기 위해 [[]]로 생성 bbox = np.array([[0,0,0,0,0]]) # data augmentation if self.transform is not None: img, bbox = self.transform((img, bbox)) # 해당 배치가 몇번째 배치인지 확인하기 위한 index if not empty_target: batch_idx = torch.zeros(bbox.shape[0]) # 객체 개수만큼 크기를 생성 target_data = torch.cat((batch_idx.view(-1,1), bbox), dim=1) # x는 1, y는 객체 개수의 array로 만들어줘서 bbox와 concat else: return return img, target_data, anno_path else: # test mode bbox = np.array([[0,0,0,0,0]]) if self.transform is not None: img, _ = self.transform((img, bbox)) return img, None, None def __len__(self): return len(self.img_data)tools.pyfrom PIL import Image, ImageDrawimport numpy as npimport matplotlib.pyplot as plt# parse model layer configurationdef parse_model_config(path): file = open(path, &#39;r&#39;) lines = file.read().split(&#39;\\n&#39;) lines = [x for x in lines if x and not x.startswith(&#39;#&#39;)] lines = [x.rstrip().lstrip() for x in lines] module_defs = [] type_name = None for line in lines: if line.startswith(&quot;[&quot;): type_name = line[1:-1].rstrip() if type_name == &#39;net&#39;: continue module_defs.append({}) module_defs[-1][&#39;type&#39;] = type_name if module_defs[-1][&#39;type&#39;] == &#39;convolutional&#39;: module_defs[-1][&#39;batch_normalize&#39;] = 0 else: if type_name == &quot;net&quot;: continue key, value = line.split(&#39;=&#39;) value = value.strip() module_defs[-1][key.rstrip()] = value.strip() return module_defs# watch parse the yolov3 configuaraion about networkdef parse_hyperparam_config(path): file = open(path, &#39;r&#39;) lines = file.read().split(&#39;\\n&#39;) lines = [x for x in lines if x and not x.startswith(&#39;#&#39;)] # #으로 시작하지 않는 줄만 저장 lines = [x.rstrip().lstrip() for x in lines] # network hyperparameter에 대한 definition module_defs = [] # convolution에 대한 parameter conv_defs = [] for line in lines: # layer devision if line.startswith(&quot;[&quot;): type_name = line[1:-1].rstrip() if type_name == &quot;net&quot;: # net은 network의 hyperparameter module_defs.append({}) # dictionary module_defs[-1][&#39;type&#39;] = type_name if type_name == &#39;convolutional&#39;: conv_defs.append({}) conv_defs[-1][&#39;type&#39;] = type_name else: key, value = line.split(&quot;=&quot;) if type_name == &#39;net&#39;: module_defs[-1][key.rstrip()] = value.strip() if type_name == &#39;convolutional&#39;: conv_defs[-1][key.rstrip()] = value.strip() return module_defs, conv_defs# get the data to want to be ours.def get_hyperparam(data): for d in data: if d[&#39;type&#39;] == &#39;net&#39;: batch = int(d[&#39;batch&#39;]) subdivision = int(d[&#39;subdivisions&#39;]) momentum = float(d[&#39;momentum&#39;]) decay = float(d[&#39;decay&#39;]) saturation = float(d[&#39;saturation&#39;]) lr = float(d[&#39;learning_rate&#39;]) burn_in = int(d[&#39;burn_in&#39;]) max_batch = int(d[&#39;max_batches&#39;]) lr_policy = d[&#39;policy&#39;] in_width = int(d[&#39;width&#39;]) in_height = int(d[&#39;height&#39;]) in_channels = int(d[&#39;channels&#39;]) classes = int(d[&#39;class&#39;]) ignore_class = int(d[&#39;ignore_cls&#39;]) return{&#39;batch&#39;: batch, &#39;subdivision&#39;: subdivision, &#39;momentum&#39;: momentum, &#39;decay&#39;: decay, &#39;saturation&#39;: saturation, &#39;lr&#39;: lr, &#39;burn_in&#39;: burn_in, &#39;max_batch&#39;: max_batch, &#39;lr_policy&#39;: lr_policy, &#39;in_width&#39;: in_width, &#39;in_height&#39;: in_height, &#39;in_channels&#39;: in_channels, &#39;classes&#39;: classes, &#39;ignore_class&#39;: ignore_class} else: continuedef xywh2xyxy_np(x:np.array): y = np.zeros_like(x) y[...,0] = x[...,0] - x[...,2] / 2 # centerx - w/2 = minx y[...,1] = x[...,1] - x[...,3] / 2 # miny y[...,2] = x[...,0] + x[...,2] / 2 # maxx y[...,3] = x[...,1] + x[...,3] / 2 # maxy return y# box_a, box_b IOU# xyxy is value for whether or not boxes are [minx,miny,maxx,maxy], eps use when we divided by zero 0 to prevent errordef bbox_iou(box1, box2, xyxy=False, eps = 1e-9): box1 = box1.T box2 = box2.T if xyxy: b1_x1, b1_y1, b1_x2, b1_y2 = box1[0],box1[1],box1[2],box1[3] b2_x1, b2_y1, b2_x2, b2_y2 = box2[0],box2[1],box2[2],box2[3] else: b1_x1, b1_y1 = box1[0] - box1[2] / 2, box1[1] - box1[3] / 2 b1_x2, b1_y2 = box1[0] + box1[2] / 2, box1[1] + box1[3] / 2 b2_x1, b2_y1 = box2[0] - box2[2] / 2, box2[1] - box2[3] / 2 b2_x2, b2_y2 = box2[0] + box2[2] / 2, box2[1] + box2[3] / 2 # intersection inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\ (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0) # union b1_w, b1_h = b1_x2 - b1_x1, b1_y2 - b1_y1 * eps b2_w, b2_h = b2_x2 - b2_x1, b2_y2 - b2_y1 * eps # get two area and subtract intersection once. union = b1_w * b1_h + b2_w * b2_h - inter * eps # IOU iou = inter / union return iou# get learning rate in optimizer def get_lr(optimizer): for param_group in optimizer.param_groups: return param_group[&#39;lr&#39;]" }, { "title": "[데브코스] 10주차 - DeepLearning Yolo v3 coding (1)", "url": "/posts/yolo/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-21 15:10:00 +0900", "snippet": "Yolo v3Architecturebackbone는 darknet53을 사용했고, 이렇게 나온 feature map과 추가적으로 연산한 후의 feature map들을 다 추출해서 합치게 된다. 이를 통해 크기가 각기 다른 객체들을 잘 탐지할 수 있다. 13x13 feature map에서는 grid가 13일 것이고, 그렇다면 큰 객체를 잘 찾을 수 있고, 52x52 feature map에서는 작은 객체를 잘 찾을 수 있을 것이다.darknet을 구현하고, 연구한 사람의 github는 다음과 같다. 이 곳을 들어가면 darknet이 구현되어 있는데, c++로 구현되어 있어 pytorch를 사용하는데 있어서 모델을 변경시키거나 커스터마이징하기가 다소 복잡하다. 따라서 직접 구현을 해볼 것이다.prepare data yolo formatyolo 데이터 폴더에는 한 폴더 안에 이미지와 annotation 파일이 함께 있다.yolodata ⊢ train ⊢ a.jpg ⊢ a.txt ⊢ b.jpg ⊢ b.txt ... ⊢ eval ∟ test따라서 jpg 파일들을 먼저 불러온 다음 그에 맞게 해당하는 annotation을 불러오도록 만든다.yolo의 labeling format은 [class_index, x_center, y_center, width, height]로 되어 있다. 이 때, x,y,w,h는 전체 이미지에 대해 normalize되어 있다. 따라서 전체 이미지를 곱해줘야 실제 위치들이 나타난다. PASCAL VOC formatvoc20xx ⊢ jpegimages ⊢ a.jpg ⊢ b.jpg ⊢ c.jpg ... ⊢ annotations ⊢ a.xml ⊢ b.xml ⊢ c.xml ∟ imagesets ⊢ train.txt ⊢ eval.txtpascal voc 의 경우 annotation과 이미지는 다른 폴더에 들어가 있고, imagesets라는 폴더는 train, eval 데이터에 대한 정보들을 저장한 곳이다. train.txt를 보게 되면 경로도 없고, 확장자도 없이 파일명만 넣어져 있다. KITTI formatkitti의 label format은 다음과 같다.Truck 0.00 0 -1.57 599.41 156.40 629.75 189.25 2.85 2.63 12.34 0.47 1.49 69.44 -1.56 type: 총 9개의 클래스에 대한 정보이다. tram은 길거리에 있는 기차를 말하고, misc는 구별하기 어려운 애매한 차량들을 지칭하는 것이고, doncares는 너무 멀리 있어서 점으로 보이거나, 잘려서 보이는 차량들에 대한 클래스로 지정해놓은 것이다. truncated : 차량이 이미지에서 벗어난 정도를 나타낸다. 0~1사이의 값으로 표현된다. occluded : 다른 오브젝트에 의해 가려진 정도를 나타낸다. 0은 전체가 다 보이는 것, 1은 일부만 ,2는 많이 가려진 정도, 3은 아예 안보이는 것으로 나타낸다. alpha : 객체의 각도를 나타낸다. bbox : left, top, right, bottom에 대한 bounding box 좌표를 나타낸다.더 깔끔한 데이터셋을 만들기 위해 PASCAL VOC format을 사용하여 모델을 구성하고자 한다. 그리고 xml파일이 아닌 txt파일로 사용할 것이다. kitti를 yolo로 바꾸고, 다시 pascal voc format으로 진행할 것이다.먼저 kitti를 yolo로 변환하는 것에는 convert2Yolo라는 깃허브 사이트가 있어서 이를 클론하여 사용하면 편하다.annotation이 잘 되어 있는지를 확인해봐야 하므로, labeling tool을 사용하여 확인해봐야 한다. 종류에는 labelimg(https://github.com/tzutalin/labelImg)와 yolo_label(https://github.com/developer0hye/Yolo_Label) 이 있다. window는 후자, linux는 전자의 프로그램을 사용하는 것이 좋을 것 같다. 이 프로그램을 통해 직접 annotation을 제작할 수도 있고, 조도를 바꾸는 등의 다양한 기능을 제공한다.이제 직접 데이터를 다운받고, 정리를 해주자.먼저 데이터를 다운받는 방법에는 KITTI Dataset 홈페이지를 들어가서 다운받아도 되지만, 코드로 구현을 해서 다운을 받을수도 있다.curl https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip --create-dirs ./datasets/KITTI/ -o ./datasets/KITTI/img_kitti.zipcd ./datasets/KITTI &amp;amp;&amp;amp; unzip -d . img_kitti.zipcurl https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip --create-dirs ./datasets/KITTI/train/Annotations -o ./datasets/KITTI/train/Annotations/lab_kitti.zipcd ./datasets/KITTI/train/Annotations &amp;amp;&amp;amp; unzip -d . lab_kitti.zipcurl &amp;lt;url&amp;gt; --create-dirs &amp;lt;folder name to download&amp;gt; -o &amp;lt;dir/zip file name.zip&amp;gt;cd &amp;lt;dir&amp;gt; &amp;amp;&amp;amp; unzip -d &amp;lt;dir to upzip&amp;gt; &amp;lt;zip filename.zip&amp;gt;자신이 다운받고자 하는 폴더의 경로를 설정해서 타이핑하면 된다. 나의 경로는 다음과 같다../datasets ∟ KITTI ⊢ train ⊢ Annotations ⊢ 000000.txt ⊢ 000001.txt ⊢ 000002.txt ... ⊢ ImageSets ∟ train.txt ∟ JPEGimages ⊢ 000000.png ⊢ 000001.png ⊢ 000002.png ... ∟ test ⊢ ImageSets ∟ test.txt ∟ JPEGimages ⊢ 000000.png ⊢ 000001.png ⊢ 000002.png ...이미지는 train과 test가 함께 있으므로 KITTI폴더에 다운 받은 후 알맞게 분배하고 이름도 변경해주고, Annotation의 경우 train에 대한 것뿐이므로 train폴더에 풀고 경로에 맞게 집어넣어준다.그 다음 imageset의 파일들을 생성하기 위해서는 다음과 같은 코드를 사용하여 파일을 만들어준다.datasets/KITTI/train&amp;gt; dir /b /s .\\Annotations\\*.txt &amp;gt;&amp;gt; ./ImageSets/train.txtC:\\Users\\dkssu\\dev\\datasets\\KITTI\\train\\Annotations\\000000.txtC:\\Users\\dkssu\\dev\\datasets\\KITTI\\train\\Annotations\\000001.txtC:\\Users\\dkssu\\dev\\datasets\\KITTI\\train\\Annotations\\000002.txtC:\\Users\\dkssu\\dev\\datasets\\KITTI\\train\\Annotations\\000003.txtC:\\Users\\dkssu\\dev\\datasets\\KITTI\\train\\Annotations\\000004.txt...여기에서 경로와 확장자를 다 지워준다.000000000001000002000003000004...test도 동일하게 만들어준다.datasets/KITTI/test&amp;gt; dir /b /s .\\JPEGimages\\*.png &amp;gt;&amp;gt; ./ImageSets/test.txt000000000001000002000003000004...configuration파라미터에 대한 파일은 다음 주소로 들어가 파일을 다운로드한다.https://github.com/dkssud8150/dev_yolov3/blob/master/config/yolov3.cfgconfig 파일에서 network 부분의 config 파라미터만 불러온다.# watch parse the yolov3 configuaraion about networkdef parse_hyperparam_config(path): file = open(path, &#39;r&#39;) lines = file.read().split(&#39;\\n&#39;) lines = [x for x in lines if x and not x.startswith(&#39;#&#39;)] # #으로 시작하지 않는 줄만 저장 lines = [x.rstrip().lstrip() for x in lines] # module에 대한 definition module_defs = [] for line in lines: # layer devision if line.startswith(&quot;[&quot;): type_name = line[1:-1].rstrip() if type_name == &quot;net&quot;: # net은 network의 hyperparameter는 저장 x module_defs.append({}) # dictionary module_defs[-1][&#39;type&#39;] = type_name if module_defs[-1][&#39;type&#39;] == &#39;convolutional&#39;: module_defs[-1][&#39;batch_normalize&#39;] = 0 # default bat normalize else: if type_name != &#39;net&#39;: continue key, value = line.split(&quot;=&quot;) module_defs[-1][key.rstrip()] = value.strip() return module_defs#로 되어 있는 것은 주석이므로 제거하고, 나머지를 lines에 저장한다. 좌우 빈칸이 존재할 수 있으므로 삭제해준다. 그 다음으로 [로 시작되는 부분은 아래 부분들이 어떤 부분의 내용인지를 나타내는 것이므로 저장을 해놓아야 한다. net이면 network, 즉 criterion이나 optimizer에 대한 hyperparameter들에 대한 것이고, convolutional은 convolution을 할 때의 filter 사이즈나 stride 값이다.batch normalize는 0으로 default로 설정하고, network에 대한 것들만 보기 위해서 net 부분에서의 key와 value를 =을 기준으로 입력해준다. 결과를 출력해주면 다음과 같다.[{&#39;type&#39;: &#39;net&#39;, &#39;batch&#39;: &#39;4&#39;, &#39;subdivisions&#39;: &#39;1&#39;, &#39;width&#39;: &#39;640&#39;, &#39;height&#39;: &#39;480&#39;, &#39;channels&#39;: &#39;3&#39;, &#39;class&#39;: &#39;8&#39;, &#39;momentum&#39;: &#39;0.9&#39;, &#39;decay&#39;: &#39;0.0005&#39;, &#39;angle&#39;: &#39;0&#39;, &#39;saturation&#39;: &#39;1.5&#39;, &#39;exposure&#39;: &#39;1.5&#39;, &#39;hue&#39;: &#39;.1&#39;, &#39;ignore_cls&#39;: &#39;99&#39;, &#39;learning_rate&#39;: &#39;0.01&#39;, &#39;burn_in&#39;: &#39;1000&#39;, &#39;max_batches&#39;: &#39;500200&#39;, &#39;policy&#39;: &#39;steps&#39;, &#39;steps&#39;: &#39;400000,450000&#39;, &#39;scales&#39;: &#39;.1,.1&#39;}]저장한 param들에서 원하는 데이터들만 추출한다.# get the data to want to be ours.def get_hyperparam(data): for d in data: if d[&#39;type&#39;] == &#39;net&#39;: batch = int(d[&#39;batch&#39;]) subdivision = int(d[&#39;subdivisions&#39;]) momentum = float(d[&#39;momentum&#39;]) decay = float(d[&#39;decay&#39;]) saturation = float(d[&#39;saturation&#39;]) lr = float(d[&#39;learning_rate&#39;]) burn_in = int(d[&#39;burn_in&#39;]) max_batch = int(d[&#39;max_batches&#39;]) lr_policy = d[&#39;policy&#39;] in_width = int(d[&#39;width&#39;]) in_height = int(d[&#39;height&#39;]) in_channels = int(d[&#39;channels&#39;]) classes = int(d[&#39;classes&#39;]) ignore_class = int(d[&#39;ignore_cls&#39;]) return{&#39;batch&#39;: batch, &#39;subdivision&#39;: subdivision, &#39;momentum&#39;: momentum, &#39;decay&#39;: decay, &#39;saturation&#39;: saturation, &#39;lr&#39;: lr, &#39;burn_in&#39;: burn_in, &#39;max_batch&#39;: max_batch, &#39;lr_policy&#39;: lr_policy, &#39;in_width&#39;: in_width, &#39;in_height&#39;: in_height, &#39;in_channels&#39;: in_channels, &#39;classes&#39;: classes, &#39;ignore_class&#39;: ignore_class} else: continue이렇게 저장된 결과물을 출력해보면 다음과 같다.{&#39;batch&#39;: 4, &#39;subdivision&#39;: 1, &#39;momentum&#39;: 0.9, &#39;decay&#39;: 0.0005, &#39;saturation&#39;: 1.5, &#39;lr&#39;: 0.01, &#39;burn_in&#39;: 1000, &#39;max_batch&#39;: 500200, &#39;lr_policy&#39;: &#39;steps&#39;, &#39;in_width&#39;: 640, &#39;in_height&#39;: 480, &#39;in_channels&#39;: 3, &#39;classes&#39;: 8, &#39;ignore_class&#39;: 99}Dataloader데이터셋을 만들고, loader까지 하기 위해 폴더를 생성하고 거기서 작업을 한다.... ⊢ dataloader ⊢ __init__.py ⊢ yolo_data.pyyolo_data.pyimport torchfrom torch.utils.data import Datasetclass Yolodata(Dataset): # torch utils data의 dataset을 상속받는다. # format path file_dir = &#39;&#39; anno_dir = &#39;&#39; file_txt = &#39;&#39; base_dir = &#39;C:\\\\Users\\\\dkssu\\\\dev\\\\datasets\\\\KITTI\\\\&#39; # train dataset path train_img = base_dir + &#39;train\\\\JPEGimages&#39; train_txt = base_dir + &#39;train\\\\Annotations&#39; # valud dataset path valid_img = base_dir + &#39;valid\\\\JPEGimages&#39; valid_txt = base_dir + &#39;valid\\\\Annotations&#39; class_names = [&#39;Car&#39;, &#39;Van&#39;, &#39;Truck&#39;, &#39;Pedestrian&#39;, &#39;Persion_sitting&#39;, &#39;Cyclist&#39;, &#39;Tram&#39;, &#39;Misc&#39;] # doncare는 x num_classes = None img_data = [] def __init__(self, is_train=True, transform=None, cfg_param=None): super(Yolodata, self).__init__() self.is_train = is_train self.transform = transform self.num_class = cfg_param[&#39;classes&#39;] if self.is_train: self.file_dir = self.train_img self.anno_dir = self.train_txt self.file_txt = self.base_dir + &#39;train\\\\ImageSets\\\\train.txt&#39; else: self.file_dir = self.valid_img self.anno_dir = self.valid_txt self.file_txt = self.base_dir + &#39;valid\\\\ImageSets\\\\valid.txt&#39; img_names = [] with open(self.file_txt, &#39;r&#39;, encoding=&#39;UTF-8&#39;, errors=&#39;ignore&#39;) as f: img_names = [i.replace(&quot;\\n&quot;, &quot;&quot;) for i in f.readlines()] for i in img_names: #print(i) if os.path.exists(self.file_dir + i + &quot;.jpg&quot;): img_data.append(i + &quot;.jpg&quot;) elif os.path.exists(self.file_dir + i + &quot;.JPG&quot;): img_data.append(i + &quot;.JPG&quot;) elif os.path.exists(self.file_dir + i + &quot;.png&quot;): img_data.append(i + &quot;.png&quot;) elif os.path.exists(self.file_dir + i + &quot;.PNG&quot;): img_data.append(i + &quot;.PNG&quot;) self.img_data = img_datadataset이 있는 곳의 dir을 base_dir로 정의해주고, 그 안에 있는 image와 annotation 파일들의 경로를 지정한다. 클래스 이름들도 미리 정의를 해주고, 생성과 편집을 편리하게 하기 위해 file_dir, anno_dir, file_txt 변수를 만들어 상황에 맞게 이곳에 집어넣는다.그 다음 이미지의 이름들이 적힌 ImageSets의 txt파일을 불러와 img_names 리스트에 저장시켜준다. 이 때, replace를 하지 않으면 1줄씩 띄워져서 저장이 되므로 enter를 지우고 저장시킨다. 저장된 파일명을 통해 image_data를 불러온다. 확장자가 어떤 것인지 모르므로 다 적용시켜서 붙여준다. image data def __getitem__(self, index): img_path = self.file_dir + self.img_data[index] with open(img_path, &#39;rb&#39;) as f: img = np.array(Image.open(img_path).convert(&#39;RGB&#39;), dtype=np.uint8) img_origin_h, img_origin_w = img.shape[:2] # img shape : [H,W,C]이미지 경로를 받아서 이미지 파일을 열어 저장하고, 이미지의 크기도 저장한다. annotation data # annotation dir이 있는지 확인, txt파일 읽기 if os.path.isdir(self.anno_dir): txt_name = self.img_data[index] for ext in [&#39;.png&#39;, &#39;.PNG&#39;, &#39;.jpg&#39;, &#39;.JPG&#39;]: txt_name = txt_name.replace(ext, &#39;.txt&#39;) anno_path = self.anno_dir + txt_name if not os.path.exists(anno_path): return bbox = [] with open(anno_path, &#39;r&#39;) as f: # annotation about each image for line in f.readlines(): line = line.replace(&quot;\\n&quot;,&#39;&#39;) gt_data = [l for l in line.split(&#39; &#39;)] # [class, center_x, center_y, width, height] # skip when abnormal data if len(gt_data) &amp;lt; 5: continue cls, cx, cy, w, h = float(gt_data[0]), float(gt_data[1]), float(gt_data[2]), float(gt_data[3]), float(gt_data[4]) bbox.append([cls, cx, cy, w, h]) bbox = np.array(bbox) # skip empty target empty_target = False # even if object does not exist in image, we have to put bbox data if bbox.shape[0] == 0: empty_target = True bbox = np.array([[0,0,0,0,0]]) # bbox의 형태가 객체가 2개일경우 [[a,b,c,d,e],[a,b,c,d,e]] 이므로, 형태를 맞추기 위해 [[]]로 생성 # data augmentation if self.transform is not None: img, bbox = self.transform((img, bbox)) # 해당 데이터가 몇번째 배치인지 확인하기 위한 index # 지금 getitem으로 얻은 데이터는 1개의 이미지에 대한 것인데, 학습을 위해서는 batch size로 묶어야 하므로 index를 추가 if not empty_target: batch_idx = torch.zeros(bbox.shape[0]) # 객체 개수만큼 크기를 생성 target_data = torch.cat((batch_idx.view(-1,1), bbox), dim=1) # batch index의 x는 1, y는 객체 개수의 array로 만들어줘서, bbox와 concat else: return return img, target_data, anno_path else: # test or valid mode bbox = np.array([[0,0,0,0,0]]) if self.transform is not None: img, _ = self.transform((img, bbox)) return img, None, None len def __len__(self): return len(self.img_data)traindef collate_fn(batch): # only use valid data batch = [data for data in batch if data is not None] # skip invalid data if len(batch) == 0: return imgs, targets, anno_path = list(zip(*batch)) # print(targets[0].shape, targets[1].shape) imgs = torch.stack([img for img in imgs]) # mk 3dim -&amp;gt; 4dim, 0index = batch for i, boxes in enumerate(targets): # insert 0 index of box of dataloader function, instead of zero boxes[:,0] = i targets = torch.cat(targets,0) # print(targets.shape) return imgs, targets, anno_pathcustomdataset을 구성하게 되면 datset은 각 이미지마다의 정보들을 불러온다. 그러나 학습에서는 batch 단위로 학습을 하기 때문에 batch 단위로 만들어줘야 한다. 이것을 위해 collate_fn을 사용한다. 대부분 이 함수를 직접 선언해주고 사용한다.batch를 인자로 받아 사용되고, batch는 batch크기로 dataset의 값들을 저장해놓은 변수이다. 이에 대해 batch의 각 단위들을 읽어불러오며 비어있지 않은 값들을 저장시킨다. Yolodata에서 리턴하는 값들은 img, target, anno_path이므로 zip(*iterables) 함수를 사용하여 batch size 단위로 묶어서 저장한다.zip(*iterables) 참고 사이트list(zip(*batch))를 통해 zip을 통해 같은 index의 위치에 있는 요소들끼리 묶어서 tuple을 만들고, 다 묶은 것들을 list로 묶어놓는다. 이렇게 만들어진 정보들을 stack을 통해 차원 하나를 더 생성한 것이다.torch.Size([3, 608, 608]) =&amp;gt; torch.Size([1, 3, 608, 608])target의 경우, 우리의 yolodata 클래스를 들어가보면 batch idx 자리가 있는데, 모두 0으로 만들어져 있다. 이것을 0 대신 해당 batch index를 삽입한다. 그리고 targets.shape에 대한 코드들을 출력해보면 다음과 같다.torch.Size([1, 6]) torch.Size([8, 6])torch.Size([9, 6])확인을 위해 batch를 2로 주었을 때의 출력이고, 위의 size를 먼저보게 되면 1,8이 객체 수를 의미한다. 각 image마다의 객체 수를 1개로 합치는 것이 torch.cat이다. 추후 코드를 짤 때도 batch마다의 총 객체 수를 통해 구현을 할 예정이므로 이렇게 묶어주었다.from torch.utils.data.dataloader import Dataloader&#39;&#39;&#39; train &#39;&#39;&#39;def train(cfg_param = None, using_gpus = None): print(&quot;train&quot;) # dataloader train_dataset = Yolodata(is_train=True, transform=None, cfg_param=cfg_param) train_loader = DataLoader(train_dataset, batch_size=cfg_param[&#39;batch&#39;], num_workers=0, pin_memory=True, drop_last=True, shuffle=True, collate_fn = collate_fn) for i, batch in enumerate(train_loader): img, target_data, anno_path = batch print(&quot;iter {}, img {}, targets {}, anno_path {}&quot;.format(i , img.shape, target_data.shape, anno_path)) print(&quot;gt_data {}&quot;.format(target_data)) break만든 dataset을 torch.utils.data.dataloader에 있는 Dataloader를 통해 학습에 맞는 형태로 생성한다. 위에서도 말했듯이 collate_fn을 생성해줘야 학습에 맞는 크기로 구성이된다. num_workers : cpu와 gpu의 데이터 교류를 담당하는데, 0 이면 default값으로 single process와 비슷하게 진행된다. 0 이상을 넣게 되면 multi threading 방식으로 진행된다. pin_memory : 이미지나 데이터 array를 gpu로 올릴 때 memory의 위치를 할당할지 말지에 대한 인자이다.출력을 보게 되면 다음과 같다.iter 0, img torch.Size([1, 375, 1242, 3]), targets torch.Size([1, 5, 6]), anno_path [&#39;C:\\\\Users\\\\dkssu\\\\dev\\\\datasets\\\\KITTI\\\\train\\\\Annotations\\\\000545.txt&#39;]target에 대해 조금 더 자세히 보기 위해 000545.txt도 함께 본다. 이 txt의 내용은 다음과 같고, 그 아래는 gt_data즉 target_data를 출력한 것이다.0 0.906 0.564 0.185 0.2121 0.499 0.54 0.043 0.1390 0.589 0.569 0.076 0.0957 0.421 0.525 0.024 0.0391 0.42 0.499 0.021 0.077gt_data tensor([[[0.0000, 0.0000, 0.9060, 0.5640, 0.1850, 0.2120], [0.0000, 1.0000, 0.4990, 0.5400, 0.0430, 0.1390], [0.0000, 0.0000, 0.5890, 0.5690, 0.0760, 0.0950], [0.0000, 7.0000, 0.4210, 0.5250, 0.0240, 0.0390], [0.0000, 1.0000, 0.4200, 0.4990, 0.0210, 0.0770]]],txt파일과 비교하여 보았을 때 target_data의 1번째는 batch index에 대한 것이고, 2번째는 object index, 3~6은 bbox에 대한 정보가 담겨져 있다.Data Transformimport numpy as npimport cv2import torchimport torchvision.transforms as transformsimport imgaug as iafrom imgaug import augmenters as iaafrom imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImagefrom util.tools import xywh2xyxy_np필요한 패키지들을 import한다. imgaug란 data augmentation(데이터 증강)에 사용되는 툴로 사용법은 깃허브에 나와있다. 일반적으로 transform을 할 때 분류 태스크에서는 큰 문제가 생기지 않으나 object detection의 경우 이미지 크기를 변환시키거나 affine하면 그에 따라 bounding box도 변형되어야 한다. 그러므로 이를 잘 지원해주는 툴이 imgaug 이다. 사용을 위해서는 설치가 필요하다.pip install imgaugdata_transform.py# absolute bboxclass AbsoluteLabels(object): def __init__(self,): pass def __call__(self, data): # yolodata코드에서 transform이 들어갈 때 (img, bbox)가 data로 들어옴 img, label = data h, w , _ = img.shape label[:,[1, 3]] *= w # cx, w *= w label[:,[2, 4]] *= h # cy, h *= h return img, label# relative bboxclass RelativeLabels(object): def __init__(self,): pass def __call__(self, data): img, label = data h, w, _ = img.shape label[:,[1, 3]] /= w label[:,[2, 4]] /= h return img, labelannotation 파일에 있는 데이터들은 이미지 크기에 대해 정규화되어 있다. 이것을 transform하기 위해 정규화를 풀어줘야 한다. 그래야 transform을 해도 정보를 잃지 않는다. 우리는 data를 (img, bbox)의 형태로 집어넣기 때문에 이에 대해 각각을 처리해준다.정규화를 풀어주고 transform을 하고 난 후에는 다시 상대적 값으로 변환해줘야 한다.class ResizeImage(object): def __init__(self, new_size, interpolation = cv2.INTER_LINEAR): self.new_size = tuple(new_size) self.interpolation = interpolation def __call__(self, data): img, label = data img = cv2.resize(img, self.new_size, interpolation=self.interpolation) return img, label이미지 data augmentation을 위해 resize하는 class를 정의한다. 이 때, interpolation이란 보간을 의미하는데, 이미지를 변환할 때 생기는 빈 공간등을 처리하는 방식을 정의한다. 그리고, cv2 툴을 사용하여 resize하는데, label은 어차피 상대적인 값이므로 resize를 하지 않아도, 나중에 resize된 w,h를 곱해주면 자동으로 적용이 된다.class ToTensor(object): def __init__(self,): pass def __call__(self, data): img, label = data img = torch.tensor(np.transpose(np.array(img, dtype=float) / 255, (2,0,1)), dtype=torch.float32) # normalize, transpose HWC to CHW label = torch.FloatTensor(np.array(label)) return img, label학습을 위해서는 tensor형태로 변환해줘야 한다. 우리가 가지고 있는 image 차원은 [H,W,C]인데, tensor의 차원은 [C,H,W]이어야 하므로 이를 transpose해야 하고, 이미지들을 정규화하여 집어넣게 된다.label은 tensor로만 변환하여 리턴한다.# util/tools.pydef xywh2xyxy_np(x:np.array): y = np.zeros_like(x) y[...,0] = x[...,0] - x[...,2] / 2 # centerx - w/2 = minx y[...,1] = x[...,1] - x[...,3] / 2 # miny y[...,2] = x[...,0] + x[...,2] / 2 # maxx y[...,3] = x[...,1] + x[...,3] / 2 # maxy return y이 부분은 편의를 위해 정의해놓은 함수로 tools.py에 위치시켜놓는다. 현재 우리가 가지고 있는 bbox는 [center_x, center_y, w, h] 이므로 imgaug의 포맷을 맞춰주기 위해 [min x, min y, max x, max y] 의 형태로 바꾼다.# augmentation template, # 앞으로 다른 augmentation을 사용할 때 이 template을 상속받아서 구현할 것이다. 공통적으로 augmentation을 할 때마다 bbox가 augmentation방식에 따라 값이 변해야 하므로class ImgAug(object): def __init__(self, augmentations=[]): self.augmentations = augmentations def __call__(self, data): # unpack data img, labels = data # convert xywh to minx,miny,maxx,maxy because of imgAug format boxes = np.array(labels) boxes[:,1:] = xywh2xyxy_np(boxes[:,1:]) # 0 index is class info # convert bbox to imgaug format bounding_boxes = BoundingBoxesOnImage( [BoundingBox(*box[1:], label=box[0]) for box in boxes], shape=img.shape) #apply augmentation img, bounding_boxes = self.augmentations(image=img, bounding_boxes=bounding_boxes) # 예외 처리, 이미지 밖으로 나가는 bounding box를 제거 bounding_boxes = bounding_boxes.clip_out_of_image() # convert bounding boxes to np.array() boxes = np.zeros((len(bounding_boxes), 5)) # memory assignment for box_idx, box in enumerate(bounding_boxes): x1, y1, x2, y2 = box.x1, box.y1, box.x2, box.y2 # x1,y1,x2,y2 멤버 변수를 가지고 있음 # return [x, y, w, h], 원래의 포맷은 xywh이므로 다시 변환 boxes[box_idx, 0] = box.label boxes[box_idx, 1] = (x1 + x2) / 2 boxes[box_idx, 2] = (y1 + y2) / 2 boxes[box_idx, 3] = x2 - x1 boxes[box_idx, 4] = y2 - y1 return img, boxesclass DefaultAug(ImgAug): def __init__(self,): self.augmentations = iaa.Sequential([ iaa.Sharpen(0.0, 0.1), iaa.Affine(rotate=(-0,0), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)) ])imgAug 클래스는 추후에 우리가 적용시킬 다양한 augmentation의 템플릿으로 사용할 용도로 정의한 것으로 image와 label을 정의한 augmentation을 적용시켜서 리턴해주는 방식이다. 각기 다른 augmnetation 클래스마다 작성해줄 수 있지만, 이렇게 하면 코드도 길어지고, 반복적인 작업을 줄일 수 있다. 먼저 numpy형태로 변환해준 bbox들을 위에서 정의해놓은 xywh2xyxy_np를 사용하여 변환시켜준다. 그렇게 변환된 [minx,miny,maxx,maxy] 형태의 bbox들은 imgaug를 사용하여 imgaug 포맷으로 변환한다. 그 후 미리 정의한 augmentation을 적용 시키고, 이미지 밖으로 나가는 boundingbox 좌표들은 제거한다. 그 후 원래의 형태인 xywh 포맷으로 변환시켜준 후 리턴한다.def get_transformations(cfg_param = None, is_train = None): if is_train: data_transform = transforms.Compose([AbsoluteLabels(), DefaultAug(), RelativeLabels(), ResizeImage(new_size = (cfg_param[&#39;in_width&#39;], cfg_param[&#39;in_height&#39;])), ToTensor(), ]) else: data_transform = transforms.Compose([AbsoluteLabels(), DefaultAug(), RelativeLabels(), ResizeImage(new_size = (cfg_param[&#39;in_width&#39;], cfg_param[&#39;in_height&#39;])), ToTensor(), ]) return data_transform다 정의해준 클래스들을 통해 transform 묶음에 집어넣는다. for i, batch in enumerate(train_loader): img, target_data, anno_path = batch drawBox(img[0].detach().cpu()) # detach는 backprop나 등등의 torch의 graph에서 떼내는 것이고, gpu이면 cpu로 데이터를 복사 break그렇게 augmentation이 적용된 이미지를 살펴보기 위해 drawbox 함수를 만들어서 선언한다. drawBox 함수는 아래와 같다.def drawBox(img): img = img * 255 if img.shape[0] == 3: img_data = np.array(np.transpose(img, (1,2,0)), dtype=np.uint8) img_data = Image.fromarray(img_data) # draw = ImageDraw.Draw(img_data) plt.imshow(img_data) plt.show()채널이 3, 즉 컬러 이미지라면 원래 형태는 tensor(C,H,W)이므로 이를 다시 표현하기 위해 (W,H,C)로 변화하고 0~255에 대한 데이터타입으로 uint8로 지정한다.Architectureutil/tools.pyyolov3.cfg에서 convolutional에 대한 것만 추출한다. 방식은 network에 대한 module_defs를 추출하는 것과 동일하다.# parse model layer configurationdef parse_model_config(path): file = open(path, &#39;r&#39;) lines = file.read().split(&#39;\\n&#39;) lines = [x for x in lines if x and not x.startswith(&#39;#&#39;)] lines = [x.rstrip().lstrip() for x in lines] module_defs = [] type_name = None for line in lines: if line.startswith(&quot;[&quot;): type_name = line[1:-1].rstrip() if type_name == &#39;net&#39;: continue module_defs.append({}) module_defs[-1][&#39;type&#39;] = type_name if module_defs[-1][&#39;type&#39;] == &#39;convolutional&#39;: module_defs[-1][&#39;batch_normalize&#39;] = 0 else: if type_name == &quot;net&quot;: continue key, value = line.split(&#39;=&#39;) value = value.strip() module_defs[-1][key.rstrip()] = value.strip() return module_defsmodel/yolov3.pyimport os, sysimport numpy as npimport torchimport torch.nn as nnfrom util.tools import *class Darknet53(nn.Module): def __init__(self, cfg, param): super().__init__() self.batch = int(param[&#39;batch&#39;]) self.in_channels = int(param[&#39;in_channels&#39;]) self.in_width = int(param[&#39;in_width&#39;]) self.in_height = int(param[&#39;in_height&#39;]) self.n_classes = int(param[&#39;classes&#39;]) self.module_cfg = parse_model_config(cfg) self.module_list = self.set_layer(self.module_cfg) def set_layer(self, layer_info): module_list = nn.ModuleList() in_channels = [self.in_channels] for layer_idx, info in enumerate(layer_info): print(layer_idx, info[&#39;type&#39;])이를 출력해보면 다음과같다.0 convolutional1 convolutional2 convolutional3 convolutional4 shortcut5 convolutional6 convolutional7 convolutional8 shortcut...81 convolutional82 yolo83 route84 convolutional85 upsample86 route87 convolutional88 convolutional89 convolutional90 convolutional91 convolutional92 convolutional93 convolutional94 yolo95 route96 convolutional97 upsample98 route99 convolutional100 convolutional...다음과 같이 convolutional과 yolo, route, upsample 등의 layer로 구성되어 있다.여기서 convolutional은 우리가 아는 convolution에 대한 값들이다. 그리고, shortcut와 route는 그림과 함께 살펴봐야 한다.먼저 그림에서 봤을 때, 분홍색 부분이 residual block이다. residual block이란 지난 값을 연산 후의 값과 더하여 연산량을 줄이는 방식을 말한다. 식은 g(x) = F(x) + x이고, 이전 layer의 출력값을 x, 연산에 의한 값을 f(x), 현재 출력값을 g(x)라고 한다. 자세한 내용은 https://coding-yoon.tistory.com/141 이 사이트를 참고하는 것이 좋을 것 같다. 그래서 residual block에서 지난 값 x를 가져올 때 shortcut의 값만큼 이전의 layer에서의 값을 가져온다는 것이다. 즉 shortcut가 -3이면 현재 layer에서 3개 layer이전의 값인 x를 더하여 g(x)를 구한다.route도 얼마나 이전의 layer인지를 뜻하는 것으로, 그림에서 3번째 강아지 사진과 2번째 사진 사이에 * 연산을 하는 곳이 있다. 이것은 concat한다는 의미로, route = -4이면 4개 이전의 layer의 feature map과 concat하는 것이고, route = 95 이면 95번째 layer의 값을 concat한다는 것이다.좀 더 자세히 설명하기 위해 config 파일을 가져와보았다.[shortcut]from=-3activation=linear######################[route]layers = -4...[route]layers = -1, 61shortcut layer의 경우 from=-3이라 되어 있다. 이는 3개 이전의 layer와 더한다는 것이다. 그러나 route의 경우 2가지 경우가 있다. layers = -4인 경우에는 그냥 4개 이전의 layer의 feature map을 그대로 가져와 input으로 사용하겠다는 의미이고, layer = -1, 61이면 1개 이전의 layer와 61번째 layer를 concat하겠다는 의미이다. def set_layer(self, layer_info): # init layer setting module_list = nn.ModuleList() # layer를 하나하나 정의하는 것은 너무 오래 걸리고 옛날방식이다. module에 대한 list를 만들어주는 메서드 in_channels = [self.in_channels] # first channels of input for layer_idx, info in enumerate(layer_info): modules = nn.Sequential() # 많은 layer들을 하나의 묶음으로 만들어주는 메서드 if info[&#39;type&#39;] == &#39;convolutional&#39;: make_conv_layer(layer_idx, modules, info) # conv layer add in_channels.append(int(info[&#39;filters&#39;])) # store each module&#39;s input channels elif info[&#39;type&#39;] == &#39;shortcut&#39;: make_shortcut_layer(layer_idx, modules) # shortcut layer add in_channels.append(in_channels[-1]) elif info[&#39;type&#39;] == &#39;route&#39;: make_route_layer(layer_idx, modules) # route layer add layers = [int(y) for y in info[&#39;layers&#39;].split(&#39;,&#39;)] # layers에 있는 정보들을 다 가져옴 if len(layers) == 1: in_channels.append(in_channels[layers[0]]) # 1개인 경우에는 거기에 있는 feature map을 그대로 사용 elif len(layers) == 2: in_channels.append(in_channels[layers[0]] + in_channels[layers[1]]) # 2개인 경우 concat elif info[&#39;type&#39;] == &#39;upsample&#39;: make_upsample_layer(layer_idx, modules, info) # upsample layer add in_channels.append(in_channels[-1]) # width, height만 커지므로 channel은 동일 elif info[&#39;type&#39;] == &#39;yolo&#39;: yololayer = Yololayer(info, self.in_width, self.in_height, self.training) modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_yolo&#39;, yololayer) in_channels.append(in_channels[-1]) module_list.append(modules) return module_listdef make_conv_layer(layer_idx : int, modules : nn.Module, layer_info : dict, in_channels : int): filters = int(layer_info[&#39;filters&#39;]) # output channel size size = int(layer_info[&#39;size&#39;]) # kernel size stride = int(layer_info[&#39;stride&#39;]) pad = (size - 1) // 2 # layer_info[&#39;pad&#39;] modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_conv&#39;, nn.Conv2d(in_channels, filters, size, stride, pad)) if layer_info[&#39;batch_normalize&#39;] == &#39;1&#39;: modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_bn&#39;, nn.BatchNorm2d(filters)) if layer_info[&#39;activation&#39;] == &#39;leaky&#39;: modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_act&#39;, nn.LeakyReLU()) elif layer_info[&#39;activation&#39;] == &#39;relu&#39;: modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_act&#39;, nn.ReLU())def make_shortcut_layer(layer_idx : int, modules : nn.Module): modules.add_module(&#39;layer_&#39;+str(layer_idx)+&quot;_shortcut&quot;, nn.Identity()) # modulelist에서 info 타입이 맞지 않으면 복잡해지므로 빈 공간으로 initdef make_route_layer(layer_idx : int, modules : nn.Module): modules.add_module(&#39;layer_&#39;+str(layer_idx)+&quot;_route&quot;, nn.Identity())def make_upsample_layer(layer_idx : int, modules : nn.Module, layer_info : dict): stride = int(layer_info[&#39;stride&#39;]) modules.add_module(&#39;layer_&#39;+str(layer_idx)+&#39;_upsample&#39;, nn.Upsample(scale_factor=stride, mode=&#39;nearest&#39;))add_module 메서드는 (이름, 함수)로 구성되어 있어 각각의 역할에 대한 이름과 기능을 추가한다. config 파일에서 conv layer 관련해서는 batch_normalize, activation 등이 존재했기 때문에 이 또한 추가해주었다.shortcut와 route는 기본적으로 구성되어야 할 함수가 없으므로 빈 공간으로 집어넣어 주었다. Identity를 하면 입력을 그대로 출력으로 내뱉어준다.upsample layer의 경우 stride가 존재하고, upsample에 대한 torch 함수가 지원되므로 이를 사용한다. upsample을 할 때 빈 공간을 어떻게 채울지에 대한 mode도 설정해준다. yolo layeryolo에 대한 layer는 연산이 복잡하기 때문에 따로 class를 구현했다. yolov3.cfg 파일에서 yolo에 대한 정보는 다음과 같고, mask과 anchor는 anchor가 총 9개가 정의되어 있는데, 각 yolo layer마다 9개 모두를 사용하지 않는다. mask에 해당하는 index의 anchor들만 사용한다. 그래서 아래의 경우는 3,4,5 즉 50,71~ 43,212 만을 사용한다. 50,71은 anchor 박스의 크기를 나타내는 것으로 width = 50, height = 71이다.classes는 class의 개수이고, ignore_thresh는 loss 계산할 때 박스들이 threshold이상인 것만 positive로 정의하고, 나머지는 negative로 정의할 때 사용되는 값이다.[yolo]mask = 3,4,5anchors = 15,36, 30,49, 19,115, 50,71, 76,106, 43,212, 115,145, 129,230, 194,280classes=8num=9jitter=.3ignore_thresh = .5truth_thresh = 1random=1class Yololayer(nn.Module): def __init__(self, layer_info : dict, in_width : int, in_height : int, is_train : bool): super(Yololayer, self).__init__() self.n_classes = int(layer_info[&#39;classes&#39;]) self.ignore_thresh = float(layer_info[&#39;ignore_thresh&#39;]) self.box_attr = self.n_classes + 5 # output channel = box[4] + objectness[1] + class_prob[n] mask_idxes = [int(x) for x in layer_info[&#39;mask&#39;].split(&#39;,&#39;)] anchor_all = [int(x) for x in layer_info[&#39;anchors&#39;].split(&#39;,&#39;)] # w1,h1 , w2,h2 , w3,h3 , ... 로 되어 있으므로 이것을 다시 w,h 를 묶어줘야 한다. anchor_all = [(anchor_all[i], anchor_all[i+1]) for i in range(0, len(anchor_all), 2)] self.anchor = torch.tensor([anchor_all[x] for x in mask_idxes]) self.in_width = in_width self.in_height = in_height self.stride = None # feature map의 1 grid가 차지하는 픽셀의 값 == n x n self.lw = None self.lh = None self.is_train = is_train def forward(self, x): # bounding box를 뽑을 수 있게 sigmoid나 exponantional을 취해줌 # x is input. [N C H W] self.lw, self.lh = x.shape[3], x.shape[2] # feature map&#39;s width, height self.anchor = self.anchor.to(x.device) # 연산을 할 때 동일한 곳에 올라가 있어야함, cpu input이라면 cpu에, gpu input이라면 gpu에 self.stride = torch.tensor([torch.div(self.in_width, self.lw, rounding_mode = &#39;floor&#39;), torch.div(self.in_height, self.lh, rounding_mode = &#39;floor&#39;)]).to(x.device) # stride = input size / feature map size # if kitti data, n_classes = 8, C = (8 + 5) * 3 = 39, yolo layer 이전의 filters 즉 output channels을 보면 다 39인 것을 확인할 수 있다. # [batch, box_attrib * anchor, lh, lw] ex) [1,39,19,19] # 4dim -&amp;gt; 5dim [batch, anchor, lh, lw, box_attrib] x = x.view(-1, self.anchor.shape[0], self.box_attr, self.lh, self.lw).permute(0,1,3,4,2).contiguous() # permute를 통해 dimension 순서를 변경, configuouse를 해야 바뀐채로 진행됨 return xstride의 경우 convolutional에서 사용되는 stride와는 다르다. 이 때의 stride는 feature map에서 grid로 나뉘어져 있는데, 이 한 grid가 차지하는 픽셀의 크기를 의미한다.x가 현재에는 4dimension [batch, box_attribute * number of anchor, lh, lw] 을 가지고 있는데, 이를 보기 편하도록 5dim [batch, anchor, lh, lw, box_attrib] 으로 변환한다. 이를 위해 view를 통해 변환해주고, box_attrib의 속성을 맨 뒤로 보내주기 위해 permute 메서드를 사용한다. 그 후 실제 변환을 유지시키기 위해 contiguous()도 추가한 후 리턴한다. model = Darknet53(args.cfg, cfg_param, training=True) for name, param in model.named_parameters(): print(&quot;name : {}, shape : {}&quot;.format(name, param.shape))이것들을 입력받아 출력을 해보면 다음과 같이 출력된다.name : module_list.0.layer_0_conv.weight, shape : torch.Size([32, 3, 3, 3])name : module_list.0.layer_0_conv.bias, shape : torch.Size([32])name : module_list.0.layer_0_bn.weight, shape : torch.Size([32])name : module_list.0.layer_0_bn.bias, shape : torch.Size([32])name : module_list.1.layer_1_conv.weight, shape : torch.Size([64, 32, 3, 3])name : module_list.1.layer_1_conv.bias, shape : torch.Size([64])...name : module_list.104.layer_104_bn.bias, shape : torch.Size([256])name : module_list.105.layer_105_conv.weight, shape : torch.Size([39, 256, 1, 1])name : module_list.105.layer_105_conv.bias, shape : torch.Size([39])추가적으로 입력의 이미지 크기가 608 x 608 이라 했을 때, 이것들이 network들을 거치면서 1/2씩 크기가 작아진다. 그래서 304, 152, 76, 38, 19 로 작아진다. 그래서 4번 reshape이 되어도 계속 짝수가 되도록 만들어줘야 한다.그리고 yolov3에서는 출력이 총 3번 이루어지는데, 19x19, 38x38, 76x76으로 output이 만들어진다. 출력에 대한 것은 뒤에 다시 살펴보도록 하자.위의 부분들이 잘 마무리되었으면 이제 실제 모델 학습에 사용될 forward를 구현해야 한다.class Darknet53(nn.Module): ... def forward(self, x): yolo_result = [] # 최종 output들은 yolo layer에서 나옴 layer_result = [] # shortcut, route에서 사용하기 위해 저장 for idx, (name, layer) in enumerate(zip(self.module_cfg, self.module_list)): if name[&#39;type&#39;] == &#39;convolutional&#39;: x = layer(x) layer_result.append(x) elif name[&#39;type&#39;] == &#39;shortcut&#39;: x = x + layer_result[int(name[&#39;from&#39;])] layer_result.append(x) elif name[&#39;type&#39;] == &#39;yolo&#39;: yolo_x = layer(x) layer_result.append(yolo_x) yolo_result.append(yolo_x) elif name[&#39;type&#39;] == &#39;upsample&#39;: x = layer(x) layer_result.append(x) elif name[&#39;type&#39;] == &#39;route&#39;: layers = [int(y) for y in name[&#39;layers&#39;].split(&#39;,&#39;)] x = torch.cat([layer_result[l] for l in layers], dim=1) layer_result.append(x) print(&quot;idx : {}, result : {}&quot;.format(idx, layer_result[-1].shape)) return yolo_result미리 전부 선언해주었기 때문에 forward는 다소 단순하게 구현할 수 있다. 미리 만들어둔 layer들과 module_cfg 파일의 값들을 1줄씩 함께 읽어나가며 연산을 진행한다. type에 따라 식들이 달라질 수 있으므로 다 처리해주고, 우리가 실제로 출력받는 값들은 yolo layer에서 출력이 되므로 이에 대한 값들을 저장하기 위홰 yolo_result 리스트를 선언하고, shortcut이나 route에서 이전 값들을 재사용해야 하므로 layer의 출력값들을 저장할 수 있는 layer_result도 만들어준다.shortcut이나 route의 경우 미리 만들어둔 layer를 사용하지 않고도 연산이 가능한데, 이 이유는 위에서도 말했듯이 zip을 진행하는데 두 개의 list의 크기가 맞지 않으면 연산이 복잡해지기 때문에 빈 공간으로 선언해둔 것이다.이들을 직접 print해보면 다음과 같은 출력들을 볼 수 있다.idx : 0, result : torch.Size([1, 32, 608, 608])idx : 1, result : torch.Size([1, 64, 304, 304])idx : 2, result : torch.Size([1, 32, 304, 304])idx : 3, result : torch.Size([1, 64, 304, 304])idx : 4, result : torch.Size([1, 64, 304, 304])...idx : 103, result : torch.Size([1, 128, 76, 76])idx : 104, result : torch.Size([1, 256, 76, 76])idx : 105, result : torch.Size([1, 39, 76, 76])idx : 106, result : torch.Size([1, 3, 76, 76, 13])총 106층의 layer가 존재하고, 그에 맞게 각각의 결과들이 존재한다. 결과의 shape은 [batch, channels, height, width] 이다. model = Darknet53(args.cfg, cfg_param, training=True) model.train() for name, batch in enumerate(train_loader): img, targets, anno_path = batch output = model(img) # yolo layer는 3개, 19x19 grid, box attr가 8개 클래스와 box 4와 objectness 1, 3개의 anchor, batch 1 # 19x19, 38x38, 76x76의 grid를 가지는 출력값 3개를 받는다. print(&quot;shape : {} {} {}&quot;.format(output[0].shape, output[1].shape, output[2].shape)) sys.exit(1)결과를 출력해보면 다음과 같은 shape들을 볼 수 있다.shape : torch.Size([1, 3, 19, 19, 13]) torch.Size([1, 3, 38, 38, 13]) torch.Size([1, 3, 76, 76, 13])우리의 입력은 608x608이다. 맞춰주기 위해 config파일에서 width, height를 608로 수정하였다. input size는 위의 idx0 에서 확인할 수 있다. 이를 $ 2^5 $로 나누게 되면 19x19 이 된다. 19x19, 38x38, 76x76의 grid에 대한 출력을 받을 수 있게 된다. 1은 batch size, 3은 anchor 개수, 13은 n_classes[8] + objectness score[1] + box point[4]이다. weight initialize모델의 파라미터들을 초기화하여 조금 더 나은 학습 환경을 만들어준다. 초기화 방법은 kaiming_uniform_을 사용한다. 이에 대해서는 다음 url을 참고하여 더 자세히 볼 수 있다.https://pytorch.org/docs/stable/nn.init.html?highlight=kaiming#torch.nn.init.kaiming_uniform_# model/yolov3.py def initialize_weights(self): # track all layers for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_uniform_(m.weight) # weight initializing if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) # scale nn.init.constant_(m.bias, 0) # shift elif isinstance(m, nn.Linear): nn.init.kaiming_uniform_(m.weight) nn.init.constant_(m.bias, 0)# yolov3.py model.train() model.initialize_weights()Traintrain을 위해 새로운 directory를 생성한다.... ⊢ __init__.py ∟ train.py# train.pyimport os, sysimport torchimport torch.optim as optimfrom util.tools import *class Trainer: def __init__(self, model, train_loader, eval_loader, hyparam, device, torchwriter): self.model = model self.train_loader = train_loader self.eval_loader = eval_loader self.max_batch = hyparam[&#39;max_batch&#39;] self.device = device self.epoch = 0 self.iter = 0 self.optimizer = optim.SGD(model.parameters(), lr=hyparam[&#39;lr&#39;], momentum=hyparam[&#39;momentum&#39;]) self.scheduler_multistep = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[20,40,60], gamma = 0.5) def run_iter(self): for i, batch in enumerate(self.train_loader): # drop the batch when invalid values if batch is None: continue input_img, targets, anno_path = batch print(input_img.shape, targets.shape) def run(self): while True: self.model.train() self.run_iter() self.epoch += 1# yolov3.py train = Trainer(model = model, train_loader = train_loader, eval_loader=None, hyparam=cfg_param) train.run()실제 학습을 진행할 코드로 model, train_loader, valid_loader, cfg파일 내용을 가져온다. cfg파일에 max_batch라는 내용도 있는데, 이는 최대 batch를 나타낸다. 즉 몇 batch까지만 진행을 할 것인지에 대한 내용이다.최적화 함수로는 SGD를 사용하고, lr은 cfg파일에 있는 그대로 사용하고, momentum또한 작성되어 있는대로 사용한다.scheduler도 선언해주었는데, 이는 학습을 진행할 때마다 learning rate값이 줄어들어야 더 정교하게 학습이 가능하다. 떨어지는 빈도를 설정해주어야 하는데, 우리는 MultiStepLR을 사용하였다. milestones의 경우 언제 learning rate를 수정할 것인지에 대한 것이고, gamma는 얼마나 수정시킬 것인지에 대한 값이다.지정해준 trainer를 run하게 되면 학습이 진행된다. 그러나 이는 epoch 단위로 돌아가게 되어있고, run_iter에서 각각의 batch마다 학습을 진행하도록 만들어진다. yolo_data.py에서 train_loader가 가지고 있는 값들에는 img, target_data, anno_path이므로 이를 받아주어 출력해보면 다음과 같다.torch.Size([1, 3, 608, 608]) torch.Size([1, 4, 6])torch.Size([1, 3, 608, 608]) torch.Size([1, 6, 6])torch.Size([1, 3, 608, 608]) torch.Size([1, 4, 6])torch.Size([1, 3, 608, 608]) torch.Size([1, 2, 6])앞은 input size에 대한 정보이고, 두번째는 target에 대한 정보이다. [batch, number of object, box attrib + class info]로 구성되어 있다." }, { "title": "[데브코스] 10주차 - DeepLearning Object Detection", "url": "/posts/od/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-20 01:10:00 +0900", "snippet": "Object detectionclassification은 단일 객체에 대해 분류를 하는 것이다. 이 단일 객체의 위치를 찾아주는 것을 localization이라 하는데, 멀티 객체애 대해 탐색을 하고자 했고, 이 방법이 object detection이다.localization에서의 출력은 classifer 및 객체 위치에 대한 x,y,w,h 이다.예전에는 sliding window와 같은 방법으로 객체를 검출했다. 그러나 이는 너무 연산량도 많고, 속도도 느리다는 단점이 존재했다. 그래서 최근에는 classifier와 regressor, 두 개를 출력하는 two stage 모델과 이 두개를 한꺼번에 하는 one stage 모델이 개발되었다.two stage에는 RCNN시리즈가 있고, one stage에는 SSD, Yolo 등이 있다.이전의 classfication의 마지막 단은 fc layer였다. 그러나 object detection에서의 마지막 단은 conv layer로 구성되어 있다. 이 둘의 차이는 flatten를 하냐 마냐의 차이다. flatten을 하게 되면 위치 정도가 사라지게 되고, conv layer로 출력이 되면 위치 정보가 그대로 유지할 수 있다.one stage의 object detection의 출력은 [H,W class num + box_offset_confidence]One stage detectionSSD, YOLO 등이 이에 해당하는데, 속도는 빠르지만 정확도가 다소 낮은 편이다.모델 구조 backboneclassification 모델에서 봤듯이 input가 입력되면 feature map이 점점 작아지면서 더 함축적인 의미의 feature map을 만드는 역할을 하는 것을 backbone 또는 feature extractor이라 한다.layer가 깊어질수록 추상화된다. neck이렇게 추출된 각기 다른 해상도의 feature map들은 neck 단계에서 합쳐진다. 두개 의 feature map을 결합을 할 때는 동일한 크기의 feature map으로 만들어 준 후 elementwise concat 또는 add을 통해 결합하게 된다.A matrix [C, H, W]와 B matrix [C, H, W]를 결합하면 concat matrix는 [2C, H, W]의 shape을 가지게 된다. add matrix는 [C, H, W]의 shape을 가진다. dense prediction합쳐진 feature map들을 통해 prediction을 한다. 중요한 것은 하나의 feature map에서 결과가 나오는 것이 아니라 여러 개의 feature map에서 나온 결과를 합치는 것이다.class와 bounding box를 출력하는 부분이다. 이를 header 또는 prediction layer이라 부른다.Two stage detectionRCNN 시리즈가 two stage detection 모델에 해당하는데, 속도가 조금 느리나 정확도가 높은 편이다.가장 큰 특징은 region proposal을 추출하고, CNN을 연산한 후 이 rego=ion proposal과 결합하여 예측한다.dense prediction까지는 동일하나 뒤에 prediction 단이 하나 더 존재한다. dense prediction에서는 후보군을 출력하고, 이에 대해 NMS를 통해 박스의 개수를 줄여 출력한다.용어object detection을 하다보면 다양한 용어들이 나온다. 그 용어들을 차근차근 살펴보고자 한다.Gridheader layer의 마지막 feature map에서의 픽셀 수를 grid라 한다. 즉, network를 통해 출력된 feature map이 13x13을 가진다면 grid도 13x13의 사이즈를 가지고 있다고 할 수 있다.마지막 feature map은 이미지의 정보를 담고 있는 것이므로 이미지에 삽입해보면 전체 이미지를 grid로 나눈 그림을 볼 수 있다.한 grid마다 box prediction, objectness score, class score의 정보를 anchor Box 개수만큼 가지고 있다. 즉, 각 BOX마다의 box prediction[tx,ty,tw,th], objectness score[confidence], class score[p1,p2,…,pc] 의 정보를 가지고, 이것이 box개수만큼 존재한다.Anchorfeature map에서 bounding box를 예측할 떄 사용되는 detector이다. anchor는 각 grid안에 특정 개수로 존재하고, 1개의 anchor 당 1개의 object를 예측한다. 이 anchor의 크기는 미리 정의해줘야 한다.이 때, 크기는 다양하게 정의해야 한다. 그 이유는 object의 모양이 어떻게 되어 있는지 확인하기 어려우므로 여러 개의 anchor size를 선언해줘야 잘 예측할 수 있다.anchor가 5개라고 하면, 각 anchor마다 box offset, objectness confidence, class confidence가 5개씩 존재한다. box offset(4) + objectness confidence(1) + class confidence(n_classes)개수만큼의 element를 가지므로, class의 개수가 20개라 하면 25개의 element를 가진다. 1개의 grid안에 5개의 anchor가 있다고 하면 feature map의 채널은 (25*5=125)채널을 가진다.objectness, class scoreobjectness score란 원하는 object에 대해서만 gt를 1로 두는 것이다. class score는 class 별로 gt를 구분해놓고 class별로 객체에 대한 class를 예측하는 부분에 대한 score이다.IOU(Intersection over union)두 bounding box를 얼마나 잘 예측했는지에 대해 비교하기 위해 IOU개념을 사용한다. 용어 그대로 영역의 결합 정도를 추론한다. 이 IOU가 threshold보다 크면 positive box, 작으면 negative box라 할 수 있다.\\[IOU = \\frac{A \\cap B}{A \\cup B}\\]NMS(Non-Maximum Suppression)1개의 grid에 대해 n개의 anchor가 있고, 1개의 anchor마다 1개의 객체를 예측하면 n개의 bounding box가 나오게 된다. 같은 객체를 검출하는 bounding box들에 대해 anchor들을 filtering하는데, 가장 큰 score을 가진 것만 남긴다.추가적으로 많이 사용되는 함수들에 대해 자세히 설명하려고 한다.softmax우리가 예측한 class score는 0.9,0.4,0.2 등의 값으로 존재한다. 이를 확률값으로 가지기 위해 softmax를 적용하면 total sum이 1이 되도록 만든다. 즉 [0.9,0.4,0.2]에 softmax를 적용하면 [0.4755, 0.2844, 0.2361]이 된다.소프트맥스는 최대(max)를 모방하여 네트워크의 출력에 대해 0 또는 1로 만드는 것이 아닌, 출력값들을 $ \\frac{e^i} {\\sum_{i=1,c} e^i} $ 으로 계산한다. 즉, 지수함수로 치환한 상태로 다 더한 값에 대한 해당 index의 값으로 나타낸다.loss functionMSE loss, MAE loss이전에 얘기했던 n1 norm, n2 norm과 비슷하게 MAE는 절대값을 사용해서 loss를 구하는 방식이고, MSE는 제곱을 통해 Loss를 구하는 방식이다. 오차가 클수록 error 값이 커지므로 정량적 성능으로 활용된다. 이 방법의 문제는 두 벡터 사이의 거리를 비교하는 것이므로 error가 큰 경우임에도 불구하고, gradient가 작아서 갱신되는 폭이 작을 수 있다. 다시 말해 grdient를 구하는 방식을 간략하게 설명하면, $$ MSE = \\frac{1}{2}   y - o   _2^2 $$ 일 때, network 구조가 wx + b = o 이고, ground truth = y일 때, 즉, 아래와 같은 구조를 가지고 있다고 가정하고, error에 대한 식을 세우면 $ e = \\frac{1}{2}(y - o)^2 = \\frac{1}{2}(y - \\sigma(wx + b))^2 $ 이다. 이 때, 각각의 gradient를 구해보면, w에 대한 error의 gradient는 $ \\frac{\\partial e}{\\partial w} = -x\\sigma’(wx + b) * x * (y - \\sigma(wx + b)) = 0.048 $ 이다. 그러나 b = 3, w = 1.9, output = 0.9971, y = 0 이라면 $ \\frac{\\partial e}{\\partial w} = 0.0043 $ 이 나온다. 후자의 경우가 더 큰 error값을 가지므로 더 큰 gradient를 가져야 학습이 잘 진행되지만, 계산의 결과 gradient가 더 작게 나왔다.그 이유는 sigmoid의 함수 형태에 있다. 함수 형태는 0에 가까울수록 gradient가 크고, 0에서 멀어질수록 gradient가 작다. 따라서 값이 크면 더 큰 gradient를 가져야 하지만, sigmoid의 형태의 특성으로 인해 이상하게 작동한다.Cross entropy losscross entropy 란 신경망이 예측할 수 있는 값이 0또는 1이고, 정답 또한 0또는 1이라 가정하고, gt가 0일 때, 신경망이 0으로 잘 출력된 확률이 0.7이라면 반대로 잘못 출력된 확률이 0.3이 된다. 또는 gt가 1일 때, 신경망이 1로 잘 출력된 확률이 0.9라면 잘못 출력된 확률은 0.1이 된다. 그렇다면 예측값이 s1, gt값이 t1이라 하면, 신경망의 출력을 식으로 나타내면 $ s1 = \\sigma(z), z = wx + b $ 일테고, $ \\sigma(wx + b) $ 를 f라고 하면, $ t1 = f(s1) $ 으로 생각할 수 있다. 이 때, t1, s1은 모두 확률값으로 생각을 하고, 만약 0일 때를 가정했는데, gt값이 0이 나올 확률을 t1, 0이 나오지 않을 확률을 (1-t1)이라 할 수 있다. 실제로는 0으로 생각했는데, gt가 0이 나오지 않을 확률은 거의 존재하지 않는다. 하지만 계산을 위해 그렇게 가정을 한다. 그래서 positive term에서 gt가 t1일 때, t1이 나올 확률 t1과 예측값 f(s1)을 곱하고, t1이 나오지 않을 확률 (1-t1)과 그에 대해 틀리게 예측한 확률 (1-f(s1))을 곱하여 이 둘을 더하면 loss가 된다.만약 0과 1의 분류에서 오분류했을 때, 실제값에 대한 확률은 거의 1에 가깝고, 예측값은 거의 0에 가까울 것이다. 따라서 이에 대한 손실은 -(1 x log0.009) - (0 x log0.991) = 2.05 - 0가 된다. 반대로 잘 분류했을 때는 -(1 x log0.991) - (0 x log0.009) = 0.0039 - 0 이 된다. 즉 잘못된 부분에서의 손실은 크게, 잘된 부분에서의 손실은 작게 할 수 있다.즉, cross entropy loss에는 잘 분류된 것에 대한 positive loss와 잘못 분류된 것에 대한 negative loss 텀이 존재하고, class에 대해 잘 예측했다면 positive loss 텀은 작아지고, 잘못 검출했다면 negative loss 텀이 커진다.내가 원하는 label이었는지 아닌지에 대해 판단하기 위해 one hot encoding을 통해 내가 원했던 label에 대해서만 1로 두고, 아닌 label에는 0으로 두는 방식을 사용하여 조금 더 명확한 loss값을 가지도록 만들어준다. 아까 얻었던 softmax의 출력값 [0.4755, 0.2844, 0.2361]과 one hot label [1, 0, 0]을 통해 cross entropy를 적용하면 0index에 대해서는 1 * log(0.4755), 1index에 대해서는 (1-0) * log(1-0.2884), 2index는 (1-0) * log(1-0.2361)이 되고, 이를 다 더하여 loss를 구한다.Data preprocessingnormalize데이터를 전처리한다는 것은 정규화(normalization)을 한다는 것이다. 데이터가 편향되어 있을 때, 각각의 클래스별로 정규화를 수행한다.먼저 데이터에 대해 평균이 0이 되도록 이동시킨다. 그 후 x,y축 각각에 대해 동일하게 표준편차가 1이 되도록 만든다. 즉, x축과 y축이 동일한 폭을 가지는 데이터의 분포로 만든다.one-hot encoding또는, one-hot encoding으로 만드는 것도 전처리에 있어서 필요한 부분이다.원핫인코딩이란 각각의 클래스에 맞는 index에만 1을 주고, 나머지는 0으로 만드는 과정을 말한다. 사진에서 볼 수 있듯이 각각의 클래스의 index에만 1, 나머지는 0으로 되어 있는 것을 볼 수 있다.초기 환경 설정Initialize weights초기 가중치를 어떻게 설정하는지에 따라 학습의 성능이 달라질 수 있다. 그래서 예전에는 weight를 가우시안이나 균일 분포에서 난수를 추출했지만, 최근에는 xhavier, kaiming 등의 다양한 방법들을 사용해서 초기화한다.초기 가중치를 초기화하는데, 너무 작은 값을 사용하면 추후 학습할 때는 모든 활성 값이 0이 된다. 즉 학습이 되지 않는다. 또는, 초기 가중치가 너무 큰 값을 사용해도 내적에 의한 출력 자체가 너무 큰 값이 나오게 될 것이고, 활성 함수에서의 gradient도 너무 커져서 saturation, 포화가 발생한다. 그렇게 되면 학습이 되지 않는다.최근에는 가중치를 배치단위로 정규화를 진행해서 다음 층에 전달하게 된다. 그렇게 되면 훨씬 더 좋은 성능을 가질 수 있다.Optimizer momentummomentum, 관성이라는 것은 이전의 방향성을 현재의 학습에 사용하겠다는 것이다. 모멘텀을 활용해서 local minima나 saddle points를 넘어갈 수 있다. local minima란 전체의 최소값이 아니지만, 지역적으로 봤을 때 최소값이 되는 구간을 말한다. saddle points란 삼차원이상의 함수에서 극값을 가지지 않는 gradient가 극소한 점을 말한다. 어떤 방향에서보면 최소, 어떤 방향에서보면 최대가 된다.원래 가중치 갱신 수식은 $ \\theta = \\theta - \\rho g$ 이다. 이 때 관성을 추가하면 $ v = \\alpha * prev_v + \\rho \\frac{\\partial g}{\\partial \\theta} $, 즉, 이전의 속도와 현재의 gradient를 더해서 현재의 속도로 만든다. 그 후 가중치를 갱신할 때는 $ \\theta = \\theta - v $ 를 사용한다.이 때, $ \\alpha $는 과거 속도에 대한 정도를 얼마나 적용시킬지에 대한 값이다. 이 값이 1에 가까울수록 이전 경사도 정보에 큰 가중치를 주는 셈이고, 이를 통해 더 매끄러운 가중치의 궤적을 그릴 수 있다. 이전 위치에서의 이동 방향 벡터와 이전 위치에서의 gradient 벡터를 더해서 현재의 진행 방향으로 만든다.이 때, nesterov momentum이라 해서 현재 위치에서 다음 이동할 방향을 예견하고, 그에 대한 경사도를 구해서 이전 벡터를 더하면 local minima나 saddle point에서의 멈춤 현상도 방지할 수 있고, 더 알맞은 방향으로 진행할 수 있도록 만든다. 이러한 방법이 원래의 momentum 방식보다 더 좋은 결과를 얻는다고 한다.momentum값은 보통 0.5, 0.9, 0.99로 사용한다. 예전에는 0.5로 사용했지만, 현재에는 0.9나 0.99로 많이 사용한다. leraning ratelearning rate, $ \\rho $ 가 너무 크면 최저점을 지나치는 overshooting이 발생할 수 있다. 그러나 너무 작으면 수렴이 느리므로 적절한 learning rate를 주는 것이 중요하다.Stochastic Gradient Descent1개의 데이터마다 gradient를 갱신한다. w = w - lr * diff이렇게 하면 너무 많은 갱신이 일어나기 때문에 좀 더 느리고, 연산량이 많아질 수있다. 그래서 MSGD(mini batch gradient descent)를 사용하기도 한다. 즉, 1개의 데이터마다 gradient를 갱신하는 것이 아닌 mini batch단위로 gradient를 연산하고, 이에 대해 갱신한다.이 SGD의 인자에는 learning rate, weight decay, momentum, dampening 등이 있다.이 때, 위의 식을 보면, momuntem은 전의 위치에서의 속도를 얼마나 가중할지에 대한 값이다. 대체로 0.9 또는 0.99로 주고, dampening은 현재 위치에서의 gradient를 얼마나 가중할지에 대한 값이다. 이것을 높이면, 현재의 gradient의 비중이 줄어든다. weight decay가 regularization 텀인데, weight 텀을 gradient에 얼마나 반영할지에 대한 값이다.optimizer = optim.SGD(model.parameters(), lr =1e-2, momentum=0.9, weight_decay=0.1)AdaGradlearning rate는 초기에는 큰 값을 통해 수렴을 빨리하고, 뒤로 갈수록 세밀한 조작에 의한 최저점 도달을 위해 작은 learning rate가 적절하다. 그렇다면 우리가 learning rate를 상황에 맞게 조정할 수 있다면 더 좋은 학습 성능을 보일 수 있다.그래서 나오게 된 방법이 adaptive learning rate를 적용한 gradient방법인 AdaGrad이다. 난수를 통해 초기화하고 학습을 통해 gradient를 구하는데, 과거의 값과 현재의 값을 비교하여 동일한 방향이라면 수렴을 하고 있다는 증거이므로 learning rate를 줄여서 더 세밀한 조작을 할 수 있도록 만든다.식을 보면, dx는 gradient, grad_squared는 과거의 gradient를 누적한 함수이다. 가중치 갱신의 식은 정리하면 $ \\theta = \\theta -\\frac{\\rho}{\\epsilon + \\sqrt{r}} * g = \\theta - \\rho’ * g $ 이다. $ \\rho $는 lr, $ \\epsilon $ 은 분모가 0이 되는 것을 방지하기 위한 값으로 보통 10^(-5) ~ 10^(-7)의 값을 가진다. 이 때, r은 이전 gradient를 누적한 벡터에 대한 값이다. r을 구하는 식은 $ r = r + g * g $, 즉 현재 gradient의 제곱을 더한다.r이 크면 전체적으로 곱해지는 값인 $ \\rho’ $가 작아져서 조금만 이동할 것이고, r이 작으면 곱해지는 값이 커져서 많이 이동하게 된다.RMSPropAdaGrad 방식의 문제점은 현재의 gradient와 과거의 gradient가 같은 비중을 가지므로 r이 점점 커져서 수렴에 방해가 될 수 있다. 그래서 가중 이동 평균 기법을 사용하여 최근 것에 비중을 더 크게 두는 방식이다. r에 대한 수식으로는 $ r = \\alpha r + (1 - \\alpha) g * g $ 이고, $ \\alpha $ 는 0.9,0.99,0.999를 많이 사용한다. 나머지는 AdaGrad와 동일한 방식이다.AdamRMSprop에 momentum(관성)을 추가한 알고리즘이다. 이전의 $ \\theta = \\theta -\\frac{\\rho}{\\epsilon + \\sqrt{r}} * g = \\theta - \\rho’ * g $ 식에서 관성까지 추가하여, 관성, v는 $ v = \\alpha_1 v - (1 - \\alpha_1)g $ 이고, 을 수행하고, 이를 다시 $ v = \\frac{1}{1-(\\alpha_1)^t}v $ 로 만든다. r 또한, $ r = \\alpha_2 r + (1 - \\alpha_2) g * g $ 을 한 후, $ r = \\frac{1}{1-(\\alpha_2)^t}r $ 로 만든다. 그 후 $ \\theta = \\theta -\\frac{\\rho}{\\epsilon + \\sqrt{r}} * v $ 을 통해 가중치를 갱신한다.일반적으로 $ \\alpha_1 $ 는 0.9, $ \\alpha_2 $ 는 0.999, learning rate는 1e-3 또는 5e-4로 설정한다. 과거의 좋은 점들은 다 추가한 알고리즘이라 할 수 있다. Adam보다는 SGD를 커스텀하는 것이 더 좋은 성능을 가질 수 있지만, 잘 모르겠지만 좋은 걸 적용하고 싶다면 Adam을 사용하는 것이 좋다. 그러나 최근에 회자되고 있는 부분으로 SGD가 Adam보다 더 좋은지에 대한 의견이 있다. Adaptive method(Adam, RMS-prop) 등의 방법이 non-Adaptive method(SGD, Momentum)보다 더 안좋다라는 해석이 있으므로 데이터셋에 맞게 잘 사용하는 것이 중요하다.위의 최적화 방법들은 1차 미분을 활용한 방법들이다. 빨간 점에서의 gradient를 계산하고, 그 정보를 이용해서 손실함수를 선형함수로 근사시킨다.2차 미분 optimizer그렇다면 2차 미분을 사용하면 더 좋은 loss함수가 생성되지 않을까?2차 미분을 사용한다는 것은 gradient와 hessian matrix을 통해 2차 근사를 사용하게 된다는 것이다. 2차 미분을 활용한 방법에는 3가지가 있다.뉴턴 방법뉴턴 방법에서는 테일러 급수를 사용해서 2차 근사를 한다.\\[J(w + \\alpha) \\approx J(w) + J&#39;(w)\\alpha + \\frac{J&#39;&#39;(w)}{2} \\alpha^2\\]이를 현재 가중치에서 다음 가중치까지의 변화량, $ \\alpha $ 에 대해 미분해서 최소점을 찾아보면, $ \\frac{\\partial J(w + \\alpha)}{\\partial \\alpha} \\approx J’(w) + \\alpha J’’(w) = 0 $이다. 이를 $ \\alpha $ 에 대해 전개하면 다음과 같다.\\[\\alpha = -\\frac{J&#39;(w)}{J&#39;&#39;(w)} = -(J&#39;&#39;(w))^{-1} J&#39;(w)\\]이는 w가 하나일 때의 식이고, w가 여러 개일 때의 수식은 Hessian matrix를 사용하여 표현된다. 2차 미분에 대한 값은 $ H = (\\nabla J(w))’ $ 로 표현되고, 1차는 J의 미분인 gradient로 표현된다.\\[\\alpha = -H^{-1} \\nabla J\\]이를 통해 다음 가중치 값 w2를 나타내면\\[w_2 = w_1 + \\alpha\\]한번에 최저점으로 도달하는 것이 불가능하므로 반복을 해서 적용해야 하는데, H를 구하는 과정에서 매개 변수의 개수가 m이라 할 때 O(m^3)의 복잡도를 가지므로 과다한 연산량이 필요하다.켤레 경사도 방법H를 구할 때 과다한 연산량이 필요했던 것을 해결하기 위해 켤레, 즉 현재의 gradient 방향과 이전의 gradient 방향을 더해서 새로운 방향의 직선을 탐색한 후 그에 대해 2차 근사를 진행한다. 뉴턴에 비해서는 더 빠르게 도달할 수 있게 되었다.유사 뉴턴 방법(quasi-Newton methods)경사 하강법, 1차 근사는 수렴의 효율성이 낮았지만, 뉴턴 방법의 경우 해시안 행렬 연산이 너무 부담이었다. 그래서 이 해시안 행렬의 역행렬을 근사하는 방법을 직접적으로 구하는 것이 아닌 근사 행렬 M을 통해 근사하는 것이 유사 뉴턴 방법이다.원래의 방법에서는 해시안 H의 역행렬을 직접적으로 구해서 다음 위치를 찾았지만, 이 역행렬을 근사해서 구한다.대표적으로 해시안을 근사화하는 LFGS가 많이 사용된다. 기계학습에서는 M을 저장하는 메모리를 적게 쓰는 L-BFGS 를 주로 사용한다. 2차 미분을 활용하는 방법이 있긴 하나, 아직 연구가 덜 되어 있는 부분이라 1차 미분을 활용한 방법들을 많이 사용한다.activation function활성함수에는 선형, 계단, tanh, sigmoid, relu, leakyrelu등이 있다. sigmoid나 tanh 함수는 입력값이 커지면 포화상태가 되고 gradient가 0에 가까운 값이 출력된다. 이에 대한 한계로 인해 현재에는 ReLU를 많이 사용한다.ReLU의 수식은 max(0, x)로 간단하다. 이 ReLU의 변형으로 leaky ReLU 함수가 있다. ReLU의 경우 0보다 작을 때는 0이므로 gradient가 0이 나오게 된다. 조금 더 유연한 함수를 사용하기 위해 0보다 작은 구간에서 leaky, 즉 미세한 경사를 만들었고, 이것이 leaky ReLU이다. 수식으로는 max(0.1x, x)이다.ReLU의 가장 큰 장점은 gradient vanishing 현상이 생기지 않고, 간단한 max함수로 구성되어 있기에 연산량이 줄어든다.Batch Normalization입력을 정규분포로 만들고 network에 넣으면, 층을 거듭할수록 각각의 layer가 가지는 weight의 분포에 따라 데이터의 분포가 달라질 것이다. 그렇다면 이 층이 깊어질수록 더더욱 변형되고 이는 학습을 방해하는 요인으로 작용한다. 즉 각 layer마다의 입력의 분포가 일정해야 출력도 일정하게 되어서 성능도 잘 나오게 된다.그래서 층 단위로 정규화를 진행하는 것을 batch normalization이라 한다. 이 때, 층은 convolution나 fc layer 이후, activation을 통과하기 이전에 정규화를 진행한다.$ x = activation(batch_norm(convolution(x))) $미니 배치 단위로 평균과 분산을 계산해서 이를 통해 정규화를 진행한다. 정규화의 장점은 신경망의 gradient 흐름도가 개선되고, 높은 학습률을 가능케 한다. 또, 초기화에 대한 의존성이 감소한다. 초기화를 이상하게 했을 때 정규화를 하지 않으면 출력도 이상하게 나올 가능성이 있다. 그래서 이상한 초기값이라도 보정을 통해 분산 정도를 감소시켜줄 수 있다. 이렇게 하면 dropout과 비슷한 효과를 발생시켜서 dropout을 사용하지 않아도 되는 효과가 있다.정규화를 진행한 후에는 학습을 잘 할 수 있도록 이동과 비례를 시켜주는 과정을 거쳐야 한다. 이동 $ \\beta $, 비례 $ \\gamma $ 을 통해 최종값인 z를 구한다.$ z’ = \\gamma * z + \\beta $이 때, 이동을 사용하면 가중치 편향의 역할을 하므로 가중치 편향을 제거시켜 줘도 된다.이러한 이동과 비례를 한 후 후처리로 전체 훈련집합에 대한 z’의 평균 $ \\mu $ 과 분산 $ \\simga^2 $ 를 구한다. 이는 학습이 다 끝난 후 구해진 $ \\gamma, \\beta, \\mu, \\sigma^2 $는 저장시켜놓고 예측 단계에서 사용된다.훈련 시에 구한 평균과 분산, 그리고 이동과 비례 파라미터들에 대해 새로운 데이터에 적용을 해서 정규화를 수행하고 이동과 비례를 적용시킨다. 즉, 새로운 데이터의 값들을 학습을 통해 얻어진 가장 잘 진행된다고 생각되는 분포로 만들어주는 것이다.CNN에서는 특징 맵 단위로 정규화를 진행하게 되므로 특징맵 크기가 w x h 라면 미니배치에 있는 샘플 n개에 대해 feature map마다 하나씩의 $ \\gamma, \\beta $ 값이 존재하고, 이 값들을 통해 나온 정규 분포를 가지고, 배치 단위의 평균 $ \\mu, \\simga^2 $ 를 구한다.정규화를 통해 0을 기준으로 분포가 구성되므로 sigmoid를 사용해도 학습이 가능해졌다.Regularization규제란 일반화 능력을 높이기 위해 학습 알고리즘을 수정하는 방법들을 모두 일컫는다. 규제에는 가중치 감쇠나 드롭아웃과 같이 목적함수나 신경망 구조를 직접 수정하는 명시적 규제와 데이터 증강, 앙상블 등을 통해 간접적으로 영향을 미치는 암시적 규제가 있다.overfitting이 발생하는 이유는 가지고 있는 데이터에 비해 용량, 즉 모델이 훈련집합을 단순 암기함으로써 파라미터의 수가 너무 많아지기 때문이다. 그래서 loss function에 regularization, 규제 항을 추가하여 가중치의 차원을 줄일 수 있다.$ loss function’(\\theta) = loss function(\\theta) + \\lambda R(\\theta) $규제 기법weight decay규제항은 훈련집합과 무관하고, 데이터 생성 과정에서 미리 생성되는 사전 지식에 해당한다. 규제항은 매개변수를 작게 유지시켜서 모델의 용량(매개변수의 수)을 제한하는 역할을 한다. 즉 쉽게 말해 10차원의 매개변수가 있는데, 이것이 모두 사용되면 과잉적합이 발생하기 쉽다. 그래서 10차 중 8차정도만 사용되도록 만들거나, 학습을 통해 계속해서 가중치의 값이 커져서 과잉적합이 발생하는 상황에서 큰 가중치의 값을 작게 눌러주는 것이 규제이다. 이 때, 작은 가중치를 사용하도록 눌러주기 위해 사용하는 것이 규제항이고, 규제항의 종류로는 L2 norm이나 L1 norm을 사용한다. L2 norm을 사용하는 규제 기법을 가중치 감쇠라 한다. L2 normalizationweight의 제곱의 합으로 패널티를 적용한다. 규제항의 식은 $ || \\theta ||_2^2 $ 이고, 이는 가중치 벡터들의 크기를 추출하고, 이를 규제항으로 넣는다. 그리고, 이 규제항도 학습에 의해 낮아져야 하므로 학습을 진행할수록 w = [w1, w2, w3, …] 의 크기가 전체적으로 작아진다.그렇다면 원래의 가중치 갱신의 수식이 $ \\theta = \\theta - \\rho * \\nabla J(\\theta) = \\rho * \\frac{\\partial J}{\\partial \\theta} $ 인데, 여기에 규제항을 추가해서 다음과 같은 식으로 만든다.\\[\\theta = \\theta - \\rho (\\nabla J(\\theta) + \\lambda * 2 * \\theta)\\]\\[= (1 - 2\\rho \\lambda)\\theta - \\rho \\nabla J(\\theta)\\]원래의 식에서 \\(-\\rho \\lambda \\theta\\) 가 추가되었다. 만약 $ \\rho = 0.01, \\lambda = 2.0 $ 이라면 $ (1 - 2 \\rho \\lambda) = 0.96 $ &amp;lt; 1 이므로 원래의 값보다 작은 값이 곱해지는 것이다.이를 그림으로 생각을 해봤을 때 원래의 최저점 위치보다 원점 방향으로 \\(2\\rho \\lambda\\) 값만큼 가까워진 것이 된다. 그러면 최저점으로 가는 벡터의 크기가 작아지는 효과가 발생하게 되고, 이것이 가중치 감쇠라 할 수 있다. L1 normalizationL1 norm을 적용해줄 수도 있다. L1이란 weight값의 절대값의 합을 페널티 항으로 사용하여 loss function에 패널티를 부과한다. 규제항이 추가된 손실함수를 미분하면 다음과 같다.\\[\\nabla J&#39;(\\theta) = \\nabla J(\\theta) + \\lambda \\sin(\\theta)\\]sin을 사용한 이유는 L1이 절대값이므로 음수면 -, 양수면 +여야 한다. 이를 적용시키기 위해 sin함수를 적용했다. 이 식을 매개변수 갱신 수식에 적용하면\\[\\theta = \\theta - \\nabla J&#39;(\\theta)\\]\\[\\theta = \\theta - \\rho(\\nabla J(\\theta) + \\lambda \\sin(\\theta))\\]\\[\\theta = \\theta - \\rho \\nabla J(\\theta) - \\rho \\lambda \\sin(\\theta)\\]원래의 식에서 $ - \\rho \\lambda \\sin(\\theta) $ 가 추가되었다. 이 항의 역할은 가중치 중 1개를 없앤다. 즉, L1 norm의 형태는 2차원을 기준으로 사각형인데, L1 규제 안에서의 최저점에 가장 가까운 위치는 축 위의 점일 것이다. 그래서 1개의 weight를 삭제시키는 역할을 한다.대체로 L2를 많이 쓰는데, L1를 사용하는 경우는 특정 feature에 대한 의미를 찾을 때, 즉 필요하지 않는 feature에 대해서는 weight를 0으로 만들어서 원하는 feature만 보고자 할 때 사용한다.정리하면, L2 규제는 대체로 weight를 0에 가깝게 만들어주는 역할을 하고, L1 규제는 특정 weight를 0으로 만들어주는 역할을 한다.early stopping학습 시 일정 반복을 지나게 되면 과익 적합 현상이 발생한다. 그래서 loss가 줄어들다가 loss가 올라갈 때 멈추는 방식이다. 학습때마다 loss를 저장했다가 loss가 5~10번 정도가 값이 상승하는 구간이 발생되면 멈추도록 한다.data Augmentation과잉적합을 방지하는 가장 확실한 방법은 훈련 집합을 키우는 것이다. 데이터의 수를 늘릴 수 있지만, weight에 맞게 데이터의 수를 증가시키는 것은 현실적으로 불가능하다. 그래서 rotation, translation, reflection 등의 affine 변환을 통해 데이터 수를 증가시킬 수 있다. 그러나 모든 부류에 같은 변형이 발생하면 모델이 이 규칙까지 인식하고 학습할 수 있다.그래서 모핑과 같이 비선형 변환을 통해 공간을 찌그러트리기도 한다. 중요한 것은 원래 데이터의 특징이 변형되지 않도록 주의해야 한다.많이 사용되는 방법은 당연히 crop, affine, 색상 변환이다. 완벽히 방지하지는 못하지만, 어느정도의 과잉적합을 방지하는데 도움이 되고, 적용하기도 간편하다.Dropoutfc layer의 노드 중 일정 비율을 임의로 선택해서 동작을 멈추게 만드는 것이 Dropout 방식이다. 완전 연결을 사용하여 학습을 많이하면 fc layer의 가중치들이 비슷해진다. 이를 방지해야 overfitting을 방지할 수 있고, 그를 위해 몇개씩을 연결을 끊어서 부분적으로 최적화하여 비슷해지는 현상을 방지한다. 또 하나의 신경망에 dropout을 적용하면 여러 형태의 부분 신경망을 만들수 있고, 이들의 결과를 앙상블할 수 있다.batch normalization을 사용했을 때 dropout을 사용하지 않아도 되는 이유는 만약 원래는 양수로만 이루어져 있는 출력을 gradient하면 값이 수렴하지 못할 것이다. 그러나 이 값을 정규화를 하면 평균이 0에 가까워질 것이고, 그러면 양수로만 이루어져 있는 출력이 음수와 양수의 값을 이분화해서 가질 수 있게 된다. 이를 activation 함수에 넣으면 특정 값에 대해서만 활성이 될 것이다. 이러한 방식이 dropout을 했을 때 특정 연결을 끊어버림으로써 얻을 수 있는 효과와 비슷하다.Ensemble서로 다른 여러 개의 모델을 결합하여 성능을 향상시키는 기법인데, 현대 기계학습에서는 상상블도 규제로 여긴다. 앙상블에는 크게 두가지 2단계가 있다. 서로 다른 예측기를 학습 서로 다른 구조의 신경망 여러 개를 학습, 같은 구조를 사용하더라도 서로 다른 초기값과 매개변수를 설정하고 학습 e.g.) bagging(훈련 집합을 여러 번 샘플링해서 서로 다른 훈련 집합을 구성) boosting(i번쨰 예측기가 틀린 샘플을 i+1번째 예측기가 잘 인식하도록 하는 연계성을 구축) 학습된 예측기를 결합(평균) 여러 모델의 출력을 평균하거나 투표하여 최종 결과를 결정 hyperparameter Optimization참고 자료모델에서 매개변수는 가중치와 하이퍼 파라미터로 두 종류가 있다. 학습에 의해 결정되는 매개변수는 가중치, 사람에 의해 결정되는 lr, filter size 등은 하이퍼파라미터라고 한다.하이퍼파라미터를 선택하는 기준에는 디폴트 값을 설정해도 되고, 경험에 의한 특정 range를 설정하고, 그 안에서 최적화시킬 수도 있다. 하이퍼 파라미터가 중요한 이유는 이로 인해 학습이 결정되기 때문이다. 특히 learning rate를 잘 설정해야 학습이 잘 진행된다.선택하는 방법은 여러 방법이 존재한다. grid search and random search임의 탐색은 난수로 하이퍼파라미터 조합을 생성해서 어떤 것들이 좋은지 체크하는 방법이다. 격자 탐색은 고정된 간격으로 샘플링해서 값을 비교하는 방법이다. 결과적으로 따져봤을 때, 임의 탐색이 조금 더 좋다. random search를 할 경우 최적의 값을 찾을 수도 있지만, grid를 사용할 경우 최적의 값이 grid 사이에 존재하면 찾을 수가 없다. 또한, grid search가 연산이 더 오래 걸린다.이 때 값을 설정할 때는 log스케일로 값을 주어야 한다. 즉 log를 사용하지 않고 찾으면 log(1e-3) ~ log(1e-6)의 범위를 찾기 떄문에 수많은 값들이 샘플링된다. 그 대신 난수를 (-3~-6)과 같이 설정한 후 정확도가 높은 범위를 재설정해서 다시 찾는다. log의 값 자체를 난수로 설정하게 되면 예를 들어 uniform(log(1e-3),log(1e-6))로 찾고자 할 때, 0,0.1,0.0001,0.52343 등의 너무 수많은 값이 존재하게 된다. 이렇게 설정하면 각각의 값들의 차별성이 모호할 수 있으므로 log(10**uniform(-5,5))의 방법으로 탐색한다.Prepare Dataobject detection에서의 data를 사용하려면 data annotation이 필요하다. 우리의 데이터를 사용하려면 데이터 전처리를 통해 각 bounding box와 label에 대한 파일을 만들어야 한다. labelimg라는 툴을 통해 많이 사용한다.실제로 딥러닝 개발자가 직접 이 label을 만들지 않더라도, 이 annotation들이 적절하게 잘 적용되어 있는지 확인하지 않으면 모델 학습 자체가 잘못될 수 있다. 그래서 직접 data annotation을 수행해보면서 생성해보는 것이 중요하다.gt 파일에는 txt나 yaml파일로 구성되어 있고, coco dataset에서의 gt파일을 살펴보게 되면 순서는 다음과 같다.45 0.479492 0.688771 0.955609 0.595545 0.736516 0.247188 0.498875 0.47641750 0.637063 0.732938 0.494125 0.51058345 0.339438 0.418896 0.678875 0.781549 0.646836 0.132552 0.118047 0.09693749 0.773148 0.129802 0.090734 0.09722949 0.668297 0.226906 0.131281 0.14689649 0.642859 0.079219 0.148063 0.148062첫번째는 class index일 것이고, 2:5번째는 bounding box의 좌표로 구성되어 있다. 어떤 coco dataset의 경우 조금 다른 포맷을 가지기도 한다.car 0.80 0 -2.09 1013.39 182.46 1241.00 374.00 1.57 1.65 1.35 4.43 1.65 5.20 -1.42첫번째는 class name, 두번째는 truncation, 즉 가려진 정도나 이미지에서 벗어난 정도를 나타내준다. 세번째는 occulation, 가려짐에 대한 레벨을 나타낸다. 그 다음은 [x min, y min, x max, y max]인 bounding box에 대한 정보를 나타낸다.여기서 coco dataset의 경우 bounding box가 [x,y,w,h]로 되어 있지만, yolo의 경우 [x center, y center, w, h]로 구성되어 있다. 따라서 자신이 사용하는 dataset과 모델을 적절하게 조정하는 것이 중요하다.아래는 kitti dataset의 gt파일이다.Pedestrian 0.00 0 -0.20 712.40 143.00 810.73 307.92 1.89 0.48 1.20 1.84 1.47 8.41 0.01첫번째는 object의 정보, 두번째는 truncated level, 세번째는 occluded level, 네번째는 object의 각도가 정의되어 있다. 마지막으로 bbox에 대한 4개의 value가 있다.데이터를 불러온 후 dataset을 설정할 때는 train:valid:test = 8:1:1 정도로 맞춘다.Evaluation metric precision내가 예측한 바운딩 박스들 중 참인 것에 대한 비율 recallgt박스들 중 내가 예측한 참인 바운딩 박스의 비율 precision과 recall은 trade-off 관계를 가지고 있다. 따라서 이 두 값을 종합해서 성능을 평가한다.F1 score$ F_1 = 2 \\frac{precision x recall}{precision + recall} $F1 score은 precision과 recall의 조화평균을 사용한 방법이다.PR curveprecision과 recall에 대한 곡선을 그리는 방법으로, confidence가 높은 것을 맨 앞으로 두고, 차례대로 그림을 그린다. 누적 TP/FP를 통해 각각의 예측값마다의 precision과 recall을 구한 후 곡선을 그려준다. 그러나 이 PR 곡선은 서로 다른 두 알고리즘을 비교하는 데에는 적합하지 않다. 그래서 나온 것이 mAP이다. mAP이는 Mean average precision을 말하는데, PR 곡선에서 아래 영역으로 계산할 수 있다. 모든 box를 confidence기준으로 내림차순하고, 이 모든 box에 대해 precision과 recall을 계산한다. 곡선 아래 면적을 계산해주면 되는데, 객체가 두 개 이상 검출되었을 경우 AP를 평균하여 계산한다.Confusion matrix각 클래스간 예측한 값들을 카운팅해서 matrix로 보여주는 방식이다. 파란색이 정답인 label이고, 나머지는 틀리게 예측한 label이다. 이를 통해 특정 label에 대해 어떤 class를 모델이 헷갈려하는지를 확인할 수 있다." }, { "title": "[데브코스] 10주차 - DeepLearning CNN", "url": "/posts/cnn/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-19 02:40:00 +0900", "snippet": "CNN을 들어가기 전에 DMLP에 대해 잠깐 살펴보고자 한다.DMLP(Deep Multi Layer Perceptron)딥러닝을 들어가기에 앞서 DMLP의 형태가 있다. 이는 다층 퍼셉트론보다 더 깊게 layer를 쌓은 것을 말하는데, 이는 완전 연결 구조를 가진다. 그에 따라 복잡도가 높아지고, 학습이 매우 느려진다. 또한 과잉적합(overfitting)이 발생할 수 있다.그러나 CNN(convolutional neural network)의 경우 부분적으로 연결되어 있는 구조로 인해 격자 구조를 갖는 데이터에 적합하다.컨볼루션 연산을 통해 특징을 추출하고 영상 분류나 문자 인식 등 인식문제에서 높은 성능을 보인다.Convolutional Neural NetworkConvolution padding : add zero in boundary of input image stride : elements of sliding window of convolution kerneloutput shape output height : (input height - kernel height + padding size * 2) // stride + 1 output width : (input width - kernel width + padding size * 2) // stride + 1A * w = B의 연산의 경우 A shape : [batch, input channel, input height, input width] w shape : [output channel ,input channel, kernel height, kernel width] B shape : [batch, output channel, output height ,output width]연산량을 따질 때는 MAC(Multiply Accumulation Operation)단위를 사용한다.convolution MAC : kw*kh*kc*oc*ow*oh*b(k:kernel, o:output, b:batch)이 convolution은 sliding window 방식을 통해 연산을 수행하기 되고, 이를 for문으로 나타내면 다음과 같다.for b in batch: for oh in output_height: for ow in output_width: for oc in output_channel: for kc in kernel_channel: for kh in kernel_height: for kw in kernel_width:총 7번의 루프로 동작한다.이는 너무 비효율적이기 때문에 IM2COL &amp;amp; GEMM 방식을 통해 더 간편한 연산 방식을 사용할 수 있다. IM2COLn-dimension의 data를 2D matrix data로 변환시켜 더 효율적으로 연산한다.data와 kernel을 2차원으로 변환하여 연산하면 2차원의 값이 출력될 것이다. 이를 다시 원래의 차원으로 변환하면 연산이 효율적으로 진행된다. kernel : [oc, kh*kw*ic] input : [kh*kw*ic, oh*ow] output : [oc, oh*ow]이렇게 변환된 matrix를 연산하는 과정 자체를 GEMM(General Matrix to Matrix Multiplication)이라고 한다.참고자료CNN (numpy)이 CNN의 연산을 pytorch나 tensorflow가 아닌 numpy만을 사용하여 구현해보고자 한다.과정 sliding window convolution IM2COL GEMM convolutionsliding window 방식 function/convolution.pyimport numpy as npclass Conv: # dilation : kernel이 얼마나 간격을 띄우고 연산할지에 대한 값 def __init__(self, batch, in_c, out_c, in_h, in_w, k_h, k_w, dilation, stride, pad): self.batch = batch self.in_c = in_c self.out_c = out_c self.in_h = in_h self.in_w = in_w self.k_h = k_h self.k_w = k_w self.dilation = dilation self.stride = stride self.pad = pad self.out_h = (in_h - k_h + 2 * pad) // stride + 1 self.out_w = (in_w - k_w + 2 * pad) // stride + 1 def check_out(self, a, b): return a &amp;gt; -1 and a &amp;lt; b # naive convolution, sliding window matric def conv(self, A, B): # A * B = C # defice C size C = np.zeros((self.batch, self.out_c, self.out_h, self.out_w), dtype=np.float32) # 7 loop for b in range(self.batch): for oc in range(self.out_c): # each channel of output for oh in range(self.out_h): for ow in range(self.out_w): # each pixel of output shape a_j = oh * self.stride - self.pad # a&#39;s y value == input&#39;s y value for kh in range(self.k_h): if self.check_out(a_j, self.in_h) == False: # a_j 가 in_h보다 크다면 연산 x C[b, oc, oh, ow] += 0 else: a_i = ow * self.stride - self.pad # a&#39;s x value == input&#39;s x value for kw in range(self.k_w): if self.check_out(a_i, self.in_w) == False: C[b, oc, oh, ow] += 0 else: C[b, oc, oh, ow] += np.dot(A[b, :, a_j, a_i], B[oc, :, kh, kw]) a_i += self.stride # add x direction moving unit for kernel a_j += self.stride # add y direction moving unit for kernel return CC는 결과를 저장하기 위한 저장소이다.batch단위별로 결과 채널만큼 반복하고, 그것을 또 높이 단위로, 넓이 단위로 하나하나 루프를 돈다. 즉, [batch, out channel, out height, out width]의 형태만큼 순환을 해야 한다. output 결과를 계산해야 하는데, 계산을 위해 kernel크기만킄도 루프를 돌아야 한다. 반복을 하기 전에 입력의 row, col을 지정해주고, 예외 처리를 하여 연산에 오류가 나지 않는지 체크한다. 오류가 난다면 연산을 하지 않는다. row를 구할 때 kernel이 stride만큼씩 움직이므로 이를 곱한다. padding은 경계면에 0을 추가하여 중앙과 가장자리의 연산 수를 동일하게 맞춰주는 용도이다. 따라서 시작을 padding을 포함하여 시작할 수 있도록 (-)를 해준다. main.pyimport numpy as npfrom function.convolution import Convdef convolution(): print(&quot;convolution&quot;) # define the shape of input &amp;amp; weight in_w = 3 in_h = 3 in_c = 1 out_c = 16 batch = 1 k_w = 3 k_h = 3 # define matrix x = np.arange(9, dtype=np.float32).reshape([batch, in_c, in_h, in_w]) w = np.array(np.random.standard_normal([out_c, in_c, k_h, k_w]), dtype=np.float32) #print(x,&quot;\\n\\n&quot;, w) Convolution = Conv(batch = batch, in_c = in_c, out_c = out_c, in_h = in_h, in_w = in_w, k_h = k_h, k_w = k_w, dilation = 1, stride = 1, pad = 0) print(&quot;x shape : &quot;, x.shape) print(&quot;w shape : &quot;, w.shape) L1 = Convolution.conv(x,w) #print(L1) print(&quot;C shape : &quot;, L1.shape) # batch, out_c, out_h, out_wif __name__ == &quot;__main__&quot;: convolution()# ------------ # x shape : (1, 1, 3, 3) # batch, in_c, in_h, in_ww shape : (16, 1, 3, 3) # out_c, in_c, k_h, k_wC shape : (1, 16, 1, 1) # batch, out_c, out_h, out_wim2col 방식 function/convolution.py # IM2COL, change n-dim input to 2-dim matrix def im2col(self, A): # define output mat = np.zeros((self.in_c * self.k_h * self.k_w, self.out_w * self.out_h), dtype=np.float32) # matrix index mat_i = 0 mat_j = 0 # transform from A to mat for c in range(self.in_c): for kh in range(self.k_h): for kw in range(self.k_w): in_j = kh * self.dilation - self.pad for oh in range(self.out_h): if not self.check_out(in_j, self.in_h): for ow in range(self.out_w): mat[mat_j, mat_i] = 0 mat_i += 1 else: in_i = kw * self.dilation - self.pad for ow in range(self.out_w): if not self.check_out(in_i, self.in_w): mat[mat_j, mat_i] = 0 mat_i += 1 else: mat[mat_j, mat_i] = A[0, c, in_j, in_i] # [batch, ic, ih, iw], batch = 1이므로 0index mat_i += 1 # 1 x direction move in_i += self.stride # move the stride unit as x axis in_j += self.stride # move the stride unit as y axis mat_i = 0 # initialization mat_j += 1 # move next row at input return mat # gemm, 2D matrix multiplication def gemm(self, A, B): a_mat = self.im2col(A) b_mat = B.reshape(B.shape[0],-1) # kernel 4차원 텐서 차원을 reshape로 바꿀 수 있음, kernel은 [output channel ,input channel, kernel height, kernel width] 로 되어 있는데, 이를 [oc, kh*kw*ic]로 변환하기에 c_mat = np.matmul(b_mat, a_mat) c = c_mat.reshape([self.batch, self.out_c, self.out_h, self.out_w]) return c이 때는 매번 루프를 돌 때마다 연산하는 것이 아닌 4d 차원을 2d로 변환한 후 연산을 진행한다. 따라서 변환해주기 위한 im2col을 먼저 선언한다. main.py L2 = Convolution.gemm(x,w) print(L2) print(&quot;L2 shape : &quot;, L2.shape) # batch, out_c, out_h, out_wpytorch와 위의 두 방식 시간 비교# main.pyimport time l1_time = time.time() for i in range(100): L1 = Convolution.conv(x,w) print(&quot;L1 time : &quot;, time.time() - l1_time) l2_time = time.time() for i in range(100): L2 = Convolution.gemm(x,w) print(&quot;L2 time : &quot;, time.time() - l2_time) # pytorch torch_conv = nn.Conv2d(in_c, out_c, kernel_size = k_h, stride = 1, padding = 0, bias = False, dtype = torch.float32) torch_conv.weight = torch.nn.Parameter(torch.tensor(w)) # 우리가 직접 생성한 weight를 집어넣음 l3_time = time.time() for i in range(100): L3 = torch_conv(torch.tensor(x, requires_grad=False, dtype=torch.float32)) # x가 numpy로 생성되었기 때문에 tensor로 변환하여 실행 print(&quot;L3 time : &quot;, time.time() - l3_time) print(L3)# ------------------- #L1 time : 0.40502333641052246L2 time : 0.017976760864257812L3 time : 0.00850367546081543L1 » L2 » L3 순으로 시간이 단축되는 것을 볼 수 있다.CNN - poolingpooling은 feature map의 크기를 줄이는 것을 말한다.종류로는 max pooling / average pooling이 있다. max pooling의 작동 방식은 다음과 같다.영역에서 최대값만 추출하므로 엣지부분을 많이 잡히게 출력된다.그에 반해 average pooling의 경우 평균값을 사용하므로 스무딩한 형상을 띄게 된다.pooling 직접 구현 function/pool.pyimport numpy as np# 2d poolingclass Pool: def __init__(self, batch, in_c, out_c, in_h, in_w, kernel, dilation, stride, pad): self.batch = batch self.in_c = in_c self.out_c = out_c self.in_h = in_h self.in_w = in_w self.kernel = kernel self.dilation = dilation self.stride = stride self.pad = pad self.out_w = (in_w - kernel + 2 * pad) // stride + 1 self.out_h = (in_h - kernel + 2 * pad) // stride + 1 def maxpool(self, A): C = np.zeros([self.batch, self.out_c, self.out_h, self.out_w], dtype=np.float32) for b in range(self.batch): for c in range(self.in_c): for oh in range(self.out_h): # output 크기만큼 결과를 낼 것이므로 a_j = oh * self.stride - self.pad # 연산 시작 row for ow in range(self.out_w): a_i = ow * self.stride - self.pad # 연산 col # kernel 크기만큼 중에서 가장 큰 값을 지정 C[b, c, oh, ow] = np.amax(A[:, c, a_j:a_j+self.kernel, a_i:a_i+self.kernel]) return Cfor문을 통해 하나하나 연산한다. amax라는 array안의 가장 큰 값을 추출해주는 메서드를 통해 2x2크기의 공간에서 최대값을 C array에 넣는다. main.py# 간단한 forward 구조 생성def forward_net(): # define batch = 1 in_c = 3 in_w = 6 in_h = 6 k_h = 3 k_w = 3 out_c = 1 x = np.arange(batch*in_c*in_w*in_h, dtype=np.float32).reshape([batch, in_c, in_w, in_h]) w1 = np.array(np.random.standard_normal([out_c, in_c, k_h, k_w]), dtype=np.float32) Convolution = Conv(batch = batch, in_c = in_c, out_c = out_c, in_h = in_h, in_w = in_w, k_h = k_h, k_w = k_w, dilation = 1, stride = 1, pad = 0) L1 = Convolution.gemm(x,w1) print(&quot;L1 shape&quot;, L1.shape) # L1 shape (1, 1, 4, 4) print(&quot;L1&quot;, L1) Pooling = Pool(batch = batch, # L1의 출력 Shape를 입력으로 넣어줘야 한다. in_c = L1.shape[1], out_c = L1.shape[0], in_h = L1.shape[2], in_w = L1.shape[3], kernel = 2, # pooling의 커널 2x2 dilation = 1, stride = 2, pad = 0) L1_max = Pooling.maxpool(L1) print(&quot;\\nL1 max shape : &quot;, L1_max.shape) print(&quot;L1 max&quot;,L1_max)# ---------------------- #L1 shape (1, 1, 4, 4)L1 [[[[484.84863 491.3175 497.78632 504.2552 ] [523.6616 530.1305 536.5993 543.0681 ] [562.47455 568.9434 575.4122 581.88104] [601.28754 607.75635 614.2252 620.694 ]]]]L1 max shape : (1, 1, 2, 2)L1 max [[[[530.1305 543.0681 ] [607.75635 620.694 ]]]]간단하게 forward과정만 보기 위해 함수를 선언해주었다. x를 input 형태로 만들어주고, w1을 생성해준다. 그 후 conv 이후에 pooling을 진행하므로 convolution을 먼저 진행해준다. conv는 gemm 함수를 사용했다. 이로 인해 출력되는 값은 (1,1,4,4) 형태로 리턴된다. 이를 pooling 해줄 때는 in_c, out_c 가 아닌 L1 conv 한 출력값 형태로 넣어줘야 한다.당연히 input -&amp;gt; conv -&amp;gt; pooling -&amp;gt; output 순서로 진행되기 때문이다. 그렇게 maxpooling을 진행하면 [batch, in_c, in_h, in_w] -&amp;gt; [batch, out_c, (in_h - kernel + 2 * pad) // stride + 1,(in_w - kernel + 2 * pad) // stride + 1] 로 변환된다.output : [1, 1, (4 - 2 + 2 * 0) // 2 + 1, (4 - 2 + 2 * 0) // 2 + 1] = [1,1,2,2]CNN - FC layerFully Connected Layer로써 2d 특징맵을 1d 특징맵으로 변환한 후 fc weight와 연산하여 최종 결과를 출력하는 층이다.이는 2d를 1d로 변환한 층이므로 연산랴이 엄청 크게 되고, 파라미터의 수가 이곳에 가장 많이 분포되어 있는 경우가 많다.그래서 이를 해결하기 위해 1x1 convolutional layer로 바꿔서 만드는 모델도 많다.fc layer 코드 구현 funtion/fc.pyimport numpy as npclass FC: def __init__(self, n_classes, in_c, out_c, in_h, in_w): self.n_classes = n_classes self.in_c = in_c self.out_c = out_c self.in_h = in_h self.in_w = in_w def fc(self, A, W): # A shape : [b,in_c, in_h, in_w] -&amp;gt; [b, in_c*in_h*in_w] a_mat = A.reshape([self.n_classes, -1]) B = np.dot(a_mat, np.transpose(W, (1,0))) return BA, 입력의 shape은 [b,in_c, in_h, in_w] 이다. 이 4-dim 을 2-dim으로 변환한 후 fc layer 연산을 수행해야 하므로 [b, in_c*in_h*in_w]로 변환해준다.B의 경우 출력값인데, vector 내적 연산을 수행하는 dot을 사용했고, w는 입력이 [1, in_c*in_h*in_w] 이므로 연산을 위해서는 순서를 바꿔줘야 한다. 그러므로 transpose 시켜준다. main.py # fully connected layer w2 = np.array(np.random.standard_normal([L1_max.shape[0], L1_max.shape[1]*L1_max.shape[2]*L1_max.shape[3]]), dtype=np.float32) Fc = FC(n_classes = L1_max.shape[0], in_c = L1_max.shape[1], out_c = 1, # 출력은 1채널이어야 함 in_h = L1_max.shape[2], in_w = L1_max.shape[3]) L2 = Fc.fc(L1_max, w2) print(&quot;L2 shape : &quot;, L2.shape) print(L2)# ------------------- #L2 shape : (1, 1)[[1205.8112]]이렇게 출력된 형태는 (1,1) == (n_classes, out_c) , 즉 각각의 클래스에 따른 확률값이다.CNN - Activationactivation, 활성 함수는 비선형 함수로 sigmoid, tanh, ReLU, LeakyReLU 등이 있다.sigmoid와 tanh는 역전파시 gradient vanishing 현상이 발생하므로 최근에는 사용하지 않는다. 또한, ReLU는 max 함수이므로 연산이 더 빠르기 때문에 ReLU를 많이 사용한다.Activation 코드 구현 function/activation.pyimport numpy as np# max(0,x)def relu(x): x_shape = x.shape x = np.reshape(x, [-1]) # 몇 차원인지 모르기 때문에 1차원으로 변환 x = [max(0,v) for v in x] x = np.reshape(x, x_shape) return xdef leaky_relu(x): x_shape = x.shape x = np.reshape(x, [-1]) x = [max(0.1*v, v) for v in x] x = np.reshape(x, x_shape) return x def sigmoid(x): x_shape = x.shape x = np.reshape(x, [-1]) x = [ 1 / (1 + np.exp(-v)) for v in x] x = np.reshape(x, x_shape) return xdef tanh(x): x_shape = x.shape x = np.reshape(x, [-1]) x = [np.tanh(v) for v in x] x = np.reshape(x, x_shape) return x main.pydef plot_activation(): x = np.arange(-10,10,1) out_relu = relu(x) out_leaky = leaky_relu(x) out_sigmoid = sigmoid(x) out_tanh = tanh(x) plt.figure(figsize=(10,5)) output = {&#39;out_relu&#39;:out_relu, &#39;out_leaky&#39;:out_leaky, &#39;out_sigmoid&#39;:out_sigmoid, &#39;out_tanh&#39;:out_tanh} key = list(output.keys()) for i in range(len(key)): out = key[i] plt.subplot(2,2,i+1) plt.plot(x, output[out], &#39;o-&#39;) plt.title(out) plt.tight_layout() plt.show()CNN 전체 구성얕은 CNN을 프레임워크를 사용하지 않고, numpy로만 구성해보고자 한다.layer는 다음과 같다. input : x [1,1,6,6] conv : w [1,1,3,3], k [3x3], stride=1, pad=0 max pooling : k [2x2], stride=2, pad=0 fc layer : w [4,1] L2 norm역전파까지 진행해서 학습이 진행되는지를 볼 것이다. 역전파를 할 때는 chain rule을 사용하여 좀 더 간편하게 weight를 갱신한다.max pooling을 역전파할 때는 다시 되돌리기 위해서는 max값을 가져온 위치를 알고 있어야 한다. 그것을 max unpooling 방식을 사용한다. 이를 통해 가져온 위치만 활성화하고, 나머지는 0으로 된다.data, label, weight, h,w 선언def shallow_network(): # input [1,1,6,6], 2 iter x = [np.array(np.random.standard_normal([1,1,6,6]), dtype=np.float32), np.array(np.random.standard_normal([1,1,6,6]), dtype=np.float32)] # Ground Truth y = np.array([1,1], dtype=np.float32) # conv1 weights [1,1,3,3] w1 = np.array(np.random.standard_normal([1,1,3,3]), dtype=np.float32) # fc weights [1,4] w2 = np.array(np.random.standard_normal([1,4]), dtype=np.float32) lr = 0.01 padding = 0 stride = 1 # L1 layer shape w,h L1_h = (x[0].shape[2] - w1.shape[2] + 2 * padding) // stride + 1 L1_w = (x[0].shape[3] - w1.shape[3] + 2 * padding) // stride + 1 print(&quot;L1 output : ({}, {})&quot;.format(L1_h, L1_w)) # (4, 4)# -------------------- # L1 output : (4, 4) x,y : 1epoch마다 2번을 진행하기 휘애 2개를 선언해주었다. 이 때 나중에 값을 비교할 때 정확한 판단을 위해 dtype을 지정해줘야 한다. 따라서 array로 생성한다. conv1 weight : shape=[1,1,3,3] == [out_c, in_c, k_h, k_w] fc weight : shape=[1,4] == [1, n_classes] lr : learning rate stride : convolution에서는 1, pooling에서는 2 L1_w, L1_h = convolution을 해서 나오는 출력 w, hconvolution, FC, pooling layer 선언 # conv1 Convolution = Conv(batch = x[0].shape[0], in_c = x[0].shape[1], out_c = w1.shape[0], in_h = x[0].shape[2], in_w = x[0].shape[3], k_h = w1.shape[2], k_w = w1.shape[3], dilation = 1, stride = stride, pad = padding) # conv1 backprop conv Conv_diff = Conv(batch = x[0].shape[0], in_c = x[0].shape[1], out_c = w1.shape[0], in_h = x[0].shape[2], in_w = x[0].shape[3], k_h = L1_h, k_w = L1_w, dilation = 1, stride = stride, pad = padding) # max pooling Pooling = Pool(n_classes = x[0].shape[0], in_c = w1.shape[1], out_c = w1.shape[0], in_h = L1_h, in_w = L1_w, kernel = 2, dilation = 1, stride = 2, pad = 0) # FC Fc = FC(n_classes = 1, in_c = x[0].shape[1], out_c = 1, in_h = L1_h/2, in_w = L1_w/2) convolution batch, in_c, in_h, in_w : input = [batch, in_c, in_h, in_w] 이므로 각각 지정 out_c, k_h, k_w : w = [out_c, in_c, k_h, k_w] 이므로 각각 지정 역전파에 사용될 convolution을 선언해준다. 자세한 내용은 아래에서 설명하겠다. conv_diff k_h, k_w : 1 conv layer의 출력값으로 지정 pooling kernel,stride : pooling에서는 kernel size를 2로 설정하고, stride를 2로 설정하여 출력 크기를 1/2로 만듦 FC n_classes, out_c : 출력 크기는 n_classes x 1 in_c : 입력 채널 in_h, in_w : FC layer는 1 conv layer의 출력값에서 max pooling하여 1/2 크기가 된 값을 입력으로 받으므로 1/2해줘야 한다. forward epochs = 100 for e in range(epochs): # 100 epoch total_loss = 0 for i in range(len(x)): # 2iter for each epoch # forward L1 = Convolution.gemm(x[i], w1) print (x[i].shape, w1.shape, L1.shape) L1_act = sigmoid(L1) # (1,1,4,4) L1_max = Pooling.maxpool(L1_act) #print (L1_max.shape) # (1,1,2,2) L1_max_flatten = np.reshape(L1_max, (1,-1)) #print (L1_max_flatten.shape) # (1,4) L2 = Fc.fc(L1_max_flatten, w2) #print (L2.shape) # (1,1) #print (L2) L2_act = sigmoid(L2) #print (L2_act) loss = np.square(y[i] - L2_act) * 0.5 total_loss += loss.item() #print (loss)# -------------------- #x1.shape : (1, 1, 6, 6), w1.shape : (1, 1, 3, 3), L1.shape : (1, 1, 4, 4)L1_act.shape : (1, 1, 4, 4), L1_max.shape : (1, 1, 2, 2)L1_max_flatten.shape : (1, 4)L2.shape : (1, 1) epochs: 반복할 횟수 지정 x의 길이만큼 반복 forward 1 layer : conv layer (b,in_c,in_h,in_w) * (out_c,in_c,k_h,k_w) = (b,out_c,out_h,out_w) (1, 1, 6, 6) * (1, 1, 3, 3) = (1, 1, 4, 4) out_h = (in_h - k_h + 2 * padding) // stride + 1 4 = (6 - 3 + 2 * 0) // 1 + 1 1 layer activation activation은 차원이 달라지지 않고, 값만 바뀐다. 1 layer max pooling max pooling에서 stride와 kernel의 크기를 통해 결과의 크기를 설정할 수 있다. 이 또한, out_h = (in_h - k_h + 2 * padding) // stride + 1 1 layer flatten fc layer에 넣기 위해 1차원으로 변환시켜준다. 2 layer : fc layer (n_classes, 1) 2 layer activation backward w2 backpropagation # backward # delta E / delta w2 diff_w2_1 = L2_act - y[i] diff_w2_2 = L2_act * ( 1 - L2_act) diff_w2_3 = L1_max diff_w2 = diff_w2_1 * diff_w2_2 * diff_w2_3 #print (diff_w2) # 2x2 인데, fc layer.shape은 1x4이므로 변환해줘야 함 diff_w2 = np.reshape(diff_w2, (1,-1))# --------------- #diff_w2_before.shape : (1, 1, 2, 2)diff_w2_after.shape : (1, 4) diff_w2_1, diff_w2_2, diff_w2_3 : chain rule을 통해 $ \\frac{\\partial E}{\\partial W_2} $ 를 구한다.구한 diff_w2_1,2,3 을 곱해서 출력값을 구하면 (1,1,2,2) shape을 얻는다. 이는 w2를 최적화하는데 사용하는데 w2의 shape은 (1,4) 이므로 이를 변환시켜줘야 한다. w1 backpropagation # delta E / delta w1 diff_w1_1 = diff_w2_1 * diff_w2_2 #print (diff_w1_1.shape) diff_w1_2 = np.reshape(w2, (1,1,2,2)) # w2 [1,4] -&amp;gt; reshape #print (diff_w1_2.shape) # 1,1,2,2 diff_w1_2 = diff_w1_2.repeat(2, axis=2).repeat(2, axis=3) # array를 n번 증폭 #print (diff_w1_2.shape) # 1,1,4,4 # diff maxpool diff_w1_3 = np.equal(L1_act, L1_max.repeat(2, axis=2).repeat(2, axis=3)) # pooling의 input, output,, 동일한 값의 인덱스를 구해줌, 동일한 행렬 크기로 만든 후 비교 #print (diff_w1_3) diff_w1_4 = L1_act * (1- L1_act) #print (diff_w1_4.shape) # 1,1,4,4 diff_w1_5 = x[i] diff_w1 = diff_w1_1 * diff_w1_2 * diff_w1_3 * diff_w1_4 # 위 4개의 결과는 4x4 이고, x[i]는 6x6이므로 x[i]에 conv를 진행해줘야 함 diff_w1 = Conv_diff.gemm(x[i], diff_w1) #print (diff_w1)chain rule에 의해 전개한 수식을 모두 곱하여 w1에 대한 diff를 구한다. 이 떄, 중요한 것은 diff_w1_5의 차원은 (1,1,6,6)인데, 나머지의 결과값들은 (!,1,4,4)이므로 이 둘을 곱하기 위해서 convolution 연산을 해야 한다. 그 이유는 6x6의 연소 개수는 36개인데, 이를 4x4에 reshape를 시켜줄 수 없다. 그러므로 연산을 위해 convolution을 진행한다. # update w2 = w2 - lr * diff_w2 w1 = w1 - lr * diff_w1 print(&quot;{} epoch loss {}&quot;.format(e, total_loss / len(x)))# ----------------- #0 epoch loss 0.183725595474243161 epoch loss 0.182699449360370642 epoch loss 0.18167604506015778...97 epoch loss 0.1018630564212799198 epoch loss 0.10122730582952599 epoch loss 0.10059575736522675구한 가중치의 gradient를 통해 learning rate와 곱해서 가중치를 업데이트한다. 구한 total_loss는 x의 길이, 즉 반복한 횟수만큼 나눠주어 평균을 출력한다. loss가 줄어들고 있는 것을 확인할 수 있다.FashionMNIST using Lenet5지난 번에 만들어주었던 Lenet5를 사용하여 fashionMNIST를 학습시키고자 한다. 여기서 dropout, activation 변화 등을 추가했고, batch normalization 텀을 추가했다.torch.nn.BatchNorm2d(num_features)batch normalization에서 나머지 인자는 디폴트 값을 사용한다. 이는 conv layer와 activation 사이에 넣는다. fc layer에는 넣지 않는다.class Lenet5(nn.Module): def __init__(self, batch, n_classes, in_channel, in_width, in_height, is_train = False): super().__init__() ... self.bn0 = nn.BatchNorm2d(6) self.bn1 = nn.BatchNorm2d(16) self.bn2 = nn.BatchNorm2d(128) self.dropout = nn.Dropout(p=0.3) # weight initialization torch.nn.init.xavier_uniform_(self.conv0.weight) torch.nn.init.xavier_uniform_(self.conv1.weight) torch.nn.init.xavier_uniform_(self.conv2.weight) torch.nn.init.xavier_uniform_(self.fc0.weight) torch.nn.init.xavier_uniform_(self.fc1.weight) def forward(self, x): x = self.conv0(x) x = self.bn0(x) x = torch.tanh(x) x = self.pool0(x) x = self.conv1(x) x = self.bn1(x) x = torch.tanh(x) x = self.pool1(x) x = self.conv2(x) x = self.bn2(x) x = torch.tanh(x) x = torch.flatten(x, start_dim=1) x = self.fc0(x) x = self.dropout(x) x = torch.tanh(x) x = self.fc1(x) x = x.view(self.batch, -1) x = nn.functional.softmax(x, dim=1) 그 후 weight의 초기값을 설정하기 위해 weight initialization을 사용했다. 이에 대한 종류로는 torch.nn.init.xavier_ 들을 많이 사용한다. 이는 학습 데이터에서 통계를 기반으로 weight를 계산한다.batchnorm만으로도 성능이 잘 나와서 dropout은 잘 사용하지 않으나, 성능 비교를 위해 마지막 layer에서 dropout을 함으로서 overfitting을 막을 수 있다. 만든 dropout은 fc layer에 추가한다." }, { "title": "[데브코스] 10주차 - DeepLearning Perception", "url": "/posts/perception/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-18 17:40:00 +0900", "snippet": "환경 설정코딩에 앞서 환경을 설정해주어야 한다. anaconda를 통해 가상환경을 설정할 수 있지만, 컴퓨터의 용량 한계로 인해 anaconda를 사용하지 않고도 가상환경을 구축할 수 있는 방법을 소개한다.가상환경 설정 (use virtualenv)설치 및 가상환경 생성pip install virtualenvvirtualenv dev --python=python3.8이 때 3.8 interpreter가 없다고 뜬다면, 3.8 python이 안깔려 있는 것이므로 3.8버전을 깔거나 다른 버전으로 실행한다.가상환경 활성화source dev/Scripts/activate필요한 패키지 설치pip install numpy가상환경 나가기deactivate자신이 설치한 패키지를 저장하기pip freeze &amp;gt; requirements.txt다시 설치pip install -r requirements.txttensor 기초import torchimport numpy as nptensor 만들기def make_tensor(): # int a = torch.tensor([[1, 2],[3, 4]], dtype=torch.int16) # float b = torch.tensor([1.0], dtype=torch.float32) # double c = torch.tensor([3], dtype=torch.float64, device=&quot;cuda:0&quot;) print(a.dtype, b.dtype, c.dtype, c.device)torch.tensor(data, dtype, device) 의 형태로 선언해준다. data는 []를 통해 작성해야 하고, dtype의 경우 float64로 하게 되면 double형태로 만들어진다. 그 뒤에 device로 gpu를 사용할지 cpu를 사용할지 지정해준다.변수의 type을 보려면 type(a)와 a.dtype이 있는데, 전자의 경우 전체 변수의 type을 보는 것이고, dtype의 경우 그 안에 들어있는 데이터 타입을 확인하는 메서드다.tensor 더하기/빼기def sumsub_tensor(): a = torch.tensor([3,2]) b = torch.tensor([5,3]) print(&quot;input : {}, {}\\nsum : {}\\tsub : {}&quot;.format(a,b, a+b, a-b)) # each element sum sum_element_a = a.sum() print(sum_element_a)# ------------------- #input : tensor([3, 2]), tensor([5, 3])sum : tensor([8, 5]) sub : tensor([-2, -1])tensor(5)생성된 tensor을 행렬로 더할 때는 a+b,a-b를 통해 진행하고, 각각의 요소를 더할 때는 sum() 메서드를 사용한다.tensor 곱하기/나누기def muldiv_tensor(): a = torch.arange(0,9,1).view(3,3) b = torch.arange(0,9,1).view(3,3) # mat mul c = torch.matmul(a,b) print(&quot;input : {} \\n{}\\n\\n mul : {}&quot;.format(a,b,c)) # elementwise multiplication d = torch.mul(a,b) print(d)# --------------- # input : tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])mul : tensor([[ 15, 18, 21], [ 42, 54, 66], [ 69, 90, 111]]) tensor([[ 0, 1, 4], [ 9, 16, 25], [36, 49, 64]])view는 차원을 바꿔주는 함수이다. 즉 view(3,3)을 하게 되면 3x3 행렬로 만들어준다. 그 후 matmul을 통해 행렬 곱을 진행한다. 단지 요소 별 곱셈을 하고 싶다면 mul을 사용한다.tensor 차원 바꾸기def reshape_tensor(): a = torch.tensor([2,4,5,6,7,8]) # view b = a.view(2,3) # transpose b_t = b.t() print(&quot;input : {} ,\\t transpose : {}&quot;.format(b, b_t))# -------------- #tensor 접근def access_tensor(): a = torch.arange(1,13).view(4,3) # first row (slicing) print(a[:,0]) # first col print(a[0,:])transform to numpydef transform_numpy(): a = torch.arange(1,13).view(4,3) # array to numpy a_np = a.numpy() print(a_np) # tensor to numpy b = np.array([1,2,3]) b_ts = torch.from_numpy(b) print(b_ts)tensor 결합# 두 dimension이 동일해야 한다.def concat_tensor(): a = torch.arange(1,10).view(3,3) b = torch.arange(10,19).view(3,3) c = torch.arange(19,28).view(3,3) abc = torch.cat([a,b,c],dim=1) print(abc) abc2 = torch.cat([a,b,c], dim=0) print(abc2) print(abc.shape, abc2.shape)tensor 결합 2# 새로운 dimension을 만들면서 결합, 위는 2d -&amp;gt; 2d, 현재는 2d -&amp;gt; 3ddef stack_tensor(): a = torch.arange(1,10).view(3,3) b = torch.arange(10,19).view(3,3) c = torch.arange(19,28).view(3,3) abc = torch.stack([a,b,c], dim=0) # 0index = a, 1index = b, 2index = c abc2 = torch.stack([a,b,c], dim=1) # 0index = a[0,:]+b[0,:]+c[0,:] abc3 = torch.stack([a,b,c], dim=2) # 0index = a[:,0]+b[:,0]+c[:,0] print(abc, &#39;\\n&#39;, abc2, &quot;\\n&quot;, abc3) print(abc.shape, abc2.shape)tensor transposedef transpose_tensor(): a = torch.arange(1,10).view(3,3) at = torch.transpose(a,0,1) print(a,&quot;\\n&quot;,at,&quot;\\n\\n&quot;) b = torch.arange(1,25).view(4,3,2) bt = torch.transpose(b, 0, 2) print(b,&quot;\\n&quot;, bt) # 여러 가지 dimension을 바꿀 떄 bp = b.permute(2,0,1) # 0,1,2 -&amp;gt; 2,0,1 print(b.shape, bp.shape)if __name__ == &#39;__main__&#39;: #make_tensor() #sumsub_tensor() #muldiv_tensor() #reshape_tensor() #access_tensor() #transform_numpy() #concat_tensor() #stack_tensor() #transpose_tensor()image classification using the MNIST Benchmarksdev ⊢ datasets ⊢ dev ⊢ loss ⊢ \\__init__.py ∟ loss.py ⊢ model ⊢ \\__init__.py ⊢ lenet5.py ∟ models.py ⊢ util ⊢ \\__init__.py ∟ tools.py ⊢ img_classify.py ∟ requirements.txtdatasets폴더에는 데이터셋을 저장하고, dev는 가상환경을 위한 폴더이다. loss는 loss function을 선언해주는 파일을 담기위한 폴더이고, model에는 모델들을 저장해놓은 폴더이다. 정의해놓은 모델을들 models.py 파일에서 불러온다. utils는 예측 결과물을 보기 위해 만든 폴더이다. tools.py에는 PIL의 ImageDraw 함수를 통해 결과물을 본다.실제 실행하는 파일은 img_classify.py 이고, 필요한 패키지를 저장해놓은 파일이 requirements.txt이다.importimport torchimport torch.nn as nnfrom torchvision.datasets import MNISTimport torchvision.transforms as transformsfrom torch.utils.data.dataloader import DataLoaderimport torch.optim as optimimport osimport argparseimport sysfrom tqdm.notebook import trangefrom model.models import *from loss.loss import *from util.tools import *parserimport argparseimport sysdef parse_args(): parser = argparse.ArgumentParser(description=&quot;MNIST&quot;) parser.add_argument(&quot;--mode&quot;, dest=&quot;mode&quot;, help=&quot;train or valid or test&quot;, default=None, type=str) parser.add_argument(&quot;--download&quot;, dest=&quot;download&quot;, help=&quot;dataset download&quot;, default=False, type=bool) parser.add_argument(&quot;--odir&quot;, dest=&quot;output_dir&quot;, help=&quot;output directory for train result&quot;, default=&quot;./output&quot;, type=str) parser.add_argument(&quot;--checkpoint&quot;,dest=&quot;checkpoint&quot;, help=&quot;checkpoint for trained model file&quot;, default=None, type=str) parser.add_argument(&quot;--device&quot;,dest=&quot;device&quot;, help=&quot;use device cpu / gpu&quot;, default=&quot;cpu&quot;, type=str) if len(sys.argv) == 1: # python main.py 하나만 했다는 뜻 parser.print_help() sys.exit() args = parser.parse_args() return args터미널에서 실행할 때 매 번 인자들을 바꾸기 힘들기 때문에, 지정을 해줄 수 있도록 argument를 지정해준다.get datasetdef get_data(): download_root = &quot;./datasets&quot; transform = { &quot;train&quot; : transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor(), transforms.Normalize((0.5,),(1.0,)) ]), &quot;test&quot; : transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor(), transforms.Normalize((0.5,),(1.0,)) ]) } # MNIST(root, train=false,transform,...,download ) train_dataset = MNIST(root=download_root, transform=transform[&quot;train&quot;], train=True, download=args.download) valid_dataset = MNIST(root=download_root, transform=transform[&quot;test&quot;], train=False, download=args.download) test_dataset = MNIST(root=download_root, transform=transform[&quot;test&quot;], train=False, download=args.download) return train_dataset, valid_dataset, test_datasetpytorch 내에 데이터셋이 MNIST, ImageNet, caltech 등이 있다. 이를 다운로드하고, transform시켜주는 함수이다.main model output directory 생성, 사용할 device 지정def main(): #print(torch.__version__) if not os.path.isdir(args.output_dir): os.makedirs(args.output_dir + &quot;/model_epoch&quot;, exist_ok=True) if args.device == &quot;cpu&quot;: device = torch.device(&quot;cpu&quot;) else: if torch.cuda.is_available(): print(&quot;gpu&quot;) device = torch.device(&quot;cuda&quot;) else: print(&quot;can not use gpu!&quot;) device = torch.device(&quot;cpu&quot;) dataset, dataloader불러온 데이터셋을 dataloader train_dataset, valid_dataset, test_dataset = get_data() # Make dataloader # image dataset과 annotation을 쉽게 넣어주기 위해 준비된 타입으로 만듦 train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True, drop_last=True, ) valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=False, ) test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=False, )DataLoader에 인자로 다양한 것들이 존재한다. batch_size : batch 크기 지정 shuffle : 데이터의 순서를 섞을지 지정 num_workers : 데이터 로더를 만들어주는 일꾼과 같이 CUDA를 사용할 때 학습과 추론에서 cpu와 gpu 사이에서 동기화를 하면서 전송될 때, 몇개를 쓰냐에 따라 성능이 차이난다. 대체로 cpu 코어수의 절반으로 지정 pin memory : true일 경우, 커스텀 타입의 데이터 요소들을 사용할 때 이를 리턴하기 전에 cuda pinned memory로부터 텐서를 복사한다. drop_last : batch size를 8로 사용하는데, 마지막에 4장이 남는다면 이걸로 batch로 만들 수 없는데, 이를 사용할지 버릴지에 대해 지정 model 불러오기model폴더에서 선언해둔 함수를 통해 모델 불러오기_model = get_model(&quot;lenet5&quot;) train에 대한 과정 # MNIST 데이터 : [1, H, W] if args.mode == &quot;train&quot;: model = _model(batch=8, n_classes=10, in_channel=1, in_width=32, in_height=32, is_train=True) model.to(device) model.train() # optimizer &amp;amp; scheduler optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) criterion = get_criterion(crit=&quot;mnist&quot;, device=device) epochs = 10 for epoch in trange(epochs): total_loss = 0 best_loss = 5 best_epoch = 0 for i, (image, label) in enumerate(train_dataloader): image = image.to(device) label = label.to(device) output = model(image) #print(output) loss_val = criterion(output, label) #print(loss_val) # backpropagation loss_val.backward() optimizer.step() optimizer.zero_grad() total_loss += loss_val.item() # if i % 1000 == 0: # print(&quot;{} epoch {} iter loss : {}&quot;.format(epoch, i, loss_val.item())) total_loss = total_loss / i scheduler.step() print(&quot;-&amp;gt;{} epoch mean loss : {}&quot;.format(epoch, total_loss)) if total_loss &amp;lt; best_loss: best_loss = total_loss best_epoch = epoch torch.save(model.state_dict(), args.output_dir + &quot;/model_epoch/&quot;+&quot;best_epoch.pt&quot;)train의 경우 batch size : 8로 설정 n_classes : 분류할 클래스의 수는 10 in_channel : MNIST 데이터의 경우 [1,H,W]로 이루어진 흑백 이미지이므로 1 in_width,in_height : MNIST 데이터의 경우 크기가 32x32이므로 32 optimizer : 최적화에 사용될 optimizer 알고리즘은 Stochastic Gradient Descent 를 사용하고, 처음 learning rate를 지정한다. 또, SGD에서 사용될 얼마나 lr을 갱신할 지에 대한 momentum도 지정해준다. scheduler : 처음에는 0.01로 시작하지만, 너무 큰 lr이면 학습이 잘 안될 수 있으므로 줄여주기 위한 용도이다. steplr은 단계적으로 lr을 갱신하는 방법으로 step size 마다 gamma 값만큼 줄인다. get_criterion : 지정해둔 mnist에 대한 손실함수를 불러와 저장한다. epoch : 얼마나 반복할지에 대한 값이므로 알아서 설정하면 된다. trange : tqdm에서 지원하는 메서드로 tqdm(range())와 같다. 루프의 진행 정도를 볼 수 있다. to(device) : gpu를 사용하여 학습할 것이므로 to(device)를 사용하여 cuda.tensor로 변경시킨다. 그 후 모델에 집어넣어 결과를 얻는다. 얻어진 결과를 바탕으로 loss를 구하고, total_loss에 더한다. 다 더해진 total_loss를 batch_size로 나누어준다. 한 epoch이 끝나면 scheduler를 진행시킨다. best_loss, torch.save : 가장 낮은 loss를 얻은 epoch을 저장해놓고 나중에 모델로 save한다. eval 과정 elif args.mode == &quot;eval&quot;: model = _model(batch=1, n_classes=10, in_channel=1, in_width=32, in_height=32) # load trained model checkpoint = torch.load(args.checkpoint) model.load_state_dict(checkpoint) model.to(device) model.eval() acc = 0 num_eval = 0 with torch.no_grad(): for i, (image, label) in enumerate(valid_dataloader): image = image.to(device) output = model(image) output = output.cpu() if output == label: acc += 1 num_eval += 1 print(&quot;evaluation score : {} / {}&quot;.format(acc, num_eval)) batch : train처럼 batch_size 단위로 학습시키는 것이 아닌 이미지 1개씩 평가를 진행해야 하므로 batch를 1로 설정 checkpoint : 저장해둔 모델을 불러온다. model.load_state_dict : 모델의 파라미터들을 불러와서 평가를 진행정확도를 얻기 위해 acc과 num_eval을 구하여 출력한다. test 과정 elif args.mode == &quot;test&quot;: model = _model(batch=1, n_classes=10, in_channel=1, in_width=1, in_height=1) checkpoint = torch.load(args.checkpoint) model.load_state_dict(checkpoint) model.to(device) model.eval() with torch.no_grad(): for i, (image, label) in enumerate(test_dataloader): image = image.to(device) output = model(image) output = output.cpu() #print(output) # show result show_img(image.detach().cpu().numpy(),str(output.item()))eval과정과 동일하나, 결과물을 얻기 위해 show_img를 사용하여 image를 본다. 이때, numpy로 변환시키기 위해 gradient 연산과 분리시키고, cuda연산을 했던 결과를 cpu로 변환하고, numpy로 변환한다.실행if __name__ == &quot;__main__&quot;: args = parse_args() main()loss/loss.pyimport torchimport torch.nn as nnimport sysclass MNISTloss(nn.Module): def __init__(self, device = torch.device(&#39;cpu&#39;)): super(MNISTloss, self).__init__() self.loss = nn.CrossEntropyLoss().to(device) def forward(self, output, label): loss_val = self.loss(output, label) return loss_valdef get_criterion(crit = &quot;mnist&quot;, device = torch.device(&#39;cpu&#39;)): if crit is &quot;mnist&quot;: return MNISTloss(device = device) else: print(&quot;unknown criterion&quot;) sys.exit(1)MNISTloss : MNIST에 대한 loss를 선언해둔다. crossentropyloss를 사용할 것이고, loss를 연산하여 리턴한다.get_criterion : 선언해둔 MNISTloss를 불러와 mnist라는 인자를 받을 경우 MNISTloss를 반환하고, mnsit가 아닐 경우 일단은 없다고 출력하고 끝낸다.modellenet5.pylenet5에 대한 레이어들을 선언해둔 파일이다.import torchimport torch.nn as nn# in == inputclass Lenet5(nn.Module): def __init__(self, batch, n_classes, in_channel, in_width, in_height, is_train = False): super().__init__() self.batch = batch self.n_classes = n_classes self.in_channel = in_channel self.in_width = in_width self.in_height = in_height self.is_train = is_train # layer define # convoluation output size : [(W - K + 2P)/S] + 1 # w : input size, k : kernel size, p : padding size, s : stride self.conv0 = nn.Conv2d(self.in_channel, 6, kernel_size=5, stride=1, padding=0) # [(32 - 5 + 2*0) / 1] + 1 = 28 self.conv1 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0) self.conv2 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0) self.pool0 = nn.AvgPool2d(2, stride=2) self.pool1 = nn.AvgPool2d(2, stride=2) self.fc0 = nn.Linear(120, 84) self.fc1 = nn.Linear(84, self.n_classes) # 실제 layer 연산 define def forward(self, x): # x&#39; shape = [B, C, H, W] x = self.conv0(x) x = torch.tanh(x) x = nn.functional.avg_pool2d(x, kernel_size=2, stride=2) # x = self.pool0(x) x = self.conv1(x) x = torch.tanh(x) x = nn.functional.avg_pool2d(x, kernel_size=2, stride=2) # x = self.pool1(x) x = self.conv2(x) x = torch.tanh(x) # change format from 3D to 2D ([B, C, H, W] -&amp;gt; B,C*H*W) x = torch.flatten(x, start_dim=1) x = self.fc0(x) x = torch.tanh(x) x = self.fc1(x) x = x.view(self.batch, -1) x = nn.functional.softmax(x, dim=1) # 학습할 때는 모든 결과를 받아야 함 if self.is_train is False: x = torch.argmax(x, dim=1) return xmodels.py위에서 선언해둔 lenet5 모델을 불러온다. 이를 생성한 이유는 lenet5 이외에 다른 모델도 사용할 때 불러오기 편하도록 하기 위해서이다.from model.lenet5 import Lenet5def get_model(model_name): if(model_name == &quot;lenet5&quot;): return Lenet5 else: print(&quot;not exist this model : {}&quot;.format(model_name))선언해둔 lenet5.py 의 Lenet5를 불러와 모벨을 리턴한다. 만약 선언되어 있지 않은 모델을 인자로 받으면 없다고 출력한다.util/tools.py예측된 결과를 출력하고, 시각화하기 위해 만든 파일이다. 출력을 위해 PIL의 ImageDraw를 사용했다. 입력 받는 데이터를 정규화를 해제하고,from PIL import Image, ImageDrawimport numpy as npimport matplotlib.pyplot as pltdef show_img(img_data, text): # 입력된 값이 0~1로 정규화된 값이므로 255를 곱함 _img_data = img_data * 255 print(_img_data.shape) # (1, 1, 32, 32) # 4d tensor -&amp;gt; 2d _img_data = np.array(_img_data[0,0], dtype=np.uint8) img_data = Image.fromarray(_img_data) draw = ImageDraw.Draw(img_data) # 예측결과을 텍스트로 보기 위해 선언 # draw text in img, center_x, center_y cx, cy = _img_data.shape[0] / 2, _img_data.shape[1] / 2 if text is not None: draw.text((cx,cy), text) plt.imshow(img_data) plt.show()입력된 값들은 pytorch에서의 4dimension이므로 이를 출력하기 위해 2dimension으로 변환한다. 그 후 Image 포맷으로 변경하고, 이미지를 그린다. 여기서 cx,cy는 정답 라벨을 화면 위에 표시하기 위한 것들이다.requirements.txtargon2-cffi==21.3.0argon2-cffi-bindings==21.2.0asttokens==2.0.5attrs==21.4.0backcall==0.2.0beautifulsoup4==4.11.1bleach==5.0.0certifi==2021.10.8cffi==1.15.0charset-normalizer==2.0.12colorama==0.4.4cycler==0.11.0debugpy==1.6.0decorator==5.1.1defusedxml==0.7.1entrypoints==0.4executing==0.8.3fastjsonschema==2.15.3fonttools==4.32.0idna==3.3ipykernel==6.13.0ipython==8.2.0ipython-genutils==0.2.0ipywidgets==7.7.0jedi==0.18.1Jinja2==3.1.1jsonschema==4.4.0jupyter==1.0.0jupyter-client==7.2.2jupyter-console==6.4.3jupyter-core==4.9.2jupyterlab-pygments==0.2.2jupyterlab-widgets==1.1.0kiwisolver==1.4.2MarkupSafe==2.1.1matplotlib==3.5.1matplotlib-inline==0.1.3mistune==0.8.4nbclient==0.6.0nbconvert==6.5.0nbformat==5.3.0nest-asyncio==1.5.5notebook==6.4.10numpy==1.22.3packaging==21.3pandocfilters==1.5.0parso==0.8.3pickleshare==0.7.5Pillow==9.1.0prometheus-client==0.14.1prompt-toolkit==3.0.29psutil==5.9.0pure-eval==0.2.2pycparser==2.21Pygments==2.11.2pyparsing==3.0.8pyrsistent==0.18.1python-dateutil==2.8.2pywin32==303pywinpty==2.0.5pyzmq==22.3.0qtconsole==5.3.0QtPy==2.0.1requests==2.27.1Send2Trash==1.8.0six==1.16.0soupsieve==2.3.2.post1stack-data==0.2.0terminado==0.13.3tinycss2==1.1.1torch==1.11.0+cu113torchaudio==0.11.0+cu113torchvision==0.12.0+cu113tornado==6.1tqdm==4.64.0traitlets==5.1.1typing_extensions==4.2.0urllib3==1.26.9wcwidth==0.2.5webencodings==0.5.1widgetsnbextension==3.6.0" }, { "title": "[lane detection] sliding window를 c++로 구현하기 ", "url": "/posts/lanedetect/", "categories": "Projects, detection", "tags": "detection, lane-detection", "date": "2022-04-17 19:02:00 +0900", "snippet": "github : https://github.com/dkssud8150/LaneDetectpjt개발 언어는 c++이고, lane detection 기법 중 sliding window를 구현해보았다.영상 처리이진화 grayscale - 원본인 컬러 이미지에서 Gray 영상으로 변환 Gaussian Blur - 노이즈를 처리 canny edge - 외곽선 추출 - lowwer threshold는 upper threshold의 2~3배가 적당 Gaussian -&amp;gt; HSV image inRange를 통한 이진화 (Threshold) - 이 때, 명도에 대한 V만 사용하여 차선의 색이나 주변 밝기를 지정해줘야 한다. - e.g. cv2.inRange(hsv, (0,0,50), (255,255,255)) or cv2.inRange(hsv, (0,0,150), (255,255,255)) canny edge Gaussian -&amp;gt; LAB image inRange를 통한 이진화 (Threshold) Canny edge이진화 후 처리 ROI 영역 설정 차선이 존재하는 위치에 지정해야 함, 또 필요없는 부분들을 잘 처리해야 한다. houghlineP로 라인 추출 threshold, maxval 등의 파라미터를 잘 설정해야 함 기울기의 절대값이 너무 작은 건 다른 물체들에 해당할 확률이 크므로 처리 x 오른쪽, 왼쪽 분리 라인들의 대표 직선 찾기 선분의 기울기 평균값, 양끝점 좌표의 평균값을 사용하여 대표직선을 찾는다. 노이즈를 제거해야 멀리 떨어져 있는 이상한 값도 함께 처리하지 않게 된다. 모든 데이터를 반영하여 계산하는 것이 아닌 노이즈 데이터를 찾아 계산에서 제외시켜야 한다. offset을 설정하여 차선 인식하고, 차선의 중간값과 화면의 중앙과 비교하여 핸들링RANSAC 알고리즘을 통해 노이즈를 제거한다.예외 상황 처리 카메라 영상에서 차선이 잡히지 않는 경우 영상처리를 위한 작업공간인 스크린 사이즈를 확대시킨다. 실제 카메라 영상 크기보다 옆으로 넓어진 가상 스크린을 사용하여 작업 e.g. 기존 크기 : 640 x 480, 좌우로 200픽셀씩 확장하여 1040x480 한쪽 차선만 보이는 경우(차선이 끊기거나 추출되지 않을 경우)에는 추출한 한쪽 차선을 활용하여 대칭을 맞춰서 예측한다. 또는 차선은 연속적인 선이므로 지난번 위치를 재사용 새로 찾은 차선의 위치가 너무 많이 차이가 날 경우 갑자기 위치가 크게 바뀔 수 없기 때문에 한계값을 정해 이를 넘어갈 경우 무시하고 지난번 위치를 재사용사용한 차선 인식 알고리즘 - sliding window 알고리즘 순서 ROI 설정 perspective transform hsv -&amp;gt; split and using only V inverse brightness processing gaussian inRange histogram -&amp;gt; argmax abount left and right -&amp;gt; sliding 차선인식 알고리즘 종류Hough transform + RANSACDeep learnging + RANSAC instance segmentationV-ROI 참고자료 : https://yeowool0217.tistory.com/558?category=803755sliding window 코드#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;#include &amp;lt;fstream&amp;gt;#include &amp;lt;algorithm &amp;gt;using namespace std;using namespace cv;vector&amp;lt;Point&amp;gt; matrix_oper(Mat frame, Mat per_mat_tosrc, int lx1, int ly1, int lx2, int ly2, int rx1, int ry1, int rx2, int ry2) { vector&amp;lt;Point&amp;gt; warp_left_line, warp_right_line; int new_lx1, new_ly1, new_lx2, new_ly2; new_lx1 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 0) * lx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 1) * ly1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * lx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ly1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); new_ly1 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 0) * lx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 1) * ly1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * lx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ly1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); new_lx2 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 0) * lx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 1) * ly2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * lx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ly2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); new_ly2 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 0) * lx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 1) * ly2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * lx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ly2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); int new_rx1, new_ry1, new_rx2, new_ry2; new_rx1 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 0) * rx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 1) * ry1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * rx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ry1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); new_ry1 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 0) * rx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 1) * ry1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * rx1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ry1 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); new_rx2 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 0) * rx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 1) * ry2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(0, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * rx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ry2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); new_ry2 = (per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 0) * rx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 1) * ry2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(1, 2)) / (per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 0) * rx2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 1) * ry2 + per_mat_tosrc.at&amp;lt;double&amp;gt;(2, 2)); warp_left_line.push_back(Point(new_lx1, new_ly1)); warp_left_line.push_back(Point(new_lx2, new_ly2)); warp_right_line.push_back(Point(new_rx1, new_ry1)); warp_right_line.push_back(Point(new_rx2, new_ry2)); line(frame, Point(new_lx1, new_ly1), Point(new_lx2, new_ly2), Scalar(0, 255, 255), 2); line(frame, Point(new_rx1, new_ry1), Point(new_rx2, new_ry2), Scalar(0, 255, 255), 2); int offset = 400; int lpos = int((offset - warp_left_line[0].y) * ((warp_left_line[1].x - warp_left_line[0].x) / (warp_left_line[1].y - warp_left_line[0].y)) + warp_left_line[0].x); int rpos = int((offset - warp_right_line[0].y) * ((warp_right_line[1].x - warp_right_line[0].x) / (warp_right_line[1].y - warp_right_line[0].y)) + warp_right_line[0].x); vector&amp;lt;Point&amp;gt; pos; pos.push_back(Point(lpos, rpos)); return warp_left_line, warp_right_line, pos;}vector&amp;lt;Point&amp;gt; n_window_sliding(int left_start, int right_start, Mat roi, Mat v_thres, int w, int h, vector&amp;lt;Point&amp;gt;&amp;amp; lpoints, vector&amp;lt;Point&amp;gt;&amp;amp; rpoints, Mat per_mat_tosrc, Mat frame) { // define constant for sliding window int nwindows = 12; int window_height = (int)(h / nwindows); int window_width = (int)(w / nwindows * 1.5); int margin = window_width / 2; // 양쪽이 인식이 되었다면 초기화하고 다시 입력 vector&amp;lt;Point&amp;gt; mpoints(nwindows); // init value setting int lane_mid = w / 2; int win_y_high = h - window_height; int win_y_low = h; int win_x_leftb_right = left_start + margin; int win_x_leftb_left = left_start - margin; int win_x_rightb_right = right_start + margin; int win_x_rightb_left = right_start - margin; lpoints[0] = Point(left_start, (int)((win_y_high + win_y_low) / 2)); rpoints[0] = Point(right_start, (int)((win_y_high + win_y_low) / 2)); mpoints[0] = Point((int)((left_start + right_start) / 2), (int)((win_y_high + win_y_low) / 2)); // init box draw rectangle(roi, Rect(win_x_leftb_left, win_y_high, window_width, window_height), Scalar(0, 150, 0), 2); rectangle(roi, Rect(win_x_rightb_left, win_y_high, window_width, window_height), Scalar(150, 0, 0), 2); // window search start, i drew the init box at the bottom, so i start from 1 to nwindows for (int window = 1; window &amp;lt; nwindows; window++) { win_y_high = h - (window + 1) * window_height; win_y_low = h - window * window_height; win_x_leftb_right = left_start + margin; win_x_leftb_left = left_start - margin; win_x_rightb_right = right_start + margin; win_x_rightb_left = right_start - margin; int offset = (int)((win_y_high + win_y_low) / 2); int pixel_thres = window_width * 0.2; int ll = 0, lr = 0; int rl = 960, rr = 960; int li = 0; // nonzero가 몇개인지 파악하기 위한 벡터에 사용될 인자 // window의 위치를 고려해서 벡터에 집어넣으면 불필요한 부분이 많아질 수 있다. 어차피 0의 개수를 구하기 위한 벡터이므로 0부터 window_width+1 개수만큼 생성 vector&amp;lt;int&amp;gt; lhigh_vector(window_width + 1); // nonzero가 몇개 인지 파악할 때 사용할 벡터 for (auto x = win_x_leftb_left; x &amp;lt; win_x_leftb_right; x++) { li++; lhigh_vector[li] = v_thres.at&amp;lt;uchar&amp;gt;(offset, x); // 차선의 중앙을 계산하기 위해 255 시작점과 255 끝점을 계산 if (v_thres.at&amp;lt;uchar&amp;gt;(offset, x) == 255 &amp;amp;&amp;amp; ll == 0) { ll = x; lr = x; } if (v_thres.at&amp;lt;uchar&amp;gt;(offset, x) == 255 &amp;amp;&amp;amp; lr != 0) { lr = x; } } int ri = 0; vector&amp;lt;int&amp;gt; rhigh_vector(window_width + 1); for (auto x = win_x_rightb_left; x &amp;lt; win_x_rightb_right; x++) { ri++; rhigh_vector[ri] = v_thres.at&amp;lt;uchar&amp;gt;(offset, x); if (v_thres.at&amp;lt;uchar&amp;gt;(offset, x) == 255 &amp;amp;&amp;amp; rl == 960) { rl = x; rr = x; } if (v_thres.at&amp;lt;uchar&amp;gt;(offset, x) == 255 &amp;amp;&amp;amp; lr != 960) { rr = x; } } // window안에서 0이 아닌 픽셀의 개수를 구함 int lnonzero = countNonZero(lhigh_vector); int rnonzero = countNonZero(rhigh_vector); // 방금 구했던 255 픽셀 시작 지점과 끝 지점의 중앙 값을 다음 window의 중앙으로 잡는다. if (lnonzero &amp;gt;= pixel_thres) { left_start = (ll + lr) / 2; } if (rnonzero &amp;gt;= pixel_thres) { right_start = (rl + rr) / 2; } // 차선 중앙과 탐지한 차선과의 거리 측정 int lane_mid = (right_start + left_start) / 2; int left_diff = lane_mid - left_start; int right_diff = -(lane_mid - right_start);#if 1 // 한쪽 차선의 nonzero가 임계값을 넘지 못할 경우 중간을 기점으로 반대편 차선 위치를 기준으로 대칭 if (lnonzero &amp;lt; pixel_thres &amp;amp;&amp;amp; rnonzero &amp;gt; pixel_thres) { lane_mid = right_start - right_diff; left_start = lane_mid - right_diff; } else if (lnonzero &amp;gt; pixel_thres &amp;amp;&amp;amp; rnonzero &amp;lt; pixel_thres) { lane_mid = left_start + left_diff; right_start = lane_mid + left_diff; }#else // 지난 프레임에서의 픽셀값을 기억하고 nonzero가 임계값을 넘지 못할 경우 지난 프레임의 해당 윈도우 번호의 값을 불러옴 if (lnonzero &amp;lt; pixel_thres &amp;amp;&amp;amp; rnonzero &amp;gt; pixel_thres) { left_start = lpoints[window].x; lane_mid = (right_start + left_start) / 2; } else if (lnonzero &amp;gt; pixel_thres &amp;amp;&amp;amp; rnonzero &amp;lt; pixel_thres &amp;amp;&amp;amp; rpoints[window].x != 0) { right_start = rpoints[window].x; lane_mid = (right_start + left_start) / 2; }#endif // draw window at v_thres rectangle(roi, Rect(win_x_leftb_left, win_y_high, window_width, window_height), Scalar(0, 150, 0), 2); rectangle(roi, Rect(win_x_rightb_left, win_y_high, window_width, window_height), Scalar(150, 0, 0), 2); mpoints[window] = Point(lane_mid, (int)((win_y_high + win_y_low) / 2)); lpoints[window] = Point(left_start, (int)((win_y_high + win_y_low) / 2)); rpoints[window] = Point(right_start, (int)((win_y_high + win_y_low) / 2)); } Vec4f left_line, right_line, mid_line; fitLine(lpoints, left_line, DIST_L2, 0, 0.01, 0.01); // 출력의 0,1 번째 인자는 단위벡터, 3,4번째 인자는 선 위의 한 점 fitLine(rpoints, right_line, DIST_L2, 0, 0.01, 0.01); fitLine(mpoints, mid_line, DIST_L2, 0, 0.01, 0.01); // 방향이 항상 아래를 향하도록 만들기 위해 단위 벡터의 방향을 바꿔준다. if (left_line[1] &amp;gt; 0) { left_line[1] = -left_line[1]; } if (right_line[1] &amp;gt; 0) { right_line[1] = -right_line[1]; } if (mid_line[1] &amp;gt; 0) { mid_line[1] = -mid_line[1]; } int lx0 = left_line[2], ly0 = left_line[3]; // 선 위의 한 점 int lx1 = lx0 + h / 2 * left_line[0], ly1 = ly0 + h / 2 * left_line[1]; // 단위 벡터 -&amp;gt; 그리고자 하는 길이를 빼주거나 더해줌 int lx2 = 2 * lx0 - lx1, ly2 = 2 * ly0 - ly1; int rx0 = right_line[2], ry0 = right_line[3]; int rx1 = rx0 + h / 2 * right_line[0], ry1 = ry0 + h / 2 * right_line[1]; int rx2 = 2 * rx0 - rx1, ry2 = 2 * ry0 - ry1; int mx0 = mid_line[2], my0 = mid_line[3]; int mx1 = mx0 + h / 2 * mid_line[0], my1 = my0 + h / 2 * mid_line[1]; int mx2 = 2 * mx0 - mx1, my2 = 2 * my0 - my1; line(roi, Point(lx1, ly1), Point(lx2, ly2), Scalar(0, 100, 200), 3); line(roi, Point(rx1, ry1), Point(rx2, ry2), Scalar(0, 100, 200), 3); line(roi, Point(mx1, my1), Point(mx2, my2), Scalar(0, 0, 255), 3); vector&amp;lt;Point&amp;gt; warp_left_line(2), warp_right_line(2), pos; warp_left_line, warp_right_line, pos = matrix_oper(frame, per_mat_tosrc, lx1, ly1, lx2, ly2, rx1, ry1, rx2, ry2); return warp_left_line, warp_right_line, pos;}int main(){ VideoCapture cap(&quot;../data/subProject.avi&quot;); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Camera open failed&quot; &amp;lt;&amp;lt; endl; return -1; } //csv 파일 생성 ofstream CSVFILE(&quot;lane_pos.csv&quot;); CSVFILE &amp;lt;&amp;lt; &quot;index&quot; &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; &quot;frame&quot; &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; &quot;lpos&quot; &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; &quot;rpos&quot; &amp;lt;&amp;lt; endl; int index = 0; // src image size int width = cvRound(cap.get(CAP_PROP_FRAME_WIDTH)); int height = cvRound(cap.get(CAP_PROP_FRAME_HEIGHT)); // warped image size int w = (int)width * 1.5, h = (int)height * 1.5; // point about warp transform vector&amp;lt;Point2f&amp;gt; src_pts(4); vector&amp;lt;Point2f&amp;gt; dst_pts(4); // 파란색 선 없는 roi src_pts[0] = Point2f(0, 395); src_pts[1] = Point2f(198, 280); src_pts[2] = Point2f(403, 280); src_pts[3] = Point2f(580, 395); dst_pts[0] = Point2f(0, h - 1); dst_pts[1] = Point2f(0, 0); dst_pts[2] = Point2f(w - 1, 0); dst_pts[3] = Point2f(w - 1, h - 1); // point about polylines vector&amp;lt;Point&amp;gt; pts(4); pts[0] = Point(src_pts[0]); pts[1] = Point(src_pts[1]); pts[2] = Point(src_pts[2]); pts[3] = Point(src_pts[3]); Mat per_mat_todst = getPerspectiveTransform(src_pts, dst_pts); Mat per_mat_tosrc = getPerspectiveTransform(dst_pts, src_pts); Mat frame, roi; vector&amp;lt;Point&amp;gt; warp_left_line(2), warp_right_line(2); vector&amp;lt;Point&amp;gt; lpoints(12), rpoints(12); while (true) { cap &amp;gt;&amp;gt; frame; if (frame.empty()) break; // perspective transform Mat roi; warpPerspective(frame, roi, per_mat_todst, Size(w, h), INTER_LINEAR); // roi box indicate polylines(frame, pts, true, Scalar(255, 255, 0), 2); // 2-1 hsv -&amp;gt; gaussian -&amp;gt; inRange -&amp;gt; canny Mat hsv; Mat v_thres = Mat::zeros(w, h, CV_8UC1); int lane_binary_thres = 125; // contrast : 155 cvtColor(roi, hsv, COLOR_BGR2HSV); // split H/S/V vector&amp;lt;Mat&amp;gt; hsv_planes; split(roi, hsv_planes); Mat v_plane = hsv_planes[2]; // inverse v_plane = 255 - v_plane; // brightness control int means = mean(v_plane)[0]; v_plane = v_plane + (100 - means); GaussianBlur(v_plane, v_plane, Size(), 1.0); inRange(v_plane, lane_binary_thres, 255, v_thres); imshow(&quot;v_thres&quot;, v_thres); // 첫위치 지정 int left_l_init = 0, left_r_init = 0; int right_l_init = 960, right_r_init = 960; for (auto x = 0; x &amp;lt; w; x++) { if (x &amp;lt; w / 2) { if (v_thres.at&amp;lt;uchar&amp;gt;(h - 1, x) == 255 &amp;amp;&amp;amp; left_l_init == 0) { left_l_init = x; left_r_init = x; } if (v_thres.at&amp;lt;uchar&amp;gt;(h - 1, x) == 255 &amp;amp;&amp;amp; left_r_init != 0) { left_r_init = x; } } else { if (v_thres.at&amp;lt;uchar&amp;gt;(h - 1, x) == 255 &amp;amp;&amp;amp; right_l_init == 960) { right_l_init = x; right_r_init = x; } if (v_thres.at&amp;lt;uchar&amp;gt;(h - 1, x) == 255 &amp;amp;&amp;amp; right_r_init != 960) { right_r_init = x; } } } int left_start = (left_l_init + left_r_init) / 2; int right_start = (right_l_init + right_r_init) / 2; vector&amp;lt;Point&amp;gt; pos; warp_left_line, warp_right_line, pos = n_window_sliding(left_start, right_start, roi, v_thres, w, h, lpoints, rpoints, per_mat_tosrc, frame); imshow(&quot;src&quot;, frame); imshow(&quot;roi&quot;, roi); //csv 파일 생성 int frame_number = cap.get(CAP_PROP_POS_FRAMES) - 1; if (frame_number % 30 == 0) { int lpos = pos[0].x; int rpos = pos[1].x; CSVFILE &amp;lt;&amp;lt; index &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; frame_number &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; lpos &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; rpos &amp;lt;&amp;lt; endl; index++; } if (waitKey(10) == 27) break; } cap.release(); destroyAllWindows();}다양한 기능을 구현해보기 위해 github를 사용했고, 기능마다의 branch를 생성했다.요약 슬라이딩 윈도우를 통한 ROI 설정 warpPerspective HSV를 통한 이진화 V 평면만 사용 반전을 통해 더 잘 탐지 평균 밝기 유지 슬라이딩 윈도우 사용 첫 시작점을 잡아줄 때, max_element를 사용하면 동일한 크기의 픽셀 중 가장 앞의 위치를 추출하기에 다른 방법을 사용 초기값 설정을 위해 왼쪽은 0, 오른쪽은 ROI 가로 사이즈인 960으로 지정 255인 위치 중 위치 지정이 안되어 있다면 해당 인덱스를 지정 해당 인덱스가 지정된 후부터 255가 끝날 때까지 끝 점을 지정해줌 255로 되어있다가 0이 되면 if문이 종료된다. 차선의 인덱스들을 통해 중앙값을 차선의 중앙으로 설정 slidiing window 진행 window개수는 최대한 줄여서 변화에 잘 적응할 수 있도록 12개 정도로 지정 window 가로와 세로는 유동적인 설정을 위해 직접 지정이 아닌 window 개수에 따라 변하도록 설정 margin은 중앙값을 기준으로 가로 길이/2이다. 즉, 중앙값을 입력으로 받았기 때문에 좌우 +- margin하여 window 좌측, 우측 좌표를 지정함 기존의 sliding window는 윈도우를 만들고 나서 해당 윈도우에서 다음 윈도우의 시작 위치를 찾아주었다. 이렇게 하면 변화에 둔해지는 현상 발생 따라서 탐색할 높이인 offset을 먼저 지정해준 후 탐지 하고 나서 그림을 그려준다. 이렇게 하면 변화에 적응을 더 잘 할 뿐더러 코너를 나름 잘 따라감 이 때도, 차선을 찾기 위해 오른쪽 차선의 시작점, 끝점을 탐지하여 지정, 왼쪽 차선의 시작점, 끝점을 탐지하고, 해당 offset에서의 nonzero값을 구해서 임계값보다 높으면 탐지한 결과값으로 진행하고, 임계값보다 낮으면 지난 픽셀의 값을 사용하거나, 반대편 차선의 대칭으로 만들어줌 이렇게 탐지한 왼쪽, 오른쪽 차선과 그로인해 구해진 중앙 차선을 변수에 저장 fitline을 통해 직선을 구한다. 이 때 출력되는 값의 0,1번째 값은 단위 벡터, 2,3번째 값은 선들의 중간 픽셀을 출력해준다. 방향을 일정하게 맞추기 위해 무조건 방향벡터에서의 y를 -로 맞춰주었다. 이를 통해 선 위의 점을 추출하고, 단위 벡터는 중앙값을 기준으로 방향이 맞춰져 있기 때문에, h로곱하는 것이 아닌 중앙값에서 h/2*단위벡터를 구했다. 중앙 픽셀과 끝 픽셀의 차를 통해 맨 아래 점을 찾아서 ROI화면의 맨 위 아래로 직선을 그렸다. 출력값을 통해 ROI를 다시 frame 형태로변환하는 행렬을 통해 원본 frame에 점 좌표를 그리거나, 직선을 그리고자 햇지만, 아직 수행하지 못했다. ROI를 파란색 선이 포함되도록 자르면 차가 조금만 움직여도 차선을 탐지하기 어려워지기 때문에 높이를 395에서 잘라서 ROI를 만들었다. 아까 그린 좌우 차선의 직선 색을 통해 차선 중앙의 x좌표를 구하고 csv파일로 입력한다. " }, { "title": "[데브코스] 9주차 - Docker (chroot, pseudo path)", "url": "/posts/docker/", "categories": "Classlog, devcourse", "tags": "devcourse, docker", "date": "2022-04-15 15:40:00 +0900", "snippet": "chrootchange root directory의 약자로 root dir를 특정 디렉토리로 변경하는 기능이다. UNIX command, c언어의 형태로 존재한다. 초기 chroot의 경우 SVr4에서 등장했고, system 설치나 복구 등에 대해 사용되었다. 부수적으로 jail(감옥)의 기능을 가지고 있다.특정 폴더를 생성하고, bin,etc,lib,usr 등 폴더를 생성하고 chroot / /mnt/chroot 를 하게 되면 루트 디렉토리가 /mnt/chroot로 변경된다. 이 후 /usr/local로 이동하면 실제로는 /mnt/chroot/usr/local로 이동이 되는 셈이다.chroot는 부수적 기능으로서 보안적 측면의 격리 기능을 가지고 있다. 이를 sandbox의 개념이라고 하는데, 특서한 목적을 위해 격리된 형태의 공간을 의미한다. chroot를 사용하게 되면 특정 디렉토리 안에서 격리가 되고, 동일한 프로그램을 다른 환경으로 복제할 수 있도록 해준다. 구현체 중에 sandboxf라는 이름이 있는데, 이것과 다른 기능이므로 주의해야 한다. 준비 작업1.먼저 ftp 서비스를 제공하는 vsftpd를 설치한다.$ sudo apt -y install vsftpd2.설치가 되었는지 확인한다.$ sudo systemctl is-active vsftpdactive2-1.active 상태가 아니라면 다음과 같이 명령한다.$ sudo systemctl start vsftpd3.ftp클라이언트 프로그램인 filezilla를 설치 후 실행$ sudo apt -y install filezilla$ filezilla설치 후 실행하면 창이 뜨는데, 좌측 상단에 제일 왼쪽 버튼을 누르면 사이트 관리자가 나온다. 여기서 새 사이트를 누르고 이름은 아무렇게나 넣으면 된다. ip주소는 127.0.0.1, 포로토콜은 FTP, 로그온은 일반으로 설정 후 시스템에 있는 일반 유저명과 암호를 넣는다. 이 때, 중요한 것은 한글이 깨지는 것을 방지하기 위해 문자셋을 설정해야 한다. 문자셋 탭에 가서 utf-8로 강제 설정을 선택한다.접속이 완료되면 창이 2개 나온다. 왼쪽이 local, 오른쪽이 remote이다. 오른쪽의 홈 디렉토리를 확인한 후 연결을 끊는다.4.vsftpd.conf 파일을 수정한다.$ sudo vim /etc/vsftpd.conf그 후 아래 2가지를 추가한다.chroot_local_user=YESallow_writeable_chroot=YES설정을 저장한 뒤 vsftpd를 재시작한다.$ sudo systemctl restart vsftpd$ sudo systemctl status vsftpd5.filezilla에 재접속 하여 home 디렉토리를 확인한다.확인해보면 오른쪽 화면에 디렉토리가 /로 되어 있을 것이다.chroot는 rescue모드 부팅에서 사용될 수 있다. 예를 들어 A와 B시스템이 있는데, A가 고장나서 부팅이 안되고, B는 정상일 때 B에 A의 디스크를 붙이고 부팅한 뒤 A를 B에 마운트한 후 chroot /mnt를 하면 B디스크에 A시스템이 붙어서 A에 있는 파일을 실행시킬 수 있다.Isolation 격리의 필요성시스템 내에 존재하는 자원은 한정적이다. 한정적인 자원을 효율적으로 분배하면 시스템의 가용성을 올릴 수 있다. 현대적인 OS는 프로세스가 독립적인 공간을 가지게 해준다. 즉 고유한 공간이기 때문에 다른 사람들이 볼 수 없다. 그러나 외부 통신을 위해 IPC를 사용해야 해서 I/O 비용이 높아진다. 여러 프로세스가 협동해야 하는 프로그램에서는 단점이 더 커진다. 예를 들어 DBMS이나 server 네트워크를 다룰 때, 한 시스템에 2개의 DBMS를 구동하기 힘들다. 그 이유는 DBMS를 구성하는 각종 프로세스들이 특정 디렉토리를 독점적으로 사용하는 경우가 많기 때문이다. 이를 함께 사용하면 충돌이 발생한다.Isolation의 활용 보안, 자원 관리적 측면특정 파일 경로의 접근을 제한할 수 있다. 예를 들어 위에서 했듯이 root directory를 변경하여 특정 공간 안에서만 이동이 가능하도록 만들면 독립적인 공간을 가질 수 있다. 또는 호스팅 업체라면 고성능의 컴퓨터 1대로 여러 사업자에게 DB나 웹을 제공할 수 있다. 호환, 충돌 측면동일한 디렉토리를 사용하는 프로세스는 독립된 실행하거나, 서로 다른 버전의 파일을 사용하는 프로세스를 사용하고자 한다면 격리된 공간을 사용해야 할 것이다. Name spacechroot에는 chroot 기능하나만 있는 것이 아니라 name space라는 기능이 또 있다.HistoryPlan 9이라는 기업에서 1992년도에 분산 컴퓨팅 시스템으로서 local system, remote system을 계층적 file system으로 표현한 기능을 만들었다. 예를 들어, /usr 폴더 안에 /a/aa/aaa 폴더가 있고, /b/bb/bbb 폴더가 있는 것이 이 계층적 file system이다. 이를 linux에도 구현이 되어 있다.isolated resouces(독립된 자원)를 NS의 계층적 file system 형태로 구현했다. 방식의 위의 폴더 방식과 동일하다. 이 name space를 줄여서 NS로 표기하기도 한다.Namespace 종류 mount UTS(UNIX Time-sharing : 유닉스 시분할 시스템) UTS는 가상머신과 비슷한 것으로 호스트 네임을 분리하여 하나의 시스템이 여러 개의 이름을 가질 수 있다. IPC network PID user cgroup 2006년도에는 process container로 만들어져, 2007년에 cgroup으로 이름이 바뀌었다. group별로 가상화된 공간을 만들고 자원을 제약할 수 있게 한다. 다른 그룹은 격리되어있으므로 물리적으로 다른 호스트처럼 인식한다. docker, hadoop, systemd 등 수많은 프로젝트들이 cgroup을 사용한다. namespace 관리하는 명령어 unshare lsns nsenterNamespace 기본 작동unshare과 lsns를 사용해보고자 한다. unshare의 경우 고유의 공간을 만들고 그 안에서 프로그램을 실행할 수 있게 해주는 기능이고, lsns를 통해 namespace를 살펴본다.# unshare -pf --mount-proc /bin/bash격리된 프로세스를 만드는데, 그 프로세스의 이름이 bash이다.옵션 -p : –pid = pid를 격리시켜서 새로운 pid를 생성 -f : –fork = 자식 프로세스를 만들어서 실행 –mount-proc : proc파일 시스템을 고유의 공간으로 가져가기 프로세스 컨트롤 블록을 고유하게 사용 # ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 57 pts/0 00:00:00 ps# exitpid는 낮은 숫자부터 실행된다. network를 격리해보기# ss -nltpState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 5 127.0.0.1:631 0.0.0.0:* LISTEN 0 32 *:21 *:* LISTEN 0 128 [::]:22 [::]:* LISTEN 0 5 [::1]:631 [::]:* host에서 5000번 포트를 listen# nc -l 5000 &amp;gt;nc_host_outout.txt &amp;amp; [1] 32400# ss -nltpState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 5 127.0.0.1:631 0.0.0.0:* LISTEN 0 1 0.0.0.0:5000 0.0.0.0:* users:((&quot;nc&quot;,pid=32400,fd=3)) LISTEN 0 32 *:21 *:* LISTEN 0 128 [::]:22 [::]:* LISTEN 0 5 [::1]:631 [::]:* # unshare -n /bin/bashport list, host의 PID까지 다 보임# ps PID TTY TIME CMD 32419 pts/0 00:00:00 sudo 32420 pts/0 00:00:00 bash 32476 pts/0 00:00:00 ps격리를 했기 때문에 나오지 않음# ss -nltState Recv-Q Send-Q Local Address:Port Peer Address:Port다시 listen# nc -l 5000 &amp;gt;nc_host_outout.txt &amp;amp; [1] 32479# ss -nltState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 1 0.0.0.0:5000 0.0.0.0:* 위의 창은 그대로 유지한채로 터미널을 한개 더 켜서 확인해본다.# lsns NS TYPE NPROCS PID USER COMMAND4026531835 cgroup 330 1 root /sbin/init splash4026531836 pid 330 1 root /sbin/init splash4026531837 user 330 1 root /sbin/init splash4026531838 uts 330 1 root /sbin/init splash...4026532583 net 2 32420 root /bin/bash4026532672 mnt 1 1274 root /usr/lib/bluetooth/bluetoothd...방금 연결한 net /bin/bash가 보인다. 여기서 unshare -pf --mount-proc /bin/bash를 하면 NPROCS가 올라가는 것을 볼 수 있다.Docker LXC(Linux Container) 현재 cononical에서 공식적으로 지원하고 있고, 초창기 리눅스 컨테이너 기술의 발판이 되었다. docker의 경우도 초창기에는 lxc를 사용했다. docker는 container runtime 기술로 2008년에 설립하여 2013년에 릴리즈했다. docker는 container를 세련된 방식으로 구현한 제품의 일종으로 격리된 자원의 묶음과 런타임으로 구성된다. 기본적으로 C/S 구조를 가지므로 daemon이 작동된다.docker는 host os위에서 작동되는 격리된 프로세스의 일종이므로 virtual machine과 달리 memory, file system 등의 문제가 발생하지 않는다. 그러나 단점으로는 daemon으로 작동하기 때문에 daemon이 버그가 걸리면 밑에 있던 모든 container가 죽어버린다. 또 docker는 관리자 권한으로 실행해야 한다. 따라서 보안적 문제가 발생하기 쉽다.왼쪽은 conatiner의 작동방식이고, 오른쪽은 virtual machine의 작동 방식이다. container는 바로 바로 전달받아서 동작하나, virtual machine의 경우 hypervisor를 통해 전달해서 virtual machine으로 들어갔다가 다시 나온다. 이 hypervisor과 virtual machine간의 통신이 성능이 매우 저하되는 곳이다.podmandocker의 보안성문제로 인해 대안책으로 podman이라는 linux container가 있다. podman은 RedHat에서 지원하고, daemon을 사용하지 않고, 관리자 권한도 사용하지 않으며, systemd와 잘 어울리므로 중앙집중이 잘된다. container 기술은 매우 자주 변화하는 기술이다. 따라서 1가지만 사용하는 것이 아닌 트랜드에 맞춰 바꿔가며 사용하는 것이 좋다.VirtualizationVirtual machine가상 머신도 격리의 일종이다. 가상머신의 경우는 full virtualization을 사용한다. 가상머신은 소프트웨어로 가상화된 하드웨어를 구현시켜준다. 이를 통해 격리된 공간을 제공한다. 하지만 이로 인해 실행을 하면 성능이 너무 안좋아진다. 또 독점적인 자원을 점유한다. 즉 서로 VM끼리는 자원을 공유할 수 없다.Sandbox사전적 의미 그대로 격리된 공간을 의미한다. 다양한 방법이 가능하다. (VM, container, chroot ,…), 이 sandbox는 테스트 유닛으로서 격리된 공간, 보안 공간, 복제된 서비스 공간에 사용된다. 장점프로그램이 작동하기 위해서 많은 외부 자원을 필요로 한다. 예를 들어 게임을 설치할 때 directX, VC+ redist, library 2014,,, 등에 대한 버전을 여러 개 설치해야 하는데, 이에 대한 충돌이 발생할 수 있는데, 이를 해결해줄 수 있다.단지 격리만을 목적이라면 VM을 사용할 필요도 없이 lightweight container을 사용하면 된다. 이는 공유할 부분은 공유하고, 따로 사용할 부분은 따로 사용할 수 있다. 그러나 host OS와 공유하는 부분이 있으므로 이기종의 OS를 사용할 수 없다.docker 설치docker 사이트old version 삭제docker 버전이 여러 개 있으면 충돌이 발생하기 때문에 옛살 버전을 삭제해야 한다.$ su -# apt list docker{,-engine,.ip} containerd runcListing... Donecontainerd/bionic-updates,bionic-security 1.5.5-0ubuntu3~18.04.2 amd64docker/bionic 1.5-1build1 amd64runc/bionic-updates 1.0.1-0ubuntu2~18.04.1 amd64[installed] 되어 있으면 제거한다.필요 패키지 설치 및 key file 추가# apt update# apt -y install apt-transport-https ca-certificates curl gnupg lsb-release# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg위 명령이 실행되면 /usr/share/keyrings/docker-archive-keyring.gpg에 key파일이 생성된다. 이는 https에 필요한 파일이다.APT 저장소 source.list 추가echo &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; &amp;gt; /etc/apt/sources.list.d/docker.list이 명령이 성공하면 /etc/apt/source.list.d에 docker.list가 생성된다. deb : 데비안 패키지 arch=amd64 : intel 호환 x86 64bit를 사용 signed-by= : 앞에서 저장한 key 파일 위치 url : 다운로드 경로 lsb_Reelase -cs : binoic stable 파일을 다운docker engine 설치# apt update# apt -y install docker-ce docker-ce-cli containerd.io docker daemon 실행 확인# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: e Active: active (running) since Fri 2022-04-15 16:39:55 KST; 14s ago Docs: https://docs.docker.com Main PID: 8891 (dockerd) Tasks: 13 CGroup: /system.slice/docker.service └─8891 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containdocker 실행# docker run hello-worldUnable to find image &#39;hello-world:latest&#39; locallylatest: Pulling from library/hello-world2db29710123e: Pull complete Digest: sha256:10d7d58d5ebd2a652f4d93fdd86da8f265f5318c6a73cc5b6a9798ff6d2b2e67Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ hello-world라는 image가 없으므로 다운 docker client가 docker daemon에 접속 docker daemon이 hello-world image를 가져온다. (amd64) image를 통해 새로운 컨테이너를 생성 만든 이미지 안에서 실행된 결과를 터미널로 가져온다.완료되면 $ docker run -it ubuntu bash 를 실행해보라는 말이 나온다. 이를 실행하면 docker안으로 들어갈 수 있게 된다.# docker run -it ubuntu bashUnable to find image &#39;ubuntu:latest&#39; locallylatest: Pulling from library/ubuntue0b25ef51634: Pull complete Digest: sha256:9101220a875cee98b016668342c489ff0674f247f6ca20dfc91b91c0f28581aeStatus: Downloaded newer image for ubuntu:latestroot@3102cd9f9b3f:/# 옵션 -i : interactive mode (open stdin) = shell을 쓸 수 있게 하는 옵션 -t : terminal (allocate a pseudo-tty , stdio) = 터미널을 쓸 수 있게 하는 옵션root@3102cd9f9b3f:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 9 pts/0 00:00:00 psroot@3102cd9f9b3f:/# cdroot@3102cd9f9b3f:~# pwd/rootroot@3102cd9f9b3f:~# ls -altotal 16drwx------ 2 root root 4096 Apr 5 05:02 .drwxr-xr-x 1 root root 4096 Apr 15 07:50 ..-rw-r--r-- 1 root root 3106 Dec 5 2019 .bashrc-rw-r--r-- 1 root root 161 Dec 5 2019 .profileroot@3102cd9f9b3f:~# exitexit이 때, root는 docker안의 루트이고, ps 를 치면 이는 격리된 공간으로서 1번을 받게 된다. ls -al을 통해 파일을 보게 되면 실제 우리 host os의 root 디렉토리가 아니라는 것을 알 수 있다.# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3102cd9f9b3f ubuntu &quot;bash&quot; 4 minutes ago Exited (0) 4 seconds ago interesting_chandrasekharcf5599884127 hello-world &quot;/hello&quot; 13 minutes ago Exited (0) 13 minutes ago practical_antonelli생성된 시간과 현재 상태를 보여준다. 여기서 중요한 것은 container의 Id와 이름 둘다 나오고 있으며, 이것들을 통해 구분이 가능하다.docker CLI(Command Line Interface)docker는 기본적으로 docker이라는 binary명령을 사용한다. docker는 docker daemon과 통신하기 때문에 먼저 daemon을 실행시켜줘야 한다. 추가적으로 docker group을 supplemetary group에 포함시켜야 사용이 가능하다. 즉 나의 일반 유저의 그룹에 추가해야 한다.$ whoamijhyoon$ sudo usermod -aG docker jhyoon$ iduid=1000(jhyoon) gid=1000(jhyoon) groups=1000(jhyoon),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare)$ id jhyoonuid=1000(jhyoon) gid=1000(jhyoon) groups=1000(jhyoon),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),999(docker)그냥 id에서는 docker가 나오지 않는 이유는 id는 런타임된 부분을 보여주고 이름까지 타이핑하면 설정된 그룹을 표시해주기 때문이다. 그룹 추가 후 session을 재생성해야 보인다. 즉, 재로그인을 해야 하므로 컴퓨터를 재부팅을 하거나 다음과 같은 방법을 사용한다. X Window 로그아웃 (우측 상단에 logout) &amp;lt;CTRL - ALT - F4&amp;gt;를 눌러 tty4로 이동한 뒤 console에서 root로 로그인 systemctl restart gdm &amp;lt;cTRL - ALT - F1&amp;gt;를 눌러 X window 로그인그 후 Docker가 들어가 있는지 확인한다.$ iduid=1000(jhyoon) gid=1000(jhyoon) groups=1000(jhyoon),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),999(docker)여기서 안나오면, 그냥 재부팅하기를 추천한다.이를 통해 루트가 아닌 개인 계정으로도 docker를 실행할 수 있게 되었다.$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3102cd9f9b3f ubuntu &quot;bash&quot; 25 minutes ago Exited (0) 20 minutes ago interesting_chandrasekharcf5599884127 hello-world &quot;/hello&quot; 34 minutes ago Exited (0) 34 minutes ago practical_antonelli그냥 ps를 치면 현재 진행중인 컨테이너를 보여주는데, 여기에 -a 옵션을 추가하면 종료된 컨테이너도 보여준다. image 부분이 컨테이너 이미지, Command는 컨테이너 안에서 실행된 명령어이다. status는 명령어에 대한 출력이다.0이면 성공을 의미한다. 이름은 랜덤으로 주어지는 이름이다.$ docker run hello-world...$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESdcb316d5d2f6 hello-world &quot;/hello&quot; 40 seconds ago Exited (0) 25 seconds ago happy_euler3102cd9f9b3f ubuntu &quot;bash&quot; 30 minutes ago Exited (0) 25 minutes ago interesting_chandrasekharcf5599884127 hello-world &quot;/hello&quot; 39 minutes ago Exited (0) 38 minutes ago practical_antonelli$ docker rm dcb316d5d2f6rm 명령을 통해 컨테이너를 삭제할 수 있다. 여기서 id또는 이름을 지정해주면 된다.컨테이너의 이름을 직접 지정해줄 수 있다.$ docker run --name hello-world_01 hello-world...$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESdcb316d5d2f6 hello-world &quot;/hello&quot; 40 seconds ago Exited (0) 25 seconds ago hello-world_013102cd9f9b3f ubuntu &quot;bash&quot; 30 minutes ago Exited (0) 25 minutes ago interesting_chandrasekharcf5599884127 hello-world &quot;/hello&quot; 39 minutes ago Exited (0) 38 minutes ago practical_antonelli$ docker rm hello-world_01이처럼 이름을 직접 지정해주면 사용 용도나 제작 의도를 알 수 있다.10개의 hello-world 컨테이너를 실행해보자.$ for ii in {1..10}; do docker run --name hello-world_${ii} hello-world; done$ docker ps -afilter 기능docker cLI의 option에는 filter 기능이 제공한다. 모든 명령어에 제공되는 것은 아니지만, ps에서는 제공된다.$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5e64b7abf4f5 hello-world &quot;/hello&quot; 2 minutes ago Exited (0) 2 minutes ago hello-world_10912c9f14910a hello-world &quot;/hello&quot; 2 minutes ago Exited (0) 2 minutes ago hello-world_9ccdda8a7d6e7 hello-world &quot;/hello&quot; 2 minutes ago Exited (0) 2 minutes ago hello-world_87e8f2369071f hello-world &quot;/hello&quot; 2 minutes ago Exited (0) 2 minutes ago hello-world_7eed82f62df9d hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago hello-world_664fb8ef6f1e3 hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago hello-world_5a93fe3dbd077 hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago hello-world_492e289160f8f hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago hello-world_35115345eec80 hello-world &quot;/hello&quot; 4 minutes ago Exited (0) 4 minutes ago hello-world_2f41fdc72a8f5 hello-world &quot;/hello&quot; 4 minutes ago Exited (0) 4 minutes ago hello-world_1dcb316d5d2f6 hello-world &quot;/hello&quot; 10 minutes ago Exited (0) 9 minutes ago happy_euler3102cd9f9b3f ubuntu &quot;bash&quot; 39 minutes ago Exited (0) 35 minutes ago interesting_chandrasekharcf5599884127 hello-world &quot;/hello&quot; 48 minutes ago Exited (0) 48 minutes ago practical_antonelli$ docker ps -af &#39;name=hello-world_[1-3]&#39; PORTS NAMES5e64b7abf4f5 hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 2 minutes ago hello-world_1092e289160f8f hello-world &quot;/hello&quot; 4 minutes ago Exited (0) 4 minutes ago hello-world_35115345eec80 hello-world &quot;/hello&quot; 4 minutes ago Exited (0) 4 minutes ago hello-world_2f41fdc72a8f5 hello-world &quot;/hello&quot; 5 minutes ago Exited (0) 5 minutes ago hello-world_1glob pattern패턴을 지원하는 것을 볼 수 있다. 그러나 extglob은 지원하지 않는다.filter에는 name외에 id, label, status 등이 가능하다.필터와는 비슷한 기능으로 format option을 사용하면 원하는 column만 뽑을 수 있다.$ docker ps -af &#39;name=hel&#39; --format &quot; &quot;5e64b7abf4f5 hello-world Exited (0) 8 minutes ago hello-world_10912c9f14910a hello-world Exited (0) 8 minutes ago hello-world_9ccdda8a7d6e7 hello-world Exited (0) 8 minutes ago hello-world_87e8f2369071f hello-world Exited (0) 9 minutes ago hello-world_7eed82f62df9d hello-world Exited (0) 9 minutes ago hello-world_664fb8ef6f1e3 hello-world Exited (0) 9 minutes ago hello-world_5a93fe3dbd077 hello-world Exited (0) 10 minutes ago hello-world_4formatting에 사용될 수 있는 항목은 많다. .ID .Image .Command .CreatedAt .Ports .Status .State앞에 항상 .이 붙어 있고, 대/소문자 구분을 반드시 해줘야 한다.삭제할 때 format을 사용하여 삭제해준다.$ docker ps -af &#39;name=hello&#39; --format &quot;&quot;$ docker ps -af &#39;name=hello&#39; --format &quot;&quot; | xargs docker rmhello-world_10hello-world_9hello-world_8hello-world_7hello-world_6hello-world_5hello-world_4hello-world_3hello-world_2hello-world_1대신 을 사용해도 된다. xargs를 통해 파이프 앞에서 출력되는 결과를 인자로 받아주어 삭제한다. 앞에서 실행한 결과의 hello-world_10을 docker rm hello-world_10로 실행된다.rm을 통해 컨테이너는 삭제했지만, 컨테이너에서 사용되었던 images를 지우지는 않는다. 그래서 rmi를 통해 삭제한다.$ docker rmi hello-worldError response from daemon: conflict: unable to remove repository reference &quot;hello-world&quot; (must force) - container cf5599884127 is using its referenced image feb5d9fea6a5이 때 에러가 나오는 이유는 container가 존재하기 때문에 실패한 것이다. 강제로 지울 수 있지만, 되도록이면 container를 삭제한 뒤 image를 삭제해야 한다.$ docker ps -af &#39;ancestor=hello-world&#39; --format &quot;&quot; | xargs docker rmhappy_eulerpractical_antonelli$ docker rmi hello-worldUntagged: hello-world:latestUntagged: hello-world@sha256:10d7d58d5ebd2a652f4d93fdd86da8f265f5318c6a73cc5b6a9798ff6d2b2e67Deleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412Deleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359anscestor은 이미지를 통해 검색하는 것이다. hello-world를 사용하는 컨테이너를 모두 삭제한다.docker image$ docker imageUsage: docker image COMMANDManage imagesCommands: build Build an image from a Dockerfile history Show the history of an image import Import the contents from a tarball to create a filesystem image inspect Display detailed information on one or more images load Load an image from a tar archive or STDIN ls List images prune Remove unused images pull Pull an image or a repository from a registry push Push an image or a repository to a registry rm Remove one or more images save Save one or more images to a tar archive (streamed to STDOUT by default) tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGERun &#39;docker image COMMAND --help&#39; for more information on a command.현재는 아직 이미지를 직접 생성하지 않을 것이므로 운용 명령인 inspect, load, ls, pull, rm, save 를 중점적으로 보도록 하자.ls$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 825d55fb6340 9 days ago 72.8MB REPOSITORY : docker image 저장소 이름 TAG : 태그 이름 ( 버전 )pull$ docker image pull nginxUsing default tag: latestlatest: Pulling from library/nginxc229119241af: Pull complete 2215908dc0a2: Pull complete 08c3cb2073f1: Pull complete 18f38162c0ce: Pull complete 10e2168f148a: Pull complete c4ffe9532b5f: Pull complete Digest: sha256:2275af0f20d71b293916f1958f8497f987b8d8fd8113df54635f2a5915002bf1Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 825d55fb6340 9 days ago 72.8MBnginx latest 12766a6745ee 2 weeks ago 142MBrun에는 이미지가 없으면 Pull하는 기능이 포함되어 있다. 그러나 run이 목적이 아니라 save나 build를 하기 위한 목적인 경우 pull한다.inspect이미지 안에 있는 설정이나 환경을 보기 위한 명령이다.$ docker image inspect nginx[ { &quot;Id&quot;: &quot;sha256:12766a6745eea133de9fdcd03ff720fa971fdaf21113d4bc72b417c123b15619&quot;, &quot;RepoTags&quot;: [ &quot;nginx:latest&quot; ], &quot;RepoDigests&quot;: [ &quot;nginx@sha256:2275af0f20d71b293916f1958f8497f987b8d8fd8113df54635f2a5915002bf1&quot; ], &quot;Parent&quot;: &quot;&quot;, &quot;Comment&quot;: &quot;&quot;, &quot;Created&quot;: &quot;2022-03-29T16:02:44...이렇게 하면 모든 환경을 출력하는데, 특정 포맷만 출력할 수도 있다.$ docker image inspect -f &#39;&#39; nginx[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin NGINX_VERSION=1.21.6 NJS_VERSION=0.7.2 PKG_RELEASE=1~bullseye]$ docker image inspect -f &#39;&#39; nginx[&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,&quot;NGINX_VERSION=1.21.6&quot;,&quot;NJS_VERSION=0.7.2&quot;,&quot;PKG_RELEASE=1~bullseye&quot;]json 포멧으로 보여달라 하면 json 옵션을 추가하면 된다.save/loaddocker에서 큰 용량을 다운받거나 계속 다운/삭제를 반복하게 되면 해당 ip가 불이익을 받을 수 있다. 그래서 자주 사용하는 이미지는 저장해놓는 것이 좋다. save를 하면 stdout형태로 출력되므로 redirection 을 통해 저장하거나 옵션 -o 를 사용한다.$ docker image save nginx &amp;gt; docker_nginx_1.21.6.tar$ file docker_nginx_1.21.6.tardocker_nginx_1.21.6.tar: POSIX tar archiveimage가 tar파일로 저장된다. 작성할 때 inspect를 통해 버전을 확인한 후 저장하는 것이 좋다.불러올 때는 stdin으로 불러와야 한다.$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 825d55fb6340 9 days ago 72.8MBnginx latest 12766a6745ee 2 weeks ago 142MB$ docker image rm nginxUntagged: nginx:latestUntagged: nginx@sha256:2275af0f20d71b293916f1958f8497f987b8d8fd8113df54635f2a5915002bf1Deleted: sha256:12766a6745eea133de9fdcd03ff720fa971fdaf21113d4bc72b417c123b15619Deleted: sha256:3ea962f6f388096ab9798790d363fc6f9c779c924a5eddf5c699d8da080114f7Deleted: sha256:091a2aef7242e42505b69f1ad027d6a442cfce2403e260ac914f0fd6cc2d275fDeleted: sha256:4e72a31f1cd6fd655cc0826c91e886967b6e965e13ac21f31f9f66c27a3b7732Deleted: sha256:e3d1cdf9772a260b3e81a22c1940d63ac45dfe67720f78f00ca73834d9498934Deleted: sha256:af40da71a8618ea9cbcdc333d5e60bd5b6df820f0d07a55f7c9a1c21fd930095Deleted: sha256:608f3a074261105f129d707e4d9ad3d41b5baa94887f092b7c2857f7274a2fce$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 825d55fb6340 9 days ago 72.8MB$ docker image load &amp;lt; docker_nginx_1.21.6.tar608f3a074261: Loading layer 83.9MB/83.9MBea207a4854e7: Loading layer 62MB/62MB33cf1b723f65: Loading layer 3.072kB/3.072kB5c77d760e1f4: Loading layer 4.096kB/4.096kBfac199a5a1a5: Loading layer 3.584kB/3.584kBea4bc0cd4a93: Loading layer 7.168kB/7.168kBLoaded image: nginx:latest이 때, image rm 이나 rmi나 동일하게 작동한다.또 위에서 배운 docker image의 모든 명령어는 다 단축이 가능하다.docker image load == docker loadrun일단 시작 전에 이미 존재하는 컨테이너를 모두 삭제하는 것이 좋다.$ docker ps -a --format &#39;&#39; | xargs docker rm터미널을 2개 실행해서 각각 docker run을 해보자.$ docker run -it --name ubuntu_top ubuntu &quot;top&quot; &quot;-d 1&quot;top - 09:59:24 up 1:48, 0 users, load average: 2.33, 2.46, 2.19Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie%Cpu(s): 34.5 us, 5.1 sy, 0.0 ni, 60.0 id, 0.0 wa, 0.0 hi, 0.4 si, 0.0 stMiB Mem : 15680.4 total, 7678.9 free, 3078.9 used, 4922.6 buff/cacheMiB Swap: 6939.0 total, 6939.0 free, 0.0 used. 11506.3 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 6088 3208 2704 R 0.0 0.0 0:00.10 top ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ$ docker run -it --name ubuntu_top ubuntu &quot;top&quot; &quot;-d 1&quot;docker: Error response from daemon: Conflict. The container name &quot;/ubuntu_top&quot; is already in use by container &quot;5d31836dac6bfee687ad4bfc050dec2893ab56426518535625a3a752250085df&quot;. You have to remove (or rename) that container to be able to reuse that name.See &#39;docker run --help&#39;.ubuntu_top이라는 이름의 anscestor이 ubuntu인 컨테이너를 실행하고, 명령어는 top, argument는 딜레이를 1준다는 것이다. 두번째에서 에러가 나는 이유는 동일한 이름을 가진 컨테이너를 만들 수 없기 때문이다.exec그 다음으로는 같은 컨테이너에서 2개의 top을 실행시켜보고자 한다. 1번 터미널의 ubuntu_top은 그대로 두고 2번에서 docker ps -a 를 확인해보면 ubuntu_top이 실행중인 것을 확인할 수 있다.$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5d31836dac6b ubuntu &quot;top &#39;-d 1&#39;&quot; 4 minutes ago Up 4 minutes ubuntu_top이 2개의 top을 실행시키기 위해 exec를 사용한다. run은 새롭게 컨테이너를 실행하는 명령이고, exec는 기존에 존재하는 컨테이너에서 실행하는 명령이다.$ docker exec -it ubuntu_top top &quot;-d 0.2&quot;top - 10:04:03 up 1:53, 0 users, load average: 2.42, 2.44, 2.26Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 5.0 us, 2.5 sy, 0.0 ni, 92.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 15680.4 total, 7561.3 free, 3139.4 used, 4979.7 buff/cacheMiB Swap: 6939.0 total, 6939.0 free, 0.0 used. 11390.6 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 6088 3208 2704 S 0.0 0.0 0:00.24 top 7 root 20 0 6088 3156 2652 R 0.0 0.0 0:00.05 top 이 때 보이는 7번이 방금 실행시킨 exec에 의한 process이다.이번에는 3번째 터미널에서 bash를 실행시켜본다.$ docker exec -it ubuntu_top bashroot@5d31836dac6b:/# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 09:56 pts/0 00:00:00 top -d 1root 7 0 0 10:03 pts/1 00:00:00 top -d 0.2root 13 0 0 10:05 pts/2 00:00:00 bashroot 22 13 0 10:05 pts/2 00:00:00 ps -ef그 후 1번 터미널에서 다시 확인해보면 bash가 추가된 것을 볼 수 있다. PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 7 root 20 0 6088 3156 2652 S 1.0 0.0 0:00.42 top 1 root 20 0 6088 3208 2704 R 0.0 0.0 0:00.31 top 13 root 20 0 4116 3460 2912 S 0.0 0.0 0:00.02 bash 컨테이너를 종료하기 위해서는 1번 터미널만 닫아도 된다. 1번이 오리지널 프로세스이므로 1번이 꺼지면 나머지는 쫓겨나게 된다.Binding컴네이너의 자원을 외부와 연결하는 명령이다. 일반적으로는 I/O 나 storage관련, 또는 환경을 연결한다.i/o에서는 대표적으로 network을 연결하는데 연결 방식은 2가지가 있다. port binding : host OS의 port 와 컨테이너의 port를 바인딩해서 port를 통해 연결 network : docker network를 사용파일에서는 대표적으로 directory나 file, block device 등을 연결한다. 이에 대해서도 3가지의 방법이 있다. mount binding : host OS의 디렉토리를 바인딩 volume : docker volume 저장소를 사용 device : host os의 device, gpu를 바인딩환경에서는 shell environment를 지정해줘서 연결할 수 있다. docker는 컨테이너라서 커스터마이징이 가능하다. 환경 변수를 통해 어떤 값이 들어오면 특정 설정으로 작동한다는 식으로 만들 수 있다.port bindingport binding은 네트워크 서비스를 사용한다는 것이므로 nginx web server를 사용한다. 그래서 nginx container가 사용할 port를 확인해보고자 한다.$ docker inspect nginx&quot;Config&quot;: { &quot;Hostname&quot;: &quot;&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;ExposedPorts&quot;: { &quot;80/tcp&quot;: {} }, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;, &quot;NGINX_VERSION=1.21.6&quot;, &quot;NJS_VERSION=0.7.2&quot;, &quot;PKG_RELEASE=1~bullseye&quot; ], &quot;Cmd&quot;: [ &quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;config부분을 확인해보면 포트를 80/tcp를 외부에 노출하고 있음을 알 수 있다. 또는 stdin/stdout을 사용하지 않는 것으로 되어 있다. 그 이유는 기본적으로 daemon서비스는 std를 사용하지 않기 때문이다. SIGTTIN/SIGTTOUT을 발생시키지 않기 위해서이다.2개의 터미널을 통해 사용한다. 1번 터미널에서 run을 한다. 8080이 host의 Port번호 컨테이너의 port번호이다. 2번에서는 listen을 통해 확인한다.$ docker run --rm -p 8080:80/tcp --name nginx_8080 nginx...2022/04/15 10:20:44 [notice] 1#1: start worker process 342022/04/15 10:20:44 [notice] 1#1: start worker process 352022/04/15 10:20:44 [notice] 1#1: start worker process 362022/04/15 10:20:44 [notice] 1#1: start worker process 372022/04/15 10:20:44 [notice] 1#1: start worker process 38$ ss -nlt &#39;sport = :8080&#39;State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 4096 0.0.0.0:8080 0.0.0.0:* LISTEN 0 4096 [::]:8080 [::]:*1번 터미널의 옵션 –rm : 컨테이너를 껐을 때 자동 삭제 -p &amp;lt;&amp;gt; : 뒤에 오는 것은 호스트 port번호와 컨테이너 port번호2번 터미널의 옵션 -n : numeric -l : listen -t : tcpfilter port가 8080인 것만 보여달라연결이 잘 되어 있는지 확인하기 위해 curl을 사용한다.$ curl 127.0.0.1:8080&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;&amp;lt;style&amp;gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&amp;lt;/style&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;For online documentation and support please refer to&amp;lt;a href=&quot;http://nginx.org/&quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;Commercial support is available at&amp;lt;a href=&quot;http://nginx.com/&quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;동작하고 있는 1번 터미널을 죽이기 위해서는 &amp;lt;CTRL-C&amp;gt;를 누른다.detach위에서는 foreground에서 실행되고 있지만, 관리하기 귀찮을 경우 background에 놓고`` 실행시키고자 한다. 대부분 background에 놓고 실행한다.$ docker ps -a$ docker run -d --rm -p 8080:80/tcp --name nginx_8080 nginx0c501f5680b67dfdbcd9585ea6798b3250e553389652fc416e0c1f3194998001$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0c501f5680b6 nginx &quot;/docker-entrypoint.…&quot; 13 seconds ago Up 4 seconds 0.0.0.0:8080-&amp;gt;80/tcp, :::8080-&amp;gt;80/tcp nginx_8080현재는 background에서 실행했기 떄문에 로그가 나오지 않는다. 이 로그들을 보기 위해서 다른 터미널을 열어서 확인한다.$ docker logs nginx_80802022/04/15 10:50:42 [notice] 1#1: start worker process 322022/04/15 10:50:42 [notice] 1#1: start worker process 332022/04/15 10:50:42 [notice] 1#1: start worker process 342022/04/15 10:50:42 [notice] 1#1: start worker process 352022/04/15 10:50:42 [notice] 1#1: start worker process 362022/04/15 10:50:42 [notice] 1#1: start worker process 372022/04/15 10:50:42 [notice] 1#1: start worker process 38이 때, 추적해서 관찰하고 싶다면 -f를 추가한다.$ docker logs -f nginx_8080위에서 docker run -it 을 사용할 때, interactive mode와 terminal을 사용하는 경우 container을 running 상태로 두고 잠시 빠져나올 때는 ^P^Q를 통해 detach를 할 수 있다. 또는 처음 실행할 때 detach mode를 추가하면 된다.$ docker run --rm -itd --name ubuntu_bash ubuntu bash이것만 실행하면 shell로 진입하지 않는다. 그래서 attach를 하여 진입한다.$ docker attach ubuntu_bashroot@4042467bb952:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 9 pts/0 00:00:00 ps^P^Q를 누르면root@4042467bb952:/# read escape sequence$ docker attach ubuntu_bashroot@4042467bb952:/# mountnginx에 외부 디렉토리를 mount해보고자 한다. 그전에 nginx 웹서버가 사용하는 디렉토리 구조를 알아야 한다. 이는 공식 사이트 에 들어가면 정리되어 있다. 설정 파일 내용은 직접 컨테이너로 들어가 보는 것도 좋다.$ docker run --rm -it nginx bash# alias ls=&#39;ls --color&#39;# cd /etc/nginx/conf.d# lsdefault.conf# more default.confserver { listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index intex.html index.htm;l }}/usr/share/nginx/html이 바로 web document root이다. 여기를 외부 host에서 mount로 binding해야 한다.host : home/nginx_doc_root -&amp;gt; container : /usr/share/nginx/html 로 바인딩한다.마운트를 위해 다시 host os로 넘어온다.$ mkdir ~/nginx_doc_root$ readlink -f ~/nginx_doc_root/home/jhyoon/nginx_doc_root$ echo &quot;Hello Document root dir&quot; &amp;gt; ~/nginx_doc_root/hello.txt$ ls ~/nginx_doc_root/hello.txt$ docker run --rm -d -p 8080:80/tcp -v /home/jhyoon/nginx_doc_root:/usr/share/nginx/html --name nginx_8080 nginx -v host_file:container_file : 지금은 1개만 했지만, 여러 개 지정해줄 수 있다.위에서 작업했던 터미널 말고 1개더 실행해서 다음을 명령한다.$ curl 127.0.0.1:8080$ curl http://127.0.0.1:8080/hello.txt$ echo &quot;Hello World&quot; &amp;gt;&amp;gt; ~/nginx_doc_root/hello.txt$ curl http://127.0.0.1:8080/hello.txt$ docker stop nginx_8080예전에 있던 파일에 바인딩했으므로 그 파일은 보이지 않고, 내가 생성한 파일로 출력된다. 이에 &amp;gt;&amp;gt;를 통해 내용을 더 추가했다. 컨테이너가 죽으면 --rm을 설정했기 때문에 바로 삭제된다.-v 대신 –mount를 사용할 수도 있다.$ docker run --rm -d -p 8080:80/tcp --mount type=bind,src=/home/jhyoon/nginx_doc_root,dst=/usr/share/nginx/html -- name nginx_8080 nginx두 방법은 동일하다. 단지 직관성이 다를 뿐이므로 마음에 드는 것을 사용하면 된다.Environment variables환경 변수 관리이다. –env KEY=value –env-file env_file$ docker run --rm -it --name ubuntu_bash ubuntu bash# echo $LANG# exit$ docker run --rm -it -e LANG=en_US.utf8 --name ubuntu_bash ubuntu bash# echo $LANGen_US.utf8–env를 사용해도 되지만, -e로 사용해도 된다.stop/startdocker stop/start는 어떤 경우에 사용하나? stop detach mode로 실행중이라면, 정지시키기 위해서 외부에서 docker stop으로 정지한다. -it를 사용하지 않는 시스템은 signal이나 docker stop으로 정지할 수 밖에 없다. start docker run –rm을 쓰지 않는 경우 exit를 하면 container가 남아있어서 stop 후 재시작할 때 start로 가능하다. $ docker run -d -p 8080:80/tcp --mount type=bind,src=/home/jhyoon/nginx_doc_root,dst=/usr/share/nginx/html -- name nginx_8080 nginx$ docker ps$ docker stop nginx_8080 &amp;amp;&amp;amp; docker ps -a$ docker start nging_8080 &amp;amp;&amp;amp; docker ps$ docker rm IDDocker Composedocker를 실행시킬 때 설정들을 저장해서 만들어놓을 수 있다. 파일의 이름은 docker-compose.yml 이다. 이를 통해 매번 길게 치지 않아도 되기에 편리하다.docker-compose [-f &amp;lt;arg&amp;gt;...] [--profile &amp;lt;name&amp;gt;...] [options] [COMMAND] [ARGS...] -f config : yaml filetype default filename : docker-compose.yml docker-compose 파일을 사용할 때는 안써도 되지만, 다른 파일을 지정하고 싶은 경우 사용 compose를 할 경우 알아서 build, rebuild, create, start 까지 해준다.더 궁금한 내용이 있다면 help(-h)를 사용한다.$ docker-compose -hdocker-compose 설치$ sudo apt -y install docker-composedocker-compose 설정설치 완료 후 간단한 설정을 만들어보자. 먼저 추후 편리하게 사용하기 위해 docker-compose를 작업할 디렉토리를 만들 것이다.$ mkdir ~/docker-compose$ cd ~/docker-compose$ vim docker-compose.ymlversion: &#39;3&#39;services: nginx_8080: image: nginx restart: always hostname: nginx1 container_name: cmp_nginx1_1 network: mynet: ipv4_address: 172.20.0.10 ports: - 8080.80 volume: - /home/jhyoon/nginx_doc_root:/usr/share/nginx/htmlnetworks: mynet: ipam: driver: default config: - subnet: 172.20.0.0/24compose사이트 version : 2, 3 등 원하는 것으로 쓰면 되지만, 3을 가장 많이 쓴다 nginx_8080 : 서비스의 이름 image : 이미지, 버전 명이 있는 경우 옆에 적으면 된다. restart : 해당컨테이너가 종료되면 자동으로 재시작 hostname : 호스트 이름 container_name : 이를 지정하지 않으면 directory이름을 prefix로 자동 생성된다. network : 쓸 network이름, 이는 아래에 선언함. port : port binding, hostport.containerport volume : mount binding, hostdir/containerdirdocker-compose 실행$ docker-compose up다른 터미널 창에서 다음을 명령한다.$ pwd/home/jhyoon/docker-compose$ docker-compose ps$ docker ps$ curl http://127.0.0.1:8080/hello.txt이를 detach로 실행하려면 -d를 추가해야 한다.$ docker-compose up -d$ docker-compose ps$ docker-compose stop$ docker-compose psweb 서버를 2개 실행2개를 실행하기 위해 docker-compose2.yml 파일을 생성한다.version: &#39;3&#39;services: nginx_8080: image: nginx restart: always hostname: nginx1 container_name: cmp_nginx1_1 network: mynet: ipv4_address: 172.20.0.10 ports: - 8080:80 volume: - /home/jhyoon/nginx_doc_root:/usr/share/nginx/html nginx_8081: image: nginx restart: always hostname: nginx2 container_name: cmp_nginx2_1 network: mynet: ipv4_address: 172.20.0.20 ports: - 8081:80 volume: - /home/jhyoon/nginx2_doc_root:/usr/share/nginx/htmlnetworks: mynet: ipam: driver: default config: - subnet: 172.20.0.0/24...여기서 DB나 API서버를 사용하여 container 끼리 서로 접속하려면 hostname을 설정해줘야 하고, 이런 경우에는 /etc/hosts에 서버를 등록한다.172.20.0.10 nginx1.domainname172.20.0.20 nginx2.domainname2개를 실행할 때는 -f를 추가해야 한다.$ docker-copmose -f compose-compose2.yml up -d$ docker-copmose -f compose-compose2.yml psnetwork$ docker network ls$ docker-compose -f docker-compose2.yml stop$ docker network rm dockercompose_mynet$ docker-compose -f docker-compose2.yml up -d실행을 하면 에라가 뜬다. 왜냐하면 network가 재생성되면서 id 값이 달라지기 때문이다. 따라서 재생성해야 한다.$ docker-compose -f docker-compose2.yml up --force-recreate -dwsl2에 systemd 사용하기wsl2에 도커를 사용하기 위해서는 설정을 해줘야 한다.git clone https://github.com/DamionGans/ubuntu-wsl2-systemd-script.gitcd ubuntu-wsl2-systemd-script/bash ubuntu-wsl2-systemd-script.sh이 후, wsl을 닫고, powershell에 들어가서 wsl --shutdown을 한다. 그리고 wsl을 다시 실행한다.만약 이 때, cannot execute daemonize to start systemd.라는 에러가 뜬다면, powershell을 다시 실행한다. 그리고 wsl에 접속해야 한다.wsl -u root그리고 daemonize 패키지가 있는지 확인한다.sudo apt install daemonize없으면 설치하고, 있다면 삭제하고 재설치한다.설치가 끝나면, wsl을 빠져나와 다시 wsl --shutdown 한다. 그리고 wsl을 실행하면 systemctl을 사용할 수 있게 될 것이다. reference https://velog.io/@guswns3371/wsl2-%EC%97%90%EC%84%9C-systemctl-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0 " }, { "title": "[데브코스] 9주차 - DeepLearning Multi Layer Perceptron", "url": "/posts/multiperceptron/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-14 15:40:00 +0900", "snippet": "신경망사람의 뉴런의 집합을 신경망이라 한다. 뉴런은 두뇌의 가장 작은 정보처리 단위이고, 세포체는 연산을하고, 수상돌기는 신호를 수신, 축삭은 처리 결과를 전송한다. 이러한 사람의 뉴런을 따럿 컴퓨터에 인공지능으로 구성하여 퍼셉트론, 인공신경망을 만들었다. 세포체, 수상돌기, 축삭, 시냅스가 인공신경망에서 각각 노드, 입력, 출력, 가중치에 해당한다.인공 신경망에서는 다양한 분류 방식에 따라 많은 종류가 있다. 신호 처리 방식에 따라 전방 신경망/순환 신경망로 나뉠 수 있고, 신경망의 깊이에 따라 얕은 신경망/깊은 신경망로 나뉠 수 있다. 또는 결정론적 신경망과 확률론적 신경망으로도 나뉠 수 있다. 결정론 신경망모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망을 말한다. 즉, 동일한 입력에서는 반드시 동일한 출력이 나오는 신경망이다. 확률론 신경망고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력을 가지는 신경망이다. 퍼셉트론node, weight, layer 과 같은 새로운 개념의 구조를 도입한 모델이다. 처음으로 학습이라는 알고리즘을 제안했다. 현재의 딥러닝 모델은 퍼셉트론을 깊게 만들어서 결과를 도출하는 것이므로 중요한 기반이 되는 모델이다.퍼셉트론의 구조 입력층퍼센트론을 포함한 모든 모델은 d차원의 입력 벡터를 받는다. i번째 노드는 특징 벡터 $ x = (x_1, x_2, \\cdots , x_d)^T $ 의 요소 x_i를 담당한다. 항상 1이 입력되는 평향(bias) 노드도 있다. 이 bias을 통해 임계점이 0으로 옮겨진다. 연산(입력과 출력 사이)i번째 입력 노드와 출력 노드를 연결할 때 가중치 w_i를 가진다. 퍼셉트론은 단일 층 구조로 간주한다. 출력층계단함수를 사용했으므로 한 개의 노드에 의해 수치(+1 or -1)을 출력한다. 퍼셉트론의 동작선형 연산과 비선형 연산이 있다. 선형 연산에는 입력값과 가중치가 곱해지고 모두 더하는 연산이고, 비선형 연산에는 활성함수(계단함수)가 이에 해당된다.$ s = w^Tx + w_0 $편향항은 b또는 w_0라고 표기하는데, 이를 w안의 벡터로 추가하게 되면, $ x = (1,x_1,x_2, \\cdots, x_d)^T, w = (w_0,w_1,w_2,\\cdots,w_d)^T $ 가 된다. 따라서 이를 간단하게 만들면 다음과 같은 식이 만들어진다.$ y = \\tau(w^Tx) $OR 논리 게이트OR 논리 게이트란 입력값이 (0,0)만 -1, 나머지인 (1,0),(0,1),(1,1) 은 모두 1로 출력되는 퍼셉트론을 말한다. 그렇다면 AND 논리 게이트는 (1,1)은 1이고, 이외에는 다 -1이 출력될 것이다.첫번째 그림은 OR, 두번째 그림은 AND 논리 게이트에 대한 좌표 그림이다.결정 직선 d(x)는 $ d(x) = d(x_1,x_2) = w_1x_1 + w_2x_2 + w_0 = 0 $ 이다. 이 때, w_1,w_2는 직선의 기울기, w_0는 절편(편향)을 결정한다. 결정 직선이란 특징 공간을 두 부분 공간으로 이분할 하는 분류기 역할을 하는 직선이다. 이 때는 결정 직선이 (0,0)이외에는 다 1이 나오기 때문에 (0,0)과 나머지를 구분해야 한다.식에서 0이 나오는 이유는 원래는 $ w_1x_1 + w_2x_2 = -w_0 $ 와 같이 w_0가 임계값에 대한 값인데, 이를 좌항으로 옮긴 것이다. -w_0인 이유는 임계값을 0으로 좌표를 옮기기 위해서이다.이를 d차원 공간으로 일반화하면 $ d(x) = w_1x_1 + w_2x_2 + \\cdots + w_dx_d + w_0 = 0 $AND 논리 게이트를 코드화하면 다음과 같다. 하지만 이는 벡터가 아닌 상수값으로 만들어진 것이다.def AND(x1,x2): w1, w2, theta = 0.5, 0.5, 0.7 tmp = x1*w1 + x2*w2 if tmp &amp;lt;= theta: return 0 elif tmp &amp;gt; theta: return 1 print(AND(0, 0))print(AND(0, 1))print(AND(1, 0))print(AND(1, 1))theta는 편향값을 말한다.상수가 아닌 벡터로 표현하게 되면 다음과 같다.import numpy as np# 함수로 표현def AND(x1,x2): # 모두 1일때만 1, 나머지는 0 x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.7 tmp = np.sum(w*x) + b if tmp &amp;lt;= 0: return 0 else: return 1def NAND(x1, x2): # 모두 1일때만 0, 나머지는 1 x = np.array([x1, x2]) w = np.array([-0.5, -0.5]) # and와 부호만 반대 b = 0.7 # and와 부호만 반대 tmp = np.sum(w*x) + b if tmp &amp;lt;= 0: return 0 else: return 1def OR(x1, x2): # 1이 존재하면 1, 나머지는 0 x = np.array([x1, x2]) w = np.array([0.5, 0.5]) # and와 가중치만 다르다. b = -0.2 tmp = np.sum(w*x) + b if tmp &amp;lt;= 0: return 0 else: return 1퍼셉트론의 학습목적함수 목적함수 상세 설계$ J(w) = \\sum_{x_k in Y} -y_k(w_Tx_k) $이 때, Y는 w가 틀리는 샘플의 집합, 즉 오판단하는 샘플의 집합을 말한다. 이 식이 퍼셉트론의 목적함수로 적합한지를 판단해보면 임의의 샘플 x_k 가 Y에 속한다면 퍼셉트론의 예측값 $w^Tx_k$와 실제값 y_k는 부호가 달라야 한다. 즉 정답이 1인데, 예측값이 -1이거나, 답이 -1인데 예측값이 1이므로 실제값과 예측값은 항상 부호가 다르다. Y가 크다는 것은 틀리는 개수가 크다는 것이고, Y가 커질수록 J(w)가 커진다. Y가 공집합, 즉 틀린 것이 없다면 J(w) = 0 이다따라서 목적함수로 사용이 가능하다.경사하강법J(w)의 기울기를 이용하여 최소값을 찾는다. 이를 위해 가중치를 갱신할 때는 $ \\theta = \\theta - \\rho g $를 사용하는데, 이 때 $\\theta$는 w와 같다. 경사도 g를 얻기 위해 w_i에 대해 편미분을 한다.$ \\frac{\\partial J(w)}{\\partial w_i} = \\sum_{x_k \\in Y} \\frac{\\partial(-y_k(w_0x_{k0} + w_1x_{k1} + \\cdots + w_ix_{ki} + w_dx_{kd}))}{\\partial w_i} = \\sum_{x_k \\in Y} -y_kx_{ki} $w_i는 w벡터의 i번째 요소이므로 가중치 w는 $ w = {w_0,w_1,\\cdots,w_i,\\cdots,w_d} $ 이다. 그렇기 때문에가중치 갱신 식은 $ w_{i+1} = w_i + \\rho\\sum_{x_k \\in Y} y_kx_{ki} $ 가 된다. 이를 델타 규칙이라 부른다. 델타 규칙은 퍼셉트론의 학습 방법에서만 해당된다. 여기서 $ \\rho $는 learning rate(학습률)이다. 이 learning rate는 학습을 할 때 가장 중요한 파라미터이다. 이 값이 너무 크면 최저점을 넘어서서 계속 진동이 되기도 하고, 너무 작으면 지역 최저점에 갇혀버리거나 너무 느리게 학습이 될 수도 있기 때문이다. 결정 직선과 가중치 w_k는 서로 직각을 이룬다.퍼셉트론의 한계가 존재한다. 데이터가 직선을 통해 이분화가 된다면 퍼셉트론을 사용하면 되지만, 거의 모든 데이터는 직선으로 분류하기 어렵다. 이를 해결하기 위해 다층 퍼셉트론을 만들게 된다.다층 퍼셉트론XOR 논리 게이트가 다층 퍼셉트론에 해당된다. 즉, 하나가 1이고, 하나가 0인 상태만 1을 출력한다. x1 x2 output 0 0 0 1 0 1 0 1 1 1 1 0 이 상황에서는 선형 분류기로는 한계가 존재한다.다층 퍼셉트론의 특징 은닉층을 둔다. 즉 입력층, 출력층 이외에 중간에 층이 하나 더 존재한다. 시그모이드 활성함수를 사용한다. 기존의 퍼셉트론은 계단함수로 활성함수를 만들었지만, 이 함수는 적절하지 않다. 그래서 다층 퍼셉트론에서는 시그모이드함수를 활성함수로 사용한다. 출력을 신뢰도로 간주함으로서 더 융통성 있게 의사결정이 가능해졌다. 오류 역전파 알고리즘을 사용한다. 여러 층이 순차적으로 이어져 있기에 역방향을 진행하면서 한층씩 그레디언트를 계산하고 가중치를 갱신한다.원래의 퍼셉트론을 2개를 병렬 결합하면 원래 공간 $ x = (x_1,x_2)^T $ 를 새로운 특징 공간 $ z = (z_1, z_2)^T $ 로 변환이 된다. 즉, 새로운 특징 공간 z에서 선형 분리가 가능해진다.다층 퍼셉트론의 용량특징 공간 x가 2개의 벡터를 가진다면, x1,x2의 2차원 공간에서 w1,w2를 통해 직선의 방정식을 그릴 수 있다. 이를 새로운 영역 z에 점으로 투영한다면 3차원의 점으로 변환될 것이고, 층을 지날수록 더 높은 차원으로 변환이 될 수 있다.그렇다면, 3개의 퍼셉트론을 결합한 경우 2차원 공간을 n개 영역으로 나누고, 각 영역을 3차원 점으로 변환하고, 계단함수를 활성함수로 가정한다면 3차원에 점으로 변환된다. 따라서 p개의 퍼셉트론을 결합하면 p차원 공간으로 변환되는 것과 같다.하나의 은닉층은 특징 공간을 다른 특징 공간으로 매핑하는 것이므로 함수의 근사표현으로 생각해볼 수 있고, 이 은닉층을 여러 개 쓴다면 내가 원하는 공간을 변환하는 근사 함수라고 표현할 수 있다.활성함수활성함수가 원래는 계단함수였으나 좀 더 부드러운 공간 분할을 위해 시그모이드 함수로 바꾸었다. 그렇게 되면 원래의 계단함수는 영역을 점으로 변환하지만, 시그모이드와 같은 함수들은 영역을 영역으로 변환한다.활성함수로 많이 사용되는 함수들은 다음과 같다.시그모이드나 tanh는 a가 커질수록 계단함수에 가까워진다. 다층 퍼셉트론에서는 sigmoid나 tanh를 많이 사용했고, 딥러닝에서는 마지막에 ReLU를 가장 많이 사용한다. 딥러닝에서 sigmoid나 tanh를 사용하지 않는 이유는 그래프의 모양과 같이 0이나 1에 가까운 값을 내는 값들에 대해서는 gradient를 없앤다.(= vanishing gradient)예를 들어 데이터 x와 출력이 존재할 때 sigmoid를 사용한 모델에서 backpropagation를 위해 gradient를 계산한다면, 일단 (dL/dσ)가 있을 것이고, sigmoid gate를 지나 local sigmoid function의 gradient인 (dL/dx)를 구하기 위해 chain rule를 적용한다.따라서 $ \\frac{\\partial L}{\\partial x} = \\frac{\\partial σ}{\\partial x} * \\frac{\\partial L}{\\partial σ} $ 가 된다.x = 0 일 때는 backprop가 잘 진행될 것이다. 그러나 x = -10 일 때의 gradient($\\frac{\\partial L}{\\partial x} $)는 그림에서 보듯이 0이다. 그렇게 되면, 0이 backpropagation 될 것이고, 그로 인해 뒤에 전달되는 모든 gradient는 모두 죽어(=0)버린다. x = 10 일 때도 마찬가지로 모든 gradient가 0 이 된다.따라서 x가 아주 크거나 아주 작다면 gradient가 계속 0으로 죽어버린다. 이를 gradient vanish 또는 vanishing gradient라 한다. 참고 자료사실 실제 뉴런의 출력도 sigmoid보다 ReLU의 형태가 더 가깝다. 따라서 ReLU를 많이 사용한다.매개변수은닉 층이 1개인 퍼셉트론을 2층 퍼셉트론, 은닉층이 2개인 퍼셉트론을 3층 퍼셉트론이라 한다. 은닉층이 4개 이상인 퍼셉트론을 깊은 신경망이라 한다. 은닉층의 개수가 너무 많으면 과잉적합(overfitting), 너무 적으면 과소적합(underfitting)이 발생한다.은닉층이 기하학적으로 봤을 때는 새로운 특징 공간으로 변환해주는 것과 같지만, 의미론적으로 바라봤을 때 이는 입력 벡터에 대해 내가 원하는 부분을 뽑아내겠다는 특징 추출기라 할 수 있다. 입력이 들어오면 내가 만들어놓은 가중치, 즉 벡터 값들과 곱해서 원하는 결과를 얻어내는 것이다. 예를 들어, 내가 이미지를 모델에 넣었을 때 학습된 가중치들과 곱해져서 컴퓨터가 해당 이미지의 특징들이 추상화된 형태가 될 것이다.현대 기계학습에서는 이를 특징학습이라 부른다.오류 역전파 알고리즘손실함수가장 일반적인 손실함수는 MSE(Mean Squared Error)이 있다. L2 norm을 사용해서 다음과 같이 정의된다. $ e = \\frac{1}{2n}\\sum_{i=1}^n   y_i - o_i   _2^2 $ 여기서 o는 출력 벡터이고, y는 실제값인데, 이를 원핫 인코딩 형태로 만들어 부류 벡터라는 벡터를 만든다.. 즉, 0과 1로만 구성되어 있는데, 기댓값 즉 내가 원하는 값인지 아닌지에 대한 값을 나타내는 것으로 이를 통해 error를 구한다.연산 그래프 (computational graph)연산을 그래프로 표현한 것을 연산 그래프라 한다.연산 그래프를 진행할 때, 그래디언트를 계산하기 위해서는 연쇄 법칙을 사용한다. 예를 들어, k(x) = f(g(h(i(x)))) 일 때, k’(x)를 구하기 위해서는 다음과 같은 식이 될 것이다.\\[i\\prime(x) = f\\prime(g(h(i(x)))) * g\\prime(h(i(x))) * h\\prime(i(x)) * i\\prime(x)\\]그렇다면 우리가 전방 계산을 할 때마다 prime을 구해놓으면 그것을 통해 역전파를 하면 된다.동일한 f(활성함수)를 연속으로 사용하고, 입력이 w -&amp;gt; x -&amp;gt; y -&amp;gt; z 일 때의 그래디언트를 구하면$ \\frac{\\partial z}{\\partial w} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x} \\frac{\\partial x}{\\partial w} $$ = f\\prime(y)f\\prime(x)f\\prime(w) $$ = f\\prime(f(f(w)))f\\prime(f(w))f\\prime(w) $우리가 실제로 궁금한 미분은 가중치에 대한 예측값의 그래디언트이다. 2층 퍼셉트론에서의 매개 변수 $ \\theta = {U^1, U^2} $ 가 있다고 하면, 손실 함수 J($\\theta$) 는 다음과 같다. $ J(\\theta) = \\frac{1}{2}   y - o(\\theta)   _2^2 $ 이 때, y는 부류 벡터, o는 예측값이다. $J({U^1, U^2}) $의 최저점을 찾기위해 이를 미분하면$ U^1 = U^1 - \\rho \\frac{\\partial J}{\\partial U^1} $$ U^2 = U^2 - \\rho \\frac{\\partial J}{\\partial U^2} $연산의 순서는 x -&amp;gt; U^1 -&amp;gt; U^2 -&amp;gt; o 이다.다시 한 번 살펴보자면, 우리가 궁금한 것은 가중치 x,y 에 대한 결과값 L의 그래디언트이다. 여기서 upstream은 뒤로 올라가는 방향의 gradient, downstream은 전방으로 전달되는 방향으로의 gradient이다.$ \\frac{\\partial L}{\\partial x} $ 은 chain rule에 의해 분해가 되어 $ \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial x} $ 로 변환될 수 있다. z도 가중치이므로 x 와 z를 연산하여 출력이 되고, 그 출력이 L이 된다. y도 동일하다.forward, backprop 두 가지를 따로 바라보게 되면 forward입력값 in, 이 활성함수 f를 거쳐 out이 된다. backward이에 대해 미분을 해보면$ \\frac{\\partial \\epsilon}{\\partial in} = \\frac{\\partial \\epsilon}{\\partial out} \\cdot \\frac{\\partial out}{\\partial in} = \\frac{\\partial \\epsilon}{\\partial out} \\cdot f\\prime(in)$이 때, $\\epsilon$ 은 error이고, $ \\frac{\\partial \\epsilon}{\\partial out} $ 은 output gradient이다. $ \\frac{\\partial out}{\\partial in} $ 은 local gradient이다.그래서 입력에 대한 에러의 gradient는 output gradient(출력에 대한 에러의 gradient) * local gradient(입력에 대한 출력의 gradient) 이 된다. local gradient는 out = f(in) 이므로 in에 대한 out의 미분은 $f\\prime(in)$ 이 된다.곱셈의 역전파forward의 식은 다음과 같다.$ out = in_1 \\cdot in_2 $이에 대해 역전파를 진행해보면$ \\frac{\\partial \\epsilon}{\\partial in} = \\frac{\\partial \\epsilon}{\\partial out} \\cdot \\frac{\\partial out}{\\partial in} = \\frac{\\partial \\epsilon}{\\partial out} \\cdot in_2$in_2가 되는 이유는 곱셈의 연산을 생각해보면 X * Y에서 X에 대해 미분을 하면 Y, Y에 대해 미분하면 X이다. 따라서 in_1에 대해 미분을 하면 in_2가 남는다.이 연산을 코드로 구현하면 다음과 같다. 이는 pytorch에서 이미 구현되어 있다. 각 연산마다의 grad_z 즉 output gradient를 저장해놓고 이를 backprop에 사용한다.class Multiply(torch.autograd.Function): @staticmethod def forward(ctx, x, y): ctx.save_for_backward(x,y) z = x * y return z @staticmethod def backward(ctx, grad_z): x,y = ctx.saved_tensors grad_x = y * grad_z # dz/dx * dL/dz grad_y = x * grad_z # dz/dy * dL/dz return grad_x, grad_y덧셈의 역전파 forward$ out = \\sum_i in_i $ backpropagation$ \\frac{\\partial \\epsilon}{\\partial in_i} = \\frac{\\partial \\epsilon}{\\partial out} \\cdot 1 = \\frac{\\partial \\epsilon}{\\partial out}$덧셈의 경우 X+Y를 X, Y 각각에 대해 미분하면 모두 1이 나온다.S형 활성함수의 역전파$ \\frac{\\partial \\epsilon}{\\partial in} = \\frac{\\partial \\epsilon}{\\partial out} \\cdot \\sigma \\prime(in) = \\frac{\\partial \\epsilon}{\\partial out} \\cdot [\\sigma(in) (1 - \\sigma(in))] $시그모이드 함수의 형태를 생각해보면 다음과 같다.이를 미분하면 $ [\\sigma(in) (1 - \\sigma(in))] $ 이 만들어진다.최대화 역전파$ out = max_i{in_i} $max{0,x} 일 경우, 0보다 작으면 0이 되고, 0보다 크면 output gradient를 전달해준다.ReLU가 이에 해당되는데, ReLU와 같은 형태는 미분을 하면 임계값까지는 0이다가, 임계값 이후에는 1이 나온다. 그래서 local gradient는 1 또는 0이 된다.전개(fanout) 역전파fanout이란 두 가지의 출력이 존재하고, 이 둘을 합쳐서 또 다른 출력을 생성한다.$ x = x(t), y = y(t) =&amp;gt; z = f(x,y) $이 때는 역전파를 하기 위해서는 x,y 방향 각각을 gradient를 구해서 더하면 된다.$ \\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{\\partial x} \\cdot \\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial t} $위는 두 가지의 출력이지만, 이를 일반화 시키게 되면$ z = f(n_1,n_2, \\cdots , n_k, \\cdots) , and , n_k = n_k(in)$의 형태라면 이를 미분하게 되면 다음과 같다.$ \\frac{\\partial z}{\\partial in} = \\sum_k \\frac{\\partial z}{\\partial n_k} \\cdot \\frac{\\partial n_k}{\\partial in} = \\sum_{k} \\frac{\\partial z}{\\partial in_k} $신경망 역전파위의 덧셈, max, 곱셈에 대한 역전파를 모두 사용한다. 여기서 gradient vanish가 발생하는 이유는 중간에 시그모이드 함수가 있을 때, 값이 너무 크거나 너무 작으면 gradient가 0으로 나오고, 그 뒤로는 계속 0이 나오게 된다.도함수의 종류를 바라보면 3가지가 있다 scalar to scalar스칼라에 대한 스칼라는 상수가 나온다. vector to scalar출력값은 하나(L)지만, 입력값이 $ x = (x_1,x_2,x_3, …)^T $ 이라면 이에 대한 미분을 gradient라 한다. vector 각각이 영향을 주고, 그에 대해 L이 달라지므로 1~N개의 미분이 다 나올 것이고, 이는 벡터 형태이다.이 때 표기를 $ \\nabla_x z $ 라 한다. 즉, 스칼라 z에 대한 벡터 x의 미분 벡터이다. vector to vector출력도 벡터, 입력도 벡터라면 y1에 대한 모든 요소 x1,x2,x3..이 있고, y2도 x1,x2,x3 에 대한 미분이 다 있을 것이다. 그렇기에 출력되는 값은 행렬로 표현된다. 이 행렬을 Jacobian이라 한다.\\[\\frac{\\partial y}{\\partial x} = [\\frac{\\partial y_1}{\\partial x_1} \\cdots \\frac{\\partial y_1}{\\partial x_m} ]= [\\frac{\\partial y_1}{\\partial x_1} \\cdots \\frac{\\partial y_1}{\\partial x_m} \\vdots \\ddots \\vdots \\frac{\\partial y_n}{\\partial x_1} \\cdots \\frac{\\partial y_n}{\\partial x_m}]\\]만약 퍼셉트론이 2층이 있어서 x -&amp;gt; y -&amp;gt; z 인데, x가 vector, y도 vector, z 가 scalar이면 $ \\frac{\\partial y}{\\partial x} $ 는 야코비안, $ \\frac{\\partial z}{\\partial y} $는 gradient 형태이다.위의 vector는 다 2차원이지만, 실제 딥러닝에 사용되는 차원은 3차원이다. 미니 배치를 사용해서 3차원을 만든 형태를 집어넣는데, 이 때문에 연산량이 배치 단위에 따라 달라지는 것이다.MLP 코드 구현Pytorch autogradpytorch의 autograd라는 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공한다. tensor torch.tensor 클래스에는 required_grad 속성이 있는데, 이를 true로 설정하면 해당 텐서에서 이루어진 모든 연산을 추적한다. 계산이 완료된 후 backward()를 호출하면 모든 그래디언트를 자동으로 계산하며 이 그래디언트는 .grad 속성에 누적된다. tensor가 기록 추적하는 것을 멈추게 하려면 해당 줄에 .detach()를 사용하거나 .with torch_no_grad()를 사용한다. 이를 사용하는 이유는 모델을 추론할 때 사용하거나 numpy로 변환할 때는 grad를 추적하지 않아야 하기 때문이다. 각 tensor는 .grad_fn 속성을 가지고 있는데, 이는 tensor를 생성한 Function을 참조한다. 그러나 사용자가 만든 tensor는 예외고, 사용자가 만들지 않은 tensor에서의 연산으로 생긴 텐서는 모두 Function을 참조한다. 도함수를 계산할 때는 .backward를 호출한다. import torchx = torch.ones(2, 2, requires_grad = True)print(x)print(x.grad_fn)# -----------------------------#tensor([[1., 1.], [1., 1.]], requires_grad=True)None사용자가 직접 선언해준 tensor이므로 None으로 출력된다.y = x + 2print(y.grad_fn)# -----------------------------#&amp;lt;AddBackward0 object at 0x7ff8f2d294d0&amp;gt;선언해주지 않았지만, tensor의 연산에 의해 만들어진 y는 True, 즉 requires_grad가 True로 설정된다.requires_grad를 True로 설정했기 때문에 x의 grad_fn은 True로 출력된다.print(x.requires_grad)x.requires_grad_(False)print(x.requires_grad)# -----------------------------#TrueFalserequires_grad_ 메서드를 통해 requires_grad를 변경시켜줄 수 있다.x.requires_grad_(True) # 위에서 False로 설정했으므로 다시 True로 변경z = y**3out = z.mean()y.retain_grad()z.retain_grad()out.backward()print(x.grad)print(y.grad)print(z.grad)print(x.is_leaf) # 이것이 leaf 노드인지 확인 -&amp;gt; 즉 가장 끝단의 노드인지 확인out.backward()# -----------------------------#tensor([[6.7500, 6.7500], [6.7500, 6.7500]])tensor([[6.7500, 6.7500], [6.7500, 6.7500]])tensor([[0.2500, 0.2500], [0.2500, 0.2500]])TrueRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.중요한 것은 backward()를 여러 번 하려면 retain_graph = True로 설정해줘야 한다. 그렇지 않으면 에러가 난다.x = torch.ones(2, 2, requires_grad = True)y = x + 2z = y**3out = z.mean()y.retain_grad()out.backward(retain_graph=True)out.backward() print(x.grad)print(y.grad)print(z.grad) # -----------------------------#tensor([[13.5000, 13.5000], [13.5000, 13.5000]])tensor([[13.5000, 13.5000], [13.5000, 13.5000]])Nonez.retain_grad() 를 호출하지 않으면 grad를 저장하지 않으므로 grad가 없다.x = torch.randn(3, requires_grad=True)print(x)y = x * 2v = torch.tensor([0.1,1.0,0.0001], dtype=torch.float)y.backward(v) # 아무것도 지정하지 않으면 default로 1이 들어가서 값이 추출된다.print(x.grad) # 미분 후에 결과값# -----------------------------#tensor([ 0.5778, 1.1385, -0.4793], requires_grad=True)tensor([2.0000e-01, 2.0000e+00, 2.0000e-04])신경망 구현하기import torchimport torch.nn as nnfrom torch.utils.data import DataLoader, TensorDatasetclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.l1 = nn.Linear(4, 128) self.l2 = nn.Linear(128, 64) self.l3 = nn.Linear(64, 32) self.l4 = nn.Linear(32, 16) self.l5 = nn.Linear(16, 3) self.bn1 = nn.BatchNorm1d(128) # batch normalization self.bn2 = nn.BatchNorm1d(64) self.bn3 = nn.BatchNorm1d(32) self.act = nn.ReLU() # activation function def forward(self, x): x = self.act(self.bn1((self.l1(x)))) x = self.act(self.bn2((self.l2(x)))) x = self.act(self.bn3((self.l3(x)))) x = self.act(self.l4(x)) x = self.l5(x) return xcriterion = nn.CrossEntropyLoss()x,y = torch.randn([4,4]), torch.tensor([1,0,2,0])net = Net()output = net(x)loss = criterion(output, y)print(loss.item())net.zero_grad() # 저장되어 있는 grad를 다 지워야 한다.print(net.l5.bias.grad) # zero grad 했으므로 noneprint(net.l5.bias.is_leaf) # leaf노드란 아래 자식 노드가 없는 노드loss.backward()print(net.l5.bias.grad) # ----------------------------- #1.0462257862091064NoneTruetensor([-0.1753, 0.1098, 0.0655])역전파를 진행해주었기 때문에 grad가 기록되어 있다.params = list(net.parameters())print(len(params)) # 각 층마다 weight 와 biasprint(params[0].size()) # 0번째 층의 weight 와 bias # ----------------------------- #16torch.Size([128, 4])각 층마다의 weight와 bias를 저장해놓고 있으며, 이를 호출하면 모든 층의 weight와 bias를 리턴할 수 있다. 각 벡터마다 측의 weight를 저장하고 있다.iris 데이터를 불러와서 MLP 모델에 학습시키기from torch.utils.data import DataLoader, TensorDatasetimport pasdas as pdfrom sklearn.datasets import load_irisdataset = load_iris()data = dataset.datalabel = dataset.targetprint(label)# ----------------------------- #.. _iris_dataset:Iris plants dataset--------------------**Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ====================...from sklearn.datasets import load_iris에는 여러 데이터셋이 저장되어 있다. dataset.DESCR을 하면 description 즉 설명들이 저장되어 있다.target은 정답 라벨을 의미한다.print(data.shape)print(label.shape)# ----------------------------- #(150, 4)(150,)150개의 데이터가 있고, 각각의 4개의 속성을 가지고 있는 것을 확인할 수 있다.from sklean.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(data, label, test_size = 0.25)print(x_train.shape)print(x_test.shape)# ----------------------------- #(122,4)(38,4)train_test_split은 데이터셋을 분리해주는 기능을 한다. test_size를 통해 test 데이터셋의 크기를 지정해준다.x_train = torch.from_numpy(x_train).float()y_train = torch.from_numpy(y_train).long()y_test = torch.from_numpy(x_test).float()y_test = torch.from_numpy(y_test).long()train_dataset = TensorDataset(x_train, y_train)train_loader = DataLoader(train_dataset, batch_size=4, shuffle = True)class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.l1 = nn.Linear(4, 128) self.l2 = nn.Linear(128, 64) self.l3 = nn.Linear(64, 32) self.l4 = nn.Linear(32, 16) self.l5 = nn.Linear(16, 3) self.bn1 = nn.BatchNorm1d(128) self.bn2 = nn.BatchNorm1d(64) self.bn3 = nn.BatchNorm1d(32) self.act = nn.ReLU() def forward(self, x): x = self.act(self.bn1((self.l1(x)))) x = self.act(self.bn2((self.l2(x)))) x = self.act(self.bn3((self.l3(x)))) x = self.act(self.l4(x)) x = self.l5(x) return xoptimizer = torch.optim.SGD(net.parameters(), lr=0.001)criterion = nn.CrossEntropyLoss()epochs = 20losses = []accures = []for epoch in range(epochs): epoch_loss = 0 epoch_accur = 0 for batch, (x, y) in train_loader: optimizer.zero_grad() output = net(x) loss = criterion(output, y) loss.backward() optimizer.step() print(output) confidence, prediction = torch.max(output, dim=1) print(&quot;pred&quot;,prediction, prediction.shape) print(&quot;conf&quot;,confidence, confidence.shape) accur = (prediction == y).sum().item() epoch_loss += loss.item() epoch_accur += accur epoch_loss /= len(train_loader) epoch_accur /= len(x_train) losses.append(epoch_loss) accures.append(epoch_accur) iris로 데이터를 불러오면 numpy로 되어 있으므로 이를 tensor로 변환한다. dataloader이라는 함수를 통해 학습에 필요한 차원으로 변경시킨다. batch_size를 지정해주고, shuffle이란 데이터 샘플의 순서를 섞을지 말지에 대한 인자이다. 최적화 알고리즘은 SGD(stochastic gradient descent)를 사용했다. 손실함수로는 crossentropy를 사용했다. epoch이란 학습 반복 횟수를 지정해주는 것이다.반복을 하면서 에측에 대한 loss를 구하고, 역전파를 진행하고, 그에 대해 가중치를 최적화 한다.output :tensor([[-0.1946, -0.2287, -0.1576], [-0.4010, -0.0588, 0.0440], [-0.1065, -0.0667, -0.1870], [ 0.0362, -0.2685, 0.0521]], grad_fn=&amp;lt;AddmmBackward0&amp;gt;) torch.Size([4, 3])pred : tensor([2, 2, 1, 2]) torch.Size([4])conf : tensor([-0.1576, 0.0440, -0.0667, 0.0521], grad_fn=&amp;lt;MaxBackward0&amp;gt;) torch.Size([4])ground truth : tensor([1, 2, 1, 0])torch.max를 하게 되면, 2가지가 출력된다. confidence라는 최대 확률값들과 prediction이라는 최대값들의 index를 리턴하게 된다. output의 shape은 [batch size, n_classes]이다. 즉 1 batch마다의 출력값들이 존재하는데, 이는 총 class의 개수만큼의 길이를 가지고 있다. 이는 각 클래스마다의 분류 확률값을 의미한다. dim=1이므로 각 한 행마다, 즉 batch마다의 최대값에 대한 값(confidence), 인덱스(prediction)을 출력할 수 있다. prediction과 GT값을 비교하여 정확도를 계산할 수 있다.이 때, argmax를 사용하면 confidence값은 받지 않고, prediction 값, 즉 index 값만 받아올 수 있다._, pred = torch.max(output, dim=1)pred = torch.argmax(output, dim=1)마지막에는 평균 loss와 accuracy를 알아야 하므로 데이터 샘플 개수만큼 나눠준다.import matplotlib.pyplot as pltplt.figure(figsize=(20,5))plt.subplots_adjust(wspace=0.2)plt.subplot(1,2,1)plt.title(&quot;loss&quot;)plt.plot(losses)plt.xlabel(&quot;epochs&quot;)plt.subplot(1,2,2)plt.title(&quot;accuracy&quot;)plt.plot(accures)plt.xlabel(&quot;epochs&quot;)결과를 그래프로 보면 다음과 같다. inferenceoutput = net(x_test)print(torch.max(output, dim=1))_, prediction = torch.max(output, dim=1)accuracy = round((prediction = y_test).sum().item() / len(y_test),4)print(round(accuracy,4))# -----------------------------#torch.return_types.max(values=tensor([0.2726, 1.2251, 0.5746, 0.2792, 0.4174, 1.6891, 0.2693, 1.3956, 0.2245, 1.5383, 0.5694, 0.3931, 1.6798, 0.4401, 0.8937, 1.5574, 1.4948, 0.4101, 2.0672, 0.9091, 0.3115, 0.8186, 1.6430, 1.2262, 1.7812, 0.5254, 1.0002, 0.0993, 0.4572, 2.2780, 0.4420, 0.5622, 0.4398, 1.8892, 0.9751, 1.6513, 0.4286, 1.4288], grad_fn=&amp;lt;MaxBackward0&amp;gt;),indices=tensor([1, 0, 2, 2, 1, 0, 1, 0, 2, 0, 2, 1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 2, 0, 2, 0]))0.9211정확도가 0.92가 나왔다. 모델의 구성에 비해 너무 높게 나왔긴 했다.Tensorflow Autograd 그래디언트 테이프텐서플로우는 자동 미분을 위한 tf.GradientTape API를 제공한다. 이는 컨텍스트 안에서 실행된 모든 연산을 테이프에 기록 한다. import tensorflow as tfx = tf.ones((2,2))with tf.GradientTape() as t: t.watch(x) # x값을 본다. y = tf.reduce_sum(x) print(&#39;y: &#39; ,y) z = tf.multiply(y, y) print(&quot;z :&quot;, z)dz_dx = t.gradient(z, x)print(dz_dx)for i in [0,1]: for j in [0,1]: assert dz_dx[i][j].numpy == 8.0 ## 값이 틀릴 경우 assertionerror가 발생dz_dy = t.gradient(z, y)print(dz_dy)assert dz_dy.numpy() == 8.0 # ----------------------------- #y : tf.Tensor(4.0, shape=(), dtype=float32)z : tf.Tensor(16.0, shape=(), dtype=float32)tf.Tensor([[8. 8.] [8. 8.]], shape=(2, 2), dtype=float32)tf.Tensor(8.0, shape=(), dtype=float32)이 때, gradient를 호출하면 gradienttape에 포함된 리소스가 해제된다. 따라서 여러 그레디언트를 계산하려면 다음과 같이 정의해야 한다.x = tf.constant(3.0)with tf.GradientTape(persistent=True) as t: t.watch(x) y = x * x z = y * y # z = x^4dz_dx = t.gradient(z, x)dy_dx = t.gradient(y, x)print(dz_dx, &quot;\\n&quot;, dy_dx)# ----------------------------- #tf.Tensor(108.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)del t반복적으로 본 후에는 삭제를 꼭 해주어야 한다.고계도(Higher-order) 그래디언트gradientTape 컨텍스트 매니저 안에 있는 연산들은 자동 미분을 위해 기록된다. 만약 이 컨텍스트 안에서 그래디언트를 계산하면 해당 그레디언트 연산 또한 기록된다.x = tf.Variable(1.0)with tf.GradientTape() as t1: with tf.GradientTape() as t2: y = x**3 dy_dx = t2.gradient(y, x)d2y_dx2 = t1.gradient(dy_dx, x)print(dy_dx.numpy())print(d2y_dx2.numpy())# ----------------------------- #3.06.0신경망 구현하기sequential을 사용했을 때의 코드는 다음과 같다.import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersmodel = keras.Sequential( [ # layers.Dense(2, activation=&#39;relu&#39;, name=&#39;layer1&#39;), layers.Dense(3, activation=&#39;relu&#39;, name=&#39;layer2&#39;), layers.Dense(4, name=&#39;layer3&#39;), ])x = tf.ones((3,3))y = model(x)print(y)# ----------------------------- #tf.Tensor([[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)3x3 행렬이 노드가 2개인 층 -&amp;gt; 3개인 층 -&amp;gt; 4개인 층을 거쳐 출력값이 한 벡터당 4개로 출력이 되는 것을 볼 수 있다.sequential을 사용하지 않고 층을 쌓을 수 있다.layer1 = layers.Dense(2, activation=&#39;relu&#39;, name=&#39;layer1&#39;)layer2 = layers.Dense(3, activation=&#39;relu&#39;, name=&#39;layer2&#39;)layer3 = layers.Dense(4, name=&#39;layer3&#39;)x = tf.ones((3,3))y = layer3(layer2(layer1(x)))print(y)# ----------------------------- #tf.Tensor([[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)Sequential을 사용하는 것이 좋아보인다. 여기서 add 함수를 사용하여 층을 쌓을 수 있다.import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersmodel = keras.Sequential()model.add(layers.Dense(2, activation=&#39;relu&#39;))model.add(layers.Dense(3, activation=&#39;relu&#39;))model.add(layers.Dense(4))x = tf.ones((3,3))y = model(x)print(y)# ----------------------------- #tf.Tensor([[-0.6980011 -1.1421962 0.5842113 1.1211115] [-0.6980011 -1.1421962 0.5842113 1.1211115] [-0.6980011 -1.1421962 0.5842113 1.1211115]], shape=(3, 4), dtype=float32)add가 되는 것처럼 pop 메서드도 사용이 가능하다.model.pop()print(len(model.layers))2패션 MNIST 사용한 분류패션 MNIST데이터에는 10개의 카테고리와 70000개의 흑백이미지가 포함되어 있다. 이미지의 해상도는 28x28이다. 훈련 데이터셋은 6만장, 테스트 데이터셋은 1만장을 사용한다.import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as pltfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()class_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]train_images.shape# ----------------------------- #(60000, 28, 28)plt.figure()plt.imshow(train_images[0])plt.colorbar()plt.show()train_images = train_images / 255.0test_images = test_images / 255.0신경망 모델에 주입하기 전에 값의 범위를 0~1로 normalize한다.plt.figure(figsize=(10,10))for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.imshow(train_images[i],cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]])plt.show()정규화된 이미지를 여러 장 본다.model = keras.Sequential([ keras.layers.Flatten(input_shape=(28,28)), keras.layers.Dense(128, activation=&#39;relu&#39;), keras.layers.Dense(10, activation=&#39;softmax&#39;),])model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])model.fit(train_images, train_labels, epochs=5)# ----------------------------- #Epoch 1/51875/1875 [==============================] - 7s 3ms/step - loss: 0.4982 - accuracy: 0.8246Epoch 2/51875/1875 [==============================] - 6s 3ms/step - loss: 0.3744 - accuracy: 0.8662Epoch 3/51875/1875 [==============================] - 7s 4ms/step - loss: 0.3359 - accuracy: 0.8782Epoch 4/51875/1875 [==============================] - 4s 2ms/step - loss: 0.3115 - accuracy: 0.8861Epoch 5/51875/1875 [==============================] - 4s 2ms/step - loss: 0.2932 - accuracy: 0.8922&amp;lt;keras.callbacks.History at 0x7ff871842590&amp;gt;모델은 이와 같고, 입력을 위해 이미지 행렬을 벡터 형태로 편다. 그리고 마지막에는 확률값 출력을 위해 softmax를 사용했다.compile은 모델에 도구들을 지정해준다. adam 이외에 SGD 등을 사용할 수 있다. loss에는 예측값이 정수값으로 나올 경우 sparse_categorical_crossentropy를 사용한다고 한다.그 후 fit을 통해 실제 학습이 진행된다.케라스에는 모델을 시각화하는 함수가 있다.keras.utils.plot_model(model, show_shapes=True) validation모델을 통해 검증을 한다.test_loss, test_acc= model.evaluate(test_images, test_labels, verbose=2)print(&quot;test loss : &quot;, test_loss)print(&quot;test accuracy : &quot;, test_acc)# ----------------------------- #313/313 - 1s - loss: 0.3475 - accuracy: 0.8763 - 792ms/epoch - 3ms/steptest loss : 0.3474982678890228test accuracy : 0.8762999773025513 inference그 후 평가를 진행한다.prediction = model.predict(test_images)prediction[0]np.argmax(prediction[0]) # 9np.argmax(prediction[0]) == test_labels[0] # True# ----------------------------- #array([2.8452354e-09, 9.4890495e-09, 9.5172211e-08, 4.2439359e-09, 1.4116972e-07, 2.8057180e-03, 4.7427403e-07, 1.1753553e-02, 1.5509060e-05, 9.8542446e-01], dtype=float32)9Trueprediction에는 모든 10개의 신뢰도를 나타낸다. 그 후 argmax를 사용하면 가장 예측값이 높은 레이블의 인덱스가 출력된다. 그것을 test_label과 비교하여 정답인지 확인한다." }, { "title": "[데브코스] 9주차 - DeepLearning Mathematics for Machine Learning", "url": "/posts/mlmath/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-13 15:40:00 +0900", "snippet": "기계 학습에서 수학은 손실함수를 정의하고 손실함수의 최저점을 찾아주는 최적화 이론에 사용된다. 제어를 함에 있어서도 수학이 필요하다.선형대수데이터는 벡터나 행렬, 텐서 형태로 되어 있는데, 이에 대한 공간을 이해하고, 연산을 하기 위해서는 선형대수가 필요하다.벡터와 행렬벡터는 요소의 종료와 크기를 표현한다.\\[x \\in R^n\\]위의 식은 벡터,x 는 실수(R)이 n개만큼 존재한다는 식이다.데이터(샘플)은 특징 벡터로 표현된다. 특징이 4개라면 4차원의 특징 벡터이다. 또는 이미지를 벡터화하면 4x4 크기의 이미지라면 16차원의 특징 벡터로 표현이 된다. 데이터 집합의 여러 개 특징 벡터는 아래 첨자(_)로 표현한다.\\[x_1 = \\left( \\begin{matrix} 5.1 \\\\ 3.5 \\\\ 1.4 \\\\ 0.2 \\end{matrix} \\right)x_2 = \\left( \\begin{matrix} 4.9 \\\\ 3.0 \\\\ 1.4 \\\\ 0.2 \\end{matrix} \\right)x_3 = \\left( \\begin{matrix} 4.7 \\\\ 3.2 \\\\ 1.3 \\\\ 0.2 \\end{matrix} \\right), \\cdots ,x_n = \\left( \\begin{matrix} 5.9 \\\\ 3.0 \\\\ 5.1 \\\\ 1.8 \\end{matrix} \\right)\\]행렬은 여러 개의 벡터를 담은 것으로 $ x_{i,j} $ 로 표현한다. 이 때, i는 행 번호, j는 열 번호이다. 훈련 데이터셋을 담은 행렬을 설계행렬(design matrix)라 부른다. 위의 n개의 샘플들을 설계 행렬 X로 표현할 수 있다.\\[X = \\left( \\begin{matrix} 5.1 \\ 3.5 \\ 1.4 \\ 0.2 \\\\ 4.9 \\ 3.0 \\ 1.4 \\ 0.2 \\\\ 4.7 \\ 3.2 \\ 1.3 \\ 0.2 \\\\ \\vdots \\\\ 5.9 \\ 3.0 \\ 5.1 \\ 1.8 \\end{matrix} \\right) =\\left( \\begin{matrix} x_{1,1} \\ x_{1,2} \\ x_{1,3} \\ x_{1,4} \\\\ x_{2,1} \\ x_{2,2} \\ x_{2,3} \\ x_{2,4} \\\\ x_{3,1} \\ x_{3,2} \\ x_{3,3} \\ x_{3,4} \\\\ \\vdots \\\\ x_{n,1} \\ x_{n,2} \\ x_{n,3} \\ x_{n,4} \\end{matrix} \\right)\\]세로를 row, 행이라 하고, 가로를 column, 열이라 한다. 전치행렬 $ A^T $행 요소와 열 요소가 뒤바뀌는 것을 말한다. 전치행렬을 사용하는 이유는 행렬 연산을 할 때 변경을 시킨다. transpose를 하여 딥러닝 연산을 한다. 예르 들어 tensorflow에서의 행렬과 pytorch에서의 행렬 형태가 다르게 입력받는다. pytorch의 경우에는 채널이 0번에 오지만, tensorflow에서는 2번에 채널이 들어간다.$ (A^T){i,j} = A{i,j} $$ (AB)^T = B^TA^T $원래의 벡터는 1열, n행으로 이루어져 있다. 이를 행렬로 표현하려면 전치를 해서 1행에 집어넣게 된다.행렬을 이용하면 방적식을 간결하게 표현이 가능하다. 또한 행렬이나 벡터을 연산을 표현하기가 편하다.특수행렬 정방행렬(정사각행렬) n x n 크기의 행렬 \\[\\left( \\begin{matrix} 2 \\ 1 \\ 2 \\\\ 4 \\ 3 \\ 1 \\\\ 7 \\ 23 \\ 3 \\end{matrix} \\right)\\] 대각행렬 우하향 대각선의 요소들만 값을 가지고 나머지는 0인 행렬 \\[\\left( \\begin{matrix} 2 \\ 0 \\ 0 \\\\ 0 \\ 3 \\ 0 \\\\ 0 \\ 0 \\ 3 \\end{matrix} \\right)\\] 단위행렬 우하향 대각선 요소들의 값이 1이고 나머지는 0인 행렬 \\[\\left( \\begin{matrix} 1 \\ 0 \\ 0 \\\\ 0 \\ 1 \\ 0 \\\\ 0 \\ 0 \\ 1 \\end{matrix} \\right)\\] 대칭행렬 우하양 대각선을 기준으로 대칭인 행렬 또는, 전치를 해도 동일한 행렬 \\[\\left( \\begin{matrix} 2 \\ 5 \\ 7 \\\\ 5 \\ 3 \\ 10 \\\\ 7 \\ 10 \\ 3 \\end{matrix} \\right)\\] 역행렬(matrix inversion)역행렬은 $ A^{-1} $ 로 표기하고, 특징은 $ AA^{-1} = 1 $ , 반드시 1이 된다.행렬 연산행렬 곱셈\\[c_{i,j} = \\sum_{k=1,s} a_{i,k}b_{k,j}\\]\\(행렬 A = \\left( \\begin{matrix} 3 \\ 4 \\ 1 \\\\ 0 \\ 5 \\ 2 \\end{matrix} \\right), 행렬 B = \\left( \\begin{matrix} 2 \\ 0 \\ 1 \\\\ 1 \\ 0 \\ 5 \\\\ 4 \\ 5 \\ 1 \\end{matrix} \\right)\\)을 곱하면 2*3 행렬 C이 된다.\\[C = \\left( \\begin{matrix} 14 \\ 5 \\ 24 \\\\ 13 \\ 10 \\ 27 \\end{matrix} \\right)\\]이는 교환법칙(AB≢ BA)이 성립되지 않지만, 분배법칙(A(B+C) = AB+AC)이나 결합법칙(A(BC) = (AB)C)은 성립된다.\\[a ∙ b = a^Tb = \\sum_{k=1,d} a_kb_k\\]내적은 유사도를 측정하는 것이다. 두 벡터가 벌어진 정도를 본다. 이 때, 크기와 방향을 둘 다 고려해서 90도 이하의 각도를 이룬다면 양, 90도보다 크다면 음, 90도라면 0이 나온다.이 내적은 신경망에서 내가 가지고 있는 값과 입력 데이터와 비교하여 얼마나 유사한지 비교할 때 사용한다.행렬의 곱을 할 때는 첫번째 행렬의 행과 두번째 행렬의 열이 같아야 한다. 2x4 행렬 A에 1x4 가중치 x를 곱하면 1x2의 행렬 b가 나오게 된다. 즉, A 행렬을 입력으로 하여 가중치를 곱했더니 저차원으로 투영된다.\\[A = \\left( \\begin{matrix} 4 \\ -3 \\ 1 \\ 3 \\\\ 2 \\ 0 \\ 5 \\ 1 \\end{matrix} \\right) ∙ \\left( \\begin{matrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{matrix} \\right) = \\left( \\begin{matrix} 5 \\\\ 8 \\end{matrix} \\right)\\]행렬의 곱이란 공간의 변환이라 할 수 있다.텐서3차원 이상의 구조를 가진 숫자 배열을 텐서라 한다. 따라서 각 차원마다의 이름은 다음과 같다. 0차 : 수(Scalar) 1차 : 벡터 2차 : 행렬 3차이상 : 텐서RGB 컬러 영상도 텐서에 해당한다. 유사도와 거리벡터를 기하학적으로 해석을 하면 두 벡터 간의 각도를 $ \\theta $ 를 가질 것이다. 이 $ \\theta $ 가 작을수록 유사하다고 판단할 수 있다.또는 코사인 유사도라는 것도 있다. 내적 등에 의해 각도를 측정하는 방법이다.\\[cosin similarity(a,b) = \\frac{a}\\|\\|a\\|\\|} \\cdot \\frac{b}{\\|\\|b\\|\\|} = cos(\\theta)\\]벡터의 거리(크기)는 놈, norm 으로 측정한다.p차 벡터에서의 놈 : \\(\\parallel x \\parallel_p = (\\sum_{i=1,d} \\|x_i\\|^v)^{\\frac{1}{p}}\\)최대 놈 : \\(\\parallel x \\parallel_\\infty = max(\\|x_1\\|,\\|x_2\\|,\\cdots,\\|x_d\\|)\\) 1차(p=1) 놈(absolute value norm or Manhattan distance) 2차(p=2) 놈(euclidean norm) 최대(p=infinite) 놈(max norm)1차, 2차, 최대 놈의 물리적 의미는 크기가 1인 요소 값들을 다 연결을 해보면 다음과 같이 만들어진다. 첫번째가 1차, 두번째가 2차, 3번째가 최대 놈이다. 1차는 다이아몬드 형태가 만들어지고, 2차는 원, 3차는 정사각형이 만들어진다고 한다.2차 놈을 계산을 해보면 x = (3 -4 1) 일 때, 2차 놈은 $ ||x||_2 = (3^2 + (-4)^2)^{\\frac{1}{2}} = 5.099 $ 가 된다.행렬의 크기를 재는 것은 프로베니우스 놈(probenius norm)이 있다.\\[\\parallel A \\parallel_F = \\left(\\sum_{i=1,n} \\sum_{i=1,m} a_{ij}^2 \\right)^{\\frac{1}{2}}\\]이를 숫자로 풀어보면 다음과 같다.\\[\\parallel ( \\begin{matrix} 2 \\ 1 \\\\ 6 \\ 4 \\end{matrix} ) \\parallel_F = \\sqrt{2^2+1^2+6^2+4^2} = 7.550\\]이 놈을 딥러닝에서 최적값을 찾기 위해 사용한다면, 찾으러 가는 방향이 달라질 수 있다. 왼쪽 부분이 최적값, 즉 optimal point인데, 이로 이동시키도록 규제를 정하는 것이다.퍼셉트론1958년에 고안한 분류기 모델로, 다양한 벡터들이 들어오면 각각을 가중치를 곱해 연산을 해서 한가지의 결과값을 얻는 구조이다.이 때, 입력 벡터를 어떻게 처리할 것인지에 대한 함수를 활성함수(activation function) 이라 하는데, 이 때는 계단 함수를 사용한다.0 = f(w∙x) 에서 f는 활성함수이고, 이 때, w∙x, 즉 유사도가 특정 값 T를 넘으면 1, 못 넘으면 -1을 주는 방식이다.퍼셉트론을 물리적 의미로 보자면, 공간을 임계값으로 나누는 threshold값이 존재하고, 이를 결정 직선(decision line)이라 한다. 이는 w에 수직하고 원점으로부터 \\(T/\\|\\|w\\|\\|_2\\) 만큼 떨어져 있다. 3차원에서는 결정 평면이 되고, 4차원에서는 결정 초평면이다.여러 개의 퍼셉트론으로 표현한 것을 멀티 퍼셉트론이라 한다. 그렇게 되면 1개의 퍼셉트론에 대한 출력이 아닌 여러 개의 출력이 생성된다. 이 출력은 벡터로 표시된다. j번째 퍼셉트론의 가중치 벡터가 존재할 것이고 이에 대한 벡터는 $ (w_{j1},w_{j2},w_{j3}, \\cdots , w_{jd})^T $ 로 표현이 된다.따라서 이를 이미지로 생각해보면, 이미지 32x32x3를 입력으로 하면 입력은 1열로 펴서 3072x1 의 벡터로 표현 되고, 출력이 10개로 분류를 한다면 출력은 10x1이 되어야 할 것이다. 따라서 W는 10x3072 의 행렬이 된다.그래서 32x32x3을 입력으로 하여 활성 함수로 집어넣으면 가중치 w와 곱해져서 퍼셉트론에 의해 출력값이 나오게 될 것이고, 그로 인해 10개의 클래스를 분류할 수 있다.이 퍼셉트론을 여러 층으로 확장한 것을 MLP(Multi Layer Perceptron)이라 한다.기저벡터(basis) a,b의 선형 결합으로 인해 만들어지는 공간을 벡터 공간(vector space)라 부른다.즉 a와 b의 결합을 하여 c라는 벡터가 추출된다면 모든 c의 경우의 점들이 그리는 공간을 벡터 공간이라 한다.위의 이미지 벡터를 f(w∙x) 로 연산하는 과정에서 역행렬을 사용한다. 우리가 학습을 할 때는 w, 가중치를 최적값으로 만드는 것이다. 그렇다면 우리가 wx = b 라는 값이 추출되는데, w에 대해 확인해야 하므로 x를 우항으로 넘겨야 한다.$ wx = b $$ x^{-1}xw = x^{-1}b $$ Iw = x^{-1}b $$ w = x^{-1}b $이 때, 선형 방정식의 경우 해가 유일한 경우에만 역행렬을 이용하여 해를 구한다. 그렇다면 x가 다음과 같은 필요충분조건을 가진다. x의 행렬식(determinant)은 0이 아니다. $ x^{-1}x $는 양의 정부호 대칭 행렬이다 x의 고유값은 모두 0이 아니다. 행렬식A 행렬의 행렬식(determinant)는 det(A)로 표기한다.$ det(A) = ad - bc $ 로 표현하고, 행렬식이 의미하는 것은 행렬의 곱에 의한 공간의 확장이나 축소를 해석해주는 것이다. 만약 det(A) = 0 이라면 하나의 차원을 따라 축소되어 부피가 줄어들게 되고, det(A) = 1이면 부피가 유지되고, 방향이 보존되는 변환이다. det(A) = -1 이라면 부피 유지되지만 방향이 보존이 되지 않는 변환이다. 또한, det(A) = 5이면 5배 부피가 확장되며 방향도 보존된다. 정부호(definiteness) 행렬양의 정부호 행렬이란 0이 아닌 모든 벡터 x에 대해 $ x^TAx &amp;gt; 0 $ 인 상태를 말한다.\\(\\left( \\begin{matrix} x_1 \\ x_2 \\end{matrix} \\right) \\left( \\begin{matrix} 1 \\ 0 \\\\ 0 \\ 2 \\end{matrix} \\right) \\left( \\begin{matrix} x_1 \\\\ x_2 \\end{matrix} \\right) = x_1^2 + 2x_2^2\\) 이르모 이 행렬은 양의 정부호 행렬이다.양의 정부호 행렬인 경우 고유값이 모두 양수이고, 역행렬도 정부호 행렬이고, det(A) =\\ 0 이다. 분해(decomposition)정수 3717은 이대로는 특성이 안보이지만, 이를 소인수 분해하면 337*59로 3으로 나뉘고, 7로도 나눌 수 있다는 특성이 보인다. 따라서 행렬도 분해를 해서 특성을 확인한다.행렬을 분해할 때는 고유값(eigenvalue) 분해를 한다. 행렬 A를 고유값 분해하는 식은 다음과 같다.\\(Av = \\lambda v\\) , 여기서 v는 고유 벡터 $ \\lambda $ 는 고유값이다.고유값 분해란 행렬 A의 특성(방향)은 변하지 않고 크기에 대해서만 커지거나 작아지는 특징을 가진다.고유값 분해를 하면 원으로 표현된 행렬이 타원으로 변환될 수 있다. 고유 분해(eigen-decomposition)$ A = QVQ^{-1} $ 이 있을 때, Q는 A의 고유 벡터를 열로 배치한 행렬이고, V는 고유값을 대각선으로 배치한 대각 행렬이다. 고유 분해는 고유값과 해당 고유 벡터가 존재하는 정사각행렬에서만 가능하다. 하지만 기계학습에서는 정사각행렬이 아닌 경우의 분해도 필요하므로 고유 분해를 사용함에 한계가 존재한다. 특이값 분해(SVD : singular value decomposition)$ A = U\\sum V^T $이 때, U는 $ AA^T $ 의 고유벡터를 열로 배치한 n x n 행렬이다. V는 $ A^TA $의 고유 벡터를 열로 배치한 m x m 행렬이다. $ \\sum $ 은 $ A^TA $의 고유값의 제곱근을 대각선에 배치한 n x m 대각 행렬이다.이 고유 분해와 특이값 분해를 통해 역행렬을 구할 때 쉽게 연산하기 위해서다. 특이값 분해에서 기하학적으로 접근을 해보면 V는 회전 변환, D는 크기 변환, U는 회전 변환이 된다.특이값 분해(SVD)는 정사각행렬이 아닌 행렬의 역행렬을 계산하는데 사용된다.$ A = U \\sum V^T =&amp;gt; A = V \\sum^{-1} U^T $확률과 통계기계 학습이 처리할 데이터는 불확실한 세상에서 발생하므로 불확실성에 대한 확률과 통계가 활용된다.확률 기초다섯가지 경우 중 한 값을 갖는 확률 변수 x는 정의역{y1,y2,y3,y4}을 가진다. 확률분포(probability distribution) 확률질량함수(probability mass function) 이산 확률 변수에 대한 함수다. 즉 각 확률이 discrete적인 것들에 대해서 설명된다. 확률밀도함수(probability density function) 연속 확률 변수에 대한 함수다. 즉 특정값을 지정하는 것이 아닌 범위적으로 확률을 구한다. 특정 x가 아닌 p(x)를 통해 입력값을 구한다. 확률벡터(random vector)확률에 대한 벡터로 나타낸 것을 말한다. 조건부 확률x에 대한 이벤트 이후에 y에 대한 이벤트가 발생하고, 특정 조건에 만족하는 상황에 대한 확률을 구하고자 한다. 그렇다면\\[P(Y = y \\| X = x) = \\frac{P(Y = y, X = x)}{P(X = x)}\\] 곱 규칙과 합 규칙곱 규칙 : \\(P(y,x) = P(x\\|y)P(y)\\)합 규칙 : \\(P(x) = \\sum_{y} P(y,x) = \\sum_{y} P(x\\|y)P(y)\\)이 식들은 조건부 확률 식에서 항을 이동한 것이다. 우리가 알고 싶은 것은 P(y,x)이므로 조건부 확률 식을 활용한다.예를 들어 8개의 공이 들어 있는 주머니에서 번호를 뽑아 그 번호의 파란공과 하얀공이 들어있는 주머니에서 공을 뽑고자 한다. 이 때 번호는 1번, 공의 색은 흰색이어야 한다면, 확률은 P(y = 1, x = 흰색) 이 되는데, 이를 구하기는 어렵기 때문에, 조건부 확률 식으로 변환하여 P(x = 하양 \\| y = 1) P(y = 1) 의 식이 된다.그렇다면 하얀 공이 뽑힐 확률은 1번에서 흰색, 2번에서 흰색, 3번에서 흰색이 나올수 있다. 그 식은 다음과 같다.\\[P(하양) = P(하양\\|1)P(1) + P(하양\\|2)P(2) + P(하양\\|3)P(3) = \\frac{9}{12}\\frac{1}{8} + \\frac{5}{15}\\frac{4}{8} + \\frac{3}{6} \\frac{3}{8}\\]정리하면 확률 P(x) 는 y에 대한 모든 경우의 P(y,x)를 다 합하는 것이다. 이 P(y,x)는 P(x\\|y)P(y)가 된다. 확률의 연쇄 법칙$ P(x_1, x_2, \\cdots , x_n) $ 이라면 이는 조건부 확률에 의해 $ P(x_1) \\Pi_{i=2}^n P(x_i \\| x_1, \\cdots , x_i-1)$ 이 된다. 독립x와 y 상황이 서로 영향을 미치지 않을 경우 독립이라 한다.따라서 $ P(X = x, Y = y) = P(X = x)P(Y = y) $ 로 표현된다. 조건부 독립독립은 현실에서는 거의 불가능한 조건이다. 따라서 조건에 성립될 때만 독립이라 하여 조건부 독립이라 한다.상황 X,Y,Z 가 있을 때 Z = z일 때만 X,Y가 독립이라면 조건부 독립이다.$ P(X = x, Y = y, Z = z) = P(X = x \\| Z = z)P(Y = y \\| Z = z) $ 기대값내가 원하는 값을 얻기 위해 특정 상수를 곱해주는 상황을 말한다. 즉, 확률 분포가 존재할 때 내가 원하는 출력값을 얻기 위해 가중치를 곱해주는 것이다.\\[E_{x~P}\\|f(x)\\| = \\sum_x P(x)f(x)\\]베이즈 정리\\(P(y,x) = P(x\\|y)P(y) = P(x,y) = P(y\\|x)P(x)\\)\\(P(y\\|x) = \\frac{P(x\\|y)P(y)}{P(x)}\\)Bayes&#39;s rule이란 P(y,x)와 P(x,y)는 같으므로 이를 활용해서 새로은 식을 만든다. 이 식은 매우 많이 사용된다.예를 들어 흰색 공이 나왔는데, 어느 주머니인지 모르지만, 흰색 공이 나올 확률을 추정하라고 한다면 argmax를 사용한다.\\[\\hat y = argmax_y P(y\\|x)\\]x가 흰색 공인데, x가 최대가 되는 y를 알려달라는 식이다.이 베이즈 정리 식에서 좌항을 사후 확률(posteriori), P(x|y)를 likelihood, P(y)를 사전확률(prior)이라 한다. 기계 학습은 사후 확률을 구하는 과정이다. 우도는 관찰값을 보고 추측하는 것과 같다.posteriori = likelihood * prior이 베이즈 정리를 iris 데이터 분류 문제에 적용을 한다면 어떻게 될까?특징 벡터 x가 있을 때 분류 y : {setosa, versicolor, virginica}이 분류 문제를 argmax로 표현하면 다음과 같다.\\[\\hat y = argmax_y P(y\\|x)\\]즉, x가 주어졌을 때, 어떤 y인지에 대한 값이 분류기다. 즉, 이미지 벡터를 추정을 하여 나온 값이 다음과 같다고 생각해보자. P(setosa|x) = 0.18 P(versicolor|x) = 0.72 P(virginica|x) = 0.10여기서 argmax 즉, 최대값은 2번째 값이 되므로 이 이미지는 versicolor 이라는 결과를 도출하게 된다.이 때 최대 확률을 최대 우도,max likelihood(ML)이라고도 한다. 그래서 이 ML을 추정한다고 하여 MLE 방법이 있다.최대 우도를 추정한다는 것은, 확률 분포 $\\theta$ 에 대해 $ \\hat \\theta = argmax_\\theta P(y|\\theta) $ 의 식을 가진다. 확인되는 데이터 y에 대한 확률이 최대가 되는 분포를 구한다는 것이다. 즉 입력 데이터와 확률 분포 $ \\theta $ 를 통해 확률을 얻는데, 이 확률이 최대가 되도록 만들어 $ \\hat \\theta $ 를 얻는다.이 최대 우도를 추정할 때는 로그 표현을 사용한다. 그 이유는 조건부 확률은 연쇄 법칙에 의해 곱셈으로 정리되는데, 이 연산을 줄이기 위해 log를 사용하여 덧셈으로 처리한다. log는 단조 증가하는 함수이므로 최대에 대한 대상은 변하지 않는다.분산의 경우 \\(\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\mu)^2\\) 로 표현된다. 분산의 경우 데이터가 얼마나 퍼져 있는지를 확인할 수 있다. 이 때, 차원이 늘어나면 차원과 차원과의 관계가 분산으로 표시될 것이다. 이를 공분산 행렬이라 한다. 공분산 행렬을 통해 차원사이의 상관 정도를 파악한다.유용한 확률 분포 가우시안 분포(Gaussian distribution)평균 $\\mu$ 와 분산 $\\sigma^2$ 으로 정의된다. 평균값과 퍼짐의 정도를 확인할 수 있다. 베르누이 분포(bernoulli distribution)성공 확률이 p이고 실패 확률이 1-p인 분포를 말한다. 이항 분포베르누이 실험을 m번 확대한 것을 말한다. 혼합 분포확률 분포를 여러개 써서 주어진 분포를 구하는 방식을 말한다. 1가지를 여러 번 사용하는 것으로 가우시안을 3개 혼합하여 사용하는 방법이 있다. 변수 변환기존의 확률 분포를 새로운 확률 분포로 바꾸는 것을 말한다. 서로의 확률 분포는 상관관계가 있어야 한다. 모델 구성에는 잘 사용하지 않을 수 있지만, 데이터 생성에 많이 사용된다.로지스틱 시그모이드 함수(logistic sigmoid function)시그모이드 함수는 일반적으로 베르누이 분포의 매개변수를 조정을 통해 얻어진다. 이는 비선형 활성 함수로 딥러닝에 많이 사용되었다.소프트플러스 함수이 또한 활성함수로 많이 사용된다.정보이론정보이론과 확률통계는 많은 교차점을 가지고 있다. 정보이론 관점에서도 기계학습에 접근이 가능하다. 정보이론이란 사건이 지닌 정보를 불확실성에 대해 정량화하여 이용하는 방법을 말한다. 이를 기계학습에 적용한 예로는 엔트로피,교차 엔트로피, KL 발산 등이 있다.확률이 작을수록 더 많은 정보를 가지고 있다. 자주 발생하는 사건보다 잘 일어나지 않는 사건의 정보량이 많다. 자기정보(self information)사건이 가지는 정보량을 나타내는 것으로 $ h(e_i) = - log_2P(e_i) $ 또는 $ h(e_i) = - log_eP(e_i) $ 를 사용한다. 2를 사용하여 로그를 쓸 경우 비트(bit), 자연상수 e를 사용할 경우 나츠(nat)라고 부른다. 대체로 계산을 쉽게 하기 위해 2를 많이 사용한다.동전이 앞면이 나오는 사건의 정보량은 $ -log_2(1/2) = 1 $이지만, 주사위에서 1이 나오는 사건의 정보량은 $ -log_2(1/6) ≓ 2.58 $ 이다. 따라서 후자의 사건이 높은 정보량을 가진다고 할 수 있다. 엔트로피(entropy)자기 정보는 사건을 1개만 보는 것이다. 그래서 이벤트 1개만이 아닌 확률 변수 x의 불확실성을 나타내는 것을 엔트로피라 한다. 따라서 모든 사건 정보량의 기대값을 표현한다.이산확률분포와 연속확률분포일 떄의 엔트로피는 다르게 표현된다. 이산확률분포 : \\(H(x) = - \\sum_{i=1,k} P(e_i)log_2P(e_i)\\) 연속확률분포 : \\(H(x) = - \\int_R P(x)log_2P(x)\\)이 때도, $log_2$ 대신 $log_e$를 사용할수도 있다.자기 정보에 확률을 곱해 전체 다 더했을 때 원래의 자기 정보가 되도록 만든다. 즉 이전의 주사위에서 정보량은 2.58이었는데, 여기서 확률 1/6을 곱하면 0.43이 된다. 이를 각각의 확률, 즉 6번을 더하면 2.58이 된다.정보량 1을 전송할 때는 1비트가 필요하다. 따라서 주사위에 대한 정보량을 전송하려면 2.58비트가 필요하다.불확실성이 가장 클 때가 엔트로피가 가장 크다.엔트로피는 통신에 많이 사용되지만, 우리는 두 확률 분포의 유사도를 측정하는 척도로 사용할 것이다.교차 엔트로피(cross entropy)두 확률 분포 P와 Q 사이의 교차 엔트로피라는 것은 이 두 분포가 얼마나 정보를 공유하고 있는지에 대한 것이다. 교차 엔트로피의 경우 위에서는 자기 자신의 확률을 곱했지만, 이 때는 다른 확률 분포의 확률을 곱한다.\\[H(P,Q) = - \\sum_x P(x)log_2Q(x) = - \\sum_{i,k}P(e_i)log_2Q(e_i)\\]이 식을 전개하면 다음과 같다.\\(H(P,Q) = - \\sum_x P(x)log_2Q(x) = - \\sum_{i,k} P(e_i) log_2Q(e_i)\\) \\(= - \\sum_x P(x) log_2 P(x) - \\sum_x P(x) log_2 P(x) - \\sum_x P(x) log_2 Q(x)\\)\\(= H(P) + \\sum_x P(x) log_2 \\frac{P(x)}{Q(x)}\\)여기서 2번째 항을 KL 발산(divergence) 수식이라 한다. 이 식에서 볼 수 있는 것은 P를 데이터 분포라 하고, Q를 예측값이라 하면 학습 과정에서는 P는 바뀌지 않는다. 따라서 교차 엔트로피를 손실함수로 사용하면 KL 발산을 최소화한다는 것과 같다.이를 딥러닝에서 손실함수로 많이 사용한다. 출력값은 확률로 내보내기 때문에 확률값과 정답(GT)을 비교하는데 이 확률과 정답에 대한 유사도를 비교해야 한다. KL 발산두 확률분포 사이의 거리를 계산할 때 사용한다.\\[KL([ \\|\\| Q]) = \\sum_x P(x) log_2 \\frac{P(x)}{Q(x)}\\]P와 Q의 차이값이 KL 영역이 된다. 그래서 P라는 데이터 분포가 있고, 내가 예측한 값을 맞추는 것이므로 Q를 P로 동일하게 만드는 것이다.최적화기계학습에서의 최적화는 훈련 데이터셋이 주어지고, 이 데이터셋에 의해 출력되는 예측값에 대해 최적의 매개변수를 찾는 것을 말한다.주로 SGD(stochastic gradient descent)를 사용한다. 손실함수를 미분하여 최저점을 구한다. 이때는 역전파(backpropagation) 알고리즘을 사용한다. 매개변수의 공간$ y = f(w \\cdot x) + b $ 가 있을 때, 이 w가 만드는 공간을 매개변수 공간이라 하고, 이 매개변수 공간에서 최소의 손실함수를 출력하는 매개변수 벡터를 찾는 것이 목표다.특징 공간은 높은 차원을 가지지만, 훈련 데이터셋의 양이 작기 때문에 완벽한 확률분포를 찾는 것은 불가능하다. 대신 적잘한 모델을 선택하고 손실함수를 정의한 후 매개변수 공간을 탐색해서 손실함수가 최저가 되는 매개변수의 점(벡터)을 찾는 전략을 사용한다.미분을 할 때는 최저점으로 도달해야 하기 때문에 미분값에 (-)를 붙여줘야 한다. 예를 들어, 초기점 x_0 = (-0.5,-0.5)^T 일 때, $f\\prime (x_0) = (-2.5125, -2.5)^T $ 라면 $f\\prime (x_0) $ 는 현재의 방향은 경사는 값이 커지는 방향으로 되어 있다. 그러나 우리가 원하는 것은 최저점이므로 반대방향으로 해야 한다. 따라서 $-f\\prime (x_0) $ 를 해서 계산해야 한다. 미분의 연쇄 법칙합성함수 f(x) = g(h(i(x))) 를 미분하면 다음과 같다.$ f\\prime (x) = g\\prime (h(i(x))) h\\prime (i(x)) i\\prime(x) $다층 퍼셉트론은 합성함수이다. 왜냐하면 입력층 -\\&amp;gt; 은닉층1 -\\&amp;gt; 은닉층2 -\\&amp;gt; 출력층이 있다면 이 출력층은 은닉층2(은닉층1(입력층))의 출력값이 될 것이다. 따라서 이에 대해서 미분을 할 때도 연쇄법칙이 적용된다.행렬 미분 방법 야코비언 행렬(Jacobian matrix)행렬을 1차 편도 미분하는 방법으로 야코비언 행렬을 사용한다. 이 야코비언은 어떤 행렬에 대해 각각의 요소를 모두 미분한 값이라 할 수 있다. 헤세 행렬(Hessian matrix)2차 편도 미분을 해서 얻은 행렬을 헤세 행렬이라 한다.경사 하강법값이 낮아지는 곳으로 찾아가는 원리를 경사 하강법이라 한다.기울기 $ g = dw = \\frac{\\partial J}{\\partial w} $에 대해 매개변수 w의 경사 하강법의 식은 다음과 같다.$ w = w - \\rho g$함수의 기울기를 구하여 기울기가 낮은 쪽으로 반복적으로 이동하여 최소값에 도달한다.batch 경사 하강 알고리즘학습을 할 때는 반복을 해서 진행을 하는데, 한 샘플마다 경사하강을 하면 너무 오랜 시간이 걸린다. 그래서 batch(집합)단위로 나누어 샘플들의 그래디언트를 구하고 평균을 한 후 마지막에 갱신을 하기도 한다.확률론적 경사 하강 알고리즘(stochastic gradient descent)그러나 위의 방법을 사용하더라도 전체 데이터셋을 다 봐야 하기 때문에 좋은 방법은 아니다. 따라서 batch(집합)단위로 나누어 그래디언트를 각각 구해 집합 단위로 평균을 내는 것은 동일하나 batch 단위마다 즉시 갱신을 한다.간단하게 말해서 데이터셋이 24개가 있고, batch size를 8이라 하면 8개씩 묶어서 3개의 집합을 만든다. 그래서 1개의 batch에 대해 각 샘플마다의 그래디언트를 구하고, 이들을 평균내서, 가중치를 갱신하고 다음 batch로 넘어간다.여기서 중요한 것은 이 경사 하강법만 사용하지 않고 제어 알고리즘을 추가해서 경사 하강한다. 제어 알고리즘에는 Momentum, Adam 등이 있다." }, { "title": "[데브코스] 9주차 - DeepLearning AI &amp; machine learning", "url": "/posts/dplearn/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-12 15:40:00 +0900", "snippet": "기계 학습어떤 컴퓨터 프로그램이 T라는 작업을 수행할 때, 이 프로그램의 성능이 P라는 척도로 평가했을 때 경험 E를 통해 성능이 개선된다면 이 프로그램은 학습한다고 말할 수 있다. 따라서 최적의 알고리즘을 찾는 행위를 기계 학습이라 할 수 있다. 기계 학습의 중심은 경험, 과업, 성능에 있다.예전에는 지식 기반의 학습을 했다. 즉, 인간이 알고 있는 사실을 기계에 전달하여 학습을 하는 것을 말한다. 그러나 인간의 지식이 실제로는 틀린 것일 수 있기 떄문에 변화되었다. 그래서 기계학습으로 넘어왔다. 또한, 사람의 신경망을 모방하여 만든 심층 학습이 나오게 되면서 기계학습 분야가 더 발전하게 되었다.기계학습으로 넘어오면서 데이터가 매우 중요해졌다. 데이터를 통해 학습하여 규칙을 찾고 그를 통해 다음을 예측하기 때문이다. 예측에는 회귀와 분류로 나뉘어진다. 회귀는 목표치가 실수이고, 분류는 특정 부류나 종류의 값을 출력한다. 예를 들어, 분류는 개와 고양이를 분류하는 것이고, 회귀는 좌표점 n개를 통해 다음 점을 예측하는 것이 회귀에 해당한다.기계학습에서 훈련이란 주어진 task에 대해 가장 정확하게 할 수 있게 최적의 매개변수를 찾는 작업이다. 처음에는 임의의 매개변수를 지정하여 시작하지만, 학습하면서 최적의 성능에 도달한다.훈련이 끝나면 추론, infernce를 수행한다. 훈련 데이터가 아닌 새로운 데이터를 입력으로 받아 예측을 한다. 기계학습의 궁극적인 목표는 추론에 대한 오류를 최소화하는 것이다. 테스트 데이터에서 높은 성능을 일반화,generalization 능력이라 부른다.그렇다면 기계학습에서 필수로 있어야 할 것들은 학습을 위한 데이터를 가지고 있어야 하고, 데이터 사이의 규칙이 존재해야 한다.데이터에는 training set, validation set, test set 이 있다. training set은 기계가 훈련하는 셋이고, test set은 훈련된 모델에 추론하고자 하는 데이터를 넣을 때 어떻게 출력되는지에 대한 셋이다. 그렇다면 validation set이 의미하는 것은, test와 비슷하다. 그러나 training set으로 훈련된 모델이 test set으로 바로 평가를 하게 되면 정확도가 떨러질 수 있다. 또한, training set만 사용하여 훈련 후 test set에 적용시키면 과적합,overfitting이 발생하기 쉽다. 그래서 validation set을 통해 훈련 시에도 평가를 함께 수행하며 학습할 수 있다.특징 공간의 이해모든 데이터는 정량적으로 표현되며, 특징 공간 상에 존재한다. 1차원, 2차원 등이 있는데, 이 각각의 차원이 의미하는 바는 각각 서술하고 있는 특징의 개수라 할 수 있다. 즉, 특징이 1개뿐이라면 1차원, 2개가 있다면 2차원이라 할 수 있다.iris 데이터는 4개의 특징 요소를 가지고 있다. 따라서 이는 4차원 특징 공간이라 할 수 있다. 사진을 생각해보면 각각의 픽셀이 값을 가지고 있으므로 4x4 픽셀이라면 각각의 픽셀값을 가질 수 있으므로 16차원이라 할 수 있다.따라서 d차원의 데이터라 하면 d개의 특징 벡터를 가진다. d차원의 특징 공간인 직선 모델을 사용할 경우, 필요한 매개변수의 개수는 d+1이다.\\[y = w1x1 + w2x2 + ... + wdxd + b\\]d+1인 이유는 차수가 1인 x에 대해 d개를 가지고, 0차수인 상수, b를 가지기 때문이다. 그렇다면 2차원 곡선 모델을 사용한다면 매개변수의 수는 지수적으로 증가하여 d^2+d+1 개가 필요하다.따라서 iris 데이터는 4차원이었으므로, 2차 곡선 모델을 사용한다면 4^2 + 4 + 1 이므로 21개의 매개변수가 필요하다. MNIST 데이터를 사용할 경우 d = 784 이므로 총 615,441개의 매개변수가 필요하다.이 특징 벡터 사이의 거리를 구할 때 다양한 거리 공식이 있다. 유클리디안 거리, 맨허튼 거리 등등이 존재하기 때문에 상황에 맞게 잘 적용해야 한다. 차원의 저주 차원이 높아짐에 따라 발생하는 현실적인 문제들이 있다. 차원이 높으면 예측이 더 잘 되지만, 차원이 높아질수록 유의미한 표현을 찾기 위해 지수적으로 더 많은 데이터가 필요하다. 그래서 이를 차원의 저주라 한다.특징 공간을 직선 모델을 사용하여 분류한다면 규칙을 찾기 어렵다. 따라서 decision boundary, 경계선을 잘 설정해야 한다. 이를 위해서 데이터를 새로운 공간 좌표로 다시 변환하여 분류할 수 있다.따라서 이 표현, 즉 데이터를 새로운 공간좌표로 변환하는 학습인 표현 학습도 중요하다. 표현 학습이란 기계가 예측하기 좋은 특징 공간을 자동으로 찾는 것을 말한다.표현 학습에는 HOG(Histogram of oriented gradients)가 있다. nearest neighbor 알고리즘이 나오기 전에 사용되었던 방법으로 이미지에 대한 히스토그램을 그려 분석하는 방법이다. 표현 학습에도 심층 학습이 있다. NN 방법 이후에 이미지의 특징 벡터를 딥러닝이 학습해서 특징을 추출하여 최적의 계층적인 특징을 학습하는 방법을 말한다. low level에서는 선이나 구석점 등을 추출하고, high level에서는 조금 더 추상화된 특징(얼굴, 바퀴)를 추출한다. 이는 이미지 뿐만 아니라 텍스트, 언어 등 많은 곳에 이용된다.데이터 생성 과정기계학습에는 모델을 만드는 것 이외에도 데이터를 전처리하거나 수집하는 것도 중요하다. 데이터를 잘 수집하고, 잘 정제해야 정확도도 향상된다. 문제는 데이터의 양이 어느정도 많아야 할까? MNIST 데이터를 기준으로 보면 차원이 784차원이다. 이것이 흑백 이미지여서 0 또는 1로 이루어져 있다고 하더라도 서로 다른 총 샘플 수는 2^784가지가 된다. 그러나 MNIST의 경우 6만 개의 샘플로만 훈련되어 있다. 그렇다면 이 매우 작은 데이터로 어떻게 높은 성능을 보이는지 파악해봐야 할 것이다. 1과 2, 또는 2과 6의 데이터간에는 군집, 데이터가 모여있는 곳이 생긴다. 즉 scatter, 흩어져 있지 않고, 이미지마다 모여 있는 공간이 존재할 것이다.또한, 온전한 이미지가 있는 이미지 외에 잘려있는 이미지는 입력시키지 않는다. 이도 비슷하게 이상한 데이터를 집어넣지 않음으로서 데이터들이 흩어져 있지 않을 것이다.고차원의 공간을 저차원으로 바라보는 방법을 매니폴드 가정 이라 한다. 고차원의 데이터의 규칙은 관련된 낮은 차원에서도 특징이 연관되어 있을 것이다. 이를 매니폴드라 한다.추가적으로, 데이터가 충분한 양만큼 수집하면 성능이 올라가지만, 데이터가 적다면 모델이 과적합(overfitting)이 발생하게 된다. 즉, 훈련 데이터에만 매우 높은 성능을 보이지만, 테스트에서는 좋지 않은 성능을 보이는 상황을 말한다. 이를 해결하기 위해서는 데이터를 많이 늘리거나, 데이터 증강을 통해 데이터를 늘려야 할 것이다.데이터 가시화4차원 이상의 초공간(hyperplane)은 한꺼번에 가시화(visualization)가 불가능하다. 따라서 여러 가지 가시화 기법을 통해 특징 벡터를 나타낼 수 있다.선형 회귀(linear regression)직선 모델을 사용하므로 두 개의 매개변수(w,b)를 가진다. 또한, 성능을 평가하기 위해 사용하는 함수를 목적 함수(object function) 또는 비용 함수(cost function)이라 한다. 선형 회귀에서는 MSE(mean squared error)를 사용한다.\\[J(\\theta) = {1 \\over n} \\sum_{i=1}^n(f_\\theta(x_i) - y_i)^2\\]f\\theta는 우리가 설정한 직선을 말하고, f(x)는 예측값, y는 실제값에 해당한다. 즉, 우리가 예측한 예측값과 실제값의 차이의 평균을 오차로 평가하는 함수를 비용 함수라 한다. 처음에는 최적의 매개변수를 모르기 때문에 임의의 난수로 설정하고 개선해 나간다.최적화에는 convex와 nonconvex가 존재한다. 회귀 문제에서는 최적값이 1개인 것과 정확한 최적값이 존재하지 않을 수 있다. 1개인 것을 convex, 신경망과 같이 완벽한 최적값이 없거나 찾기 너무 어려운 것을 non convex라 한다.기계학습은 개선을 통해 오류를 줄이고, 그를 통해 최적의 해를 찾아간다. 이를 공식화하면 다음과 같다.\\[\\widehat{\\theta} = argmin(J(\\theta))\\]non convex에서 최적값을 찾기 위해서는 gradient를 사용한다. 그 이유는 미분값이 작아지는 방향으로 계속 가다보면 오류가 줄어들고, 결국에는 최적의 상황에 도달하기 떄문이다. 이는 산에서 골짜기를 연상하면 이해하기 쉽다.과적합(overfitting)/과소적합(underfitting)과소적합이란 모델의 용량이 작아 오차가 클 수 밖에 없는 현상을 말한다. 간단하게 선형 모델을 사용하면 당연히 경계선을 설저아기 어려울 것이다. 차수가 높을 수록 주어진 데이터에 적합하게 설정할 수 있다. 그러나 너무 큰 차수를 사용하거나 데이터의 양이 너무 작아 훈련 데이터셋에만 적합시키게 되면 test시 성능이 잘 나오지 않는다. 모델의 용량이 너무 크기 떄문에 학습 과정에사 잡음까지 수용하여 훈련시켰기 때문일 것이다. 이를 간단하게 말해서 훈련 데이터셋을 단순 암기했다고 할 수 있다.1~2차의 모델을 사용할 경우 훈련 데이터셋과 테스트 데이터셋 모두 낮은 성능을 보이는 과소적합이 발생한다. 12차와 같이 높은 차원의 모델의 경우 훈련 데이터셋에서는 높은 성능을 보이나 테스트 데이터셋에서는 낮은 성능을 보이는 과적합이 발생한다. 이는 낮은 일반화 능력을 가졌다고 할 수 있다.대체로 딥러닝에는 파라미터가 매우 많기 때문에 과소적합보다는 과적합이 더 많이 발생한다.편향(bias) and 분산/변동(variance)훈련 데이터셋을 여러 번 수집하여 1~12차에 반복 적용해서 실험해보면 2차는 매번 큰 오차 -&amp;gt; 편향이 큼(underfitting), 하지만 비슷한 모델을 얻음 -&amp;gt; 낮은 변동 12차는 매번 작은 오차 -&amp;gt; 편향이 작음, 하지만 크게 다른 모델을 얻음 -&amp;gt; 높은 변동 -&amp;gt; overfitting 발생일반적으로 용량이 작은 모델은 편향이 크고 분산이 작다. 그러나 복잡한 ㅗㅁ델의 경우 편향이 작고 분산이 크다. 즉 낮은 차수의 경우 비슷한 모델이 수집되지만, 높은 차수의 경우 매번 아예 다른 모델이 수집될 수 있다.기계 학습의 목표는 낮은 편향과 낮은 분산을 가진 예측 모델을 만드는 것이 목표다. 하지만 모델의 편향과 분산은 상충 관계에 있다. 따라서 적절한 optimal capacity를 잡아 편향을 최소로 유지하고 분산도 최대로 낮추는 전략이 필요하다. 최근에는 capacity를 크게 잡고, 점점 줄여나가는 방식을 사용한다.그러나 요즘에는 서로 다른 차수를 선택해서 학습하지 않는다. 지금은 용량이 충분히 큰 모델을 선택한 후 선택한 모델이 정상을 벗어나지 않도록 규제(regularization) 기법을 적용한다. 이 때 말하는 규제는 데이터 증강(data augmentation)이나 learning rate, weight decay 설정 등을 말한다.regularizationValidation set 설정검증 데이터를 사용하여 훈련 데이터셋으로 훈련된 모델의 성능을 측정한다. 그래서 가장 높은 성능을 보인 모델을 선택한다. 그것을 사용하여 테스트에서 예측을 수행하면 좋은 성능을 보인다.K-fold Cross validation데이터의 양이 적은 경우 검증 데이터셋을 돌아가며 사용하는 것을 말한다. k-fold라는 것은 훈련 데이터셋을 10개로 나누고, epoch, 반복마다 1개씩 validation set으로 설정하여 검증한다. 그래서 추출된 검증 데이터셋의 성능이 총 10개가 나오는데, 이를 평균내서 가장 좋은 성능의 모델을 선택한다.kfold cross validation 코드부트스트랩(bootstrap)데이터 분포가 불균형할 때 사용하는 방법으로, 임의의 복원 추출 샘플링(sampling with replacement)을 반복한다. 데이터 불균형은 이상탐지(보안 분야)에서 많이 발생한다.예를 들어, 1과 2를 분류하는데 1에 대한 데이터셋이 10000개고 2에 대한 데이터셋이 100개라면 이는 데이터 불균형에 해당된다. 200개의 데이터셋을 사용하고자 할 때는 1에 대해서는 계속 다른 데이터를 사용할 수 있지만, 2에 대해서는 100개 전체를 계속해서 추출해서 학습을 시킨다. 그렇게 많은 성능들을 어샘블링해서 판단한다.RegularizationData Augmentation데이터를 많이 수집할수록 일반화 능력이 향상된다. 그러나 수집할 수 있는 데이터의 양은 한정적이다. 또한 데이터를 실측자료(ground truth)를 사용하려면 사람이 직접 라벨링해야 한다. 그러므로 현재 가지고 있는 데이터로 crop, perspective, rotation, warping 등과 같은 이미지를 변형(transform)함으로써 데이터를 증강시켜 robust한 모델을 생성한다. 단 원 데이터의 고유 특성이 변하지 않도록 주의해야 한다.Weight Decay가중치를 줄이는 기법으로, 모델의 용량을 낮추면서 overfitting을 방지한다. overfitting이라는 것은 모델의 용량이 커서 발생하는 것이므로, 이를 감소시키게 하여 overfitting이 발생하지 않도록 조절해야 한다.\\[J(\\theta) = {1 \\over n} \\sum_{i=1}^n(f_\\theta(x_i) - y_i)^2 + \\lambda \\parallel \\theta \\parallel ^2\\]원래 있던 훈련 집합의 오차에 선호(preference) 항을 추가하여 가중치를 감소시키도록 최적화를 시킨다.이는 optimizer을 생성할 때 weight decay라는 인자를 통해 설정한다.overfitting 방지하는 방법규제에는 다양한 방법들이 있다. parameter norm penalty early stopping bagging (bootstrap aggregation) dropout ensemble여기서 dropout과 ensemble을 많이 사용한다.dropout/ensemble 참고 자료기계 학습 유형지도 방식에 따른 유형 지도 학습(supervised learning)특징 벡터 X와 목표치 Y가 모두 주어지는 상황을 말한다. 즉 데이터와 라벨이 모두 주어진다. 회귀와 분류 문제로 구분한다. 비지도 학습(unsupervised learning)특징 벡터 X만 주고, 라벨 Y를 주지 않는 상황을 말한다. 군집화(clustering)작업을 할 때 사용한다. 또는 밀도 추정, 특징 공간 변환 작업도 한다. 강화 학습(reinforcement learning)목표치가 주어지는데 지도 학습과는 다른 형태로, 강화학습에는 에이전트, 환경, 행동이 존재한다. 에이전트는 행동에 따른 적절한 보상(reward)를 받는다. 에이전트는 어떤 행동을 했을 때 보상을 받는지에 대해 학습한다.바둑에서 강화학습이 사용되는데, 수를 두는 행위가 샘플이고, 게임이 끝나면 목표치 하나가 부여된다. 이기면 1, 패하면 -1이다. 예를 들어, 고양이가 있고, 빨간색 버튼을 누르면 간식이 나오는 기계가 있다고 가정해보자. 고양이는 어떻게 해야 간식이 나오는지 모를 것이다. 아무거나 다 시도해보다가 버튼을 눌렀을 때 간식이 나오는 것을 확인한다. 이 때, 고양이(agent)는 간식이 나오는 기계(environment)의 버튼을 누르면(action) 간식 나온다는(rewards) 것을 학습한 것이다. 준지도 학습(semi-supervised learning)일부는 X와 Y를 모두 가지지만, 나머지는 X만 가진 상황이다. 대부분의 데이터가 X의 수집은 쉽지만 Y는 수작업이 필요했기 때문에 주목받고 있다.다양한 기준에 따른 유형 오프라인 학습(offline learning)과 온라인 학습(online learning)오프라인 학습이란 데이터를 수집해놓고 수집된 데이터로부터 학습을 진행하는 것을 말하고, 온라인 학습은 스트리밍 형태로 데이터를 계속 받으면서 능동적으로 학습하는 것을 말한다. 보통은 오프라인 학습을 다룬다. 온라인 학습은 IoT 등에서 추가로 발생하는 데이터 샘플을 가지고 점증적 학습을 수행한다. 결정록적 학습(deterministic learning)과 확률적 학습(stochastic learning)결정론적 학습은 같은 데이터를 주면 똑같은 결과가 나오는 것이고, 확률적 학습은 같은 데이터라도 확률 분포를 사용하여 다른 모델이 만들어질 수 있는 것을 말한다. RBM/DBN이 확률적 학습에 해당된다. 분별 모델(discriminative models)와 생성 모델(generative models)분별 모델은 부류 예측에만 관심이 있다. 즉, x에 대한 타겟 y를 추정하는 P(y|x)에 관심이 있지만, 생성 모델은 P(x) 또는 P(x|y) 처럼 x에 관심을 두는 것을 말한다. 따라서 새로운 샘플을 생성할 수 있다. GAN/RBM이 이에 해당된다.generative models" }, { "title": "[데브코스] 9주차 - DeepLearning Numpy &amp; Matplotlib", "url": "/posts/numpy/", "categories": "Classlog, devcourse", "tags": "devcourse, deeplearning", "date": "2022-04-11 14:40:00 +0900", "snippet": "NumpyNumpy 설치 및 불러오기pip install numpyimport numpy as np파이썬의 리스트는 머신러닝에서 가장 많이 사용되는 구조 중 하나일 것이다. 그러나 이 리스트는 연산 속도가 느리다. 그래서 연산을 효과적으로 할 수 있도록 하기 위해 만든 것이 Numpy이다.Numpy 연산 속도numpy와 list의 연산 속도를 비교해보고자 한다.import numpy as npL = range(1000)%timeit [i**2 for i in L]N = np.arange(1000)%timeit N**21000 loops, best of 5: 264 µs per loop1000000 loops, best of 5: 1.41 µs per loop이 때 %timeit은 뒤의 연산에 대한 속도를 측정해준다. 시간을 보게 되면 리스트는 1000번의 루프동안 264µs 이지만, numpy의 경우 1000000번의 루프동안 1.41µs밖에 안걸린다. 동일한 루프 동안 걸리는 시간이 18만 배 정도가 차이난다.지금은 단순히 1차원 배열이지만, 이것이 2차원, 3차원이 되면 더더욱 차이가 많이 나게 될 것이다.리스트를 numpy로 직접 변환할 수도 있다.&amp;gt; li = [1,2,3]&amp;gt; ar = np.array([1,2,3])&amp;gt; arr = np.array(li)&amp;gt; print(li,&quot;\\n&quot;,type(li))[1, 2, 3] &amp;lt;class &#39;list&#39;&amp;gt;&amp;gt; print(ar,&quot;\\n&quot;,type(ar))[1 2 3] &amp;lt;class &#39;numpy.ndarray&#39;&amp;gt;&amp;gt; print(arr,&quot;\\n&quot;,type(arr))[1 2 3] &amp;lt;class &#39;numpy.ndarray&#39;&amp;gt;Numpy 연산vector와 scalar 연산벡터의 각 원소에 대해 연산을 진행해보자.\\[y = \\left( \\begin{matrix} 1 \\\\ 3 \\\\ 5 \\end{matrix} \\right) \\quad z = 5\\]x = np.array([1,2,3])c = 5x각각의 원소에 c를 더하기/곱하기/나누기 등을 해주자.&amp;gt;print(&quot;더하기 : {}\\n빼기 : {}\\n곱하기 : {}\\n나누기 : {}&quot;.format(x+c,x-c,x*c,x/c))더하기 : [6 7 8]빼기 : [-4 -3 -2]곱하기 : [ 5 10 15]나누기 : [0.2 0.4 0.6]vector와 vector 연산벡터끼리는 같은 인덱스끼리 연산이 진행된다.\\[y = \\left( \\begin{matrix} 1 \\\\ 3 \\\\ 5 \\end{matrix} \\right) \\quad z = \\left( \\begin{matrix} 2 \\\\ 9 \\\\ 20 \\end{matrix} \\right)\\]&amp;gt; y = np.array([1,3,5])&amp;gt; z = np.array([2,9,20])&amp;gt; print(&quot;더하기 : {}\\n빼기 : {}\\n곱하기 : {}\\n나누기 : {}&quot;.format(y+z,y-z,y*z,y/z))더하기 : [ 3 12 25]빼기 : [ -1 -6 -15]곱하기 : [ 2 27 100]나누기 : [0.5 0.33333333 0.25 ]numpy 인덱싱list에서는 2차원 배열의 경우 k[a][b] 와 같이 인덱싱했다. numpy에서는 이와 유사하게 [a,b]로 인덱싱한다. 차이점은 numpy의 경우 ,를 사용한다.&amp;gt; l = np.array([[1,2,3],[4,5,6]])&amp;gt; l[1,2]6slicing(:) 하는 방법도 list와 동일하다.&amp;gt; l[0:2,1:3]array([[2, 3], [5, 6]])array의 broadcastingnumpy에는 broadcasting이라는 특수한 기능이 있다. broadcasting이란 피연산자가 연산이 가능하도록 변환이 가능한 경우 변환하여 연산해주는 기능을 말한다. MxN 과 Mx1 을 연산하고자 한다면, 선형 대수 연산에서는 계산이 불가능하다.그러나 이 때는 Mx1 배열을 복사해서 N개를 다 연산해준다.예를 들어,\\[x = \\left( \\begin{matrix} 1 \\ 2 \\ 3 \\\\ 4 \\ 5 \\ 6 \\\\ 7 \\ 8 \\ 9 \\end{matrix} \\right) \\quad y = \\left( \\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix} \\right)\\]이러한 배열이 있다. 원래 선형 대수 연산에서는 차원이 다르므로 계산이 불가능하지만, numpy에서는 y를 3x3 크기로 만들어 연산한다.MxN, 1xN 에서도 동일하게 동작한다. 그렇다면 Mx1 과 1xN 의 연산은 어떻게 동작할까?이 경우에는 Mx1 을 MxN으로, 1xN을 MxN으로 만들어서, MxN 두개의 행렬을 연산한다.# M by N, M by 1x1 = np.array([[1,2,3],[4,5,6],[7,8,9]])y1 = np.array([0,1,0]) # 이 때, y는 행 벡터이다. M은 행을 나타내고 있으므로, y를 전치해줘야 한다.# 전치하는 방법은 여러 가지가 있다.y1 = y1[:, None]# M by N, 1 by Ny2 = np.array([0,1,-1]) # M by 1, 1 by Nx3 = np.array([1,2,3])x3 = x3[:,None]y3 = np.array([2,0,-2])&amp;gt; print(x1*y1, &quot;\\n\\n&quot;, x1*y2, &quot;\\n\\n&quot;, x3*y3)[[0 0 0] [4 5 6] [0 0 0]] [[ 0 2 -3] [ 0 5 -6] [ 0 8 -9]] [[ 2 0 -2] [ 4 0 -4] [ 6 0 -6]]Numpy 응용 - Linear Algebra with numpy여러 형태의 행렬 생성 함수# 영행렬&amp;gt; zero = np.zeros((3,3)) # zeros(dim)&amp;gt; zero[[0.,0.,0.], [0.,0.,0.], [0.,0.,0.]]# 일행렬&amp;gt; one = np.ones((3,3)) # ones(dim)[[1.,1.,1.], [1.,1.,1.], [1.,1.,1.]]# 대각행렬&amp;gt; diag = np.diag((1,3,5)) # diag((numbers))[[1,0,0], [0,3,0], [0,0,5]]# 항등행렬&amp;gt; eye = np.eye(2, dtype=int) # eye(dim, data type)[[1,0], [0,1]]행렬 곱/나누기행렬간의 곱연산은 np.dot() 또는 @ 을 사용한다.&amp;gt;mat1 = np.array([[1,4],[2,3]])&amp;gt;mat2 = np.array([[7,9],[0,6]])&amp;gt;mat1.dot(mat2)array([[ 7, 33], [14, 36]])&amp;gt;mat1 @ mat2array([[ 7, 33], [14, 36]])트레이스(trace)트레이스란 main diagnoal의 합을 말한다. 즉, 배열 원소들 중 대각 행렬의 합이다.&amp;gt;arr = np.array([[1,2,3],[4,5,6],[7,8,9]])&amp;gt;arr.trace()15 # 1+5+9행렬식(determinant)행렬을 대표하는 값들 중 하나를 말한다. 행렬을 선형 변환을 진행했을 때 얼마나 원벡터가 변하는가에 대한 척도이다. 행렬식을 구하는 방법은 2x2에서 ad - bc에 대한 식이다. 예를 들어 아래와 같은 행렬이 있다고 하자.\\[arr = \\left( \\begin{matrix} 1 \\ 2 \\\\ 3 \\ 4 \\end{matrix} \\right)\\]이에 대해 행렬 식을 구하면 1x4 - 2x3 = -2가 된다. 이 행렬식이 0이 나온다면 full rank가 아니라는 것을 의미한다. 즉 선형 변환을 하면 차원의 손실이 일어난다.행렬식을 구하는 함수는 np.linalag.det() 이다.&amp;gt;arr2 = np.array([[2,3],[1,6]])&amp;gt;np.linalg.det(arr2)9.000000000000002 *선형 독립이란? 같은 수의 성분을 가진 n개의 벡터 a1,a2,…,an에 대해 벡터의 1차 결합(linear combination)인 $ C1a1 + C2a2 + … + Cnan = 0 $을 만족하는 상수 C1,C2,…,Cn이 모두 0이면 이 벡터 a1,a2,…,an은 선형 독립에 해당한다. 그러나 하나라도 0이 아닌 Ci가 존재하면 벡터 a1,a2,…,an은 선형종속 또는 1차 종속에 해당한다. 예를 들어 행렬 A,B가 있을 때, $ AC1+BC2 = 0 $에 대해 상수 C1=0,C2=0일때만 만족해야 선형 독립이고, C1=0,C2=0 이외에 C1=1,C2=-1일 때도 0이 된다면 이는 선형 종속이다. https://rfriend.tistory.com/163 *rank란? 행렬의 일차 독립인 행 또는 열의 최대 개수를 rank라 한다. 임의의 행렬 A의 랭크는 rank(A)로 표기한다. 행렬의 랭크는 그 행렬의 열벡터들에 의해 생성된 벡터 공간의 차원이다. m x n 행렬은 m개의 행과 n개의 열을 가진 행렬이다. 이 행렬의 랭크는 m개의 행 중 일차 독립인 행의 최대 개수 또는 n개의 열 중 일차 독립인 열의 최대 개수와 같다. 모든 행이 서로 일차독립이면 랭크는 m이 될 것이고, 모든 열이 서로 일차독립이면 랭크는 n이 된다. 그렇다면 m x n 행렬에서 모든 행끼리, 모든 열끼리 서로 일차독립이라면 어떻게 될까? 사실 그런 일은 존재하지 않는다. 왜냐하면 최대 일차독립인 행 또는 열의 개수는 한 행렬에서 나온다. 즉, 최대 행 개수와 최대 열 개수는 항상 동일하고, 이는 두 개의 차원이 같다는 것을 의미한다.\\[A = \\left( \\begin{matrix} 1 \\ 2 \\ 1 \\ 1 \\\\ 1 \\ 1 \\ -1 \\ 1 \\end{matrix} \\right)\\] 이 행렬 A의 랭크를 구해보자. 1열과 4열은 동일하고, (1,1),(2,1)은 서로 독립이다. (1,1),(2,1),(1,-1)은 서로 종속이다. 왜냐하면 C1=0,C2=0,C3=0 이외에도, C1=3,C2=-2,C3=1일 때도 영벡터가 된다. 따라서 rank(A) = 2이다. 행의 관점에서 보면 1,4열은 이미 같으므로 1행과 2행에서 각각 2,3열만 비교하면 된다. (2,1)과 (1,-1)은 일직선상에 있지 않으므로 독립이다. 따라서 1행과 2행은 독립이다. 또는 어떤 상수 a,b에 대해 $ a(1,2,1,1) + b(1,1,-1,1) = 0 $이 성립하려면 a+b=0, 2a+b=0,a-b=0이어야 하므로 이를 만족하는 유일해는 a = b = 0이므로 독립이다. 따라서 rank(A) = 2이다.\\[B = \\left( \\begin{matrix} 1 \\ 4 \\\\ -3 \\ -12 \\end{matrix} \\right)\\] 행렬 B의 경우 행 관점에서 볼 때 (1,4)를 -3배 하면 2행 (-3,-12)가 얻어지므로 두 벡터는 종속이다. 열 관점에서 봐도 (1,-3)을 4배 하면 (4,-12)가 되므로 역시 종속이므로 rank(B) = 1 종속인지 독립인지를 판단할 때는 벡터를 그림으로 그리거나 정의를 생각하면 이해하기 쉽다. 두 벡터가 동일선상, 즉 같은 직선이 될 때 이 두 벡터가 종속 관계라 한다. 예를 들어 (6,7,8)과 (1,1,1)은 같은 방향에 놓여 있지 않다. 따라서 독립이다. 참고 사이트 https://gosamy.tistory.com/16 https://rfriend.tistory.com/163 *full rank란? full rank는 해당 행렬의 행 또는 열 중 작은 값과 rank가 같은 경우를 말한다. 즉 3x2 행렬 A가 있을 때, 이 행렬의 rank(A) = 2라면 이는 full rank라 할 수 있다.역행렬(inverse matrix)행렬 A에 대해 AB = BA = I를 만족하는 행렬 B를 구할 수 있다. 즉 B = A^-1을 만족한다.이를 구하는 함수는 np.linalg.inv()이다.&amp;gt; arr = np.array([[1,4],[2,3]])&amp;gt; arr_inv = np.linalg.inv(arr)&amp;gt; arr_invarray([[-0.6, 0.8], [ 0.4, -0.2]])&amp;gt; arr.dot(arr_inv)array([[ 1.00000000e+00, 0.00000000e+00], [-1.11022302e-16, 1.00000000e+00]])# array([[1,0],# [0,1]])# == I고유값과 고유벡터(eigenvalue and eienvector)정방행렬(NxN) A에 대해 $Ax = \\lambda x$를 만족하는 상수 ⋋와, 이에 대응하는 벡터이다. 즉, $ (A- \\lambda I)x = 0 $에 만족해야 하는데, 이를 확인하기 위해 determinant를 사용하여 (A-⋋I)에 대해 0가 되는지 확인하면 고유값을 구할 수 있다.그러나 numpy에 이를 구하는 함수가 존재한다. 함수는 np.linalg.eig()이다.&amp;gt; arr = np.array([[2, 0, -2],[1, 1, -2],[0, 0, 1]])&amp;gt; np.linalg.eig(arr) # output : (⋋, x) 열을 기준으로 1에 대응되는 벡터는 [0,1,0]이다.(array([1., 2., 1.]), array([[0. , 0.70710678, 0.89442719], [1. , 0.70710678, 0. ], [0. , 0. , 0.4472136 ]])) 고유값이란?행렬 A를 선형변환으로 봤을 때, 선형 변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터라 하고, 이 상수배 값을 고유값이라 한다.이 고유값과 고유벡터가 맞는지 확인해보자.&amp;gt; eig_val, eig_vec = np.linalg.eig(arr)&amp;gt; mat @ eig_vec[:,0] # Axarray([0., 1., 0.])&amp;gt; eig_val[0] * eig_vec[:,0] # (lambda)xarray([0., 1., 0.])동일하게 추출되는 것을 통해 $ Ax = \\lambda x $ 를 만족한다는 것을 확인할 수 있다.Exercise이 때까지 배운 내용들을 통해 2가지를 직접 생성해보고자 한다. L2 norm을 구하는 함수 : get_L2_norm() 어떤 행렬이 singular matrix인지 확인하는 함수 : is_singular()1. get_L2_norm() 매개변수 : 1차원 벡터, np.array() 반환값 : 인자로 주어진 벡터의 L2 norm 값, number2. is_singular() 매개변수 : 2차원 벡터, np.array() 반환값 : 인자로 주어진 벡터가 singular이면 true, 아니면 falseMatplotlib파이썬의 데이터 시각화 라이브러리로 시각화할 때 자주 사용한다. matplotlib 설치pip install matplotlib쥬피터나 코랩에서 matplotlib을 사용하기 위해서는 %matplotlib inline을 통해 활성화해야 한다.%matplotlib inline Matplotlib 사용해보기import matplotlib.pyplot as plt%matplotlib inlineplt.plot([1,2,3,4,5]) # 실제 plotting 하는 함수plt.show() # plt를 확인하는 명령 figure이 그래프의 크기를 직접 지정해줄 수 있다.plt.figure(figsize=(6,6)) # 6x6 픽셀 크기의 도면을 선언plt.plot([0,2,3,2,5])plt.show()2차함수 그리기x = np.array([1,2,3,4,5]) # 정의역y = np.array([1,4,9,16,25]) # f(x)plt.plot(x,y)plt.show()위의 방법으로는 부드럽게 그려지지 않고 있다. 그래서 더 부드럽게 그리기 위해 전에 배웠던 np.arange를 사용하거나 np.linspace를 통해 점들의 분포를 정의역으로 선언해준다.x1 = np.arange(-10,10,0.01) # arange(min,max,간격)plt.plot(x1, x1**2)plt.title(&quot;arange&quot;)plt.show()x2 = np.linspace(-10,10,2000)plt.plot(x2, x2**2)plt.title(&quot;linspace&quot;)plt.show()plot의 추가적인 기능위에서 사용했던 title도 이에 해당되는 추가적인 기능이다. 그래프가 어떤 것을 의미하는지를 나타내기에 유용하다.추가적으로 다양한 것들이 있다.1.x,y 축에 설명 추가plt.xlabel(&quot;x value&quot;)plt.ylabel(&quot;f(x) value&quot;)2.범위 설정plt.axis([-5, 5, 0, 25]) # [x_min, x_max, y_min, y_max]3.x,y축에 눈금 설정plt.xticks([i for i in range(-5,6,1)]) # x축의 눈금 설정, -5,-4,-3...plt.yticks([i for i in range(0,27,3)]) # y축의 눈금 설정 범위 설정과 눈금설정의 차이는 범위는 그래프의 범위를 설정해서 해당범위만 보겠다는 명령이고, 눈금 설정은 x,y축의 눈금 간격을 직접 세분화하거나 거시화하는 것이다.4.titleplt.title(&quot;y = x^2 graph&quot;)5.legendplt.plot(x,x**2,label=&quot;trend&quot;) # label에 이름을 설정만 함plt.legend() # label을 매핑해서 보여줌, plot이후에 적어줘야 함Matplotlib의 plot 종류1. 꺽은선 그래프(plot)#random.seed(34) # 난수 일정하게 만들기 위한 설정x = np.arange(0,20) # 0~20y = np.random.randint(0,20,20) # 난수 20번 생성plt.axis([0,20,0,20])plt.yticks([0,5,10,15,20])plt.plot(x,y)plt.show()이 그래프는 시계열 데이터에서 가장 많이 사용한다.2. 산점도(scatter plot)plt.scatter(x,y)plt.show()이 그래프는 x와 y가 별개의 변수일 때 많이 사용한다. 별개의 변수 사이에서 어떤 상관관계를 가질지에 대해 파악할 수 있다.3. 박스 그래프(box plot)단일 변수가 있을 때 이 변수의 전반적인 분포를 볼 때 많이 사용한다. 특히 수치형 데이터에 대해 많이 사용한다. 이 그래프는 단일 변수도 가능하지만, 2개 이상의 변수도 가능하다.plt.boxplot((x,y))plt.show()이 그래프는 위 아래로 상한선(max)과 하한선(min)이 존재하고, 박스 그림에서 가장 밑부분, 주황색 선, 가장 윗부분은 각각 Q1(25%),Q2(50%),Q3(75%)에 해당하는 값들이다.4. 막대 그래프(bar plot)이 그래프는 범주형 데이터에 많이 사용한다. 범주형 데이터의 값과 그 값의 크기를 직사각형으로 나타낸 그림이다.plt.bar(x,y)plt.xticks(np.arange(0,20,1))plt.show()5. 히스토그램(histogram)도수분포를 직사각형의 막대 형태로 나타낸다.0,1,2…이 아니라 0~2까지의 범주형 데이터로 구성 후 그림을 그려준다.plt.xticks(np.arange(0,20,2))plt.hist(y,bins=np.arange(0,20,2))plt.show()막대 그래프와의 차이점은 박스들이 이어져있다. 그 이유는 히스토그램의 경우 대체로 이어져있는 연속적 데이터를 사용하기 때문이다.6. 원형 그래프(pie chart)데이터에서 전체에 대한 부분의 비율을 부채꼴로 나타낸 그래프다. 다른 그래프들과는 다르게 비율을 나타낸다.z = (100,300,200,400)plt.pie(z, labels=[&quot;first&quot;,&quot;second&quot;,&quot;third&quot;,&quot;fourth&quot;])plt.show()SeabornMatplotlib을 기반으로 더 다양한 시각화 방법을 제공하는 라이브러리다. 다양한 그래프들을 그려볼 수 있다. 커널 밀도 그래프 카운트그래프 캣그래프 스트립그래프 히트맵 seaborn 설치 및 임포트!pip install seabornimport seaborn as snsseaborn을 통한 시각화 그래프 종류커널밀도그래프 (Kernel Density plot)히스토그램과 같은 연속적인 분포를 곡선화해서 그린다.x = np.arange(0,22,2)y = np.random.randint(0,20,20)plt.xticks(np.arange(0,20,2))plt.hist(y,bins=x)plt.show()sns.kdeplot(y)plt.show()첫번째 그래프가 히스토그램이고, 두번째가 커널밀도 그래프이다.이 때, kdeplot에는 shade인자가 있다. 이는 그래프 아래의 영역을 색칠해주는 기능이다.sns.kdeplot(y,shade=True)plt.show()카운트그래프(count plot)범주형 column의 빈도수를 시각화하는 기능이다. 이는 groupby 를 한 후의 도수를 하는 것과 동일한 효과를 가진다.vote_df = pd.DataFrame({&quot;name&quot;:[&quot;Andy&quot;, &quot;Bob&quot;, &quot;Cat&quot;], &quot;vote&quot;:[True,True,False]})vote_df이를 matplotlib을 통해 생성하려면 groupby를 통해 그려야 했다,.# in matplotlibvote_count = vote_df.groupby(&#39;vote&#39;).count()vote_countplt.xlabel(&quot;vote&quot;)plt.ylabel(&quot;count&quot;)plt.bar(x=[False, True], height=vote_count[&#39;name&#39;])plt.show()# in sns countplotsns.countplot(x=vote_df[&#39;vote&#39;])plt.show()알아서 색상을 지정해서 분류해주고, xlabel,ylabel을 자동으로 지정해준다.캣 그래프(cat plot)숫자형 변수와 하나 이상의 범주형 변수의 관계를 보여주는 함수다. 이 그래프는 복잡한 데이터에 적용하는 것이 좋다. cat이 concat, 여러 개를 연결해주는 그래프라는 의미이다. 그래프의 데이터로 사용할 dataset은 다음 사이트에서 다운 받길 바란다. country_wise_lastest.csv : https://www.kaggle.com/code/imdevskp/covid-19-analysis-visualization-comparisons/data?select=country_wise_latest.csvcovid = pd.read_csv(&quot;/content/covid_19_Country_Wise_Lastest.csv&quot;)covid.head(5)s = sns.catplot(x=&quot;WHO Region&quot;, y=&quot;Confirmed&quot;, data=covid)s.fig.set_size_inches(10,6) # 이름들이 겹쳐 나오는 것을 보기 좋게 만들기 위해 figsizeplt.show()원래는 2차원으로 데이터를 나타냈다면, 추가적으로 hue인자를 통해 각 점의 범주 또는 수치별 관계를 표현할 수도 있다.또는 kind 인자를 통해 다른 형태의 그래프로도 만들 수 있다. default값은 strip이지만, violin으로 지정해주면 또 다른 형태의 그래프를 그릴 수 있다.스트립 그래프(strip plot)위의 캣 그래프의 기본 형태가 스트립 그래프이다.sns.stripplot(x=&quot;WHO Region&quot;, y=&quot;Recovered&quot;, data=covid)plt.show() swarmplot스트립 그래프와 거의 동일한 그래프이다. 동일한 값을 가지는 데이터들이 뭉쳐져 있으면 얼마나 많은 점들이 뭉쳐져 있는지 확인하기 어렵기에 이것들을 양옆으로 분산해준다.sns.swarmplot(x=&quot;WHO Region&quot;, y=&quot;Recovered&quot;, data=covid)plt.show()히트맵(heatmap)데이터의 행렬을 색상으로 표현해주는 그래프이다.covid.corr() # 행렬의 상관관계를 숫자로 표현이처럼 숫자로 확인하면 어떤 데이터인지 확인하기 어렵다. 따라서 이를 히트맵으로 생성하면 행렬을 시각화할 수 있다.sns.heatmap(covid.corr()) # dataplt.show()" }, { "title": "[데브코스] 7주차 - OpenCV corner detection and feature point detection ", "url": "/posts/corner/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-31 17:40:00 +0900", "snippet": "코너 검출코너의 특징 평탄한 영역(flat) &amp;amp; 에지(edge)영역은 고유한 위치를 찾기 어렵다 코너(corner)는 변별력이 높은 편이며, 영상의 이동 및 회전 변환에 강인하다.특징, 즉 feature은 한 이미지를 표현할 수 있는 속성, 다른 영상과 구분할 수 있는 속성을 말한다. 그래서 한 이미지의 feature map은 고유한 속성을 가진 맵이다. 히스토그램 또한 특징이 된다.openCV에서 제공하는 코너 검출 방법이 있다.가장 최근 방법이 FAST로 많이 빠르게 동작한다.harris는 코너 점 좌표를 출력하는 것이 아니라 dst에는 입력 영상과 똑같은 크기의 행렬을 출력한다. 이는 float 타입을 가지고 있는데, 이 값이 충분히 크면 이를 코너라고 간주하는 것이다.해리스 코너 검출 방법을 기반으로 비최대 억제,NMS를 적용하고, 출력은 vector&amp;lt;Point2f&amp;gt;로 점들의 좌표를 제공해주기 때문에 다소 편하다. 그러나 해리스 방식을 사용하므로 조금 구시대적이다.주변의 16개 픽셀을 찾아서 그 점보다 9개 정도가 특정 값 이상 이라면 코너라 인식한다.keypoint클래스라는 것에는 pt.x, pt.y 라는 객체가 포함되어 있다. threshold가 이 특정 값에 해당하고, NMS를 수행할지 결정할 수 있다.FAST 사이트 GFTT &amp;amp; FAST 적용 코드int main(){ Mat src = imread(&quot;building.jpg&quot;, IMREAD_GRAYSCALE); vector&amp;lt;Point2f&amp;gt; corners; goodFeaturesToTrack(src, corners, 400, 0.01, 10); vector&amp;lt;KeyPoint&amp;gt; keypoints; FAST(src, keypoints, 60, true);} 결과 비교FAST 방법은 반복 검출률이 대체로 높다. 즉, 처음에 코너라 인식한 것은 다음에도 검출할 확률이 높다는 것이다. 다만 FAST 방법은 노이즈에 민감하다.이러한 코너 검출들은 이동, 회전 변환에 강인하지만 크기 변환에는 취약하다. 따라서 크기 변환에도 강인한 다양한 크기 관점에서 특징을 검출할 수 있는 알고리즘을 사용해야 한다.크기 변환에도 강인한 방법으로는 특징점(feature point) = 키포인트(key point) = 관심점(interest point) 라고 하는 픽셀값으로부터 의미가 있는 정보를 추출하는 방식이 있다. 추출한 후에는 각 픽셀에 대해 정수가 아닌 실수값으로 표현을 하여 더 정밀하게 표현한다. 이 값들을 통해 에지 히스토그램(그래디언트 히스토그램)을 만든다. 이 때, 한 특징점에 대한 값을 특징 벡터(feature vector) = 기술자(desciptor)라 한다. 크기 불변 특징점 검출 방법SIFT, KAZE, AKAZE, ORB 등 다양한 특징점 검출 방법에서 스케일 스페이스, 이미지 피라미드를 구성하여 크기 불변 특징점을 검출한다.아래 첫번째 이미지는 scalespace에 대한 것이다. 사이즈도 계속 변경하고, 가우시안 블러도 계속 적용해준다. 두번째 이미지는 이미지 피라미드에 대한 이미지이다. 다양한 크기의 이미지들에 대해 처리한다.이 특징점 검출에 가장 많이 사용하는 알고리즘이 SIFT(Scale Invariant Feature Transform)이다.SIFT의 계산 단계에는 detector과 descriptor, 2가지가 있다. SIFT에서는 DOG(diffence of gaussian)이라 하여 인접해있는 가우시간 블러를 적용한 2개의 이미지의 차이를 구해준다. 이를 이용하여 가우시안 함수를 만들고, maxima(최댓값)와 minima(최솟값)를 나타낸다. 이 DOG 방식을 사용하기 이전에는 LOG(Laplacian)을 사용했었다. 그러나 이 방식은 연산이 오래 걸려서 그래프가 비슷한 DOG로 근사화시켜 연산을 함으로서 연산 속도를 줄였다.이 때, low level에서 특징 벡터를 구했는지, high level에서 특징 벡터를 구했는지에 따라 검출의 정확도가 다 달라진다.한 픽셀의 대표 특징 벡터(키포인트 기술자)는 각 키포인트 위치에서 스케일과 기준 방향을 고려하여 사각형 영역을 선택한다. 이 사각형 영역을 다시 4x4구역으로 만들고, 8개 방향으로 각 방향 성분을 나타내어 히스토그램을 구한다. 그러면 4x4크기의 8방향이므로 4x4x8 = 128바이트 크기에 실수 표현을 하므로 총 4바이트씩 더 곱해져 512바이트의 벡터가 생성된다.기타 특징점 기법 BRIEF(Binary Robout Independent Elemetary Features)이진 기술자를 이용한 빠른 키포인트 기술 방법이다. 이는 detector이 아니다. 부분 영상을 잘라서 그 영상에 대해 키포인트 주변 픽셀 쌍을 미리 정하고, 픽셀 값의 크기를 비교하여 0 또는 1로 특징을 기술한다. 이는 계산이 단순하고, 이진 표현이므로 크기가 다른 것보다 확실히 작아진다.특정 패치(p)에서 point pair(x,y)의 픽셀 값 크기 비교 표현식:nd차원 특징 벡터(기술자):매칭시 Hamming distance를 사용하여 XOR 논리 연산 후 0의 개수를 카운트한다. SURF(Speed-Up Robust Features)SIFT를 기반으로 속도를 향상시킨 크기 불변 특징점 검출 방법이 있다. DOG 함수를 단순한 이진 패턴으로 근사화시켰다. 적분 영상을 이용하여 속도 향상을 하였다.http://www.vision.ee.ethz.ch/~surf/ 그러나 특허가 있어 사용하기 까다롭다. KAZEKAZE : 비선형 scale space에서 공기의 흐름가우시안 함수 대신 비선형 확산 필터를 이용하여 특징점을 검출한다. SURF보다는 느리지만 SIFT보다는 빠르고 비슷한 성능이라고 한다.http://www.robesafe.com/persional/pablo.alcantarilla/kaze.html ORB(Oriented FAST and Rotated BRIEF)OpenCV 특징점 클래스Feature2D 클래스와 파생 클래스들특징 벡터를 검출하고 두 장의 영상 데이터를 매칭하는 것까지의 흐름이다. 알고리즘이라는 가장 높은 클래스가 있고, 이 클래스를 상속받은 feature2D가 있고, 그것들을 다 상속받는 SIFT, KAZE,AKAZE 등이 있다. SIFT는 무료이나 SURF는 특허가 있어 무료가 아니고, 성능도 다른 것들과 비슷하여 잘 사용하지 않는다. SIFT와 KAZE는 방향 성분에 대한 히스토그램을 사용하고, AKAZE와 ORB는 이진 바이너리를 사용한다. 이렇게 추출된 것들을 keypoint 클래스가 상속받아 사용된다.검출만 하려면 detect를 사용하고, 계산만 하려면 compute를 하는데, 둘다 진행하려면 detectandcompute를 사용한다.생성을 할 때는 create라는 함수를 사용하면 된다.static Ptr&amp;lt;SIFT&amp;gt; cv::SIFT.::create()static Ptr&amp;lt;KAZE&amp;gt; cv::KAZE::create()static Ptr&amp;lt;AKAZE&amp;gt; cv::AKAZE::create()static Ptr&amp;lt;ORB&amp;gt; cv::ORB::create()이 때, create 함수의 인자가 엄청 많지만, default인자가 다 있어서 이를 사용하면 된다. Ptr 이라는 것은 opencv에서 사용하는 스마트 포인터다. 특징점 추출 코드#include &quot;opencv2/opencv.hpp&quot;#include &amp;lt;iostream&amp;gt;using namespace cv;using namespace std;int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } TickMeter tm; tm.start(); Ptr&amp;lt;SIFT&amp;gt; detector = SIFT::create(); // SIFT, KAZE, AKAZE, ORB vector&amp;lt;KeyPoint&amp;gt; keypoints; detector-&amp;gt;detect(src, keypoints); // shift 연산자 tm.stop(); cout &amp;lt;&amp;lt; &quot;Elapsed time: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms.&quot; &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;keypoints.size(): &quot; &amp;lt;&amp;lt; keypoints.size() &amp;lt;&amp;lt; endl; Mat dst; drawKeypoints(src, keypoints, dst, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS); imshow(&quot;dst&quot;, dst); waitKey(); return 0;}이 때는 검출만 하기 위해 detect를 사용했다.Ptr&amp;lt;ORB&amp;gt; detector = ORB::create();ORB는 동심원이 많다. 즉 같은 픽셀을 참조하는 keypoint가 많다.Ptr&amp;lt;KAZE&amp;gt; detector = KAZE::create();Ptr&amp;lt;AKAZE&amp;gt; detector = AKAZE::create();AKAZE는 SIFT보다 약간 빠르다.대체로 속도가 필요하다면 ORB, 아니면 SIFT를 사용한다.drawKeypoints(src, keypoints, dst, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);이는 각각의 픽셀에 대해 그림을 그려주는 함수로 for문으로 간단히 구현할 수 있지만, 지원하는 함수를 사용했다. 또, 이 때 scalar::all(-1) 은 Scalar(-1,-1,-1,-1)과 같다.기술자(dscriptor, feature vector)기술자란 각각의 특징점 근방의 부분 영상을 표현하는 실수 또는 이진 벡터를 말한다. OpenCV에서는 기술자를 Mat객체로 표현한다. 기술자 알고리즘에 의해 추출된 특징 벡터는 이어 붙여져 행렬로 만들어진다. 이를 기술자 행렬이라 한다.SIFT의 경우 각 128개의 특징 벡터를 이어 붙여서 행렬로 만든다. KAZE의 경우에는 128이 아닌 64개를 사용한다.이진 기술자이진 기술자란 이진 테스트를 통해 부분 영상의 특징을 기술하는 방법을 말한다. 보통 uchar자료형을 사용하여 비트 단위로 영상 특징 정보를 저장한다. 이진 기술자를 사용하는 알고리즘에는 AKAZE,ORB,BRIEF가 있다. 이진 기술자는 해밍 거리(hamming distance)를 사용하여 유사도를 판단한다. 해밍 거리란 두 값과의 차를 합한 것을 말한다.특정 패치(p)에서 point pair(x,y)의 픽셀 값 크기 비교 표현식:nd차원 특징 벡터(기술자):실수 기술자기술자 사용하려면 compute를 사용하면 된다. 함수virtual void Feature2D::compute(InputArrayOfArrays images, std::vector&amp;lt;std::vector&amp;lt;KeyPoint&amp;gt; &amp;gt;&amp;amp; keypoints, OutputArrayOfArrays descriptors) image : (입력) 입력 영상 keypoints : (입력)검출된 특징점 정보, vector&amp;lt;keypoint&amp;gt; 자료형 descriptors : (출력)특징점 기술자 행렬, Mat 자료형 특징점 계산 코드int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); Ptr&amp;lt;SIFT&amp;gt; detector = SIFT::create(); // SIFT, KAZE, AKAZE, ORB vector&amp;lt;KeyPoint&amp;gt; keypoints; Mat descriptor; detector-&amp;gt;compute(src, keypoints, descriptor); // shift 연산자 Mat dst; drawKeypoints(src, keypoints, dst, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS); imshow(&quot;dst&quot;, dst); waitKey();}검출과 계산을 함께 하기 위해서는 detectAndCompute 특징점 검출과 계산 코드Mat desc;detector-&amp;gt;detectAndCompute(src, Mat(), keypoints, descriptor);두번째 인자는 마스크 행렬에 해당한다. 그리고 마지막 인자는 출력 인자이다. desc.size : [바이트 크기 x 특징점 개수]특징점 매칭두 영상에서 추출한 특징점 기술자를 비교하여 유사한 기술자끼리 선택하는 작업을 한다. 즉 두 영상에서 같은 특징 벡터를 계산해서 비교한다. 두 벡터의 비교는 유클리디안 거리(L2 norm)를 사용한다.원본에는 있지만, 변환한 영상에서 없는 3번의 경우는 3개 중 가장 비슷한 것으로 매칭을 해버린다.이렇게 지저분하게 처리되므로 후처리를 해줘야 한다.매칭을 할 때는 descriptorMatcher를 사용한다. 이는 가상 클래스이고, 이를 상속받아 사용되는 BFMatcher이나 FlannBasedMatcher를 사용하면 되는데, 대체로는 BFMatcher를 사용한다. 이 BF(Brute Force)는 전수 조사라 하여 모든 것들을 비교해보는 것을 말한다. Flann(Fast Library for Approximate Nearnest Neighbor)은 근사화시키는 것으로 속도가 빨라보이나 FHD 정도의 크기가 아닌 이상 두 방법의 속도는 비슷하다.DscriptorMatcher안에는 Match(), knnMatch(), radiusMatch() 가 있다. 많이 사용되는 것은 Match(), knnMatch()이다. knnmatch는 1개가 아닌 2개의 매칭값을 찾는 함수다.여기에 연관되어 존재하는 DMatch라는 클래스가 있다. DMatch 클래스 정의class DMatch{ public: DMatch(); DMatch(int _queryIdx, int _trainIdx, float _distance); DMatch(int _queryIdx, int _trainIdx, int _imgIdx, float _distance); int queryIdx, trainIdx, imgIdx; // 1th image(compare image), 2th image(compared image), image index float distance; // 두 특징점 사이의 거리 bool operator&amp;lt;(const DMatch &amp;amp;m) const; //compare 연산자}queryIdx가 1번 이미지, trainIdx가 2번 이미지로 이 두개를 비교한다. imgIdx라는 것도 있는데, 이는 비교되는 이미지가 여러 개일 수 있어서 어떤 것에 대해 매칭을 할 것인지에 대한 번호이다. 특징점 매칭 결과 영상 생성 함수void drawMatches(InputArray img1, const std::vector&amp;lt;KeyPoint&amp;gt;&amp;amp; keypoints1,InputArray img1, const std::vector&amp;lt;KeyPoint&amp;gt;&amp;amp; keypoints1, const std::vector&amp;lt;DMatch&amp;gt;&amp;amp; matches1to2, InputOutputArray outImg, const Scalar&amp;amp; matchColor=Scalar::all(-1), const Scalar&amp;amp; singlePointColor=Scalar::all(-1), const std::vector&amp;lt;char&amp;gt;&amp;amp; matchesMask=std::vector&amp;lt;char&amp;gt;(), int flags=DrawMatchesFlags::DEFAULT); img1,img2,keypoint1,keypoint2 : img1 영상에서 추출한 키포인트 keypoint1, img2 영상에서 추출한 키포인트 keypoint2 matches1to2 : 두 키포인트 사이의 매칭 결과 outImg : 매칭 결과를 저장 matchesMask : 매칭 정보를 선택하여 그릴 때 사용할 마스크로 그냥 std::vector&amp;lt;char&amp;gt;()를 지정하면 모든 매칭 결과를 그려준다. flags : DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS를 지정하면 매칭되지 않은 특징점을 그리지 않는다.위에서 했던 지저분한 영상을 후처리할 때 좋은 매칭만 선별하는 방법에는 몇가지 방법이 있다. sort distance matching 가장 좋은 매칭 결과에서 DMatch::distance 값을 기준으로 정렬 후 상위 N개 선택 DMatch 클래스에 크기 비교 연산자(&amp;lt;) 오버로딩이 사용할 수 있도록 미리 정의되어 있다.Ptr&amp;lt;DescriptorMatcher&amp;gt; matcher = BFMatcher::create(); //이진 기술자 사용시 BFMatcher::create(NORM_HAMMING);vector&amp;lt;DMatch&amp;gt; matches;matcher-&amp;gt;match(desc1, desc2, matches);std:sort(matches.begin(),matches.end());vector&amp;lt;DMatch&amp;gt; good_matches(matches.begin(), matches.begin()+80);오름차순으로 정렬을 한 후 상위 80개만을 선택해서 그림을 그리면 더 깔끔하게 그림이 그려지는 것을 볼 수 있다. using knnMatch()두 개의 매칭값을 가지고 처리를 하기 위해 knnMatch()를 사용한다. 가장 좋은 매칭 결과의 distance값과 두 번째로 좋은 매칭 결과의 distance 값의 비율을 계산한다. 이 비율이 임계값보다 작으면 선택된다.Ptr&amp;lt;DescriptorMatcher&amp;gt; matcher = BFMatcher::create(); vector&amp;lt;DMatch&amp;gt; matches;matcher-&amp;gt;knnmatch(desc1, desc2, matches,2);vector&amp;lt;DMatch&amp;gt; good_matches;for (auto match : matches) { if (match[0].distance / match[1].distance &amp;lt; 0.7) good_matches.push_back(match[0]);}호모그래피(Homography)원근법이 적용되어 있는 사진을 평평하게 만들고자 한다. 이 과정이 투시 변환과 거의 동일하다. 최소 4개의 대응점(코너 좌표)이 필요하기에 8DOF이다.투시 변환을 진행한 것처럼 getPerspectiveTransform과 warpPerspective를 통해 변환된 직사각형이 나올텐데, 이 매칭값들 중에서 잘못된 매칭이 조금이라도 존재하게 되면 잘못된 변환이 된다. 그래서 이 때는 RANSAC 알고리즘을 사용하여 잘된 매칭만 골라내어 모델링한다. 호모그래피 행렬 구하는 함수Mat findHomography(InputArray srcPoints, InputArray dstPoints, int method = 0, double ransacRepojThreshold = 3, OutputArray mask = noArray(), const int maxIters = 2000, const double confidence = 0.995); srcPoints : src points, CV_32FC2 행렬 또는 vector&amp;lt;Point2f&amp;gt; dstPoints : dst points, CV_32FC2 행렬 또는 vector&amp;lt;Point2f&amp;gt; method : 호모그래피 행렬 계산 방법 0, LMEDS, RANSAC, RHO ransacRepojThreshold : 대응점들을 inlier로 인식하기 위한 최대 허용 에러(픽셀 단위), [RANSAC, RHO] mask : 출력 Nx1 마스크 행렬, RANSAC, RHO 방법 사용 시 inlier로 사용된 점들을 1로 표시한 행렬 maxIters : RANSAC 최대 반복 횟수 confidence : 신뢰도 레벨, 0~1 사이의 실수로 지정 반환값 : 3x3 호모그래피 행렬, CV_64C1, 만약 호모그래피를 계산할 수 없는 상황이면 비어 있는 Mat 객체가 반환된다.이 함수는 투시 변환과 거의 동일하나 행렬을 계산하는 방법과 출력값이 다르다. method 이후의 값들은 디폴트값을 사용하는 게 좋다. find_homographyint main(){ Mat src1 = imread(file1, IMREAD_GRAYSCALE); Mat src2 = imread(file2, IMREAD_GRAYSCALE); if (src1.empty() || src2.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return; } vector&amp;lt;KeyPoint&amp;gt; keypoints1, keypoints2; Mat desc1, desc2; feature-&amp;gt;detectAndCompute(src1, Mat(), keypoints1, desc1); feature-&amp;gt;detectAndCompute(src2, Mat(), keypoints2, desc2); Ptr&amp;lt;DescriptorMatcher&amp;gt; matcher = BFMatcher::create(); vector&amp;lt;DMatch&amp;gt; matches; matcher-&amp;gt;match(desc1, desc2, matches); std::sort(matches.begin(), matches.end()); vector&amp;lt;DMatch&amp;gt; good_matches(matches.begin(), matches.begin() + 80); Mat dst; drawMatches(src1, keypoints1, src2, keypoints2, good_matches, dst, Scalar::all(-1), Scalar::all(-1), vector&amp;lt;char&amp;gt;(), DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS); vector&amp;lt;Point2f&amp;gt; pts1, pts2; for (size_t i = 0; i &amp;lt; good_matches.size(); i++) { pts1.push_back(keypoints1[good_matches[i].queryIdx].pt); pts2.push_back(keypoints2[good_matches[i].trainIdx].pt); } Mat H = findHomography(pts1, pts2, RANSAC); vector&amp;lt;Point2f&amp;gt; corners1, corners2; corners1.push_back(Point2f(0, 0)); corners1.push_back(Point2f(src1.cols - 1.f, 0)); corners1.push_back(Point2f(src1.cols - 1.f, src1.rows - 1.f)); corners1.push_back(Point2f(0, src1.rows - 1.f)); perspectiveTransform(corners1, corners2, H); vector&amp;lt;Point&amp;gt; corners_dst; for (Point2f pt : corners2) { corners_dst.push_back(Point(cvRound(pt.x + src1.cols), cvRound(pt.y))); } polylines(dst, corners_dst, true, Scalar(0, 255, 0), 2, LINE_AA); imshow(&quot;dst&quot;, dst); waitKey(); destroyAllWindows();} RANSAC이란?RANSAC : RANdom SAmple Consensus이상치(outlier)가 많은 원본 데이터로부터 모델 파라미터를 예측하는 방법으로 다양한 점들에 대해 적절한 직선을 찾고자 할 때, 직선과 점들과의 거리를 오차로 두고, 오차가 가장 작은 직선을 구한다. 그러나 점의 분포가 너무 다양하다면 점들의 분포를 가장 잘 표현할 수 있는 직선을 찾기는 어려울 수 있다.그래서 RANSAC 알고리즘을 통해 찾는다. 방법은 다음과 같다. 무작위로 2개의 점을 골라 직선을 구한다. 모든 점들과 이 직선의 거리를 다 구해서 거리가 특정값(margin)안에 들어오는 점들의 개수를 구한다. 이 margin안에 들어오는 점을 inlier, 밖에 있는 점을 outlier 라 한다. 또 다른 무자구이의 2개 점을 골라 직선을 골라 1~2번을 반복한다. 그렇게 계속 찾다보면 가장 높은 수를 가진 직선을 찾게 될 것이다.무작위로 찾기에 미세하게 값이 달라질수는 있다. 이진 기술자를 사용할 경우 create(NORM_HAMMING);으로 작성해야 한다. hamming distance를 사용하라는 인자다. 이 인자를 주지않아도 동작을 하긴 하나 조금 변형된 결과를 얻을 수도 있다.요약 OpenCV 주요 특징점 알고리즘과 기술자 특성 특징점 알고리즘 기술자 차원 depth 이진 기술자 Extra Module 비고 SIFT 128 CV_32F x x   SURF 64 CV_32F x o 알고리즘 특허 -&amp;gt; 사용 시 비용 KAZE 64 CV_32F x x   AKAZE 61 CV_8U o x SIFT와 ORB의 중간 정도 성능 및 속도 ORB 32 CV_8U o x 가장 빠름 연산 시간 비교" }, { "title": "[데브코스] 7주차 - OpenCV parallel computing ", "url": "/posts/parallel/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-31 14:40:00 +0900", "snippet": "Parallel computing영상의 병렬 처리란 cpu하나로만 처리하는 것이 아니라 코어를 다 이용해서 처리한다는 것이다. 기본적인 코드를 사용하면 첫번째에 있는 코어만 사용하게 된다. 이 처리를 코어를 다 사용한다는 것은 대체로 행 단위로 나누어 각각 처리한 후 병합하여 결과를 도출한다.병렬 프로그래밍 기법 intel TBB(threading building blocks) - intel HPX(High Performance ParalleX)** OpenMP(Open multi-processing)** APPLE GCD(Grand Central Dispatch) - apple Windows RT concurrency Windows concurrency - windows(visual studio에 적용되어 있음) Pthreads - linuxcmd를 켜서 opencv_version -v를 쳐보면 리스트가 죽 나올 것이다. 거기서 아래 parallel framework를 보면 concurrency가 되어 있을 것이다.&amp;gt; opencv_version -v...Parallel framework: Concurrency ... 병렬처리용 for루프 코드parallel_for_만 알아도 어떤 프레임워크를 사용하든 알아서 병렬 처리를 해준다.void parallel_for_(const Range&amp;amp; range, const ParallelLoopBody&amp;amp; body, double nstripes = -1.)void parallel_for_(const Range&amp;amp; range, std::function&amp;lt;void(const Range&amp;amp;)&amp;gt; functor, double nstripes = -1.) range : 병렬 처리를 수행할 범위(start~end) body : 함수 객체, ParallelLoopBody 클래스를 상속받은 클래스 또는 C++ 람다 표현식(opencv 3.3 이상부터 가능)반드시 parallelloopbody를 상속받도록 작성해야 하고, 연산자를 재정의해줘야 한다. parallel - 1.1 : 클래스 정의class ParallelContrast : public ParallelLoopBody{public: ParallelContrast(Mat&amp;amp; src, Mat&amp;amp; dst, const float alpha) : m_src(src), m_dst(dst), m_alpha(alpha) // 상속 클래스 정의, 생성자 멤버 변수 초기화 { //m_dst = Mat::zeros(src.rows, src.cols, src.type()); } virtual void operator ()(const Range&amp;amp; range) const // ()연산자 재정의, 이 부분이 실제 연산이 진행되는 부분이다 { for (int r = range.start; r &amp;lt; range.end; r++) { uchar* pSrc = m_src.ptr&amp;lt;uchar&amp;gt;(r); uchar* pDst = m_dst.ptr&amp;lt;uchar&amp;gt;(r); for (int x = 0; x &amp;lt; m_src.cols; x++) pDst[x] = saturate_cast&amp;lt;uchar&amp;gt;((1 + m_alpha)*pSrc[x] - 128 * m_alpha); } } ParallelContrast&amp;amp; operator =(const ParallelContrast &amp;amp;) { return *this; };private: Mat&amp;amp; m_src; Mat&amp;amp; m_dst; float m_alpha;};int main() { Mat src = imread(&quot;hongkong.jpg&quot;, IMREAD_GRAYSCALE); Mat dst; parallel_for_(Range(0, src.rows), ParallelContrast(src, dst, alpha)); ...}여기서 m_dst는 새로 초기화 안해도 된다.()연산자의 경우 원래는 y=0,x=0부터로 작성했지만, 이 때는 인자로 넘어온 range에 대해 r=range.start, range.end를 사용해야 한다. 이 range에는 각각 처리해줄 범위가 다 들어있어야 한다. 즉 예를 들어 0~512 까지행을 처리할 것이라면 range(0,64), range(64,128)… 가 들어가는데, 이는 opencv에서 알아서 해준다. range를 사용한다는 것 이외에는 다 이전과 동일하다.operator = 연산자 또한, 출력을 위해 반드시 작성해줘야 한다.다 정의한 paralleConstrast는 아래에 parallel_for_을 통해 사용된다. parallel - 2 : 람다 표현식int main() { Mat src = imread(&quot;hongkong.jpg&quot;, IMREAD_GRAYSCALE); Mat dst(src.rows,src.cols, src.type()); float alpha = 1.f; parallel_for_(Range(0, src.rows), [&amp;amp;](const Range&amp;amp; range) { for (int r = range.start; r &amp;lt; range.end; r++) { uchar* pSrc = src.ptr&amp;lt;uchar&amp;gt;(r); uchar* pDst = dst.ptr&amp;lt;uchar&amp;gt;(r); for (int x = 0; x &amp;lt; src.cols; x++) { pDst[x] = saturate_cast&amp;lt;uchar&amp;gt;((1 + alpha)*pSrc[x] - 128 * alpha); } } });클래스를 정의한 것보다 훨씬 더 간단해졌다. 여러 연산 비교하기 일반 연산자(+,-,*) 사용 연산자 직접 구현 연산자 직접 구현 parallelConstrast 클래스 구현 람다 표현식 사용#include &quot;opencv2/opencv.hpp&quot;#include &quot;opencv2/core/ocl.hpp&quot; // 이를 실행해야 ocl코드를 사용할 수 있음#include &amp;lt;iostream&amp;gt;using namespace cv;using namespace std;class ParallelContrast : public ParallelLoopBody{public: ParallelContrast(Mat&amp;amp; src, Mat&amp;amp; dst, const float alpha) : m_src(src), m_dst(dst), m_alpha(alpha) { m_dst = Mat::zeros(src.rows, src.cols, src.type()); } virtual void operator ()(const Range&amp;amp; range) const { for (int r = range.start; r &amp;lt; range.end; r++) { uchar* pSrc = m_src.ptr&amp;lt;uchar&amp;gt;(r); uchar* pDst = m_dst.ptr&amp;lt;uchar&amp;gt;(r); for (int x = 0; x &amp;lt; m_src.cols; x++) pDst[x] = saturate_cast&amp;lt;uchar&amp;gt;((1 + m_alpha)*pSrc[x] - 128 * m_alpha); } } ParallelContrast&amp;amp; operator =(const ParallelContrast &amp;amp;) { return *this; };private: Mat&amp;amp; m_src; Mat&amp;amp; m_dst; float m_alpha;};int main(){ ocl::setUseOpenCL(true); // opcnCL을 사용하라는 코드, 밑에서 시간 측정 시 opencv버전마다 다르지만 코드를 실행할 때 openCL을 사용할지에 대한 체크를 하려는 오류가 있기도 해서 미리 정리해줌 Mat src = imread(&quot;hongkong.jpg&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } cout &amp;lt;&amp;lt; &quot;getNumberOfCPUs(): &quot; &amp;lt;&amp;lt; getNumberOfCPUs() &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;getNumThreads(): &quot; &amp;lt;&amp;lt; getNumThreads() &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;Image size: &quot; &amp;lt;&amp;lt; src.size() &amp;lt;&amp;lt; endl; namedWindow(&quot;src&quot;, WINDOW_NORMAL); namedWindow(&quot;dst&quot;, WINDOW_NORMAL); resizeWindow(&quot;src&quot;, 1280, 720); resizeWindow(&quot;dst&quot;, 1280, 720); Mat dst; TickMeter tm; float alpha = 1.f; // 1. Operator overloading tm.start(); dst = (1 + alpha) * src - 128 * alpha; tm.stop(); cout &amp;lt;&amp;lt; &quot;1. Operator overloading: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); // 2. Pixel access by at() (No parallel) dst = Mat::zeros(src.rows, src.cols, src.type()); tm.reset(); tm.start(); for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;((1 + alpha)*src.at&amp;lt;uchar&amp;gt;(y, x) - 128 * alpha); // 2 * src -&amp;gt; 기울기 2로 만듬 } } tm.stop(); cout &amp;lt;&amp;lt; &quot;2. Pixel access by at(): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); // 3. Pixel access by ptr() (No parallel) dst = Mat::zeros(src.rows, src.cols, src.type()); tm.reset(); tm.start(); for (int y = 0; y &amp;lt; src.rows; y++) { uchar* pSrc = src.ptr&amp;lt;uchar&amp;gt;(y); uchar* pDst = dst.ptr&amp;lt;uchar&amp;gt;(y); for (int x = 0; x &amp;lt; src.cols; x++) { pDst[x] = saturate_cast&amp;lt;uchar&amp;gt;((1 + alpha)*pSrc[x] - 128 * alpha); } } tm.stop(); cout &amp;lt;&amp;lt; &quot;3. Pixel access by ptr(): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); // 4. cv::parallel_for_ with ParallelLoopBody subclass dst = Mat::zeros(src.rows, src.cols, src.type()); tm.reset(); tm.start(); parallel_for_(Range(0, src.rows), ParallelContrast(src, dst, alpha)); tm.stop(); cout &amp;lt;&amp;lt; &quot;4. With parallel_for_ (ParallelLoopBody): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); // 5. cv::parallel_for_ with lambda expression dst = Mat::zeros(src.rows, src.cols, src.type()); tm.reset(); tm.start(); parallel_for_(Range(0, src.rows), [&amp;amp;](const Range&amp;amp; range) { for (int r = range.start; r &amp;lt; range.end; r++) { uchar* pSrc = src.ptr&amp;lt;uchar&amp;gt;(r); uchar* pDst = dst.ptr&amp;lt;uchar&amp;gt;(r); for (int x = 0; x &amp;lt; src.cols; x++) { pDst[x] = saturate_cast&amp;lt;uchar&amp;gt;((1 + alpha)*pSrc[x] - 128 * alpha); } } }); tm.stop(); cout &amp;lt;&amp;lt; &quot;5. With parallel_for_ (lambda expression): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); return 0;}병렬처리를 통해 훨씬 빠른 연산 속도를 확인할 수 있다.유용한 OpenCV 활용 기법메모리 버퍼로부터 Mat 객체 생성사용자가 할당한 메모리 버퍼로부터 Mat 객체 생성하기 외부 함수 또는 외부 라이브러리로부터 생성된 영상 데이터 메모리 버퍼가 있을 경우, 해당 메모리 버퍼를 참조하는 Mat 객체를 생성하여 사용 가능하다. 이는 Mat객체 생성후 OpenCV 라이브러리를 사용할 수 있다.VideoCapture은 범용성을 중시하기에 최고의 성능을 내기는 어렵다. 영상 데이터에 대한 Mat객체를 복사하는 것은 최소화해야 속도가 빠르다. 그 대신 포인터 함수로 메모리를 가리키도록만 해주면 복사는 없어지게 된다.Mat mst; // 복사 -&amp;gt; 비효율적uchar* data; // 메모리의 시작점을 가리키는 포인터 사용자가 할당한 메모리 버퍼를 이용하여 Mat객체 생성Mat::Mat(int rows, int cols, int type, void* data, size_t step=AUTO-STEP); rows : 행의 개수 cols : 열의 개수 type : 행렬 원소의 타입 data : 사용자가 할당한 메모리 버퍼 주소 step : 한 행이 차지하는 바이트 수 (패딩할 경우 주의해야 함)#include &quot;opencv2/opencv.hpp&quot;#include &amp;lt;fstream&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace cv;using namespace std;int main(void){ // Raw 파일 열기, 512x512 FILE* fp = fopen(&quot;elaine.raw&quot;, &quot;rb&quot;); if (fp == NULL) { cout &amp;lt;&amp;lt; &quot;File load error!&quot; &amp;lt;&amp;lt; endl; return -1; } // 연속된 메모리 동적 할당 &amp;amp; 파일 읽기 int width = 512; int height = 512; int area = width * height; int step = 512; uchar* pixels = new uchar[area]; int ret = (int)fread(pixels, sizeof(uchar), area, fp); fclose(fp); // 이미 존재하는 메모리 버퍼를 참조하는 Mat 객체 생성 Mat src(height, width, CV_8UC1, pixels, step); // OpenCV Operations Mat dst; bilateralFilter(src, dst, -1, 20, 5); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); delete [] pixels; return 0;}이렇게 src를 생성하면 area공간의 메모리를 가리키는 포인터를 받아서 그 area만을 가리키게 된다.new연산자를 통한 동적 메모리 할당을 진행하면 반드시 해제해줘야 한다. 내가 직접 생성한 변수를 통해 src를 생성했기 때문에, 직접 생성한 변수만 메모리 할당을 해제해주면 되고, src에 대해서는 자동으로 해제가 된다.delete [] pixels;이 메모리 버퍼 방식은 예전에 했던 data를 참조하는 방식과 동일하다.float data[] = { 1,1,2,3 };Mat mat1(2, 2, CV_32FC1, data); // 메모리 버퍼를 참조하는 객체 생성룩업 테이블(LUT:Lookup Table)특정 연산에 대해 미리 결과 값을 계산하여 배열 등으로 저장해놓은 것을 말한다. 픽셀 값을 변경하는 경우 256x1 크기의 unsigned char 행렬에 픽셀 값 변환 수식 결과 값을 미리 저장한 후, 실제 모든 픽셀에 대해 실제 연산을 수행하는 대신 행렬(룩업 테이블) 값을 참조하여 결과 영상 픽셀 값을 설정한다.파이썬의 dictionary에 저장해놓고 불러오는 것과 비슷한 것으로 특정 값(픽셀 가로 idx)에 대한 결과값을 저장한다. 룩업 테이블 연산 함수void LUT(InputArray src, InputArray lut, OutputArray dst); src : 입력 영상, 8비트 lut : n개 원소를 갖는 룩업 테이블, 보통은 1x256(1행 256열) dst : 출력 영상, src와 같은 크기, 같은 채널수, lut와 같은 깊이lut 룩업 테이블로부터 값을 받아와 dst 픽셀 값을 설정하는데, 이때 인덱스 정보는 입력 행렬의 픽셀 값을 사용한다. 룩업 테이블을 이용한 명암비 향상 코드int main(vold){ Mat src = imread(&quot;hongkong.jpg&quot;,IMREAD_GRAYSCALE); Mat lut(1, 256, CV_8U); uchar* p = lut.ptr(0); for (int i = 0; i &amp;lt; 256; i++) { p[i] = saturate_cast&amp;lt;uchar&amp;gt;(2 * i - 128); } LUT(src, lut, dst); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();} 결과 비교 일반 연산자 사용 ptr을 통한 직접 구현 LUT 사용#include &quot;opencv2/opencv.hpp&quot;#include &quot;opencv2/core/ocl.hpp&quot;#include &amp;lt;iostream&amp;gt;using namespace cv;using namespace std;int main(void){ ocl::setUseOpenCL(false); // 진행 코드에서 OpenCL확인 코드로 인해 혹시 모를 불필요를 막기 위함 Mat src = imread(&quot;hongkong.jpg&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } cout &amp;lt;&amp;lt; &quot;getNumberOfCPUs(): &quot; &amp;lt;&amp;lt; getNumberOfCPUs() &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;getNumThreads(): &quot; &amp;lt;&amp;lt; getNumThreads() &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;Image size: &quot; &amp;lt;&amp;lt; src.size() &amp;lt;&amp;lt; endl; namedWindow(&quot;src&quot;, WINDOW_NORMAL); namedWindow(&quot;dst&quot;, WINDOW_NORMAL); resizeWindow(&quot;src&quot;, 1280, 720); resizeWindow(&quot;dst&quot;, 1280, 720); Mat dst; TickMeter tm; // 1. Operator overloading tm.start(); dst = 2 * src - 128; tm.stop(); cout &amp;lt;&amp;lt; &quot;1. Operator overloading: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); // 2. Pixel access by ptr() tm.reset(); tm.start(); dst = Mat::zeros(src.rows, src.cols, src.type()); for (int j = 0; j &amp;lt; src.rows; j++) { uchar* pSrc = src.ptr&amp;lt;uchar&amp;gt;(j); uchar* pDst = dst.ptr&amp;lt;uchar&amp;gt;(j); for (int i = 0; i &amp;lt; src.cols; i++) { pDst[i] = saturate_cast&amp;lt;uchar&amp;gt;(2 * pSrc[i] - 128); } } tm.stop(); cout &amp;lt;&amp;lt; &quot;2. Pixel access by ptr(): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey(); // 3. LUT() function Mat lut(1, 256, CV_8U); uchar* p = lut.ptr(0); for (int i = 0; i &amp;lt; 256; i++) { p[i] = saturate_cast&amp;lt;uchar&amp;gt;(2 * i - 128); } tm.reset(); tm.start(); LUT(src, lut, dst); tm.stop(); cout &amp;lt;&amp;lt; &quot;3. LUT() function: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot; ms.&quot; &amp;lt;&amp;lt; endl; imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}룩업 테이블을 사용하는 예로는 영상 데이터를 회전할 때 sin,cos연산이 되게 느리게 동작한다. 그래서 이를 0.1도씩 룩업 테이블을 만들어놓고 쓰면 훨씬 빨라질 수 있다." }, { "title": "[데브코스] 7주차 - ROS steering angle control using PID ", "url": "/posts/pid/", "categories": "Classlog, devcourse", "tags": "devcourse, ros", "date": "2022-03-30 15:40:00 +0900", "snippet": "필터 기반 조향각 제어조향각이 계속 변하면 차량이 계속 좌우로 흔들리게 된다. 그래서 조향각에 대해서도 필터링을 하면 부드러운 핸들링이 될 수 있다.필터는 평균 필터, 이동 평균 필터, 가중 이동 평균 필터, 저주파 통과 필터(지수 가중 이동 평균 필터) 등이 있다.핸들링을 조작하는데 과거 값은 필요가 없고, 최신 데이터가 중요하므로 가중 이동 평균 필터는 적절하지 않다.그렇다면 차선인식 데이터를 기반으로 하거나 조향각 데이터를 기반으로 필터를 적용하는 것이 좋을 것 같다. 그러나 차선의 위치값에도 노이즈가 존재할 수 있으니 이에 대해서도 필터를 적용해야 한다.차선 위치정보를 저장하는 변수를 생성해서 데이터를 계속 모은다. 왼쪽, 오른쪽 각각의 데이터를 이용하여 계산한 조향각 angle값에 대해 필터를 적용한다.가중 이동 평균 필터를 적용하는데 최신 데이터에 더 많은 가중치를 부여하는 방식으로 적용한다.class MovingAverage: # 1. def __init__(self, n): self.samples = n self.data = [] # 2. self.weights = list(range(1, n+1)) def add_sample(self, new_sample): if len(self.data) &amp;lt; self.samples: # 3. self.data.append(new_sample) else: # 4. self.data = self.data[1:] + [new_sample] print(&quot;samples: %s&quot; % self.data) def gem_mm(self): # 5. return float(sum(self.data)) / len(self.data) def get_wmm(self): # 6. s = 0 for i, x in enumerate(self.data): s += x * self.weights[i] return float(s) / sum(self.weights[:len(self.data)]) 1.n으로 초기화, n은 데이터 개수를 의미한다.2.가중치 값 만들기3.새로운 샘플을 맨 뒤에 추가4.리스트가 꽉차면 오래된 데이터는 버리기5.일반 평균 필터6.가중 이동 평균 필터def start(): global pub, image, cap, video_mode, width, height mm1 = MovingAverage(50) rospy.init_node(&#39;auto_drive&#39;) pub = rospy.Publisher(&#39;xycar_motor&#39;, xycar_motor, queue_size = 1) image_sub = rospy.Subscriber(&quot;/usb_cam/image_raw&quot;, Image, img_callback) rospy.sleep(2) while True: # 1. while not image.size == (640*480*3): continue # 2. lpos, rpos = process_image(image) # 3. center = (lpos, rpos) / 2 angle = (center - width/2) mm1.add_sample(angle) # 4. wmm_angle = mm1.get_wmm() # 5. driver(wmm_angle, 30) 1.카메라 영상 이미지를 한장씩 처리하기 위함2.왼쪽 차선과 오른쪽 차선의 위치 찾기3.차선의 중심과 화면 중앙과의 차이값을 조향각으로 계산4.50개 샘플에서 가중이동 평균값 구하기5.구해진 조향각을 모터제어노드로 보내기그렇다면 조향각 데이터가 아닌 차선 위치 데이터에 적용을 해보면 어떨까? 또는 차량의 속도가 느릴 때와 빠를 때 각각 어떤 필터가 효과적인지도 확인해볼 필요가 있다.PID 기반 조향각 제어control 기법에는 크게 2가지가 있다. open loop control controller이 입력을 받아 control signal을 보내고 프로세스에서 출력한다. 결과를 확인할 수 없다. closed loop control 센서 등을 통해 데이터를 수집하고 수집한 데이터를 기반으로 반복적인 피드백으로 제어한다. 대표적인 기법 : PID control 피드백 제어란 process를 거쳐서 나온 output이 input에 영향을 미치는 loop를 말한다. 예를 들어 시속 60km/h로 달리려는 자동차가 있다고 할 때, 단순히 에너지 공급/차단에 대한 on/off 방식은 출력값의 변화가 너무 크고, 시간이 지나도 목표값과의 오차가 줄어들지 않는다. 그래서 PID제어기를 사용한다. Proportional : 비례 Integral : 적분 Differential : 미분제어 대상의 목표값(desired value)과 출력값(output)과의 차이로 제어하는 방식이다. 오버슈트 : 최종 정상상태 값(1)을 넘어서는 상승 오차 피크 시간 : 가장 큰 overshoot(최대오버슈트)가 발생했을 때 시간 상승 시간 : output의 0.1부터 0.9까지 걸리는 시간 정착 시간 : 최종 정상상태에 도달하는 시간\\(MV(t) = Kp * e(t) + (Ki \\int e(t)dt - Ki \\int e(0)dτ) + Kd * {de \\over dt}\\)각각 비례, 적분, 미분에 대한 것이다.P 제어피드백 제어 신호가 오차에 비례하게 만드는 비례 제어기를 말한다.\\[Kp * e(t) = Pgain x error(오차)\\]편차에 따라 반비례한 값으로 조작한다. Kp(p상수)가 클 때 오차가 작아도 출력값이 크게 변한다. 장점 : 빠른 응답속도를 가진다. 단점 : 오버슈트가 발생 Kp가 작을 때 오차가 작을수록 출력값이 작게 변한다. 장점 : 오버슈트가 적게 발생 단점 : 느린 응답속도 P제어기만 사용할 경우 정상상태 오차가 발생한다. 정상 상태 오차(steady state error)란 반응이 일정수준에 정작한 이후에도 존재하는 오차를 말한다. 즉, 시간을 무제한으로 늘려도 목표값에 완전히 도달하지는 못한다.이 그래프는 예전에 직접 실험했던 자료를 가져왔다. 맨 처음 그래프는 Kp = 0 인 상태이다. 증가하다가 목표값을 지나 다시 내려온다. 이 때, Kp값을 증가시키면 진동이 일어나긴 하나 목표값에 더 빠르게 도달한다. Kp값을 더 높이게 되면 진동이 더 심하게 일어나긴 하나 빠르게 목표값에 도달한다. 또한, 10일때는 목표값인 1에 도달하지 않고 다소 크게 오차가 있다. 100일 경우 목표값과의 오차가 줄어든 것을 볼 수 있다.I 제어적분을 이용하여 비례 제어에서 남아있는 오차를 제거하는 제어 방법을 말한다. 출력값이 목표값에 빠르게 도달하고 수렴하게 한다.\\[Ki \\int e(t)dt - Ki \\int e(0)dτ = Igain x 누적 error\\]이 때는 순간 오차가 아닌 누적오차를 이용하여 조작한다. Ki(i상수)가 클 때 누적 오차가 빠르게 증가 장점 : 빠른 응답 속도를 가짐 단점 : 오버 슈트가 크게 발생 Ki가 작을 때 누적 오차가 느리게 증가 장점 : 오버슈트가 작게 발생 단점 : 느린 응답 속도를 가짐 I 제어기를 사용할 경우 정상상태 오차를 줄일 수 있다. 그러나 오차가 없는 상태에도 I 제어기에 남아있는 누적오차때문에 제어값이 계속 발생한다.응답 속도는 비슷하나 Kp값을 고정한 채로 Ki값을 증가시키니 목표값과의 오차가 거의 없어졌지만, 계속 흔들리는 상태이 존재한다.D 제어미분을 이용하여 진동을 줄이고 안정성을 향상하는 제어 방법이다. 급격한 출력값의 변동이 발생했을 때 급격하게 변하지 않도록 조정한다. 오차가 상수일 경우 D제어기의 출력은 0이 되어 정상상태 오차를 줄일 수 없다. 특정 신호가 급현하는 경우 미분 제어기의 출력이 급격하게 커져 시스템을 파괴하는 경우도 있어 주의해야 한다.\\[Kd * {de \\over dt} = Dgain x error 변화량\\] Kd가 클 때 장점 : 오버슈트가 작게 발생 단점 : 신호가 급변하는 경우 시스템이 파괴될 수 있다 Kd가 작을 때 장점 : 신호가 급변해도 적절한 피드백이 가능 단점 : 오버슈트가 다소 크게 발생 D 제어기를 사용할 경우 시스템의 안정도를 증가시킬 수 있다. 미분이 불가능한 오차인 경우 적절한 제어가 되지 않을 수 있다.Kd를 적용하지 않을 때는 오버슈트가 발생하는데, Kd를 적용하니 오버슈트가 거의 발생하지 않는 것을 볼 수 있다.따라서 이상적인 제어 결과는 다음 조건을 만족해야 한다. 빠른 응답속도 오버슈트 발생 x 정상상태 오차 발생 x그러나 gain값들 사이의 trade-off가 존재한다. 즉, 1개가 좋아지면 다른 하나가 나빠지거나 한다. 따라서 기준을 잡고 이에 만족하도록 최적화를 해야 한다. 주로 5% 이하의 오버슈트, 0.2초 이내의 정착 시간으로 설정한다.PID 제어를 통한 조향각 제어참고 영상핸들 조작에 PID 제어를 적용해보고자 한다. 이 때는 CTE(Cross Track Error) 값을 0 으로 만드는 것을 목표로 한다. 즉, 목표 궤적이 있고, 그와 실제 주행중인 거리에 대한 오차를 CTE라고 한다.P 제어P제어의 경우 steering angle = P * ep 식이 적용되고, 이 때 ep가 CTE이다. P제어를 핸들링에 적용할 경우 목표 지점과의 오차가 크면 핸들을 많이 꺾고, 작으면 적게 꺾는다.P gain을 작으면 목표지점까지 오래 걸리고, 높으면 금방 도달하는 것을 볼 수 있다.PD 제어P gain 값에 D 제어를 추가하여 steering angle = P * ep + D * ed를 적용해보았다. 이 때, ed는 CTE rate, 즉 error 변화량을 나타낸다.P 제어는 오차를 빨리 줄이기 위한 초록색 화살표에 해당한다. 오버슈트를 줄이기 위한 D gain는 주황색 화살표에 해당한다. 그래서 실제 주행 방향은 이 둘은 더한 방향인 진한 주황색 화살표가 된다.D gain이 작으면 오버슈트가 발생하고, 높으면 너무 심하게 제어된다.PID 제어이번에는 I 제어까지 추가해서 steering angle = P * ep + D * ed + I * ei 를 적용했다.이는 돌이나 특정 물체로 인해 궤적을 벗어낫을 때 다시 되돌아오는 상황을 살펴보았다. I gain 이 작으면 벗어나고 다시 돌아오기 어려워진다. 그러나 I gain이 높으면 너무 심하게 핸들을 꺽어 심하게 흔들린다.이 때 closed loop control 방식을 사용한다.PID 실제 적용카메라를 통해 핸들을 조정할 때 PID를 적용한다면, CTE = 화면 중앙과 좌우 차선의 중점 사이의 간격 이 될 것이다. 그래서 이에 대해 PID 제어를 적용한다.P term : \\(Pgain x error(오차)\\)I term : \\(Igain x 누적 error\\)D term : \\(Dgain x error 변화량\\)MV(t) = P term + I term + D term이를 파이썬에 적용하기 위해서 클래스를 구현한다.class PID(): def __init__(self,kp,ki,kd): self.kp = kp self.ki = ki self.kd = kd self.p_error = 0.0 self.i_error = 0.0 self.d_error = 0.0 def pid_control(self, cte): # 1. self.d_error = cte-self.p_error # 2. self.p_error = cte # 3. self.i_error += cte return self.kp*self.p_error + self.ki*self.i_error + self.kd*self.d_error 1.d값은 현재 cte값과 이전 p값과의 차이값(변화량)을 적용2.p값은 cte 값 그대로 적용3.i값은 cte값을 계속 더해서 누적한 값을 적용과정을 조금 더 자세하게 설명하자면 다음과 같다.CTE = 목표값 - 현재값 목표값 : 영상처리를 통해 찾아낸 좌우 차선의 중점 위치 현재값 : 화면 중앙점 위치 (가로 640) =&amp;gt; CTE = (rpos + lpos)/2 - 320 PID 조향각 설정 코드class PID(): def __init__(self,kp,ki,kd): self.kp = kp self.ki = ki self.kd = kd self.p_error = 0.0 self.i_error = 0.0 self.d_error = 0.0 def pid_control(self, cte): self.d_error = cte-self.p_error self.p_error = cte self.i_error += cte return self.kp*self.p_error + self.ki*self.i_error + self.kd*self.d_errordef process_image(image): global Width global Offset, Gap # gray gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) # blur kernel_size = 5 blur_gray = cv2.GaussianBlur(gray,(kernel_size, kernel_size), 0) # canny edge low_threshold = 60 high_threshold = 70 edge_img = cv2.Canny(np.uint8(blur_gray), low_threshold, high_threshold) # HoughLinesP roi = edge_img[Offset : Offset+Gap, 0 : Width] all_lines = cv2.HoughLinesP(roi,1,math.pi/180,30,30,10) # divide left, right lines if all_lines is None: return 0, 640 left_lines, right_lines = divide_left_right(all_lines) # get center of lines frame, lpos = get_line_pos(frame, left_lines, left=True) frame, rpos = get_line_pos(frame, right_lines, right=True) # draw lines frame = draw_lines(frame, left_lines) frame = draw_lines(frame, right_lines) frame = cv2.line(frame, (230, 235), (410, 235), (255,255,255), 2) # draw rectangle frame = draw_rectangle(frame, lpos, rpos, offset=Offset) #roi2 = cv2.cvtColor(roi, cv2.COLOR_GRAY2BGR) #roi2 = draw_rectangle(roi2, lpos, rpos) # show image cv2.imshow(&#39;calibration&#39;, frame) return lpos, rposdef start(): global pub, image, cap, width, height rospy.init_node(&#39;auto_drive&#39;) pub = rospy.Publisher(&#39;xycar_motor&#39;, xycar_motor, queue_size=1) image_sub = rospy.Subscriber(&quot;/usb_cam/image_raw&quot;, Image, img_callback) rospy.sleep(2) while True: while not image.size == (640*480*3): continue lpos, rpos = process_image(image) center = (lpos + rpos)/2 error = (center - width/2) pid = PID(0.5,0.0005,0.05) angle = pid.pid_control(error) drive(angle,30) # drive(angle, speed)실험 과정 gain값을 임의로 설정한다. P gain은 1이하, I gain은 매우 작게 0.001이하, D gain은 0.1 이하로 설정 대체로 P gain = 0.5, I gain = 0.0005, D gain = P gain의 1/10 값을 넣고 차를 구동시켜서 현상을 관찰하며 적절하게 gain값을 조정 P gain = 0.5, I gain = 0.0005, D gain = 0.05 차가 진동하면서 이동한다면 오버슈트가 발생해서 목표값 이상의 값을 만들기 때문에 발생하는 현상이므로 P gain을 감소시켜 오버슈트를 감소시킴 P gain = 0.45, I gain = 0.0005, D gain = 0.05 차의 진동이 완화되었지만, 여전히 진동이 발생한다면 D gain값이 작아서 오버슈트를 줄이지 못하기 때문일 수 있으므로 D gain을 증가시킨다. P gain = 0.5, I gain = 0.0005, D gain = 0.15 차의 진동은 잡았지만 중앙선에서 벗어난 주행을 한다면 정상상태 오차이므로 I gain을 증가시킨다. P gain = 0.5, I gain = 0.0007, D gain = 0.15 차가 안정적인 주행을 한다면 성공 이 때 차량의 속도를 높이면 PID 제어를 다시 해야 한다. 속도를 높인다면 정착시간까지 반응이 일어나지 않기 때문에 I 제어기가 필요하지 않다. 따라서 PD 제어기만 사용하여 P = 0.55, D = 0.4로 지정해본다." }, { "title": "[linux] error 모음집 및 해결 ", "url": "/posts/linuxerror/", "categories": "Review, linux", "tags": "linux, error", "date": "2022-03-30 01:17:00 +0900", "snippet": "sof-audio-pci ~https://bugzilla.kernel.org/show_bug.cgi?id=205959여기서$ echo &quot;options snd-intel-dspcfg dsp_driver=1&quot; &amp;gt; /etc/modprobe.d/alsa.conf그래도 안되면https://askubuntu.com/questions/1243369/sound-card-not-detected-ubuntu-20-04-sof-audio-pci$ sudo gedit /etc/default/grubGRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash snd_hda_intel.dmic_detect=0&quot;$ sudo update-grub$ reboothdaudio hdaudioC0D2: Unable to bind the codechttps://ubuntuforums.org/showthread.php?t=2437409nomodeset을 삭제한다. nvidia드라이버를 설치하면 nomodeset이 필요없기 때문이다.$ sudo gedit /etc/default/grubGRUB_CMDLINE_LINUX_DEFAULT=&quot;snd_hda_intel.dmic_detect=0&quot;$ sudo update-grub$ reboothttps://ubuntuforums.org/showthread.php?t=2444070$ pacmd list-sources | grep input위에서 나온 name을 지정$ pacmd set-default-source alsa_input.usb-046d_HD_Pro_Webcam_C920_76B4D93F-02.analog-stereo.2그래도 안된다면$ journalctl -b -p err출력된 에러들을 다 처리해줘야 한다.Nvidia 그래픽 드라이버 설치https://phoenixnap.com/kb/install-nvidia-drivers-ubuntu#ftoc-heading-17" }, { "title": "[논문 리뷰] YOLO-based lane detection system", "url": "/posts/yolo/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, YOLO", "date": "2022-03-30 01:17:00 +0900", "snippet": "YOLO based lane detection system 논문에 대한 리뷰입니다. 본 논문은 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract자율주행과 반자율주행 자동차가 개발되고 이러한 기술들은 주변 환경 문제로 인해 차선 이탈 경우와 자율주행 자동차에서 판단하지 못하는 상황이 생기고, 차선 검출기에서는 차선을 인식하지 못하는 경우가 있다. 이런 문제점에 대한 성능을 향상시키기 위해 본 논문에서는 YOLO(You Only Look Once)의 특성인 빠른 인식을 사용하고 CSI Camera를 사용하여 주변 환경으로부터 영향을 받는 상황을 인지하고 주행 데이터를 수집하여 관심 영역을 추출하는 차선 검출 시스템을 제안한다.서론자율주행 분야는 운전자의 조작이 없이 주행 상황을 판단하고 스스로 운행을 제어하여 스스로 주어진 목적지까지 주행하는것을 목표로 한다. 첨단 감지 센서, 통신 및 영상 장비 등을 사용하여 운전자에게 서비스를 제공하는 ADAS(Advanced Driver Assistance Systems)가 있는데, 이 기술에는 차선 검출과 트롤리 딜레마(trolley dilemma) 기술 등이 포함된다.차선 검출 기술은 주변 환경의 영향을 받아 차선을 인식하지 못하는 문제가 존재한다. 따라서, 본 논문에서 제안하는 차선 이탈에 대한 성능을 향상하기 위해 객체 인식 기술을 활용하여 차선 검출 시스템에 관한 연구를 제안한다.CSI Camera를 사용하여 YOLO 기반 실시간 영상에서 차선의 특징을 추출하여 차선이 지워지거나 흐려진 차선을 판별하고 직진과 커브 같은 수많은 상황의 주행 데이터를 수집 및 저장하고 차선 검출을 하며 차선 검출을 잘 하기 위해 노출 및 밝기 등을 활용하여 인식률을 향상시켜야 한다. CSI camera란?카메라 직렬 인터페이스(camera serial interface)는 MIPI(Mobile Industry Processor Interface)의 일종이다. 이는 카메라와 프로세서 사이의 연결을 의미하는데, CSI-1,CSI-2,CSI-3 등이 있다. 이는 카메라와 호스트 프로세서 사이의 연결을 규정하는 아키텍처 버전을 의미한다. 이와 비슷한 것으로 DSI(Display Serial Interface)가 있다. 이 둘은 OSI 7계층 중 프로토콜 계층에 속한다. 위키디피아 참고 블로그시스템 설계여기서는 yolo를 통한 차선 인식 시스템을 설계하고, CSI Camera에서 촬영된 영상을 크기와 대비를 조정하여 전처리를 제어하는 과정을 나타낸다. 사용자 차량의 블랙박스에 CSI Camera를 설치하여 영상을 촬영하여 데이터를 수집한다. 수집된 데이터는 전처리 과정을 통해 차선을 추출하고, 관심 영역(ROI)를 자른 후 차선의 특성을 추출한다. 후처리로는 차선 검출 결과는 사용자에게 차선 검출이 되지 않았을 경우 경고 시스템을 작동시켜 알림 서비스를 제공한다.시스템 흐름아래 그림은 데이터 처리 과정이다.이 논문에서 Jetson nano developer kit에 micro SD카드를 부팅 장치나 주저장장치로 사용했다고 한다. CSI Camera에서 데이터를 추출해서 jetson nano로 보내서 데이터를 처리한다. 처리 과정에는 흰색 차선 검출, ROI(region of interest)를 자르고, 라인 특징을 인식한다. 그리고 그 결과를 통해 라인과 떨어져 있는지, 라인이 없는지 확인하여 사용자에게 경고 알림을 보낸다.1. 관심 영역 추출주행 영상에서 얻은 데이터를 사용해서 DarkNet을 기반으로 관심 영역을 추출하고 객체마다 바운딩박스를 그려주어 관심 영역으로 설정한다. 아래 그림은 이를 시각화한 그림이다.원본 영상에서 단일 프레임을 추출해서 관심 영역 인식을 하였고, 이미지 크기를 작게 만들어 연산량을 줄였다. 줄인 이미지는 CNN에 보내져 특징맵(feature map)으로 리턴되고, 여기서 찾아진 바운딩박스에서 같은 물체를 가르키는 박스들을 전부 제거해줘야 한다. 이 때 IOU(intersection of union), 즉 겹치는 정도가 지정해둔 임계값(threshold)보다 높으면 같은 물체를 가르키고 있다고 판단하여 NMS(non-max supression)을 통해 제거해준다. NMS는 말 그대로 가장 높은 것만 놔두고 나머지는 제거하는 방식이다.이 때, 차선이 아닌 객체만 인식하도록 클래스를 지정하여 객체만 인식시킨다. 이 상자 좌표를 기반으로 관심 영역을 추출하고 자른 이미지를 따로 저장한다.(나의 의견으로, 자른 이미지를 저장하는 이유는 프레임에서 물체를 인식하여 저장해서 객체 인식 훈련에 사용될 데이터 셋을 쌓는 것 같다.)2. 차선 인식주행 영상을 Video 폴더에 저장하고, 차선을 검출하지 못했을 경우에는 출력화면에 경고 알림으로 사용자에게 알린다.이에 대한 자세한 설명은 아래 전처리에서 설명한다.3. 데이터베이스 설계PyMySQL의 라이브러리를 사용하여 시스템을 설계했다고 한다. 카메라에서 실시간 영상을 실시간으로 받아 저장된 데이터는 데이터베이스에 저장된다. 설계한 데이터베이스는 3가지 테이블을 갖추고 있다. Video storage : 실시간으로 받아진 영상 데이터 ROI setting : CNN을 통해 추출된 ROI로 자른 이미지 Lane detection : 차선 영상을 저장1. 전처리수집된 영상 데이터는 전처리된다. 전처리라 함은 이미지 크기 조정 및 관심 영역 추출이다.위는 전처리 과정을 나타낸 것으로, CNN을 통해 추출된 차선의 특징맵(feature map)에서 후보 순위를 두는 proposal과 ROI들을 뽑아낸다. 그 후 수많은 ROI를 pooling하여 score가 가장 높은 것만 남겨서 개수를 축약시킨 뒤 proposal과 겹치는 바운딩 박스들을 classifier로 보낸다. classifier는 보통 FC layer을 사용하거나 1-dimension convolutional layer를 사용한다. classifier를 통해 차선을 검출한다.전처리 과정에서 차선의 특징 추출을 통해 흰색 차선을 관심 영역으로 자르고 차선의 특성에 대해 감지한다. 후처리로 차선 검출의 결과값을 받아 차선 검출을 확인하여 사용자에게 제공하고, 되지 않으면 경고 시스템을 작동시킨다. 아래 그림은 시스템의 검출 과정을 나타낸 것이다.시스템 구현촬영된 영상과 실시간 촬영으로 수집된 데이터를 기반으로 ROI 설정과 관심 영역설정에 대해 설명한다.1. 객체 인식 기반 차선 검출 구현수집된 영상 데이터에서는 출력 크기를 조정하기 위해 훈련된 모델을 활용하여 검출 module을 초기화했다. 출력 크기를 임의로 설정하고 검출기에서 특징점 module 초기화를 통해 shape_predictor를 사용한다.차선을 찾는 기본적인 방법은 차선의 특징점을 찾아 RGB 색상으로 인해 흰색 또는 노란색 차선, 직선 또는 곡선 차선의 특징을 파악하여 ROI를 설정하고 관심 영역을 추출하여 차선을 검출한다.그림 9는 흰색 차선 시각화 프로세스, 그림 10은 관심 영역설정과 흰색 차선 추출, 그림 11은 ROI 설정과 RGB색 변경, 그림 12는 결과 차선검출 화면을 나타낸다.그림9의 차선 시각화라 함은 이미지의 노출이나 밝기 등을 조절하여 찾기 쉽도록 설정하는 것이고, 그림 10은 차선이 있는 영역을 추출해서 그에 대한 흰색 차선 추출, 그림 11은 관심 영역 추출이라는 것이 흰색 차선 부분만 출력한다는 말인 듯하고, 이를 통해 추출된 흰색 차선을 RGB 색상으로 변경하였고, 그림12는 검출된 차선을 원본 이미지에 오버레이한 것이다.2. 데이터 셋 생성 및 데이터 증각데이터 증강을 위해, 수집한 데이터에 ImageDataGenerator 클래스를 활용해서, CNN을 통해 차선 검출 과정 이미지를 분류하고, 이 이미지를 데이터셋으로 생성했다.reference YOLO based lane detection system darknet 참고 자료" }, { "title": "[데브코스] 7주차 - ROS wraping transformation and lane detection by sliding window ", "url": "/posts/warping/", "categories": "Classlog, devcourse", "tags": "devcourse, sliding window", "date": "2022-03-29 15:01:00 +0900", "snippet": "Warping사전적으로 뒤틀림이라는 의미로, 영상에서 말하는 warping은 이동, 회전, 크기 변환 등을 말한다. 찌그러진 이미지를 복원할 수도 있다.ROS 환경 설정1.패키지 생성 sliding_drive 패키지를 생성$ catkin_create_pkg sliding_drive std_msgs rospy2.src 폴더 아래 girl.png, chess.png 파일 복사Translation 변환1.평행이동 이미지를 이동하려면 원래 있던 좌표에 이동시키련느 거리만큼 더하면 된다. x_new = x_old + d1 y_new = y_old + d2 이를 변환 행렬로 표현하면따라서 x_old + d1 = 1 * x_old + 0 * y_old + d1 y_old + d2 = 0 * x_old + 1 * y_old + d2dst = cv2.warpAffine(src, matrix, dsize, dst, flags, borderMode, borderValue) src : 원본 이미지, numpy 배열 matrix : 2x3 변환 행렬, dtype=float32 dsize : 결과 이미지의 크기 dst : option,결과 이미지 flags : option, 보간법 알고리즘 플래그 cv2.INTER_LINEAR : default, 인접한 4개 픽셀 값에 거리 가중치 사용 cv2.INTER_NEAREST : 가장 가까운 픽셀 값 아용 cv2.INTER_AREA : 픽셀 영역 관계를 이용한 재샘플링 cv2.INTER_CUBIC : 인접한 16개 픽셀 값에 거리 가중치 사용 borderMode : option, 외곽영역 보정 플래그 cv2.BORDER_CONSTANT : 고정 색상 값 cv2.BORDER_REPLICATE : 가장자리 복제 cv2.BORDER_WRAP : 반복 cv2.BORDER_REFLECT : 반사 borderValue : option, 외곽영역 보정 플래그가 cv2.BORDER_CONSTANT일경우 사용할 색상 값 translation.pyimport cv2import numpy as npimg = cv2.imread(&#39;girl.png&#39;)0rows, cols = img.shape[0:2]dx,dy=100,50mtrx = np.float32([[1,0,dx],[0,1,dy]])dst = cv2.warpAffine(img,mtrx, (cols+dx, rows+dy))dst2 = cv2.warpAffine(img,mtrx, (cols+dx, rows+dy),None, cv2_INTER_LENEAR, cv2_BORDER_CONSTANT, (255,0,0))dst3 = cv2.warpAffine(img,mtrx, (cols+dx, rows+dy),None, cv2_INTER_LENEAR, cv2_BORDER_RELECT)cv2.imshow(&quot;src&quot;,img)cv2.imshow(&quot;dst&quot;,dst)cv2.imshow(&quot;constant&quot;,dst2)cv2.imshow(&quot;reflect&quot;,dst3)cv2.waitKey(0)cv2.destroyAllWindows() 실행 $ python translation.py 2.확대축소일정 비율로 확대 및 축소기존 좌표에 특정 값을 곱하면 된다. x_new = a1 * x_old = a1 * x_old + 0 * y_old + 0 * 1 y_new = 0 * x_old = a1 * x_old + a2 * y_old + 0 * 1이를 변환 행렬로 표현하면 다음과 같다. scaling.pyimport cv2import numpy as npimg = cv2.imread(&#39;girl.png&#39;)0rows, cols = img.shape[0:2]mtrx_small = np.float32([[0.5,0,0],[0,0.5,0]])mtrx_big = np.float32([[2,0,0],[0,2,0]])dst = cv2.warpAffine(img,mtrx, m_small, (int(height*0.5), int(width*0.5)))dst2 = cv2.warpAffine(img,mtrx, m_small, (int(height*0.5), int(width*0.5)), None, cv2_INTER_AREA)dst3 = cv2.warpAffine(img,mtrx, m_big, (int(height*2), int(width*2)))dst4 = cv2.warpAffine(img,mtrx, m_big, (int(height*2), int(width*2)),None, cv2_INTER_CUBIC)cv2.imshow(&quot;src&quot;,img)cv2.imshow(&quot;small&quot;,dst)cv2.imshow(&quot;small_inter_area&quot;,dst2)cv2.imshow(&quot;big&quot;,dst3)cv2.imshow(&quot;big_inter_cubic&quot;,dst4)cv2.waitKey(0)cv2.destroyAllWindows()이미지를 축소할 때는 INTER_AREA를 권장하고, 확대할 때는 INTER_LINEAR 또는 CUBIC을 추천한다.affine 이외에 다른 함수를 이용해서도 크기를 조정할 수 있다.cv2.resize(src,dsize,dst,fx,fy,interpolation) src : 입력 원본 이미지 dsize : 출력 영상 크기, 지정하지 않으면 fx,fy 배율을 적용 fx,fy : 크기 배율, dsize를 입력하면 dsize를 우선 적용함 interpolation : 보간법 알고리즘 dst : 결과 이미지 resizing.pyimport cv2import numpy as npimg = cv2.imread(&#39;girl.png&#39;)height, width = img.shape[0:2]dst1 = cv2.resize(img, (int(width*0.5), int(height*0.5)), interpolation=cv2.INTER_AREA)dst2 = cv2.resize(img, None, None, 0.5,1.5, interpolation=cv2.INTER_CUBIC)cv2.imshow(&quot;src&quot;,img)cv2.imshow(&quot;small&quot;,dst)cv2.imshow(&quot;small_inter_area&quot;,dst2)cv2.waitKey(0)cv2.destroyAllWindows() 실행 $ python resizing.py 3.회전이미지 회전을 위한 변환 행렬식은 다음과 같다.이 2x2 행렬은 2x3을 요구하는 affine행렬에서는 사용할 수 없고, 사용히려면 2x3행렬로 변환해야 한다. 변환해주는 cv2.getRotationMatrix2D 함수가 존재한다.회전 행렬을 직접 구현한 코드이다. rotation1.pyimport cv2import numpy as npimg = cv2.imread(&#39;girl.png&#39;)rows, cols = img.shape[0:2]d45 = 45.0 * np.pi / 180d90 = 90.0 * np.pi / 180mtrx1 = np.float32([[np.cos(d45), -1*np.sin(d45),cols//2],[np.sin(d45),np.cos(d45),-1*rows//4]]) mtrx2 = np.float32([[np.cos(d90), -1*np.sin(d90),cols],[np.sin(d90),np.cos(d90),0]])dst1 = cv2.warpAffine(img,mtrx1,(cols,rows))dst2 = cv2.warpAffine(img,mtrx2,(cols,rows))cv2.imshow(&quot;src&quot;,img)cv2.imshow(&quot;45&quot;,dst)cv2.imshow(&quot;90&quot;,dst2)cv2.waitKey(0)cv2.destroyAllWindows()컴퓨터는 반시계를 +로 잡기 때문에 시계방향으로 돌리기 위해 -1을 곱했고, 원점을 화면의 중간으로 잡기 위해 마지막 인자를 주었다.회전 행렬을 함수를 사용하여 구현한 코드이다.mtrx = cv2.getRotationMatrix2D(center, angle, scale) Center : 회전축 중심 좌표 angle : 회전할 각도, 60진법 scale : 확대 축소 비율 rotation2.pyimport cv2import numpy as npimg = cv2.imread(&#39;girl.png&#39;)rows, cols = img.shape[0:2]d45 = 45.0 * np.pi / 180d90 = 90.0 * np.pi / 180mtrx1 = cv2.getRotationMatrix2D((cols/2,rows/2), 45, 0.5)mtrx2 = cv2.getRotationMatrix2D((cols/2,rows/2), 90, 1.5)dst1 = cv2.warpAffine(img,mtrx1,(cols,rows))dst2 = cv2.warpAffine(img,mtrx2,(cols,rows))cv2.imshow(&quot;src&quot;,img)cv2.imshow(&quot;45&quot;,dst)cv2.imshow(&quot;90&quot;,dst2)cv2.waitKey(0)cv2.destroyAllWindows()Affine 변환Affine 변환 행렬은 2x3 행렬로 cv2.getAffineTranform 함수를 통해서 얻을 수 있다. affine.pyimport cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&#39;chess.png&#39;)rows, cols = img.shape[0:2]pts1 = np.float32([[50,50],[200,50],[50,200]])pts2 = np.float32([[10,100],[200,50],[100,250]])mtrx = cv2.getAffineTransform(pts1,pts2)dst = cv2.warpAffine(img,mtrx,(cols,rows))plt.subplot(121),plt.imshow(img), plt.title(&#39;input&#39;))plt.subplot(122),plt.imshow(dst), plt.title(&#39;output&#39;))plt.show()Perspective 변환원근법을 적용한 변환으로 직선의 성질만 유지가 되고 선의 평행성은 유지되지 않는 변환이다. 즉 원근법이 되어 있는 이미지를 평평하게 펴준다. 반대의 변환도 가능하다.cv2.getPerspectiveTransform 함수를 통해서 얻을 수 있다. 이동할 4개의 점의 좌표가 필요하다. 결과값은 3x3 행렬이다. cv2.warpPerpective()함수에 변환 행렬값을 적용하면 이미지가 변환된다. 점의 순서는 좌상단부터 시작하여 반시계방향이다. perspective.pyimport cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&#39;chess.png&#39;)rows, cols = img.shape[0:2]pts1 = np.float32([[20,20],[20,280],[380,20],[380,280]])pts2 = np.float32([[100,20],[20,280],[300,20],[380,280]])cv2.circle(img,(20,20),20, (255,0,0),-1)cv2.circle(img,(20,280),20, (0,255,0),-1)cv2.circle(img,(380,20),20, (0,0,255),-1)cv2.circle(img,(380,380),20, (255,255,255),-1)mtrx = cv2.getPerspectiveTransform(pts1, pts2)dst = cv2.warpPerspective(img, mtrx, (400,300))plt.subplot(121),plt.imshow(img), plt.title(&#39;input&#39;))plt.subplot(122),plt.imshow(dst), plt.title(&#39;output&#39;))plt.show()이 변환을 차선 검출에 적용한다면 perspective 변환을 적용할 것이다. 이렇게 변환한 영상을 bird eye view라고 한다. 이렇게 변환한다면 차선을 찾기가 편하게 된다.원근 변환을 이용한 차선 이미지 영상처리perspective 변환을 통해 차선을 찾는다.원근 변환과 슬라이딩 윈도우단계 camera calibration bird`s eye view 이미지 임계값 및 이진화 슬라이딩 윈도우로 차선 위치 파악 파악된 차선 위치 원본이미지에 표시1. Camera Calibration(카메라 보정)카메라는 곡면 렌즈를 사용해서 이미지를 형성하기 때문에 왜곡되어 보인다. 왜곡됨으로 인해 물체의 크기, 모양이 변경되기도 하고, 시야의 위치에 따라 모양이 변경되기도 한다. 따라서 이 왜곡을 변화시켜야 한다.또한, 렌즈-이미지 센서와의 거리, 렌즈와 이미지 센서가 이루는 각도 등에 의해서도 왜곡이 발생할 수 있다. 실제적으로 보이게 보정하는 것을 camera calibration이라 한다.체스보드는 정사각형으로 이루어져 있으므로 컴퓨터가 인식하기 쉽다. 그래서 체스판을 사용해서 왜곡을 보정할 수 있다.1-1. 자동으로 체스판을 찾아서 패턴을 매핑 cv2.findchessboardCorners(), cv2.drawchessboardcorners() 함수 사용 cv2.findchessboardCorners() : 체스 판의 코너들을 찾는다. cv2.drawchessboardcorners() : 찾은 체스 판의 코너들을 그린다. 1-2. 교정 및 undistortion 계산 cv2.calibrateCamera(), cv2.undistort() 함수 사용 cv2.calibrateCamera() : camera matrix, 왜곡 계수, 회전/변환 벡터들을 리턴 cv2.undistort() : 이미지를 펴서 왜곡을 없어지게 한다. 체스판을 통해 얻어진 calibrate를 통해 차선이미지를 calibration을 진행할 수 있다.int main(){ vector&amp;lt;String&amp;gt; img_paths; glob(&quot;./data/*.bmp&quot;, img_paths); for (const auto&amp;amp; path : img_paths) { Mat img = imread(path); if (img.empty()) { cout &amp;lt;&amp;lt; &quot;Image load failed: &quot; &amp;lt;&amp;lt; path &amp;lt;&amp;lt; endl; continue; } img_size = img.size(); Mat gray; cvtColor(img, gray, COLOR_BGR2GRAY); vector&amp;lt;Point2f&amp;gt; corners; bool found = findChessboardCorners(gray, pattern_size, corners); if (found) { TermCriteria criteria(TermCriteria::Type::EPS | TermCriteria::Type::MAX_ITER, 30, 0.001); cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1), criteria); obj_pts.push_back(objp); img_pts.push_back(corners); drawChessboardCorners(img, pattern_size, corners, found); imshow(&quot;img&quot;, img); waitKey(); } } cv::destroyAllWindows(); // Calculate intrinsic_matrix, distortion_coeffs Mat intrinsic_matrix, distortion_coeffs; calibrateCamera(obj_pts, img_pts, img_size, intrinsic_matrix, distortion_coeffs, // 왜곡 계수 noArray(), noArray(), CALIB_ZERO_TANGENT_DIST | CALIB_FIX_PRINCIPAL_POINT); // save them in a XML file FileStorage fs(&quot;intrinsics.xml&quot;, FileStorage::WRITE);}저장된 정보는 불러올 수 있다.int main(){ // Load intrinsic_matrix, distortion_coeffs from a XML file FileStorage fs(&quot;intrinsics.xml&quot;, FileStorage::READ); Mat intrinsic_matrix, distortion_coeffs; fs[&quot;camera_matrix&quot;] &amp;gt;&amp;gt; intrinsic_matrix; fs[&quot;distortion_coefficients&quot;] &amp;gt;&amp;gt; distortion_coeffs; Mat map1, map2; initUndistortRectifyMap(intrinsic_matrix, distortion_coeffs, noArray(), intrinsic_matrix, img_size, CV_16SC2, map1, map2); Mat frame; while (true) { cap &amp;gt;&amp;gt; frame; if (frame.empty()) break; Mat dst; remap(frame, dst, map1, map2, INTER_LINEAR); imshow(&quot;frame&quot;, frame); imshow(&quot;dst&quot;, dst); if (waitKey(10) == 27) break; }}2. Bird’s eye View왜곡이 없는 차선 이미지를 bird’s eye view 구도로 변환한다. 원근 변환을 하는 것이므로 cv2.getPerspectiveTransform(src,dst)를 사용하여 원근 변환 행렬을 얻는다.원래 이미지 -\\&amp;gt; 조감도 -\\&amp;gt; 원래 이미지 의 방식으로 두 번 변환해야 한다.원근 변환을 위한 4개의 점을 어떻게 얻을까?위에서 도로를 내려다 볼때 직사각형을 나타내야 하므로 사다리꼴 모양의 4개 점을 찾아야 한다. 색상이나 속성 등을 분석해서 선택한다.3. 이미지 이진화임계값을 주어 이진화를 진행한다. 그를 위해 이미지를 grayscale로 변환한 후 이진화한다. 색상 표현 HSV H: 색조, S: 채도, V: 명도 명도가 낮을 수록 검은색, 명도가 낮고 채도가 낮을 수록 흰색 LAB 사람 눈이 감지할 수 있는 색차와 색공간에서 수치로 표현한 색차를 거의 일치시킬 수 있는 색공간 L: 명도, A: Red and Green, B: Yellow and Blue 노란색 차선을 인식할 때 B를 사용하면 좋은 성능을 낼 수 있다고 한다. HLS 색상 균형, HSV의 V를 L로 바꾼 것 H: 색조, L: 밝기, S: 채도 밝기가 낮을 수록 검은색, 밝기가 높을 수록 흰색 흰색 차선을 인식할 때 L을 사용하면 좋은 성능을 낸다고 한다. 4. 슬라이딩 윈도우차선을 찾는 방법 히스토그램도로 이미지에 보정, 임계값 및 원근 변화을 적용하여 차선이 두드러지는 이진 이미지를 얻은 후 어떤 픽셀이 라인의 일부이고 이게 왼쪽인지 오른쪽인지를 결정해야 한다. 이를 위해 히스토그램을 사용한다. 각 열에 따라 픽셀 개수를 더하면 히스토그램에서 가장 눈에 띄는 두 개의 peak, 즉 높은 막대기 2개가 생성되고, 이 2개의 위치가 차선의 x위치라 할 수 있다. 슬라이딩 윈도우선 중심 주변에 배치된 슬라이딩 윈도우를 사용해서 프레임 상단까지 선을 찾아 따라가게 하여 곡선을 찾는다. 방금 찾은 히스토그램의 차선 x위치의 시작점, 즉 제일 하단점을 이용하여 첫 윈도우의 위치를 정한다. 그 위도우안의 점들의 평균점을 기준으로 바로 위에 쌓는다. 그렇게 최상단까지 쌓다보면 곡선을 찾을 수 있다.파악을 한 후 윈도우의 중심을 통해 선을 그리는데, polyfit 함수를 사용해서 2차원을 찾아준다. 이 선은 원본 이미지에 차선을 그리기 위한 선이다. 이 2차원 함수 2개사이의 영역을 polygon 함수를 통해 다각형을 원본 이미지에 그린다. 실행 결과차선 인식 구현 summary 1. Image read 2. warping (원근 변환) 3. gaussian blur (노이즈 제거) 4. threshold (이진 이미지 변환) 5. histogram (차선 위치 추출) 6. sliding window (윈도우 생성) 7. polyfit (2차 함수 그래프로 차선 그리기) 8. 차선 영역 표시 (원본 이미지에 영역 오버레이) 9. 핸들 조향각 결정 10. 핸들 조종1. 작업 환경 설정필요한 파일 sliding_drive 패키지에서 작업 sliding_find.py car_track1.avi,road_video1.mp4,road_video2.mp4 sliding_find.py 영상 프레임 추출 카메라 calibration 설정값을 통해 이미지 보정 원근 변환을 통해 bird’s eye view로 변환 노이즈 제거, HLS포맷으로 변경, 이진화 처리 히스토그램을 통해 좌우 차선 시작 위치 파악 슬라이딩 윈도우를 좌우 9개씩 쌓아 올리기 9개 윈도우 안의 중앙점을 모두 지나는 2차함수 찾기 원본 이미지에 차선 영역을 표시하는 영역 오버레이#!/usr/bin/env python# -*- coding: utf-8 -*-import numpy as npimport cv2, random, math, copywidth = 640height = 480cap = cv2.VideoCapture(&quot;xycar_track1.mp4&quot;)window_title = &#39;camera&#39;# 1.warp_img_w = 320 # w * 1/2warp_img_h = 240 # h * 1/2warpx_margin = 20warpy_margin = 3# 2.nwindows = 9# 3.margin = 12# 4.minpixel = 5lane_bin_th = 145# 5.warp_src = np.array([ [230-warpx_margin, 300-warpy_margin], [45-warpx_margin, 450+warpy_margin], [445+warpx_margin, 300-warpy_margin], [610+warpx_margin, 450+warpy_margin]], dtype=np.float32)# 6.warp_dst = np.array([ [0,0], [0,warp_img_h], [warp_img_w,0], [warp_img_w, warp_img_h]], dtype=np.float32)# 7.calibrated = Trueif calibrated: mtx = np.array([ [422.037858, 0.0, 245.895397], [0.0, 435.589734, 163.625535], [0.0, 0.0, 1.0] ]) dst = np.array([-0.289296, 0.061035, 0.001786, 0.015238, 0.0]) cal_mtx, cal_roi = cv2.getOptimalNewCameraMatrix(mtx, dst, (width, height), 1, (width, height))# 8.def calibrate_image(frame): global width, height global mtx, dst global cal_mtx, cal_roi tf_image = cv2.undistort(frame, mtx, dst, None, cal_mtx) x, y, w, h = cal_roi tf_image = tf_image[y:y+h, x:x+w] return cv2.resize(tf_image, (width, height))# 9.def warp_image(img, src, dst, size): m_to_dst = cv2.getPerspectiveTransform(src, dst) m_to_src = cv2.getPerspectiveTransform(dst, src) warp_img = cv2.warpPerspective(img, m_to_dst, size, flags=cv2.INTER_LINEAR) return warp_img, m_to_dst, m_to_src# 10.def warp_process_image(img): global nwindows global margin global minpixel global lane_bin_th # 11. blur = cv2.GaussianBlur(img,(5, 5), 0) # take the value, only B(yellow and blue) to detect yellow line easily # but, in video, there is no yellow line, so in this code, do not take B # _, _, B = cv2.split(cv2.cvtColor(blur, cv2.COLOR_BGR2LAB)) # 12. _, L, _ = cv2.split(cv2.cvtColor(blur, cv2.COLOR_BGR2HLS)) # 13. _, lane = cv2.threshold(L, lane_bin_th, 255, cv2.THRESH_BINARY) # 14. histogram = np.sum(lane[lane.shape[0]//2:,:], axis=0) # 15. midpoint = np.int(histogram.shape[0]/2) # 16. leftx_current = np.argmax(histogram[:midpoint]) # 17. rightx_current = np.argmax(histogram[midpoint:]) + midpoint # 18. window_height = np.int(lane.shape[0]/nwindows) nz = lane.nonzero() left_lane_inds = [] right_lane_inds = [] lx, ly, rx, ry = [], [], [], [] # 19. out_img = np.dstack((lane, lane, lane))*255 for window in range(nwindows): # 20. win_yl = lane.shape[0] - (window+1)*window_height win_yh = lane.shape[0] - window*window_height # 21. win_xll = leftx_current - margin win_xlh = leftx_current + margin win_xrl = rightx_current - margin win_xrh = rightx_current + margin # 22. cv2.rectangle(out_img,(win_xll,win_yl),(win_xlh,win_yh),(0,255,0), 2) cv2.rectangle(out_img,(win_xrl,win_yl),(win_xrh,win_yh),(0,255,0), 2) # 23. good_left_inds = ((nz[0] &amp;gt;= win_yl)&amp;amp;(nz[0] &amp;lt; win_yh)&amp;amp;(nz[1] &amp;gt;= win_xll)&amp;amp;(nz[1] &amp;lt; win_xlh)).nonzero()[0] good_right_inds = ((nz[0] &amp;gt;= win_yl)&amp;amp;(nz[0] &amp;lt; win_yh)&amp;amp;(nz[1] &amp;gt;= win_xrl)&amp;amp;(nz[1] &amp;lt; win_xrh)).nonzero()[0] left_lane_inds.append(good_left_inds) right_lane_inds.append(good_right_inds) # 24. if len(good_left_inds) &amp;gt; minpixel: leftx_current = np.int(np.mean(nz[1][good_left_inds])) if len(good_right_inds) &amp;gt; minpixel: rightx_current = np.int(np.mean(nz[1][good_right_inds])) # 25. lx.append(leftx_current) ly.append((win_yl + win_yh)/2) rx.append(rightx_current) ry.append((win_yl + win_yh)/2) # 26. left_lane_inds = np.concatenate(left_lane_inds) right_lane_inds = np.concatenate(right_lane_inds) #left_fit = np.polyfit(nz[0][left_lane_inds], nz[1][left_lane_inds], 2) #right_fit = np.polyfit(nz[0][right_lane_inds] , nz[1][right_lane_inds], 2) # 27. lfit = np.polyfit(np.array(ly),np.array(lx),2) rfit = np.polyfit(np.array(ry),np.array(rx),2) # 28. out_img[nz[0][left_lane_inds], nz[1][left_lane_inds]] = [255, 0, 0] out_img[nz[0][right_lane_inds] , nz[1][right_lane_inds]] = [0, 0, 255] cv2.imshow(&quot;viewer&quot;, out_img) #return left_fit, right_fit return lfit, rfitdef draw_lane(image, warp_img, m_to_src, left_fit, right_fit): global width, height ymax = warp_img.shape[0] ploty = np.linspace(0, ymax - 1, ymax) # 29. color_warp = np.zeros_like(warp_img).astype(np.uint8) # 30. left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2] right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2] # 31. pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))]) pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))]) pts = np.hstack((pts_left, pts_right)) # 31. color_warp = cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0)) newwarp = cv2.warpPerspective(color_warp, m_to_src, (width, height)) return cv2.addWeighted(image, 1, newwarp, 0.3, 0)def start(): global width, height, cap _, frame = cap.read() while not frame.size == (width*height*3): _, frame = cap.read() continue print(&quot;start&quot;) while cap.isOpened(): _, frame = cap.read() image = calibrate_image(frame) warp_img, m_to_dst, m_to_src = warp_image(image, warp_src, warp_dst, (warp_img_w, warp_img_h)) left_fit, right_fit = warp_process_image(warp_img) lane_img = draw_lane(image, warp_img, m_to_src, left_fit, right_fit) cv2.imshow(window_title, lane_img) cv2.waitKey(1)if __name__ == &#39;__main__&#39;: start() 1.기존의 크기보다 1/2 크기의 출력 영상에 대한 크기2.슬라이딩 윈도우 개수3.슬라이딩 윈도우 넓이4.선을 그리기 위해 최소한 있어야할 점의 개수, lane_binary_threshold(선을 판단하기 위한 이진화의 임계값)5.변환 전 좌표 행렬6.변환 후 좌표 행렬7.자이카 카메라로 촬영된 동영상이므로 자이카 카메라의 calibration보정값을 사용한다.8.위에서 구한 보정 행렬값을 적용하여 이미지를 반듯하게 수정하는 함수9.변환 전과 후의 4개 점 좌표를 전달받아서 이미지를 원근 변환 처리하여 새로운 이미지를 만든다.10.이미지를 처리하는 함수11.5x5 kernel filter을 적용한 가우시안 블러링12.흰색선을 잘 찾기 위해 HLS로 변환한 후 L값만 추출, 영상에서 노란색선이 없어서 LAB으로 변환은 하지 않았으나 있다면 LAB로 변환하여 B값만 추출한다.13.임계값을 이용하여 이진화한다. lane은 이진화된 조감도 이미지의 행렬(320,240)일 것이다.14.히스토그램의 x,y축을 정의한다. x축 : 픽셀의 x좌표값, y축 : 특정 x좌표값을 갖는 모든 흰색 픽셀의 개수15.x축을 반으로 나누어 왼쪽 차선과 오른쪽 차선을 구분한다.16.왼쪽 절반 구역에서 흰색 픽셀의 개수가 가장 많은 위치를 슬라이딩 윈도우 중심 좌표로 두어 왼쪽 윈도우의 시작 위치로 잡는다. 시작 위치에서만 이 값을 사용하고 나머지는 아래의 평균값을 사용한다.17.픽셀의 개수가 가장 많은 위치를 오른쪽 윈도우의 시작 위치를 잡는다.18.1개의 윈도우의 높이를 구함19.20.각 윈도우의 상단, 하단 y좌표21.윈도우 넓이를 구하는데, win_xlh - win_xll = 윈도우 가로 값, xrh-xrl = 윈도우 세로 값22.윈도우마다 사각형 그리기, (dst, 좌상단 좌표,우하단 좌표,색상,두께)23.슬라이딩 윈도우 박스 하나 안에 있는 흰색 픽셀의 x좌표들을 모두 모든다. 왼쪽 오른쪽은 따로 계산한다. zero가 아닌 것들에 대한 점들을 다 구하고 x좌표만 추출한다.24.위에서 구한 x좌표 리스트에서 흰색점이 5개 이상인 경우에 한해서 x좌표의 평균값을 구한다. 이 값을 위에 쌓을 슬라이딩 윈도우의 중심점으로 사용한다.25.슬라이딩 윈도우의 중심점을 저장한다. 이는 2차함수 그릴 때 사용한다.26.27.슬라이딩 윈도우의 중심 좌표 9개를 가지고 2차함수를 만들어낸다.28.기존 하얀색 차선 픽셀을 왼쪽 오른쪽 각각 파란색과 빨간색으로 색상 변경29.zeros matrix, warp_img 크기의 0으로 된 행렬을 생성한다.30.31.구해놓은 2차함수의 상수들을 이용해서 사다리꼴 이미지 외곽선 픽셀 좌표를 계산한다.32.사다리꼴 이미지를 칼라로 그리고 거꾸로 원근 변환하여 원본 이미지와 오버레이 한다. &amp;gt;ROS에서 슬라이딩 윈도우 기반 차선 인식 주행 sliding_drive.launch&amp;lt;launch&amp;gt; &amp;lt;!-- 노드 실행 : 자이카 모터 제어기 구동 --&amp;gt; &amp;lt;include file=&quot;$(find xycar_motor)/launch/xycar_motor.launch&quot; /&amp;gt; &amp;lt;!-- 노드 실행 : 자이카 카메라 구동 --&amp;gt; &amp;lt;node name=&quot;usb_cam&quot; output=&quot;screen&quot; pkg=&quot;usb_cam&quot; type=&quot;usb_cam_node&quot;&amp;gt; &amp;lt;param name=&quot;video_device&quot; value=&quot;/dev/videoCAM&quot; /&amp;gt; &amp;lt;param name=&quot;autoexposure&quot; value=&quot;false&quot; /&amp;gt; &amp;lt;param name=&quot;exposure&quot; value=&quot;50&quot; /&amp;gt; &amp;lt;param name=&quot;image_width&quot; value=&quot;640&quot; /&amp;gt; &amp;lt;param name=&quot;image_height&quot; value=&quot;480&quot; /&amp;gt; &amp;lt;param name=&quot;pixel_format&quot; value=&quot;yuyv&quot; /&amp;gt; &amp;lt;param name=&quot;camera_frame_id&quot; value=&quot;usb_cam&quot; /&amp;gt; &amp;lt;param name=&quot;io_method&quot; value=&quot;mmap&quot; /&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;!-- 노드 실행 : 슬라이딩 윈도우 기반 주행 프로그램인 sliding_drive.py 실행 --&amp;gt; &amp;lt;node name=&quot;auto_drive&quot; output=&quot;screen&quot; pkg=&quot;sliding_drive&quot; type=&quot;sliding_drive.py&quot; /&amp;gt;&amp;lt;/launch&amp;gt; sliding_drive.py작업 흐름도 카메라 노드가 보내는 토픽에서 영상 프레임 추출 카메라 calibration 설정값으로 이미지 보정 원근 변환으로 차선 이미지를 Bird’s eye view로 변환 openCV영상처리 Gaussian blur (노이즈 제거) cvtColor (BGR2HLS) threshold (이진화 처리) 히스토그램을 사용해서 좌우 차선의 시작 위치 파악 슬라이딩 윈도우를 좌우 9개씩 쌓아 올리기 왼쪽 오른쪽 차선의 위치 찾기 적절한 조향값 계산하고, 모터 제어 토픽 발행차선에 대한 2차함수가 쌓여 있고, 윈도우가 총 9개 있을 때, 내가 보고자 하는 윈도우의 번호에 있는 x좌표를 구해서 두개의 평균을 구한다. 그리고 그것과 이미지의 중앙값을 비교하여 조향각을 결정한다./usb_cam -\\&amp;gt; /usb_cam/image_raw/ -\\&amp;gt; /sliding_drive -\\&amp;gt; /xycar_motor -\\&amp;gt; /xycar_motor" }, { "title": "[데브코스] 7주차 - GitHub로 협업하기 ", "url": "/posts/git/", "categories": "Classlog, devcourse", "tags": "devcourse, git", "date": "2022-03-28 15:01:00 +0900", "snippet": "github로 협업하기 요약 git은 분산 버전 관리 시스템이다 github는 코드 호스팅 플랫폼이다. commit은 의미 있는 변화다. branch는 목적에 따른 분기다. centralized vs distributed저장소가 분리되어 있다. 내 컴퓨터에도 저장소가 있고, 깃허브에도 저장소가 있으며 다른 컴퓨터에도 저장소가 되어 있는 것을 분산이라 한다.# apt update# apt install git# git --version# mkdir catkin_ws# cd catkin_ws# git init# git status // 추가가 잘 되었는지 확인# git clone https://github.com/prgrms-ad-devcourse/ad-3-practice-assignment.git# touch first# git add first // 모든 파일을 하고 싶은 경우 git add .# git commit -m “자율이 3기 화이팅” // &quot;메세지 내용&quot; # git push# git push origin master협업을 진행하기 전 고려해야 할 사항에는 어떤 것이 있을까? 코딩 스타일 = 코딩 컨벤션사람마다의 네임 지정과 스타일이 다르다. 회사에 취직하더라도 회사만의 스타일(컨벤션)에 맞춰야 한다. 탭과 스페이스키는 둘 다 상관없지만, 함께 사용하면 오류가 난다. 일을 분배하는 기준내가 잘하는 부분과 다른 사람이 잘하는 부분이 다르기 때문에 이에 대해 잘 분배를 해야 한다. 시간의 제약도 존재한다. 그래서 어떻게 일을 분배할지에 대해서도 고려해야 한다. 의사 소통 규칙특정 시간에 서로의 작업 사항을 공유하는 것도 의사소통이고, 특정 문제를 어떻게 해결할지에 대한 아이디어를 도출하는 회의를 만들어놔야 한다. 누군가의 아이디어를 반박할 때는 그 아이디어보다 더 좋은 아이디어를 제시하면서 반박해야 한다는 규칙을 만드는 것도 좋다. Workflow Repository 운영 branching 전략 issue, PR 관리 coding styleimport rospyfrom geometry_msgs.msg import Twishfrom std_msgs.msg import Float32cmd_vel_pub = rospy.Publisher(&quot;cmd_vel_mux/input/&quot;,Twist,queue_size=10,)import rospyfrom geometry_msgs.msg import Twishfrom std_msgs.msg import Float32cmdvelpub = rospy.Publisher(&quot;cmd_vel_mux/input/&quot;,Twist,queue_size=10)여기서 다른 것은 ,이다. 깃에서 버전 관리를 할 때 ,가 있으면 1줄만 추가하도록 되지만, 아래처럼 ,가 없으면 2줄이 추가된 것으로 인식된다. 또한, 위의 코드에서 맨 아래 1줄 띄우기를 진행하는 것도 다르며, __(underscore)을 추가하는 것은 파이썬 공식 스타일 가이드에 적혀있는 것처럼 추가하는 것이 좋다.코딩 스타일을 정해야 충돌이 발생하지 않는다.formatter이라는 코드를 정렬해주는 도구가 있다. git에 커밋하기 전에 black을 한 번 돌린 후 commit하면 충돌할 일이 없다. black yapf isort(import 특화)변수명을 쓸 때 snakeclass를 사용하거나 변수명에 대한 코멘트에 대한 추천을 해주는 장치인 linter가 있다.black{ &quot;[python]&quot;: { &quot;editor_tapsize&quot;=4, ... }}Workflowcentralized(중앙 집중식) vs forking(distributed)(분산식)contralized는 관리 포인트가 하나여서 push하면 바로 저장소에 저장이 된다. 내가 변경하여 push하면 다른 사람들에게 바로 변경사항이 추가될 수 있어서 여러 시도를 하기 힘들다. forking은 원본 저장소가 따로 있고, 자신이 commit한 내용을 PR하므로 자유롭게 시도를 적용해볼 수 있다. 그 대신 관리 포인트가 늘어난다. git flow어떤 버전을 합칠지, 어떤 버전은 버릴지를 관리하는 방법이다. 여러 개의 브랜치로 인해 매우 복잡하다. github flow그에 반해 이 것은 브랜치가 적어서 main에서 추출해서 추가할 브랜치를 추가해서 합친다.브랜치를 생성할 때 자신이 fix한 내용을 추가해서 feature/fix 등과 같이 작성하는 것이 좋다.Issueissue는 문제, 할 일, 관심사 등으로 표현할 수 있다. 코딩 스타일을 어떻게 맞출 것인지에 대한 것도 이슈라고 할 수 있다. issue labels이슈를 활용하여 assignee 탭을 통해 누가 맡아서 처리할 것인지를 설정할 수도 있다.Pull Request코드를 통합하는 방법 중 하나다. merge를 하면 그냥 혼자 통합하는 것이므로 내 코드가 틀린지 맞는지에 대해 상관하지 않고 합치는 것이다. 그러나 pull request를 하게 되면 나의 코드 검토를 요청할 수 있고, 어떤 기능에 대한 것인지, 어떤 이슈에 대한 것인지 팀원들에게 알려줄 수 있다.conflict(충돌)충돌인 채로 merge를 하면 문제가 많아진다. 내가 수정한 파일을 commit한 내용과 팀원이 수정한 파일을 commit한 내용이 다르면 main에서 merge를 할 때 충돌이 일어난다. 여기서 충돌이 일어나면 누가 코드를 commit한 것인지 나오기 때문에 서로 협의를 본 후에 수정해야 한다.conflict를 해결하는 방법에는 파일을 분리한다. 내가 수정하는 파일과 팀원이 수정하는 파일이 다르다면 conflict가 생길 확률이 줄어든다. 또는 branch를 분리해도 생길 확률을 줄일 수 있다.conflict가 발생했을 때 파일의 상단에 보면 둘 다 추가할 것인지, 하나만 추가할 것인지 클릭하는 부분이 있다. 그래서 변경 사항을 수정한 후 스테이징상태로 둔 뒤에 merge를 사용하면 된다.merge 중에 fast-forward 전략이라고 있다. main브랜치보다 다른 브랜치가 더 앞에 있을 때 merge를 하게 되면 그 브랜치로 main이 이동하게 된다. 즉, 그 위치로 main을 이동시킴으로서 변경사항을 pull하는 것이다.여기서 굳이 그 브랜치를 만들어두고 싶다면, --no-ff라는 인자를 넣어줘야 한다.git merge --no-ff fast-forwardmain에서 git pull을 사용할 때 git config --global 이라는 메시지가 나온다면 ff.only 즉, fast-forward일 때만 pull이 가능하도록 설정해야 한다. 이렇게 설정하게 되면 main브랜치가 merge하려는 브랜치보다 뒤에 있어서 fast-worward가 될 때만 pull을 하겠다는 것이다.이 때 억지로 가져오고 싶다면 rebase방식을 사용한다. 이는 main 버전에서 가장 마지막에 오도록 위히시키고, 그 전에 commit된 내용들을 모두 검사하고 충돌이 일어나지 않으면 다 다운받아서 pull을 한다.pull이 충돌이 되었을 때는 서로 작업 사항이, 수정사항이 맞지 않아서 생기는 것이다. 합치는 방식에는 2가지가 있다. merge방식 rebase방식merge를 하면 main에서 수정했던 변경 사항과 함께 이전의 변경 사항들을 다 저장한 branch가 생기고, 그것을 merge, 합쳐지게 된다.$ git pull --no-ffrebase를 하면 main 변경 사항의 맨 마지막으로 위치시키는 것으로 이전의 모든 변경 사항들을 다운받아지면서 업데이트가 된다.$ git pull --rebaseGithub 추가 기능Templates.github/.github/안에 템플릿을 만들어두면 issue를 생성할 때 자동으로 추가되는 템플릿을 추가할 수 있다.Project &amp;amp; Milestoneproject를 생성함으로서 노션의 board와 같이 날짜별로 시각화할 수 있다. milestone을 설정하여 due date를 설정할 수 있다.Git Hookspre-commit이라고 해서 커밋하기 전에 수행해주는 기능들에 대한 설정을 할 수 있다. pre-commitGit actionspush했을 때 자동으로 돌아가도록 하는 세팅을 말한다." }, { "title": "[데브코스] 6주차 - Carla abstrat and install ", "url": "/posts/carla/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-26 14:01:00 +0900", "snippet": "carla 설치 terminator 설치여러 터미널을 사용하기 용이한 terminator을 설치한다.단축키 ctrl + w 창닫기 ctrl + o 아래 방향에 새로운 창 ctrl + e 왼쪽으로 새로운 창 alt + 방향키 carla 설치sudo add-apt-repository &quot;deb [arch=amd64] http://dist.carla.org/carla $(lsb_release -sc) main&quot;sudo apt-get update # Update the Debian package indexsudo apt-get install carla-simulator # Install the latest CARLA version, or update the current installationcd /opt/carla-simulator # Open the folder where CARLA is installedapt-cache madison carla-simulatorsudo apt-get install carla-simulator=0.9.2 carla bridge 설치깃허브로 설치깃허브로도 설치가 가능하다.https://github.com/carla-simulator/carla/releases/tag/0.9.10/위의 설치를 진행하다가 오류가 난다면 여기 사이트를 들어가서 tar.gz파일을 설치한다. 그리고 원래 있던 패키지를 삭제한다.sudo apt remove carla-simulator자율주행 시뮬레이터자율주행 개발은 복잡한 일이다. 모든 과정들이 연결되어 있어서 중간의 단계를 실험해보기 위해서는 전의 단계를 완료해야 한다. 또한, 직접 실험을 하기 위해서는 테스트베드가 필요할 것이다. 또, 딥러닝으로 차량을 주행시키기 위해 테스트베드에서 테스트 후에 실차에 적용하는 방식을 사용할 것이다.차량용 시뮬레이션이란?실제 환경과 어느정도 비슷하게 만들어서 시뮬레이션을 진행하는 프로그램을 말한다.carla 유튜브carla 깃허브칼라의 장점 : 센서를 사용하는데 있어서 거의 다 가능하다. lidar부터 object 센서까지 제공된다. object센서를 통해 object의 위치를 탐지해준다. 맵 제작 및 환경 설정이 용이하다. 날씨 및 햇빛 조절이 가능하고 맵 호환성이 좋다. matlab을 통해 맵을 제작할 수 있다. LKS차선 유지에는 sliding window 혹은 houghline transformation을 이용해서 주행을 하게 된다. 이를 LKS(차선 유지 시스템)이라 한다. LKS는 핸들값을 조정하여 맞춰준다. 그래서 차량의 Steering 값을 주고 있지만 조금 더 현실적으로 steering이 아닌 steering wheel을 바꿔주는 시스템을 통해 사용자가 steering wheel값을 잘못 준 경우 이를 보조해주는 시스템으로 변경해볼 수 있다. 마리오카트 라이브carla를 이용하면 모든 센서가 있는 것 처럼 주행이 가능하다. carla topic과 xycar topic을 맞춰서 실제 환경에서 차량이 움직이지만 센서 값은 모두 혹은 일부 carla에서 받아와서 주행해볼 수 있다.carla 차선 인식 주행ros bridge를 이용하게 되면 ros topic을 던져줄 수 있다. 여기서 handwidth와 Hz가 안 찍힌다면 토픽이 잘못되어 있다는 것이다. 이를 통해 데이터를 처리하고 주행할 수 있다. houghline transformation을 통해 차선을 인식할 수 있다." }, { "title": "[데브코스] 6주차 - OpenCV Hough transform and using GPU ", "url": "/posts/opencvhoughandgpu/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-25 14:01:00 +0900", "snippet": "허프 변환(Hough transform) 직선 검출2차원 영상 좌표에서의 직선의 방정식을 파라미터 공간으로 변환하여 직선을 찾는 알고리즘이다.y = ax + b =\\&amp;gt; b = -xa + y(x,y) 평면에서 한 직선을 표현하는 a’,b’가 있고, 이 직선을 지나는 점을 (xi,yi), (xj,yj) 라 한다면 직선의 방정식은 다음과 같다. b = -xi*a + yi b = -xj*a + yj이 두 직선은 점 (a’,b’)를 지난다.축적 배열직선 성분과 관련된 원소 값을 1씩 증가시키는 배열직선의 방정식 y= ax+b를 사용할 때의 문제점은 y축과 평행한 수직선은 표현하지 못한다. 그래서 극좌표계 직선의 방정식을 사용한다.x*cosϴ + y*sinϴ = ρ극 좌표를 사용할 경우 원래는 xy평면에서의 점은 ab평면에서는 직선, xy평면에서의 직선은 ab평면에서는 점으로 구성되지만, 극좌표계를 사용하면 ϴρ평면에서 곡선으로 나오게 된다.직선을 찾기 위해서는 ab평면이든 ϴρ평면이든 그리드 형태로 나눠야 한다. 이 때, 나누는 단위를 정해야 하는데, 예를 들어 ϴ를 0.1씩 나누어 그리드를 그린다면 직선을 찾는 성능은 좋아질 것이지만, 시간이 오래 걸릴 수 있다. 허프 변환 직선 검출 함수void HoughLines(InputArray image, OutputArray lines, double rho, double theta, int threshold, double srn = 0, double stn = 0, double min_theta = 0, double max_theta = CV_PI); image : 그레이스케일의 에지 영상 lines : 직선의 파라미터(rho,theta) 저장할 출력 벡터 vector&amp;lt;Vec2f&amp;gt; rho : 축적 배열에서 rho 값의 간격 e.g. 1.0 -&amp;gt; 1픽셀 간격 theta : 축적 배열에서 theta 값의 간격 e.g. CV_PI/180 -&amp;gt; 1도 간격 threshold : 축적 배열에서 직선으로 판단할 임계값 stn, srn : 멀티스케일 허프 변환에서 rho 해상도를 나누는 값, srn에 양의 실수를 지정하면, rho 해상도와 rho/srn 해상도를 각각 이용하여 멀티스케일 허프 변환을 수행한다. srn과 stn이 모두 0이면 일반 허프 변환을 수행 min_theta : 검출할 직선의 최소 theta 값 max_theta : 검출할 직선의 최대 theta 값 코드 구현Mat src = imread(&quot;building.jpg&quot;, IMREAD_GRAYSCALE);Mat src_edge;Canny(src, src_edge, 50, 150);vector&amp;lt;Vec2f&amp;gt; lines1;HoughLines(src_edge, lines1, 1, CV_PI / 180, 250); // 1픽셀, 1도 간격Mat dst1;cvtColor(src_edge, dst1, COLOR_GRAY2BGR); // 검출 직선의 색을 입히기 위해 변환// 반한된 직선의 방정식을 화면에 출력for (size_t i = 0; i &amp;lt; lines1.size(); i++) { float r = lines1[i][0], t = lines1[i][1]; double cos_t = cos(t), sin_t = sin(t); // cos, sin 은 float타입으로반환되기 때문에 정밀한 측정을 위해 double로 반환한 후 계산 double x0 = r*cos_t, y0 = r*sin_t; double alpha = 1000; Point pt1(cvRound(x0 + alpha*(-sin_t)), cvRound(y0 + alpha*cos_t)); Point pt2(cvRound(x0 - alpha*(-sin_t)), cvRound(y0 - alpha*cos_t)); line(dst1, pt1, pt2, Scalar(0, 0, 255), 2, LINE_AA);}houghlines를 사용하면 반환된 값을 화면에 출력하는 것이 조금 복잡하다. 그래서 이를 보완하여 나온 함수가 있다. 확률적 허프 변환에 의한 선분 검출 함수void HoughLinesP(InputArray image, OutputArray lines, double rho, double theta, int threshold, double minLineLength = 0, double maxLineGap = 0) image : 그레이스케일 에지 영상 lines : 선분의 시작, 끝 좌표(x1,y1,x2,y2)를 저장할 출력 벡터 vector&amp;lt;Vec4i&amp;gt; rho : 축적 배열에서 rho값의 해상도, 픽셀 단위 1.0 -&amp;gt; 1픽셀 간격 theta : 축적 배열에서 theta값의 간격, 라디안 단위 CV_PI/180 -&amp;gt; 1도 간격 threshold : 축적 배열에서 직선으로 판단할 임계값 minLineLength : 검출할 선분의 최소 길이, 특정 길이보다 짧은 것은 검출하지 않을 길이를 정하는 것 maxLineGap : 직선으로 간주할 최대 에지 점 간격, 직선이 끊어져서 있을 경우 어느 정도의 거리보다 작을 때는 항상 이어주라는 간격을 정하는 것P는 proportion, 확률에 대한 함수로, 이 함수를 사용하여 반환된 값은 선분, 즉 시작점과 끝점을 받는다. 코드 구현vector&amp;lt;Vec4i&amp;gt; lines2;HoughLinesP(src_edge, lines2, 1, CV_PI / 180, 160, 50, 5);for (size_t i = 0; i &amp;lt; lines2.size(); i++) { Vec4i l = lines2[i]; line(dst2, Point(l[0], l[1]), Point(l[2], l[3]), Scalar(0, 0, 255), 2, LINE_AA);}line들을 화면에 출력하는데 있어서 위의 houghlines 함수보다 훨씬 더 간단해진 것을 볼 수 있다. 들어 있는 것을 바로 그려주면 된다.나뭇잎의 경우 울퉁불퉁한 에지도 직선으로 판단되고 있는 것을 볼 수 있다.TickMeter tm;tm.start();vector&amp;lt;Vec2f&amp;gt; lines1;HoughLines(src_edge, lines1, 1, CV_PI / 180, 250);tm.stop();tm.reset();tm.start();vector&amp;lt;Vec4i&amp;gt; lines2;HoughLinesP(src_edge, lines2, 1, CV_PI / 180, 160, 50, 5);tm.stop();시간 차는 조금 나지만, for문을 돌려 식을 구성하고, 계산하는데도 시간이 걸리기 때문에 비슷하다.이를 차선 인식에 대입해서 사용해보면 다음과 같이 코드를 작성할 수 있다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;../data/lane03.bmp&quot;); imshow(&quot;src&quot;, src); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } TickMeter tm; tm.start();// Rect roi = selectROI(src); // Mat roi_crop = src(roi);// imshow(&quot;crop&quot;,roi_crop);// int roi_crop_mean = mean(roi_crop)[0]; vector&amp;lt;Point&amp;gt; pts; pts.push_back(Point(70, 350)); pts.push_back(Point(300, 200)); pts.push_back(Point(390, 200)); pts.push_back(Point(640, 350)); Mat mask = Mat::zeros(src.rows, src.cols, CV_8UC1);// mask = roi_crop_mean; fillPoly(mask, pts, Scalar(255), LINE_AA); Mat crop; copyTo(src, crop, mask); cvtColor(crop, crop, COLOR_BGR2GRAY); Mat crop_edge; GaussianBlur(crop, crop, Size(), 1);#if 0 crop = crop &amp;gt; 150;#else threshold(crop, crop, 170, 255, THRESH_BINARY);#endif Canny(crop, crop_edge, 50, 150); imshow(&quot;crop_edge&quot;,crop_edge); vector&amp;lt;Vec4i&amp;gt; lines; HoughLinesP(crop_edge, lines, 1, CV_PI / 180, 30, 20, 100);// for (auto line : lines) // cout &amp;lt;&amp;lt; line &amp;lt;&amp;lt; endl; tm.stop(); cout &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms.&quot; &amp;lt;&amp;lt; endl; for (Vec4i line : lines) { float dx = float(line[2] - line[0]); float dy = float(line[3] - line[1]); float angle = float(atan2f(dy, dx) * 180 / CV_PI); // x,y 변위를 통해 삼각함수를 사용하여 직선의 각도를 구함 if (fabs(angle) &amp;lt;= 10) continue; // 하늘과 나무 등 차선이 아닌 것들에 대한 직선도 많이 그려지기 때문에 이를 삭제하기 위한 코드 if (angle &amp;gt; 0) cv::line(src, Point(line[0], line[1]), Point(line[2], line[3]), Scalar(0, 0, 255), 1); // 각도가 +이면 빨강 else cv::line(src, Point(line[0], line[1]), Point(line[2], line[3]), Scalar(0, 255, 0), 1); // 각도가 -이면 초록 } imshow(&quot;src&quot;, src);// imshow(&quot;dst&quot;, dst); waitKey(); return 0;}이 차선을 동영상에서 잘 인식하기 위해서는 과거의 데이터, 즉 1~2초 전에 라인을 인식한 데이터를 가져와서 찾는 것이 잘 찾아진다. 왜냐하면 현재의 사진에 대해서만 검출한다면 점선인 차선이 너무 띄엄띄엄 있을 경우 잘 인식을 못하기도 하고, 노이즈가 많이 생기기도 한다.또는관심 영역을 설정해서 그 안에서만 검출하는 방법도 있다. 아니면 차선이 거의 흰색이므로 그런 색상 정보를 이용할 수 있다. 그림자의 경우에도 에지로 찾아질 수 있다. 그러므로 도로의 픽셀값보다 커지는(차선이 흰색) 에지에 대해서만 검출을 시키면 될 것이다.관심영역을 설정하는데는 두가지가 있다. selectROI 함수 사용 사각형만 가능하여 제한적이지만, 간단하고 쉽다. polylines 그린 후 mask 연산하여 관심 영역만 copyTo(src, mask) 1번보다 구현하기는 복잡하나 원하는 영역 모양으로 구성할 수 있다. 딥러닝을 사용하려면 이 방법을 사용해야 할 것이다. #include &amp;lt;iostream&amp;gt;#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;../data/lane03.bmp&quot;); imshow(&quot;src&quot;, src); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } TickMeter tm; tm.start();// Rect roi = selectROI(src); // Mat roi_crop = src(roi);// imshow(&quot;crop&quot;,roi_crop);// int roi_crop_mean = mean(roi_crop)[0]; /* lane03.bmp point */ vector&amp;lt;Point&amp;gt; pts; pts.push_back(Point(70, 350)); pts.push_back(Point(300, 200)); pts.push_back(Point(390, 200)); pts.push_back(Point(640, 350)); Mat mask = Mat::zeros(src.rows, src.cols, CV_8UC1);// mask = roi_crop_mean; fillPoly(mask, pts, Scalar(255), LINE_AA); Mat crop; copyTo(src, crop, mask); cvtColor(crop, crop, COLOR_BGR2GRAY); Mat crop_edge; GaussianBlur(crop, crop, Size(), 1);#if 0 crop = crop &amp;gt; 150; Canny(crop, crop_edge, 50, 150);#else threshold(crop, crop, 170, 255, THRESH_BINARY); Canny(crop, crop_edge, 50, 150);#endif vector&amp;lt;Vec4i&amp;gt; lines; HoughLinesP(crop_edge, lines, 1, CV_PI / 180, 30, 20, 100);// for (auto line : lines) // cout &amp;lt;&amp;lt; line &amp;lt;&amp;lt; endl; tm.stop(); cout &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms.&quot; &amp;lt;&amp;lt; endl; for (Vec4i line : lines) { float dx = float(line[2] - line[0]); float dy = float(line[3] - line[1]); float angle = float(atan2f(dy, dx) * 180 / CV_PI); if (fabs(angle) &amp;lt;= 10) continue; if (angle &amp;gt; 0) cv::line(src, Point(line[0], line[1]), Point(line[2], line[3]), Scalar(0, 0, 255), 1); else cv::line(src, Point(line[0], line[1]), Point(line[2], line[3]), Scalar(0, 255, 0), 1); } imshow(&quot;src&quot;, src);// imshow(&quot;dst&quot;, dst); waitKey(); return 0;}이 때, roi를 설정해서 직선을 그리게 되면, 배경과 mask사이에도 직선이 그려진다. mask의 배경의 픽셀값으 0이고, 관심 영역안의 픽셀과 차이가 나기 때문이다. 따라서 픽셀 값의 차에 대한 이진화를 추가해야 테두리에 대한 직선을 제거시킬 수 있다. thresholdthreshold를 사용한다는 것은 이진화를 하는 것을 얘기한다. 대체로 특정 값보다 큰 것을 true, 특정 값보다 작은 것을 false로 해서 배경과 물체를 나눈다. 영상마다 threshold를 다르게 적용해야 하는데, 이를 자동으로 조절이 되도록 하는 오츠 알고리즘라는 것이 있다.입력 영상의 히스토그램에 대해 threshold값을 0부터 255까지 증가시키면서 빨간 곡선이 어떤 형태로 되는지 확인한다.그 후 빨간 곡선이 최대가 되는 값에서 이진화가 가장 잘된다고 판단한다.이를 OpenCV에 적용하는 방법은 간단하다. threshold를 실행하면 된다.Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);Mat dst1,dst2;// otsu 이진화 적용double thres = threshold(src, dst1, 130, 255, THRESH_BINARY);double thresotsu = threshold(src, dst2, -1,255, THRESH_OTSU);// 계산된 Threshold 출력cout &amp;lt;&amp;lt; thres &amp;lt;&amp;lt; &quot;\\n&quot; &amp;lt;&amp;lt; thresotsu &amp;lt;&amp;lt; endl;imshow(&quot;src&quot;, src);imshow(&quot;dst1&quot;, dst1);imshow(&quot;dst2&quot;, dst2);waitKey();}dst1는 130 threshold를 적용한 것이고, dst2는 otsu알고리즘을 사용한 threshold를 적용한 영상이다. lenna영상에서는 117이 나왔다. threshold를 넘으면 흰색, 넘지못하면 검정색이 될 것이다.픽셀값이 낮은 어두운 부분들은 다 검정색, 픽셀값이 높은 부분들은 흰색이 된다.이 때, src의 밝기를 낮추게 되면 출력 영상도 많이 달라진다.Mat src3 = src - 100;Mat dst3;double thresotsu2 = threshold(src3, dst3, -1, 255, THRESH_OTSU);imshow(&quot;otsuthresholddark&quot;, dst3);threshold를 출력해봐도 낮게 나오는 것을 확인할 수 있다.참고 블로그1참고 블로그2차선이 검정색인데, 환경이 어두워진다면 에지검출이 잘 되지 않는다. 명암비를 높이거나, canny함수의 최소,최대를 바꾸는 방법을 사용해야 할 것이다. equalize를 수행하면 작은 픽셀값의 차이가 강조가 된다. 대신 노이즈가 증가할수도 있다.차선에 적용해보았는데, 마스크를 이진화해서 그런지 차선이 사라졌다.threshold(crop, crop_thr, -1, 255, THRESH_OTSU);Canny(crop_thr, crop_edge, 50, 150);그래서 원본에 적용을 했더니 차선은 인식이 된다. ROI를 하지 않는 코드에서는 잘 사용될 것 같다.threshold(src, crop_thr, -1, 255, THRESH_OTSU);Canny(crop_thr, crop_edge, 50, 150);otsu 대신 threshold에 존재하는 다양한 방식을 적용하여 차선인식을 해보았다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture cap(&quot;../data/test_video.mp4&quot;); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;video open failed!&quot; &amp;lt;&amp;lt; endl; return -1; } int w = cvRound(cap.get(CAP_PROP_FRAME_WIDTH)); int h = cvRound(cap.get(CAP_PROP_FRAME_HEIGHT)); Mat mask = Mat::zeros(h, w, CV_8UC1); vector&amp;lt;Point&amp;gt; pts; pts.push_back(Point(400, 340)); pts.push_back(Point(680, 340)); pts.push_back(Point(1200, 720)); pts.push_back(Point(0, 720)); pts.push_back(Point(0, 480)); fillPoly(mask, pts, Scalar(255)); Mat frame, dst_mask, dst, roi; while (true) { cap &amp;gt;&amp;gt; frame; if (frame.empty()) break; copyTo(frame, roi, mask); Mat roi_gray, roi_edge; cvtColor(roi, roi_gray, COLOR_BGR2GRAY); GaussianBlur(roi_gray, roi_gray, Size(), 1.0);#if 0 roi_gray = roi_gray &amp;gt; 100;#else /* 추가로 otsu 알고리즘 및 다양한 threshold 방식을 적용하여 threshold를 적용하여 이진화해보았습니다.*/ threshold(roi_gray, roi_gray, 100, 255, THRESH_BINARY);// threshold(roi_gray, roi_gray, 100, 255, THRESH_BINARY_INV); // 반전된 에지 영상이 출력되지만, 선은 에지에 대한 검출이므로 잘 적용됨// threshold(roi_gray, roi_gray, 80, 255, THRESH_TRUNC); // 점선은 인식이 잘 되지 않음// threshold(roi_gray, roi_gray, -1, 255, THRESH_OTSU); // -1을 적용함으로서 경계를 지정하지 않음, 차선인식이 잘 되지 않음 /* adaptivethreshold라는 것도 있어 참고해서 추가해보았지만, mask를 통한 경계선도 같이 인식되어 threshold가 잘 되지 않았습니다. */// adaptiveThreshold(roi_gray, roi_gray, 255, ADAPTIVE_THRESH_MEAN_C, THRESH_BINARY, 5, 5);// adaptiveThreshold(roi_gray, roi_gray, 255, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY, 7, 3);#endif Canny(roi_gray, roi_edge, 50, 150); vector&amp;lt;Vec4i&amp;gt; lines; HoughLinesP(roi_edge, lines, 1, CV_PI/180, 100, 20, 100); for (Vec4i line : lines) { float dx = float(line[2] - line[0]); float dy = float(line[3] - line[1]); float angle = float(atan2f(dy, dx) * 180 / CV_PI); if (fabs(angle) &amp;lt;= 10) continue;// if (line[2] &amp;lt;= 400 || line[2] &amp;gt;= 1190 &amp;amp;&amp;amp; line[0] &amp;gt;= 680)// continue; cv::line(frame, Point(line[0], line[1]), Point(line[2], line[3]), Scalar(0, 0, 255), 2); } imshow(&quot;dst&quot;, frame); imshow(&quot;roi_edge&quot;, roi_edge); if (waitKey(10) == 27) break; }} THRESH_BINARY THRESH_BINARY_INV THRESH_TRUNC THRESH_OTSU ADAPTIVE_THRESH_MEAN_C ADAPTIVE_THRESH_GAUSSIAN_C참고 사이트참고 사이트허프 변환 원 검출허프 변환을 응용하여 원을 검출할 수 있다. 이는 잘 사용하지 않는 방법이기도 하고, 필요할 때 코드만 가져와서 사용하면 될 것 같다.원의 방정식 : (x-a)^2 + (y-b)^2 = c^2 -\\&amp;gt; 3차원 축적 평면원래는 y=ax+b이므로 2차원 평면에 표시했었다. 그래서 이를 3차원을 사용할 수도 있지 않을까 하는 생각에서 나왔다.하지만 OpenCV에서는 원 검출 대신 속도 향상을 위해 Hough gradient method를 사용한다. 입력 영상과 동일한 2차원 평면 공간에서 축적 영상을 생성한다. 에지 픽셀에서 그래디언트 계산 그래디언트 방향에 따라 직선을 그리면서 해당 픽셀에 값을 누적한다. 여기서는 단순하게 평면을 변환하는 것이 아니라 그래디언트를 구해서 그 직선에 따라 1씩 더하는 것이다. 높은 값을 가진 픽셀을 원 중심으로 잡고 에지와 가장 많이 겹치는 적절한 반지름을 검출한다. 단점 : 여러 개의 동심원은 검출하지 못한다. 가장 작은 원 하나만 검출한 후 끝난다.void HoughCircles(InputArray image, OutputArray circles, int method, double dp, double minDist, double params1 = 100, double params2 = 100, int minRadius = 0, int maxRadius = 0) image : 입력 영상, 에지가 아닌 일반 영상 circles : (cx,cy,r)의 정보를 담을 Mat(CV_32FC3) 도는 vector&amp;lt;Vec3f&amp;gt; method : HOUGH_GRADIENT or HOUGH_GRADIENT_ALT 지정, 이 두개를 사용할 때 각각 param1,param2의 값이 달라지게 되어 주의해야 한다. HOUGH_GRADIENT : 원래 사용하던 버전으로 동심원을 찾지 못했음 HOUGH_GRADIENT_ALT : 최신 버전으로 좀 더 잘 검출하고, 동심원도 찾을 수 있게 됨 dp : 입력 영상과 축적 배열의 크기 비율, 1이면 동일 크기, 2이면 축적 배열의 가로, 세로 크기가 입력 영상의 1/2 minDist : 검출된 원 중심점들의 최소 거리 params1 : canny 에지 검출기의 높은 임계값, 이 함수안에서 canny에지 검출을 해주기 때문에 지정하는 값 HOUGH_GRADIENT : sobel 을 사용한다. 약 (50,150) 이므로 최대 값을 지정해주기 위해 150 HOUGH_GRADIENT_ALT : scharr 을 사용하므로 조금 큰 값을 사용해야 한다. 약 300 정도의 값을 줘야 한다. params2 : 축적 배열 임계값, HOUGH_GRADIENT_ALT는 원의 perfectness 값(1에 가까운 실수를 사용) HOUGH_GRADIENT_ALT : 0.8~1.0 값으로 지정 minRadius, maxRadius : 검출할 원의 최소/최대 반지름이를 통해 동전 이미지에 대한 검출이 가능하다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;coins.jpg&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } Mat dst1, dst2; cvtColor(src, dst1, COLOR_GRAY2BGR); cvtColor(src, dst2, COLOR_GRAY2BGR); // HOUGH_GRADIENT Mat blr; GaussianBlur(src, blr, Size(), 1.0); // 입력 영상을 블러링하여 잡음을 제거해야 한다. vector&amp;lt;Vec3f&amp;gt; circles1; HoughCircles(blr, circles1, HOUGH_GRADIENT, 1, 10, 150, 30, 10, 50); for (size_t i = 0; i &amp;lt; circles1.size(); i++) { Vec3i c = circles1[i]; circle(dst1, Point(c[0], c[1]), c[2], Scalar(0, 0, 255), 2, LINE_AA); } // HOUGH_GRADIENT_ALT vector&amp;lt;Vec3f&amp;gt; circles2; HoughCircles(src, circles2, HOUGH_GRADIENT_ALT, 1.5, 10, 300, 0.9, 10, 50); for (size_t i = 0; i &amp;lt; circles2.size(); i++) { Vec3i c = circles2[i]; circle(dst2, Point(c[0], c[1]), c[2], Scalar(0, 0, 255), 2, LINE_AA); } imshow(&quot;src&quot;, src); imshow(&quot;dst1&quot;, dst1); imshow(&quot;dst2&quot;, dst2); waitKey();}GPU 활용CUDACUDA란 GPU에서 수행하는 병렬 처리 알고리즘을 C 프로그래밍 언어를 비롯한 산업 표준 언어를 사용하여 작성할 수 있도록 하는 GPGPU(General Purpose computin on GPU) 기술이다. CUDA는 엔비디아가 개발해오고 있어서 엔비디아 그래픽 카드에서만 가동이 된다.Main메모리에서 GPU메모리로 데이터를 복사하고, CPU에서 CUDA로 명령을 내려서 실행하고 결과가 GPU메모리에 들어가고, 그것을 다시 Main메모리로 복사한다.c언어, c++에서는 gpu를 사용하는 방법으로는 __x__처럼 CUDA문법인 __를 추가해주면 된다.__global__ void add(int *a, int *b, int *c) { *c = *a + *b;}int main() { int a = 2, b=7,c; int *d_a,*d_b,*d_c; cudaMalloc((void**)&amp;amp;d_a, sizeof(int)); cudaMalloc((void**)&amp;amp;d_b, sizeof(int)); cudaMalloc((void**)&amp;amp;d_c, sizeof(int)); cudaMemcpy(d_a, &amp;amp;a, sizeof(int), cudaMemcpyHostToDevice); cudaMemcpy(d_b, &amp;amp;b, sizeof(int), cudaMemcpyHostToDevice); add&amp;lt;&amp;lt;1,1&amp;gt;&amp;gt;(d_a,d_b,d_c); // cuda 문법 &amp;lt;&amp;lt;&amp;gt;&amp;gt; 코어를 어떻게 쓸지 cudaMemcpy(&amp;amp;c, d_c, sizeof(int), cudaMemcpyDeviceToHost); cudaFree(d_a); cudaFree(d_b); cudaFree(d_c); return 0;}openCV에서 CUDA를 사용하려면 2가지를 해야 한다. CUDA Toolkit 설치 : https://developer.nvidia.com/cuda-downloads OpenCV 직접 빌드 : http://youtu.be/Gfl6EylhFvM opencv_contrib 소스 코드 다운로드 및 CMake에서 OPENCV_EXTA_MODULES_PATH 설정 CMake 에서 WITH_CUDA 설정 선택 CUDA_TOOLKIT_ROOT_DIR 확인 CUDA를 이용한 Hough 선분 검출 예제 코드자세한 설명이 잘 없고, opencv/sources/samples/gpu/*여기에 있는 파일들이 opencv에서 제공하는 gpu를 사용한 샘플 파일들이다.#include &quot;opencv2/cudaimgproc.hpp&quot; // cuda관련 헤더 파일을 모듈별로 포함int main(void) { Mat src = imread(&quot;building.jpg&quot;,IMREAD_GRAYSCALE); Mat edge; cv::Canny(src,edge,100,200,3); // cuda::GpuMat 클래스 사용 cuda::GpuMat d_edge(edge); cuda::GpuMat d_lines; // cuda용으로 구현된 함수 및 클래스 사용 Ptr&amp;lt;cuda::HoughSegmentDetector&amp;gt; hough = cuda::createHoughSegmentDetector(1.0f, (CV_PI/180.0f), 50, 5); hough -&amp;gt; detect(d_edge, d_lines);}cpu와 gpu의 시간 차이는 대체로 15~20배 정도 차이난다. 그러나 cpu와 gpu의 결과가 다르게 나오기도 한다. 그래서 이에 대한 처리도 추가해서 해줘야 한다.OpenCL(Open Computing Language)OpenCV에서 CUDA를 사용하려면 빌드를 해야 하는데, 좀 복잡하고 오래 걸린다. 그래서 그 대신 OpenCL을 사용하기도 한다.여러 개의 CPU, GPU, DSP 등의 프로세서로 이러우저니 이종 플랫폼에서 동작하는 프로그램 코드 작성을 위한 개방형 범용 병렬 컴퓨팅 프레임워크 Open,royalty-free,cross-platform,parallel programming, heterogeneous애플에서 최초 개발해서 Intel, AMD, ARM,nVidia 등이 참여했다.GPU-Z 프로그램에서 OpenCL 항목을 확인할 수 있다. openCL을 사용한 코드예전에는 ocl이라는 네임스페이스를 통한 ocl클래스를 사용했어야 했다. 그러나 3.x 버전으로 발전되면서 Mat 함수를 생성할 때 UMat으로만 바꿔주면 openCL을 사용할 수 있게 되었다. 이것만 바꿔줘도 나머지는 gpu를 사용하는 방식으로 작동하게 된다.VideoCapture cap(&quot;../data/test_video.mp4&quot;);CascadeClassifier fd(&quot;haar.xml&quot;);UMat frame, gray,blr,dst; // Mat frame, gray,blr,dst;vector faces;while (true){ cap &amp;gt;&amp;gt; frame; TickMeter tm; tm.start(); cvtColor(frame, gray, COLOR_BGR2GRAY); GaussianBlur(gray, blr,Size(), 2); bilateralFilter(blr,dst,-1,10,5); tm.stop(); Cout &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; endl; imshow(&quot;dst&quot;,dst); imshow(&quot;frame&quot;,frame); if(waitKey(10) == 27) break;}MAT -&amp;gt; UMAT 로만 바꿔줌으로서 최소한의 소스 코드 변경을 통해 gpu를 사용하여 HW 가속이 가능하다. cap &amp;gt;&amp;gt; frame; 이 UMat에서 사용이 가능하게 되어 있다. 선언으로 이동해보면 UMat에 대한 동작이 되도록 선언되어 있다. 처음에는 Mat으로 받아서 알아서 UMat으로 변환해준다. 또한 cvtColor도 확인해보면 UMat인지에 대한 판단이 들어가 있다.그러나 imshow를 하거나, cap&amp;gt;&amp;gt;frame과 같이 frame을 받아오는 과정에서 gpu형태에서 cpu형태로 바꿔지기 때문에 이 것들에 대한 시간을 고려해야 한다. 그래서 UMat 사용이 항상 좋은 것은 아니나, 영상 관련해서는 분명히 빨라진다고 한다. gpu 사용 cpu 사용 Mat to UMatMat mat = imread(&quot;lenna.bmp&quot;)UMat umat1;mat.copyTo(umat1);UMat umat2 = mat.getUMat(ACCESS_READ); // transform to UMatACCESS_READ는 opencv에 설명이 부실하다. ACCESS_WRITE와 두개가 있는데 대체로 ACCESS_READ를 하면 잘 작동한다. UMat to MatUMat umat;videoCap &amp;gt;&amp;gt; umat;Mat mat1;umat.copyTo(mat1);Mat mat2 = umat.getMat(ACCESS_READ);UMat이 연산 속도가 빠르긴 하나 변환하는데도 속도가 조금 걸리기 때문에, 영상을 표현하는 Mat에 대해서만 UMat을 사용하는 것이 좋다. border 처리 연산시에도 BORDER_REPLICATE옵션을 사용하는 것이 조금 더 좋다고 한다.getUMat, getMat을 사용할 때 주의해야 할 것이 있다. getUMat 함수를 통해 UMat객체를 생성할 경우, 원본과 UMat, 이 두 개는 얕은 복사가 된 것이므로, 나중에 원본을 사용하기 전에 새로 생성한 UMat 객체가 완전히 소멸한 후 원본 Mat 객체를 사용해야 한다.cv::Mat mat1(height, width, CV_32FC1);mat1.setTo(0);{ cv::UMat umat1 = mat1.getMat(cv::ACCESS_READ);} 딥러닝 학습은 파이토치나 텐서플로우로 하고, 결과를 저장한 후 그 파일을 opencv에서 불러와 사용할 수 있다. 파이토치의 경우 onnx로 변환해서 불러오는 방법을 사용해야 한다고 한다. 실제로 차선 검출에서 hough변환을 사용하지는 않는다. 속도나 정확도가 뛰어나지 않기 때문이다. 그러나 컴퓨터비전을 처음 배우는 사람들에게는 적용해보기 쉬운 알고리즘이라서 필수적으로 배우는 내용이다. cv::LineSegmentDetectorhttps://docs.opencv.org/4.5.5/db/d73/classcv_1_1LineSegmentDetector.htmlcv 클래스에 라인을 따주는 클래스가 있다. 그러나 허프변환과 속도가 비슷하기도 하고, 정확도가 더 뛰어나다고 할 수도 없다. 어떤 경우든 튜닝을 잘 해야 한다." }, { "title": "[데브코스] 6주차 - OpenCV specific color and edge extraction ", "url": "/posts/opencvedge/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-24 14:01:00 +0900", "snippet": "특정 색상 영역 추출RGB, HSV, YCrCb 등의 색 공간에서 각 색상 성분의 범위를 지정하여 특정 색상 성분만 추출할 수 있다. RGB는 계산이 이상하게 나올 수 있으므로 HSV나 YCrCb를 사용한다.void inRange(InputArray src,InputArray lowerb, InputArray upperb, OutputArray dst); src : 입력 행렬 lowerb : 하한 값 (Mat or Scalar) upperb : 상한 값 (Mat or Scalar) dst : 입력 영상과 동일 크기, CV_8UC1 타입, 범위 안에 들어가는 픽셀 값만 255로 설정 단일 채널 -&amp;gt; dst = lowerb &amp;lt;= src &amp;lt;= upperb 다중 채널 -&amp;gt; dst1 = lowerb3 &amp;lt;= src1 &amp;lt;= upperb1 dst2 = lowerb2 &amp;lt;= src2 &amp;lt;= upperb2 dst3 = lowerb1 &amp;lt;= src3 &amp;lt;= upperb3 노란색만 추출하고 싶을 때, HSV 기준으로 5~30 정도 또는 15~40 정도의 값을 추출하면 된다.int pos_hue1 = 5, pos_hue2 = 30, pos_sat1 = 200, pos_sat2 = 255;Mat src, src_hsv, dst, dst_mask;if (argc &amp;lt; 2) { src = imread(&quot;flower1.png&quot;, IMREAD_COLOR);} else { src = imread(argv[1], IMREAD_COLOR);}cvtColor(src, src_hsv, COLOR_BGR2HSV);Scalar lowerb(pos_hue1, pos_sat1, 0);Scalar upperb(pos_hue2, pos_sat2, 255);inRange(src_hsv, lowerb, upperb, dst_mask);cvtColor(src, dst, COLOR_BGR2GRAY); // gray로 변환하여 연산하고자 함cvtColor(dst, dst, COLOR_GRAY2BGR); // 나중에 출력하는 것에 색을 입히고 싶으므로 bgr로 변형해놓는다.src.copyTo(dst, dst_mask); // 마스크에 대한 0이 아닌 값만 복사imshow(&quot;dst_mask&quot;, dst_mask);imshow(&quot;dst&quot;, dst);imshow(&quot;src&quot;, src);waitKey();히스토그램 역투영 (histogram backprojection)앞서 사용했던 방법에서 원색을 찾는 것이라면 HSV가 편하지만, Hue값이 애매하게 지정해야만 한다면 조금 어려울 수 있다. 그래서 이 색과 비슷한 것들만 추출해달라는 방법을 사용하면 편하다. 히스토그램 역투영은 주어진 히스토그램 모델(영상에서 특정 사각형을 쳐서 그 안의 픽셀들을 추출)에 영상의 픽셀들이 얼마나 일치하는지를 검사하는 방법을 말한다. 임의의 색상 영역을 검출할 때 효과적이다. 이 때, YCrCb로 변환시켜서 적용해야 잘 추출된다.특정 사각형의 픽셀들을 2차원 히스토그램을 그린 후 그레이스케일 범위로 정규화를 한 후 그래프를 그리게 되면 작은 점으로 표시될 것이다. 이 때, 앞서 배운 inrange는 마스크 영상으로 0 or 255이지만, 이번에는 0~255사이의 값들이 나오게 되어 결과 영상을 만든다.이 때 추출된 색상에 가우시안을 적용해서 추출하면 조금 더 부드러운 영상을 얻을 수 있다. 2차원 히스토그램Mat hist;int channels[] = {1, 2};int cr_bins = 128; int cb_bins = 128;int histSize[] = {cr_bins, cb_bins};float cr_range[] = {0, 256};float cb_range[] = {0, 256};const float* ranges[] = {cr_range, cb_range};// 부분 영상에 대한 히스토그램 계산calcHist(&amp;amp;crop, 1, channels, Mat(), hist, 2, histSize, ranges);컬러 영상의 히스토그램을 그려야 하기 때문에 조금 복잡하다. 먼저 ycrcb로 변환시킨 후 선택할 영역을 크롭한 후 그 공간에서 히스토그램을 하는데, y는 무시하고 crcb만 구할 것이므로 1,2만 출력하고, 히스토그램 사이즈를 정한 후, 범위를 0~255이지만 (0,256)으로 지정한 후 calcHist를 계산한다.hist는 2차원으로 나올 것이다. 역투영Mat backproj;calcBackProject(&amp;amp;src_ycrcb, 1, channels, hist, backproj, ranges, 1, true);imshow(&quot;back&quot;, backproj); 마스크 복사src.copyTo(dst, backproj);copyto는 0이 아닌 픽셀들에 대해 복사하므로 다 수행된다. selectROIMat src = imread(&quot;cropland.png&quot;, IMREAD_COLOR);Rect rc = selectROI(src);Mat src_ycrcb;cvtColor(src, src_ycrcb, COLOR_BGR2YCrCb);Mat crop = src_ycrcb(rc);selectROI라는 함수를 사용하면 window에서 사용자가 관심 공간을 지정해줄 수 있다. 지정된 공간을 잘라서 여기에 대해서만 관심을 두겠다는 것이다. 역투영 함수calcBackProject(const Mat* images,int nimages,const int* channels,InputArray hist,OutputArray backproject, const float** ranges, double scale = 1, bool uniform = true) images : 여러 Mat을 넣어 줄 수 있다. nimages : 몇 개의 Mat을 사용했는지에 대한 값 몇 채널만 사용했는지 hist : calcHist에서 출력된 hist backproject : 출력 영상 ranges : calcHist에서 사용했던 ranges잘 찾기 위해서는 1장의 이미지만 사용하는 것이 아니라 살색을 찾기 위해서는 살색 영상을 집어넣어서 연산을 해야 한다. 잘 나온 히스토그램은 동그랗게 나올 것이다.여기서 그냥 backprojection을 하면 배경에 있는 이미지들중에서도 비슷한 색상이 있다면 배경도 함께 추출될 수 있다. 그러므로 가우시안 블러를 한 후 이진화를 통해 0과 255로 변수를 재생성하면 좀 더 색상에 부합하는 것을 추출할 수 있다.Mat backproj;calcBackProject(&amp;amp;src_ycrcb, 1, channels, hist, backproj, ranges, 1, true);GaussianBlur(backproj, backproj, Size(), 1.0);backproj = backproj &amp;gt; 50;또한, 히스토그램을 보기 위해 아래 코드도 실행하여 0~255로 정규화 한 후 윈도우로 출력해본다.calcHist(&amp;amp;ref_ycrcb, 1, channels, mask, hist, 2, histSize, ranges);Mat hist_norm;normalize(hist, hist_norm, 0, 255, NORM_MINMAX, -1);imshow(&quot;hist_norm&quot;, hist_norm); 전체 코드#include &quot;opencv2/opencv.hpp&quot;#include &amp;lt;iostream&amp;gt;using namespace cv;using namespace std;int main(){ // Calculate CrCb histogram from a reference image Mat ref, ref_ycrcb, mask;// ref = imread(&quot;lenna.bmp&quot;, IMREAD_COLOR); ref = imread(&quot;ref.png&quot;, IMREAD_COLOR); mask = imread(&quot;mask.bmp&quot;, IMREAD_GRAYSCALE); cvtColor(ref, ref_ycrcb, COLOR_BGR2YCrCb); Mat hist; int channels[] = { 1, 2 }; int cr_bins = 128; int cb_bins = 128; int histSize[] = { cr_bins, cb_bins }; float cr_range[] = { 0, 256 }; float cb_range[] = { 0, 256 }; const float* ranges[] = { cr_range, cb_range }; calcHist(&amp;amp;ref_ycrcb, 1, channels, mask, hist, 2, histSize, ranges);#if 1 Mat hist_norm; normalize(hist, hist_norm, 0, 255, NORM_MINMAX, -1); imshow(&quot;hist_norm&quot;, hist_norm);#endif Mat src, src_ycrcb; src = imread(&quot;kids.png&quot;, IMREAD_COLOR); cvtColor(src, src_ycrcb, COLOR_BGR2YCrCb); Mat backproj; calcBackProject(&amp;amp;src_ycrcb, 1, channels, hist, backproj, ranges, 1, true); GaussianBlur(backproj, backproj, Size(), 1.0); backproj = backproj &amp;gt; 50; Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3); src.copyTo(dst, backproj); imshow(&quot;ref&quot;, ref); imshow(&quot;mask&quot;, mask); imshow(&quot;src&quot;, src); imshow(&quot;backproj&quot;, backproj); imshow(&quot;dst&quot;, dst); waitKey(); return 0;}에지 검출과 영상의 미분edge : 영상에서 픽셀의 밝기 값이 급격하게 변하는 부분을 말한다. 일반적으로 배경과 객체, 또는 객체와 객체의 경계가 이에 해당한다.기본적으로 에지를 검출하는 방법은 영상을 (x,y) 변수의 함수로 간주했을 때 이 함수의 1차 미분값이 크게 나타나는 부분(순간 변화량)을 검출한다. 그러나 픽셀 그래프틑 step function의 형태, 즉 계단 형식으로 정수값으로만 변화한다. 입력 영상에 가우시안 블러를 적용해서 잡음을 제거한 후 에지를 검출하는 것이 바람직하다.미분값이 threshold(임계값)을 넘어가면 에지라고 할 수 있다.1차 미분의 근사화 전진 차분 : [I(x+h) - I(x)] / h 후진 차분 : [I(x) - I(x - h)] / h 중앙 차분 : [I(x+h) - I(x-h)] / 2h이 때, 중앙 차분이 가장 잘 근사화가 된다. 마스크를 간단화하기 위해 중앙 차분의 1/2를 빼서 계산한다.영상에서의 ∆x의 최소값은 1pixel이다. 그리고, 영상에서는 x방향, y방향 두 개를 계산해야 한다.중요한 것은 잡음 제거와 마스크에서의 합이 0이 되면 영상이 전체적으로 어두워지므로 3x3 크기로 만든다. 즉 x방향의 [-1 0 1] 의 마스크가 있다고 하면가로 방향[-1 0 1][-1 0 1][-1 0 1]세로 방향[-1 -1 -1][0 0 0][1 0 1]또는 가우시안 형태를 따서 만든 형태를 많이 사용한다.가로 방향[-1 0 1][-2 0 2][-1 0 1]세로 방향[-1 -2 -1][0 0 0][1 2 1]가로 방향에서의 아래 필터를 사용했을 때의 에지 출력이다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); Mat dst(src.rows, src.cols, CV_8UC1); for (int y = 1; y &amp;lt; src.rows-1; y++) { for (int x = 1; x &amp;lt; src.cols-1; x++) { /* 가로 방향 필터 */ int v1 = src.at&amp;lt;uchar&amp;gt;(y - 1, x + 1) + src.at&amp;lt;uchar&amp;gt;(y, x + 1) * 2 + src.at&amp;lt;uchar&amp;gt;(y + 1, x + 1) - src.at&amp;lt;uchar&amp;gt;(y - 1, x - 1) - src.at&amp;lt;uchar&amp;gt;(y, x - 1) * 2 - src.at&amp;lt;uchar&amp;gt;(y + 1, x - 1); /* 세로 방향 필터 */ int v2 = src.at&amp;lt;uchar&amp;gt;(y - 1, x - 1) + src.at&amp;lt;uchar&amp;gt;(y - 1, x) * 2 + src.at&amp;lt;uchar&amp;gt;(y - 1, x + 1) - src.at&amp;lt;uchar&amp;gt;(y + 1, x - 1) - src.at&amp;lt;uchar&amp;gt;(y + 1, x) * 2 - src.at&amp;lt;uchar&amp;gt;(y + 1, x + 1); dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(v1); dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(v2); } } imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}여기서 문제점은 미분값이 -가 나올 수도 있는데, 포화연산을 통해 아래부분은 다 날아가게 된다. 따라서 128을 더하여 확인을 해본다.//dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(v1);dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(v1+128);이는 가로 방향 필터를 사용한 에지 검출이다. 여기서 흰색의 부분은 밝아지는 부분, 검은색의 부분은 어두워지는 부분이고, 회색은 아무것도 아닌 부분이다.위의 그림은 세로 방향 필터를 사용한 에지 검출이다. 동일하게 흰색 부분은 밝아지는 부분이고, 검은색 부분은 어두워지는 부분이다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); Mat dst1(src.rows, src.cols, CV_8UC1); Mat dst2(src.rows, src.cols, CV_8UC1); for (int y = 1; y &amp;lt; src.rows-1; y++) { for (int x = 1; x &amp;lt; src.cols-1; x++) { /* 가로 방향 필터 */ int v1 = src.at&amp;lt;uchar&amp;gt;(y - 1, x + 1) + src.at&amp;lt;uchar&amp;gt;(y, x + 1) * 2 + src.at&amp;lt;uchar&amp;gt;(y + 1, x + 1) - src.at&amp;lt;uchar&amp;gt;(y - 1, x - 1) - src.at&amp;lt;uchar&amp;gt;(y, x - 1) * 2 - src.at&amp;lt;uchar&amp;gt;(y + 1, x - 1); /* 세로 방향 필터 */ int v2 = src.at&amp;lt;uchar&amp;gt;(y - 1, x - 1) + src.at&amp;lt;uchar&amp;gt;(y - 1, x) * 2 + src.at&amp;lt;uchar&amp;gt;(y - 1, x + 1) - src.at&amp;lt;uchar&amp;gt;(y + 1, x - 1) - src.at&amp;lt;uchar&amp;gt;(y + 1, x) * 2 - src.at&amp;lt;uchar&amp;gt;(y + 1, x + 1); dst1.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(v1+128); dst2.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(v2+128); } } imshow(&quot;src&quot;, src); imshow(&quot;dst1&quot;, dst1); imshow(&quot;dst2&quot;, dst2); waitKey();}에지 검출을 위해서는 이 둘을 함께 사용해야 정확한 에지를 구할 수 있다.영상의 그래디언트함수 f(x,y)를 x축과 y축으로 각각 편미분(partial derivative)하여 벡터 형태로 표현한 것이다.∇f = [fx fy] 그래디언트 크기(magnitude) : ∇f = sqrt(fx^2 + fy^2) 그래디언트 방향(phase) : ϴ = tan^(-1)(fy/fx)이를 코드화 시키면 다음과 같다.int mag = (int)sqrt(v1 * v1 + v2 * v2);dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(mag);에지만 검출하기 위해서는 이진화를 작성해야 한다.dst = dst &amp;gt; 120;이와 같이 threshold를 주게 되면 에지만 검출하고 나머지는 검은색으로 추출된다.그래디언트 크기와 방향크기는 픽셀 값의 차이 정도, 변화량이고, 방향은 픽셀 값이 가장 급격하게 증가하는 방향을 의미한다.sobel연산자를 이용한 미분 함수가 있다. 함수void Sobel(InputArray src, OutputArray dst, iont ddepth, int dx, int dy, int ksize = 3, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT); src,dst : 입력, 출력 영상, 둘은 같은 크기 및 같은 채넗 수 ddepth : 출력 영상 깊이 dx, dy : x방향과 y방향으로의 미분 차수, 대부분 (0,1) 또는 (1,0)을 넣는다. ksize : 커널 크기 1 : 3x1 or 1x3 CV_SCHARR : 3x3 scharr 커널 3,5,7 : 3x3,5x5,7x7 scale : option, 연산 결과에 추가적으로 곱할 값 delta : option, 연산 결과에 추가적으로 더할 값 borderType : 가장 자리 픽셀 확장 방식ksize는 거의 필수로 3(default) 작성, CV_SCHARR은 scharr 필터를 사용하는 방법이다. (굳이는.. 안사용함)void magnitude(InputArray x, InputArray y, OutputArray magnitude); x : 2D 벡터의 x좌표 행렬, 실수형 y : 2D 벡터의 y좌표 행렬, x와 같은 크기, 실수형 magnitude : 2D 벡터의 크기 행렬, X와 같은 크기 sovel마스크를 이용한 에지 검출Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return;}Mat dx, dy;Sobel(src, dx, CV_32FC1, 1, 0);Sobel(src, dy, CV_32FC1, 0, 1);Mat mag;magnitude(dx, dy, mag);mag.convertTo(mag, CV_8UC1);Mat edge = mag &amp;gt; 150;imshow(&quot;src&quot;, src);imshow(&quot;mag&quot;, mag);imshow(&quot;edge&quot;, edge);convertTo 를 사용하거나 normalize를 사용캐니 에지 검출캐니 에지 검출 알고리즘openCV에 함수가 잘 되어 있다.좋은 에지 검출기란 정확한 검출 : 에지가 아닌 점을 에지로 찾거나 또는 에지를 검출하지 못하는 확률을 최소화 정확한 위치 : 실제 에지의 중심을 검출 단일 에지 : 하나의 에지는 하나의 점으로 표현이들을 수행하지 위한 canny edge detector을 사용한다.캐니 에지 검출 방법 가우시안 필터링 : 잡음 제거(option) 그래디언트 계산 소벨 마스크를 사용하여 그래디언트의 크기와 방향을 계산 방향을 4구역으로 단순화한다. 즉, 330~23,23~75… NMS(non-maximum suppression) 하나의 에지가 여러 개의 픽셀로 표현되는 현상을 없애기 위해 그래디언트 크기가 local maximum인 픽셀만을 에지 픽셀로 지정 그래디언트 방향에 위치한 두 개의 픽셀과 local maximum을 검사 그래디언트 방향 두개를 선정해서 물체의 수직 방향으로 체크하고자 함 이중 임계값을 이용한 히스테리시스 에지 트래킹 조명에 의한 에지가 끊기는 부분을 해결하기 위함 두개의 임계값을 사용 :Tlow, Thigh 강한 에지 :   f   &amp;gt;= Thigh =&amp;gt; 반드시 에지로 선정 약한 에지 : Tlow &amp;lt;=   f   &amp;lt;= Thigh -&amp;gt; 강한 에지와 연결된 픽셀만 최종 에지로 선정 Tlow &amp;gt;=   f   -&amp;gt; 에지 아닌 것으로 판단 void Canny(InputArray image, OutputArray edges, double threshold1, double threshold2, int aperturesize = 3, L2gradient = false) image : 입력 영상 edges : 에지 영상 threshold1 : 하단 임계값 threshold2 : 상단 임계값, 1과 2의 비율을 1:2, 1:3을 권장한다. aperture size: 소벨 연산을 위한 커널 크기 L2gradient : L2 norm 사용 여부이처럼 실행하면 된다. 함수를 직접 구현하고 싶다면, opencv 소스코드를 참고하면 좋다. 소스코드 찾기찾고자 하는 함수에 정지(F9) 후 디버깅(F5) -&amp;gt; 프로시저 단위로 실행(F11) -&amp;gt; outputarray/inputarray가 나오면 빠져나오기(shift+F11) -&amp;gt; 함수를 찾을 수 없다고 나오면 opencv/sources/modules/ -&amp;gt; 이정도만 와도 알아서 인식한다. 안되면 -&amp;gt; imgproc/src/canny.cpp/" }, { "title": "Coding Test[C++] - 덱과 우선 슌위 큐", "url": "/posts/codingtestcheapq/", "categories": "Classlog, coding test review", "tags": "coding test, deque", "date": "2022-03-24 02:10:00 +0900", "snippet": "프로그래머스에 기재되어 있는 c++ coding test에 대해 리뷰하고자 합니다. 그 중에서도 연습문제에 해당하는 문제를 이 곳에 작성할 예정입니다. 다른 것을 참고하시려면 아래 링크를 클릭하시기 바랍니다. 해시 정렬 연습문제 카카오 블라인드 채용 문제회전하는 큐 deque, list 사용#include &amp;lt;iostream&amp;gt;#include &amp;lt;deque&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;algorithm&amp;gt;using namespace std;int num, N, M, n;deque&amp;lt;int&amp;gt; dq;int main(void) { // 배열 크기와 출력하고 싶은 원소 개수 입력 cin &amp;gt;&amp;gt; N &amp;gt;&amp;gt; M; // N 크기의 배열 생성 for (auto i = 1; i &amp;lt;= N; ++i) { dq.push_back(i); } int cnt{ 0 }; for (auto k = 0; k &amp;lt; M; k++) { // 빼내고 싶은 원소 번호를 입력 cin &amp;gt;&amp;gt; n; // 빠른 탐색을 위해 원소 번호의 위치 계산 int range = distance(dq.begin(), find(dq.begin(), dq.end(), n)); // deque의 크기 지정 int size = dq.size(); // 원소의 위치가 deque의 크기/2 보다 작다면 앞에서 탐색 if (range &amp;lt;= int(size / 2)) { // 맨 앞에서 빼낼 수 있으므로 원하는 원소가 맨 앞에 올때까지 뒤로 미루기 while (n != dq.front()) { dq.push_back(dq.front()); dq.pop_front(); ++cnt; } } // 원소 위치가 deque의 크기/2 보다 크다면 뒤에서 탐색 else { // 빼낼 수 있는 곳이 맨 앞뿐이므로 빼내고 싶은 원소를 맨 앞으로 이동할때까지 앞으로 이동시키기 while (n != dq.front()) { dq.push_front(dq.back()); dq.pop_back(); ++cnt; } } // 탐색이 끝나면 맨 앞에 원하는 원소가 있을 것이므로 빼내기 dq.pop_front(); } cout &amp;lt;&amp;lt; cnt;}아래는 테스트를 위해 추가한 코드다. int main() 함수 밖에 추가해주면 된다.// 테스트를 위해 작성한 listsvector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; lists{ {1,2,3}, {2,9,5}, {27,16,30,11,6,23}, {1,6,3,2,7,9,8,4,10,5}};// lists의 몇번째 데이터인지 입력cout &amp;lt;&amp;lt; &quot;Number : &quot;;cin &amp;gt;&amp;gt; num;// 입력한 숫자에 대한 vector 가져오기vector&amp;lt;int&amp;gt; index = lists[num-1];deque회전하는 자료형을 사용하기 위해 deque를 사용했다. deque에서는 vector에 있는 push_back()과 pop_back()이외에, push_front(), pop_front()도 지원한다. 그리고 ++i 처럼 앞에다 ++를 작성해준 이유는 메모리 관점에서 뒤에 작성하게 되면 메모리상에 i가 존재하고, i+1이 또 생성되는 반면 앞에다 적어주면 이러한 불필요를 줄일 수 있기 때문이다.distance()시작위치부터 특정 원소의 위치를 추출해주는 함수인데, 간단하게 특정원소의 위치를 찾아준다. 사용 인자는 다음과 같다.int std::distance&amp;lt;iterator _First, iterator _Last); _First : 거리를 측정할 시작 위치 _Last : 거리를 측정할 마지막 위치 반환값 : _Last - _First;이것과 find 함수를 사용하여int distance(dq.begin(), find(dq.begin(), dq.end(), n));시작 위치부터 원소 n의 위치까지의 거리를 반환할 수 있다. 예를 들어, [1,2,3,4] 배열이 있고, 3의 위치를 distance로 찾으면 2을 반환해준다.cin » m입력 스트림으로 출력 스트림의 반대로 입력을 받는다.  » 연산자는 피연산자의 타입에 따라 적절하게 입력값을 가공하여 넣어준다. int나 double을 입력데이터를 받는다면 입력 데이터를 수로 판단하여 가공한다. 위 코드를 실행하면 입력을 받게 되고, 입력을 하면 그 값이 m으로 들어간다. m의 타입은 » 연산자가 알아서 판단해준다.더 맵게 priority_queue 사용#include &amp;lt;iostream&amp;gt;#include &amp;lt;queue&amp;gt;using namespace std;int solution(vector&amp;lt;int&amp;gt; scoville, int K) { int answer = 0; // 우선 순위 큐를 사용하여 삽입 시 알아서 들어가도록 만듦, greater를 넣어서 오름차순이 되도록 했다. priority_queue&amp;lt;int, vector&amp;lt;int&amp;gt;, greater&amp;lt;int&amp;gt;&amp;gt; pq(scoville.begin(), scoville.end()); // 맨 앞의 값, 즉 제일 작은 값이 K보다 크다면, 모든 리스트의 값들이 k보다 큼을 의미 while (pq.top() &amp;lt; K) { // 크기가 1이라면 더이상 축소가 안되므로 리턴 if (pq.size() == 1) return -1; // 제일 작은 값 2개를 추출 auto f = pq.top(); pq.pop(); auto s = pq.top(); pq.pop(); auto mix_value = f + (s * 2); pq.push(mix_value); ++answer; } return answer;}priority_queue우선순위 큐는 선입선출 순이 아니라 우선순위가 높은 항목이 가장 앞에 오도록 하는 큐이다.template &amp;lt;typename _Ty, typename _Container = vector&amp;lt;_Ty&amp;gt;, typename _Pr = less&amp;lt;_Ty&amp;gt;&amp;gt; class priority_queue; _Ty : 요소의 타입을 지정 _container : 바탕 컨테이너로 vector 이나 deque을 사용할 수 있다. _Pr : 우선순위의 비교에 사용될 비교 연산을 나타내는 타입이다. less나 greater를 사용하여 오름차순, 내림차순을 정할 수 있다. 기본값은 less로 operator &amp;lt;를 기준으로 비교하게 되어 있다.우선순위 큐는 front()나 back()을 지원하지 않고, top()메서드를 사용하여 룩업(검색)을 한다.원소 삽입 시 우선순위 큐는 힙 구조를 유지하면서 넣어주기 때문에 O(NlogN)의 시간복잡도를 가진다." }, { "title": "[데브코스] 6주차 - OpenCV Color Space ", "url": "/posts/opencv4/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-23 14:01:00 +0900", "snippet": "컬러 영상을 그레이 스케일로 변환Y = 0.299R + 0.587G + 0.114Bint main(){ Mat src = imread(&quot;lenna.bmp&quot;);#if 0 Mat dst = Scalar(255, 255, 255) - src;#else Mat dst(src.rows, src.cols, CV_8UC1); for (int y = 0; y &amp;lt; dst.rows; y++) { for (int x = 0; x &amp;lt; dst.cols; x++) { uchar b = src.at&amp;lt;Vec3b&amp;gt;(y, x)[0]; uchar g = src.at&amp;lt;Vec3b&amp;gt;(y, x)[1]; uchar r = src.at&amp;lt;Vec3b&amp;gt;(y, x)[2]; // Vec3b v = src.at&amp;lt;Vec3b&amp;gt;(y, x); 실제로는 이 방법을 사용해야 시간이 덜 걸리나, // uchar b = v[0]; visual studio에서 자체로 최적화를 해주어서 위의 식을 이 식처럼 변환해준다.// uchar g = v[1];// uchar r = v[2]; dst.at&amp;lt;uchar&amp;gt;(y, x) = uchar(0.299 * b + 0.587 * g + 0.114 * r); // 앞의 숫자들의 합이 1이 되어야 평균 밝기가 유지된다. // 합이 절대 255가 넘지 않아서 포화 연산은 할 필요는 없다. } }#endif imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}src를 grayscale로 불러오는 것과 위의 식으로 진행하는 것과 같게 나온다.또는 아래의 코드처럼 IMREAD_GRAYSCALE로 하는 것과 위의 코드로 하여 출력하는 것이나 같다.Mat src = imread(&quot;lenna.bmp&quot;);cvtColor(src, dst, COLOR_BGR2GRAY);그레일 스케일로 변환하면 속도는 빨라지지만,// dst.at&amp;lt;uchar&amp;gt;(y, x) = uchar(0.299 * b + 0.587 * g + 0.114 * r);dst2.at&amp;lt;uchar&amp;gt;(y,x) = (r + g + b) / 3 // 이렇게 표현할수도 있다.위의 식 대신 아래로 표현할 수 있지만, 이것이 정답이 아니라 단순하게 표현하기 위함이라 오류가 날 수도 있다.그러나, 문제는 0.299 * b + 0.587 * g + 0.114 * r 이 식이, 매 픽셀마다 실수 연산을 넣어줘야 하기 때문에 느려질 수 있다. 그래서dst.at&amp;lt;uchar&amp;gt;(y, x) = uchar((299 * b + 587 * g + 114 * r) / 1000);실제로는 이렇게 사용하는데, 나눗셈이 사칙연산에서 제일 오래 걸린다. 따라서, 0.299,0.587,0.114에 2^14를 곱하여 연산하고 »(shift연산자)를 통해 빠르게 계산한다. 14라 하면 2^14로 나눠지게 된다.#define RGB2GRAY(r,g,b) ((4899*r + 9617*g + 1868*b) &amp;gt;&amp;gt; 14); dst.at&amp;lt;uchar&amp;gt;(y,x) = (uchar)RGB2GRAY(r,g,b);14인 이유는 2^16이 범위이므로 이를 절대 넘지 않는 가장 큰 숫자를 곱한다는 것이다.🎈 가장 쉽고 빠른 방법은 cvtColor(src, dst, COLOR_BGR2GRAY)를 사용하는 것이다. 이 방법은 CPU 코어를 다 사용하는 것이고, for문을 하면 단일 코어를 사용하는 방법이다.색 공간 변환(color space)영상 처리에서는 특정한 목적을 위해 RGB 색 공간을 Gray, HSV, YCrCb, Lab 등의 다른 색 공간으로 변환하여 처리하는 것을 말한다. 변환하는 방식은 이 사이트를 참고 하면 된다.영상 처리에서는 RGB보다는 HSV나 YCrCb를 많이 사용하고, 논문 같은 곳들을 보면 Lab도 사용한다. 디스플레이 용도로는 RGB를 사용한다. 색 공간 변환 함수void cvtColor(InputArray src, OutputArray dst, int code, int dstCn = 0); src,dst : 입력, 출력 영상 code : 색 변환 코드, 참고 COLOR_BGR2GRAY / COLOR_GRAY2BGR : BGR &amp;lt;-&amp;gt; GRAY COLOR_BGR2RGB / COLOR_RGB2BGR : BGR &amp;lt;-&amp;gt; RGB COLOR_BGR2HSV / COLOR_HSV2BGR : BGR &amp;lt;-&amp;gt; HSV COLOR_BGR2YCrCb / COLOR_YCrCb2BGR : BGR &amp;lt;-&amp;gt; YCrCb dstCn : 결과 영상의 채널 수, 0이면 자동 결정된다. bmp파일 - B.File.Header - B.I.H - Palette ( RGBQUAD ==&amp;gt; BGR 순서) - Pixel jpg의 경우 압축을 풀 경우 IJPG이라는 라이브러리를 사용한다. 이는 RGB순서로 되어 있다.RGB 색 공간빛의 삼원색인 R,G,B를 혼합하여 색상을 표현한다. e.g. TV/모니터, 카메라 센서 Bayer 필터, 비트맵RGB 색 공간에서는 두 가지의 색에 대한 좌표상의 거리를 따진다고 하면, 한 점을 기준으로 특정 반지름을 가진 구를 지나는 점들은 다 같다고 계산된다.HSV 색 공간 Hue : 색상, 색의 종류 Saturation : 채도, 색의 탁하고 선명한 정도 Value : 명도, 빛의 밝기HSV 값 범위 CV_8U 영상의 경우 0 &amp;lt;= H &amp;lt;= 179 0 &amp;lt;= S &amp;lt;= 255 0 &amp;lt;= V &amp;lt;= 255 무지개 색을 표현하기에 적합한 방법이다.hue의 경우 각도로 표현하는 방법을 사용하기 때문에 [0,360)도로 표현이 될 수 있다. 그러나 HSV는 uchar로 표현되는데, 이는 0~255 사이에 존재해야 한다. 그래서 이렇게 적용하기 위해 H/2로 사용하는 것이다. 그래서 빨간색을 보자면 만약 빨간색이 0 ~ 30도 또는 330 ~ 360도라고 하면 범위를 다음과 같이 표현할 수 있다.` 0 &amp;lt;= H &amp;lt;= 15 or 165 &amp;lt;= H &amp;lt; 180` 가 될 것이다.v의 경우 맨 아래가 0이고 ,S의 경우 중앙이 0이다. 그리고 빨간색이라 하면 S와 V도 어느정도 커야 빨간색이라는 것을 인지할 수 있다.YCrCb(YCbCr, YUV)HSV는 원색들을 비교하기 좋지만, 세상에는 원색이 아닌 것이 더 많기 때문에 YCrCb를 사용하기도 한다. YCrCb란 PAL,NTSC,SECAM 등의 컬러 비디오 표준에 사용되는 색 공간이다. 영상의 밝기 정보와 색상 정보를 따로 분리하여 부호화한다. Y가 그레이스케일을 표현하기 위한 것이다. 그렇기에 YCrCb는 흑백 TV 호환이 된다. Y : 밝기 정보(iuma) Cr, Cb : 색차(색상 성분)(Chroma) Cr : red에 대한 색차 Cb : blue에 대한 색차 YCrCb 값 범위CV_8U 영상의 경우 0 &amp;lt;= Y,Cr,Cb &amp;lt;= 2551번째 그림의 경우 CbCr 평면이다. 또한, Y = 128로 고정한 그림이다.여기서는 거리 계산시에 y성분은 무시한 채로 (Cr,Cb) 에 대한 distance를 본다. 이것도 거리 계산에 애매한 부분이 있으나 RGB보다는 훨씬 좋다. 채널 분리void split(const Mat&amp;amp; src, Mat* mvbegin);void split(InputArray src, OutpuqArrayofArrays mv); src : 입력, 다채널 행렬 mv 합치기void merge()BGR, HSV, YCrCb 각각의 채널을 분리해보고 살펴보았다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;void split_bgr();void split_hsv();void split_ycrcb();int main(){ for (auto i=0; i&amp;lt;3;++i) { if (i ==0) split_bgr(); else if (i == 1) split_hsv(); else split_ycrcb(); }}void split_bgr(){ Mat src = imread(&quot;candies.jpg&quot;, IMREAD_COLOR); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return; } vector&amp;lt;Mat&amp;gt; bgr_planes; split(src, bgr_planes); imshow(&quot;src&quot;, src); imshow(&quot;B&quot;, bgr_planes[0]); imshow(&quot;G&quot;, bgr_planes[1]); imshow(&quot;R&quot;, bgr_planes[2]); waitKey();}void split_hsv(){ Mat src = imread(&quot;candies.jpg&quot;, IMREAD_COLOR); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return; } cvtColor(src, src, COLOR_BGR2HSV); vector&amp;lt;Mat&amp;gt; hsv_planes; split(src, hsv_planes); imshow(&quot;src&quot;, src); imshow(&quot;H&quot;, hsv_planes[0]); imshow(&quot;S&quot;, hsv_planes[1]); imshow(&quot;v&quot;, hsv_planes[2]); waitKey(); }void split_ycrcb(){ Mat src = imread(&quot;candies.jpg&quot;, IMREAD_COLOR); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return; } cvtColor(src, src, COLOR_BGR2HSV); vector&amp;lt;Mat&amp;gt; ycrcb_planes; split(src, ycrcb_planes); imshow(&quot;src&quot;, src); imshow(&quot;Y&quot;, ycrcb_planes[0]); imshow(&quot;cr&quot;, ycrcb_planes[1]); imshow(&quot;cb&quot;, ycrcb_planes[2]); waitKey();}첫번째로 BGR에 대한 채널 분리이다. RGB 의 경우 각각에 해당하는 색들이 높게 나온다.두번째로 HSV에 대한 채널 분리이다. HSV 의 경우 H에서 빨간색은 0보다 조금 더 큰 것일 수도 있고, 360도에 가까운 값을 수도 있기에 크게 나올수도 있고, 작게 나올 수도 있다. 그리고는 빨주노초파남보로 갈수록 밝아진다. S나 V에서 되게 밝은 부분들이 있지만, H는 대체로 어둡다. 왜냐하면 H는 최대가 179이기 때문에 좀 어둡고, S,V는 최댓값이 255이기 때문이다. S는 사진이 거의 원색이므로 다 크게 나온다. V는 grayscale에 가까운 화면이다.세번째로 YCrCb에 대한 채널 분리이다. YCrCb 의 경우 Y는 순수하게 grayscale이고, 나머지는 크게 중요하지 않다. 단지 Cr은 red에 대한 것이므로 red가 높게 나오고, Cb는 blue에 대한 색차이므로 blue가 높게 나올 것이다.보통은 HSV, YCrCb로 변환을 해서 분리한다. V나 Y는 밝기 정보이다.추가적으로 YCrCb에서 CrCb를 합친 것을 추출하는 코드를 실행하여 살펴보았다.void split_ycrcb_merge(){ Mat src = imread(&quot;candies.jpg&quot;, IMREAD_COLOR); cvtColor(src, src, COLOR_BGR2HSV); Mat cpsrc; src.copyTo(cpsrc); vector&amp;lt;Mat&amp;gt; ycrcb_planes; split(cpsrc, ycrcb_planes); Mat gray = ycrcb_planes[0].clone(); ycrcb_planes[0] = 128; Mat dst; merge(ycrcb_planes, dst); imshow(&quot;src&quot;, src); imshow(&quot;gray&quot;, gray); imshow(&quot;dst&quot;, dst); waitKey();} White balance(색 온도) 조절에 따른 색 차이사진을 색 온도에 따라서 따듯해보이기도, 차가워보이기도 한다. 이것은 Cr과 Cb의 차이로 인한 것인데, 이 둘을 grayscale로 변환하면 동일하게 나온다. 카메라를 사용할 때 화질의 측면에서 3가지 알고리즘이 들어 있다. Auto W.B,Auto Focus,Auto Expo 각각 색온도 조절, 포커스 조절, 노출 조절이다. 이 때 색온도가 잘못되면 그 컬러 사진의 색상에 대해 모든 값이 잘못되어 있을 수 있다.그래서 사진의 정보를 조금 수정하고 싶다면 YCrCb 중에서 Y값만 변경해준다.컬러 영상을 YCrCb 각 채널을 분리하여 표현해보면 Cr과 Cb는 디테일이 표현되지 않는다. 그래서 Cr,Cb 영상의 크기를 가로1/2, 세로 1/2로 줄이고, DCT(discrete cosine transform)를 통해 압축하면 저주파 성분이 크게 나오고, 고주파는 작게 나오기 때문에 0으로 나온다. 그래서 0은 버리고, 값이 있는 것만 가지고 가기 때문에 jpg가 압축률이 좋은 것이다.컬러 영상의 히스토그램 평활화직관적 방법 : R,G,B 각 색 평면에 대해 히스토그램 평활화R,G,B 각각을 히스토그램 평활화하여 합칠 수 있다. 그러나 이를 수행하면 이상하게 나온다.equalizeHist(bgr[0], bgr[0]);equalizeHist(bgr[1], bgr[1]);equalizeHist(bgr[2], bgr[2]);이유는 각각의 색상의 그래프는 다 비율이 다르기 때문이다.따라서 밝기 성분에 대해서만 히스토그램 평활화를 수행해야 한다.즉, YCrCb에서 Y만 명암비를 조절해줘야 한다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;lenna.bmp&quot;); Mat src_ycrcb; cvtColor(src, src_ycrcb, COLOR_BGR2YCrCb); vector&amp;lt;Mat&amp;gt; planes; split(src_ycrcb, planes); equalizeHist(planes[0], planes[0]);// planes[0] += 50; // y값 증가 Mat dst_ycrcb; merge(planes, dst_ycrcb); Mat dst; cvtColor(dst_ycrcb, dst, COLOR_YCrCb2BGR); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();} 리매핑(remapping)영상의 특정 위치 픽셀을 다른 위치에 재배치하는 일반적인 프로세스를 의미한다.dst(x,y) = src(mapx(x,y),mapy(x,y))어파인 변환, 투시 변환을 포함한 다양한 변환을 리매핑으로 표현이 가능하다. 이동 변환mapx(x,y) = x’ - 200mapy(x,y) = y’ - 100중요한 것은 우항의 x’와 y’는 출력 영상에서의 좌표이고, mapx가 반환하는 값은 입력 영상의 x, mapy가 반환하는 값은 입력 영상의 y이다.쉽게 표기하면x = x’ - 200y = y’ - 100 상하 대칭mapx(x,y) = xmapy(x,y) = h - 1 - v (h는 영상의 세로, v는 가로 크기) 크기 변환mapx(x,y) = x / 2mapx(x,y) = y / 2 리매핑 함수void remap(InputArray src, OutputArray dst, InputArray map1, InputArray map2, int interpolation, int borderMode = BORDER_CONSTANT, const Scalar&amp;amp; borderValue = Scalar()); src : 입력 영상 dst : 결과 영상, 이는 map1과 크기와 타입이 같아야 함 map1 : 결과 영상의 각 픽셀이 참조할 입력 영상의 (x,y) 좌표 또는 x좌표를 담고 있는 행렬 CV_16SC2, CV_32FC2, CV_32FC1 map2 : 결과 영상의 (x,y) 좌표가 참조할 입력 영상의 y좌표를 담고 있는 행렬, CV_16UC1, CV_32F1 interpolation : 보간법 INTER_LINEAR INTER_CUBIC borderMode : 가장자리 픽셀 확장 방식 borderValue : BORDER_CONSTANT일 때 사용할 상수 값 코드 구현호수 사진을 불러와 이 사진에 대한 기하학적 변환을 수행하는 코드#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;tekapo.bmp&quot;); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return -1; } int w = src.cols; // 가로 int h = src.rows; // 세로 크기 Mat map1 = Mat::zeros(h, w, CV_32FC1); Mat map2 = Mat::zeros(h, w, CV_32FC1); for (int y = 0; y &amp;lt; h; y++) { for (int x = 0; x &amp;lt; w; x++) { map1.at&amp;lt;float&amp;gt;(y, x) = (float)x; // 가로는 그대로 // 1. 상하 대칭 //map2.at&amp;lt;float&amp;gt;(y, x) = (float)h - 1 - y; // 상하 대칭 // 2. sin함수 모양으로 울퉁불퉁하게 map2.at&amp;lt;float&amp;gt;(y,x) = (float)y + 10 * sin(x / 32.f); } } Mat dst; remap(src, dst, map1, map2, INTER_LINEAR); //remap(src, dst, map1, map2, INTER_LINEAR, BORDER_DEFAULT); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();} 상하 대칭 sin파형여기서 빈 공간에 검정색부분을 바꿀 수 있다.remap(src, dst, map1, map2, INTER_LINEAR, BORDER_DEFAULT);default를 하면, 영상의 가장자리를 0이 아닌 픽셀값이 가장자리에서 대칭적으로 발생하다고 판단해서 자동으로 채워진다. 영상 확대영상을 확대해서 보고 싶은 경우 map과 for문을 수정해야 한다. Mat map1 = Mat::zeros(h*2, w*2, CV_32FC1); // 입력 영상의 2배 크기로 생성 Mat map2 = Mat::zeros(h*2, w*2, CV_32FC1); // 입력 영상의 2배 크기로 생성 for (int y = 0; y &amp;lt; h*2; y++) { for (int x = 0; x &amp;lt; w*2; x++) { // 이는 출력 영상의 좌표이므로 *2를 해야 한다. map1.at&amp;lt;float&amp;gt;(y, x) = (float)x/2; // 좌항이 입력, 우항이 출력 좌표이므로 확대하려면 출력을 /2해야 한다. map2.at&amp;lt;float&amp;gt;(y, x) = (float)y/2; } }" }, { "title": "[데브코스] 6주차 - OpenCV Image Geometric Transformation", "url": "/posts/opencv3/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-22 14:01:00 +0900", "snippet": "컬러 영상 처리의 기초컬러 영상의 픽셀 값 참조 OpenCV에서 컬러 영상 표현 방법 빨강,초록, 파랑 색 성분을 256단계로 표현 opencv에서는 RGB순서가 아니라 BGR순서임 OpenCV에서 컬러 영상 다루기Mat img1 = imread(&quot;lenna.bmp&quot;,IMREAD_COLOR);Mat img2(rows,cols, CV_8UC3); // 3 channelsMat img3 = imread(&quot;lenna.bmp&quot;,IMREAD_GRAYSCALE);Mat img4;cvtColor(img3,img4,COLOR_GRAY2BGR); // 눈으로 보기에는 동일하게 회색으로 나오게 된다. 이 때는 B=G=R 값이 같기 때문이다. 그러므로 1픽셀당 3byte를 차지하고 있다.circle(src,Point(200,200), 100, Scalar(255,0,0),3); // grayscale이므로 밝기가 된다. 그래서 색이 아닌 밝기가 255circle(dst,Point(200,200), 100, Scalar(255,0,0),3); // truecolor이므로 bgr순서의 스칼라값이 된다. 그래서 파란색Mat src = imread(&quot;lenna.bmp&quot;);Mat dst = 255 - src; // 이렇게 하면 파란색으로 된다.이 결과는 올바르지 않다. 이 코드는 Mat dst = Scalar(255,0,0,0) -src;로 인식되므로 blue성분만 처리된다.따라서 아래와 같이 3채널로 지정해줘야 반전이 된다.Mat dst = Scalar(255,255,255) - src; 반전을 직접 구현하면 다음과 같다.Mat dst(src.rows, src.cols, CV_8UC3);for (int y = 0; dst.rows; y++) { for (int x = 0; x &amp;lt; dst.cols; x++) { Vec3b&amp;amp; p1 = src.at&amp;lt;Vec3b&amp;gt;(y, x); Vec3b&amp;amp; p2 = dst.at&amp;lt;Vec3b&amp;gt;(y, x); p2[0] = 255 - p1[0]; //Blue p2[1] = 255 - p1[1]; //Green p2[2] = 255 - p1[2]; //Red }}여기서 중요한 것은 원래는 uchar이었던 자료형대신 Vec3b 자료형을 사용해야 한다. 전체 구현/* 1번째 방법 */Vec3b&amp;amp; p1 = src.at&amp;lt;Vec3b&amp;gt;(y, x);Vec3b&amp;amp; p2 = dst.at&amp;lt;Vec3b&amp;gt;(y, x);p2[0] = 255 - p1[0]; //Bluep2[1] = 255 - p1[1]; //Greenp2[2] = 255 - p1[2]; //Red/* 2번째 방법 */Vec3b&amp;amp; p1 = src.at&amp;lt;Vec3b&amp;gt;(y, x);Vec3b&amp;amp; p2 = dst.at&amp;lt;Vec3b&amp;gt;(y, x);p2 = Vec3b(255, 255, 255) - p1;/* 3번째 방법 */dst.at&amp;lt;Vec3b&amp;gt;(y, x) = Vec3b(255, 255, 255) - src.at&amp;lt;Vec3b&amp;gt;(y, x);이 3가지 방법은 다 동일하게 동작한다.이동 변환과 전단 전환 강체변환(Rigid-body) : 크기 및 각도가 보존되는 변환 (translation,rotation) 유사변환(similarity) : 크기는 변하고 각도는 보존되는 변환 (scaling) 선형변환(linear) : vector공간에서의 이동 affine : 선형변환과 이동변환까지 포함, 선의 수평선은 유지 perspective : affine 변환에 수평성도 유지되지 않음, 원근 변환영상의 기하학적 변환(geometric transformation)영상을 구성하는 픽셀의 배치 구조를 변경함으로써 전체 영상의 모양을 바꾸는 작업이다. 전처리 작업, 영상 정합(image registration), 왜곡 제거 등이 이에 해당한다.영상의 이동 변환이동 변환(translation transform) 가로 또는 세로 방향으로 영상을 특정 크기만큼 이동시키는 변환 x축과 y축 방향으로의 이동 변위를 지정할 수 있다.출력 영상을 (x’, y’)이라 할 때 x’ = x + a y’ = y + b이를 행렬로 표기하면 다음과 같다.아래 행렬에서 1은 행렬의 수식을 위해 존재하는 추가항이다. 이 때, 2x3 행렬을 2x3 어파인 변환 행렬이라 한다. 코드 구현Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);Mat dst = Mat::zeros(src.size(), CV_8UC1);for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { int x_ = x + 100; int y_ = y + 100; if (x_ &amp;lt; 0 || x_ &amp;gt;= dst.cols) continue; if (y_ &amp;lt; 0 || y_ &amp;gt;= dst.rows) continue; dst.at&amp;lt;uchar&amp;gt;(y_, x_) = src.at&amp;lt;uchar&amp;gt;(y, x); } }그레이 스케일 영상을 (100,100) 이동하는 변환 프로그램을 나타낸 것이다. 이중 for 루프를 이용하여 직접 구현했다. 이를 구현한 이유는 OpenCV에서는 이동 변환이나 전단 변환에 대한 함수가 따로 지원되지 않아서 알고리즘을 직접 구현하거나 행렬을 직접 생성해서 warpAffine을 시킬 수 있다.Mat trans = (Mat_&amp;lt;float&amp;gt;(2, 3) &amp;lt;&amp;lt; 1, 0, 100, 0, 1, 100); // (1 0 100; 0 1 100) ==&amp;gt; 가로 100, 세로 100 이동하는 이동 변환 행렬Mat dst; warpAffine(src, dst, trans, Size()); 행렬(trans)에서 1대신 0.5씩 주면 1/2배 축소되어 출력되는 것을 볼 수 있다.Mat trans = (Mat_&amp;lt;float&amp;gt;(2, 3) &amp;lt;&amp;lt; 0.5, 0, 100, 0, 0.5, 100); Mat dst; warpAffine(src, dst, trans, Size(700,700)); // 사이즈를 키워서 하게 되면 이동을 하면서 사라진 부분까지 볼 수 있다. Size(700,700)이를 통해 크기 변환과 이동 변환을 함께 줄 수 있다.영상의 전단 변환(shear transformation)직사각형 형태의 영상을 한쪽 방향으로 밀어서 평행사변형 모양으로 변형되는 변환, 층밀림 변환이라고도 한다. 가로 방향 또는 세로 방향으로 정의된다.첫번째는 x좌표가 이동하는 형태를 통해 변형되는 것이고, 두번째는 y좌표가 변형되는 것이다. 코드 구현Mat dst(src.rows * 3 / 2, src.cols, src.type(), Scalar(0));double m = 0.5;for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { int nx = x; int ny = int(y + m*x); dst.at&amp;lt;uchar&amp;gt;(ny, nx) = src.at&amp;lt;uchar&amp;gt;(y, x); }}코드를 자세히 살펴보게 되면, 먼저 dst의 크기를 원래 크기의 1.5배 해줌으로써 나중에 예외처리를 하지 않아도 되도록 또는 이미즹 변형된 형태를 보기 위해서 1.5배 해주었다. x, 즉 rows방향만 변형되기 때문에, cols에 대해서는 크기를 늘려주지 않은 것을 볼 수 있다.그 후 for 루프를 도는데, (y+m*x)에서 m이 0.5이므로 최대 원본의 1.5배까지 커지도록 되어 있음을 알 수 있다.마지막으로 결과 영상의 픽셀 값을 입력 영상의 픽셀 값으로 집어 넣어주면 된다.영상의 크기 변환(scale transform)영상의 크기를 원보 영상보다 크게 또는 작게 만드는 변환을 말한다. x축과 y축 방향으로의 스케일 비율(scale factor)를 지정할 수 있다.이 때, scale factor을 보면 Sx = w&#39;/w,Sy = h&#39;/h 로 표현된다. 코드 구현Mat dst = Mat::zeros(src.rows * 2, src.cols * 2, CV_8UC1);for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { int x_ = x * 2; int y_ = y * 2; dst.at&amp;lt;uchar&amp;gt;(y_, x_) = src.at&amp;lt;uchar&amp;gt;(y, x); }}단순히 2배로 키우게 되면 중간중간에 검정색이 존재하게 된다. 위의 코드를 순방향 매핑(foreward mapping)이라고 하는데, 입력 영상의 좌표만큼을 for문을 돌면서 입력 영상의 좌표값을 출력 영상의 좌표값으로 설정하는 코드를 맗한다. 이 경우 채워지지 않은 픽셀이 존재하게 된다. 이 문제를 해결하기 위해 역방향 매핑을 진행한다.역방향 매핑(backward mapping)순방향 매핑의 경우 x’ = Sx * x y’ = Sy * y그러나 역방향 매핑의 경우 x = x’ / Sx y = y’ / Sy를 통해 결과 영상의 좌표(x’,y’)에서 원본 영상의 좌표(x,y)를 참조하도록 만든다. 이를 코드로 구현하면 다음과 같다.Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);Mat dst = Mat::zeros(src.rows * 2, src.cols * 2, CV_8UC1);for (int y_ = 0; y_ &amp;lt; dst.rows; y_++) { for (int x_ = 0; x_ &amp;lt; dst.cols; x_++) { int x = x_ / 2; int y = y_ / 2; dst.at&amp;lt;uchar&amp;gt;(y_, x_) = src.at&amp;lt;uchar&amp;gt;(y, x); }}픽셀값들이 계단형태로 나오고 있다. 그 이유는 scale factor이 4라고 가정할 때, x_ = 0,1,2,3 일 때는 x=0 인 픽셀값을 참조하고, x_ = 4,5,6,7일 때는 x=1인 픽셀값을 참조하기 때문이다. 이 문제를 해결하기 위해 보간법을 사용한다.보간법역방향 매핑에 의한 크기 변환 시, 참조해야 할 입력 영상의 (x,y) 좌표가 실수 좌표일 경우 (x,y)와 가장 가까운 정수 좌표의 픽셀 값을 참조하거나 (x,y) 근방의 정수 좌표 픽셀 값을 이용하여 실수 좌표 위치의 픽셀 값을 추정해야 한다.보간법은 실수 좌표 상에서의 픽셀 값을 결정하기 위해 주변 픽셀 값을 이용하여 값을 추정하는 방법을 말한다.주요 보간법 최근방 이웃 보간법(nearest neighbor interpolation) 양선형 보간법(bilinear interpolation) 3차 보간법(cubic interpolation) 스플라인 보간법(spline interpolation) 란쵸스 보간법(lanczos interpolation)가장 단순한 방법은 최근방 이웃 보간법이다. 즉 가장 가까운 위치에 있는 픽셀의 값을 참조하는 방법을 말한다. 장점은 빠르고 구현하기 쉽다. 단점으로는 계단 현상이 발생한다. 방금 우리가 해봤던 방법이 이에 해당한다.이 계단 현상을 해결하기 위해 양선형 보간법을 많이 사용한다. 실수 좌표를 둘러싸고 있는 네 개의 픽셀 값에 가중치를 곱한 값들의 선형 합으로 결과 영상의 픽셀 값을 구하는 방법이다. 최근방 이웃 보간법에 비해서는 느리지만 비교적 빠르며 계단 현상이 크게 감소한다. 양선형 보간법 구현 방법실수 좌표를 둘러싸고 있는 네 개의 픽셀 값을 이용해야 한다. 예를 들어 점(double p,double q)가 있다고 하고, 이를 (10.3,20.5) 에 해당한다고 하면이를 둘러싼 4개의 픽셀 값은 (10,20),(11,20),(10,21),(11,21) 이 될 것이다.a,b,c,d는 픽셀 값의 크기를 의미한다. 이를 통해 x는 (1-p) * a 와 p * b 를 더하면 된다. x = (1-p)a + pb y = (1-p)c + pd z = (1-q)x + qyz는 보간법을 통해 얻은 해당 픽셀의 픽셀 값이다. 따라서 우리는 z만 쓰면 된다. 코드 구현void resizeBilinear(const Mat&amp;amp; src, Mat&amp;amp; dst, Size size){ dst.create(size.height, size.width, CV_8U); int x1, y1, x2, y2; double rx, ry, p, q, value; double sx = static_cast&amp;lt;double&amp;gt;(src.cols - 1) / (dst.cols - 1); // 가로 방향 scale factor double sy = static_cast&amp;lt;double&amp;gt;(src.rows - 1) / (dst.rows - 1); // 세로 방향 scale factor for (int y = 0; y &amp;lt; dst.rows; y++) { for (int x = 0; x &amp;lt; dst.cols; x++) { rx = sx * x; ry = sy * y; x1 = cvFloor(rx); y1 = cvFloor(ry); x2 = x1 + 1; if (x2 == src.cols) x2 = src.cols - 1; y2 = y1 + 1; if (y2 == src.rows) y2 = src.rows - 1; p = rx - x1;q = ry - y1; value = (1. - p) * (1. - q) * src.at&amp;lt;uchar&amp;gt;(y1, x1) // src.at&amp;lt;uchar&amp;gt;(y1, x1) == x1,y1에서의 픽셀 값, a + p * (1. - q) * src.at&amp;lt;uchar&amp;gt;(y1, x2) // b + (1. - p) * q * src.at&amp;lt;uchar&amp;gt;(y2, x1) // c + p * q * src.at&amp;lt;uchar&amp;gt;(y2, x2); // b dst.at&amp;lt;uchar&amp;gt;(y, x) = static_cast&amp;lt;uchar&amp;gt;(value + .5); } }}코드에서 (x1,y1),(x2,y2)는 아래 그림을 참고하면 된다.int main(void){ Mat src = imread(&quot;camera.bmp&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return; } Mat dst; resizeBilinear(src, dst, Size(1024, 1024)); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}이 코드를 실행하게 되면 아래와 같다. 아래는 1024,1024 크기로 키운 결과이다. 3차 보간법(bicubic interpolation)방금까지는 2차원 보간법을 진행했다. 그러나 3차 즉 3차원의 픽셀값을 계산하는 방법이다. 그래서 총 16개의 픽셀 값에 3차 함수를 이용한 가중치를 부여하여 결과 영상 픽셀의 값을 계산한다.이는 그냥 직접 구현하기보다 OpenCV에서 제공하는 함수를 사용해볼 것이다.위의 코드들은 알고리즘을 직접 구현했다. 그러나 OpenCV의 resize()라는 함수를 통해 쉽게 사용할 수 있다.void resize(InputArray src, OutputArray dst,Size dsize, double fx = 0, double fy = 0, int interpolation = INTER_LINEAR); src,dst : 입력, 출력 영상 dsize : 결과 영상의 크기, Size()로 지정하면 fx,fy에 의해 자동 결정된다. fx,fy : x,y방향 스케일 비율, dsize값이 0일 때 유효하다. interpolation : 보간법 지정 상수 INTER_NEAREST : 최근방 이웃 보간법 INTER_LINEAR : 양선형 보간법 (2x2 이웃 픽셀 참조) INTER_CUBIC : 3차회선 보간법 (4x4 이웃 픽셀 참조) INTER_LANCZOS4 : Lanczos 보간법 (8x8 이웃 픽셀 참조) INTER_AREA : 영상 축소 시 효과적인 방법 Mat src = imread(&quot;rose.bmp&quot;);Mat dst1, dst2, dst3, dst4;resize(src, dst1, Size(), 4, 4, INTER_NEAREST); // 이를 1번더 한 이유는 이미지를 불러올 때 시간이 걸려서 정확한 판단이 잘 안될 수 있기 때문에 미리 1번 불러와서 정확한 시간을 측정하기 위함tm.start();resize(src, dst1, Size(), 4, 4, INTER_NEAREST);tm.stop();cout &amp;lt;&amp;lt; &quot;INTER_NEAREST: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms. &quot; &amp;lt;&amp;lt; endl;tm.reset(); // reset을 하지 않을 경우 뒤에 측정한 것이 앞에 것과 합쳐져 출력된다.tm.start();resize(src, dst2, Size(1920, 1280));tm.stop();cout &amp;lt;&amp;lt; &quot;INTER_LINEAR: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms. &quot; &amp;lt;&amp;lt; endl;tm.reset();tm.start();resize(src, dst3, Size(1920, 1280), 0, 0, INTER_CUBIC);tm.stop();cout &amp;lt;&amp;lt; &quot;INTER_CUBIC: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms. &quot; &amp;lt;&amp;lt; endl;tm.reset();tm.start();resize(src, dst4, Size(1920, 1280), 0, 0, INTER_LANCZOS4);tm.stop();cout &amp;lt;&amp;lt; &quot;INTER_LANCZOS4: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms. &quot; &amp;lt;&amp;lt; endl;imshow(&quot;src&quot;, src);imshow(&quot;dst1&quot;, dst1(Rect(400, 500, 400, 400)));imshow(&quot;dst2&quot;, dst2(Rect(400, 500, 400, 400)));imshow(&quot;dst3&quot;, dst3(Rect(400, 500, 400, 400)));imshow(&quot;dst4&quot;, dst4(Rect(400, 500, 400, 400)));waitKey();총 4가지의 방법으로 진행해서 비교해보았다. dst1,2,3,4 다 동일한 크기로 resize된다. dst1 : fx,fy를 지정해줌으로써 size가 알아서 결정된다. dst2 : size를 지정해주어서 fx,fy를 지정해주지 않았다. 그리고 interpolation은 default가 INTER_LINEAR이므로 지정하지 않을 경우 INTER_LINEAR 방식을 사용한다. dst3 : INTER_CUBIC dst4 : INTER_LANCZOS4그리고 출력 영상이 너무 크기 때문에 특정 크기만큼 잘라서 보고자 했다. (400,500)위치에서 가로 400, 세로 400 크기만큼만 화면에 출력하도록 했다.이와 같이 비교해보면 3,4가 화질이 좋다. 그러나 연산량을 비교해보면 3번이 2번보다 1.5배 정도 더 걸리고, 4번의 경우 3번의 2배 정도의 시간이 더 걸린다. 영상의 축소 시 고려할 사항 한 픽셀로 구성된 성분들은 영상을 축소할 때 사라지는 경우가 발생할 수 있다. 입력 영상을 부드럽게 필터링(블러링)한 후 축소하거나 다단계 축소를 권장한다. OpenCV의 resize() 함수에서는 INTER_AREA 플래그를 사용하면 축소할 때 발생하는 화질의 열화를 방지할 수 있다. int main(void){ Mat src(1280, 1280, CV_8UC3, Scalar(255, 255, 255)); rectangle(src,Rect(600,600,100,100),Scalar(0,0,0)); rectangle(src, Rect(500, 630, 50, 50), Scalar(0, 0, 0)); Mat kernel = Mat::ones(3, 3, CV_32FC1) / 9.f; Mat dst1, dst2, dst3, dst4; GaussianBlur(src, dst3, Size(), 1.0); GaussianBlur(src, dst4, Size(), 1.0); resize(src, dst1, Size(320, 320),INTER_AREA); resize(src, dst2, Size(320, 320),INTER_LINEAR); resize(src, dst3, Size(320, 320), INTER_LINEAR); resize(src, dst4, Size(320, 320), INTER_AREA); imshow(&quot;src&quot;, src); imshow(&quot;dst1&quot;, dst1); imshow(&quot;dst2&quot;, dst2); imshow(&quot;dst3&quot;, dst3); imshow(&quot;dst4&quot;, dst4); waitKey();}흰색 배경에 검은색 두께 1인 사각형 2개를 그리고, 축소를 시켰더니 선들이 사라지는 것을 볼 수 있다. 나의 경우에는 INTER_AREA를 사용하거나, 블러링하고 축소를 해도 사각형이 사라지는 현상이 발생했다.영상의 회전 변환(rotation transform)영상을 특정 각도만큼 회전시키는 변환이다. OpenCV에서는 반시계 방향을 기본으로 사용한다.노란색이 입력 영상, 초록색이 회전된 영상을 표현한다.이 때, 회전했을 때 y가 0보다 작으면 오류가 나므로 회전한 좌표가 유효한 좌표인지 검사를 하는 것이 중요하다. 또한, 정방향 매핑이 아닌 역방향 매핑을 해야 빈 픽셀이 발생하지 않는다.입력 영상은 출력 영상과 역행렬을 곱한 것으로 표현할 수 있다.추가적으로 보간법도 선택하여 진행한다. 함수 - 영상의 회전 변환 행렬Mat getRotationMatrix2D(Point2f center, double angle, double scale); center : 회전 중심 좌표 angle : (반시계 방향) 회전 각도(degree), 음수는 시계 방향 scale : 회전 후 확대 비율 반환값 : 2x3 double(CV_64F) 행렬 (어파인 변환 행렬)이 때, center 좌표를 단순히 0,0으로 두려면 Point2f pt;로 할 수 있지만, 대체로 영상 중심의 좌표를 center로 두기 때문에Point2f pt(src.cols/2.f, src.rows/2.f)주의해야 할 것은 cols==y가 먼저 나온다는 것이다.반환하는 행렬은 다음과 같다. 함수 - 어파인 변환void warpAffine(InputArray src, OutputArray dst, InputArray M, Size dsize, int flags= INTER_LINEAR, int borderMode = BORDER_CONSTANT, const Scalar&amp;amp; borderValue = Scalar()); src,dst : 입력, 출력 영상, 두 개는 같은 타입이어야 함 M : 2x3 어파인 변환 행렬, CV_32F or CV_64F, 위의 회전 변환 행렬의 반환값을 여기에 입력 dsize : 결과 영상의 크기, 입력 영상과 동일하게 하려고 하면 Size()로 지정 flags : 보간법 선택 borderMode : 가장자리 픽셀 처리 방식 borderValue : BORDER_CONSTANT 모드 사용 시 사용할 픽셀 값실제로 회전을 시키기 위해서는 이 함수를 선언해야 한다. 코드 구현Mat src = imread(&quot;lenna.bmp&quot;);float degree = 50;Point2f pt(src.cols / 2.f, src.rows / 2.f);Mat rot = getRotationMatrix2D(pt, degree, 1.0);Mat dst;warpAffine(src, dst, rot, Size(700,700)); // 512,512 를 700,700으로 키워서 잘리는 부분까지 추출imshow(&quot;dst&quot;, dst);waitKey();이동, 크기, 회전 변환 조합 크기 -&amp;gt; 회전 변환할 경우Sx,Sy 는 크기에 대한 scale factor, 그 앞에 있는 2x2 행렬이 회전에 대한 변환이다. 뒤에서부터 계산하여 출력 영상을 만들어낸다. 크기 -&amp;gt; 회전 -&amp;gt; 크기 변환할 경우크기 변환에 대한 변환을 1번 더 진행한다. 이 3개를 하나의 2x2 행렬로 표현이 가능하다. 이동 -&amp;gt; 크기 변환할 경우이동 변환을 먼저 수행하기 때문에 x방향으로 a, y방향으로 b만큼을 이동한 결과를 크기 변환한다. 이동 -&amp;gt; 크기 -&amp;gt; 회전 변환할 경우이 경우에도 2x2행렬 * [x,y] + 2x1행렬 로 나타낼 수 있는데, 덧셈이 들어가게 되면 연산이 번거로워진다. 연산의 번거로움을 해결하기 위해 동차 좌표계를 사용한다.동차 좌표계(homogenous coordinates)차원의 좌표를 1차원 증가시켜서 표현하는 방법을 말한다. 예를 들어 2차원(x,y)좌표를 (x,y,1)로 표현할 수 있다. 이를 통해 각각의 변환들을 표현할 수 있다.이처럼 오른쪽 식으로 변환할 수 있다. 이 때 마지막 항은 항상 1이다.동차 좌표계를 활용하여 이동 변환 -&amp;gt; 크기 변환 -&amp;gt; 회전 변환을 수행한다면 아래 식처럼 쓸 수 있다.getRotationMatrix2D() 함수를 이용하여 영상의 중앙 기준 회전한다고 할 때, 한 이미지를 (-cx,cy)만큼 이동 변환하여 영상의 중심이 (0,0)에 오도록 한다. 그 후 이미지를 회전 변환하고, 다시 (cx,cy)만큼 이동 변환하여 제자리로 이동시킨다.이를 행렬로 나타내면 다음과 같다.이를 전개하면 위에서 봤던 getRotationMatrix2D() 함수의 반환값과 똑같은 형태가 된다.영상의 대칭 변환(flip,reflection)영상의 상하/좌우/원점 대칭이 있다. 상하/좌우 대칭의 경우 축을 기준으로 반전시킨 후 이동 변환을 통해 원래 자리로 되돌리는 것이다. 함수void flip(InputArray src, OutputArray dst, int flipCode); src, dst : 입력, 출력 영상 flipCode : 대칭 방향 지정 양수(+1) : 좌우 대칭 0 : 상하 대칭 음수(-1) : 원점 대칭 어파인 변환과 투시 변환어파인 변환(affine transform) vs. 투시 변환(perspective transform == projective transfrom)어파인 변환 translation shear scaling rotation parallelograms이 어파인 변환은 3x3행렬로 표현되지만, 마지막 열은 [0,0,1] 이므로 2x3 행렬(6 DOF == 6개의 미지수)이라고도 말할 수 있다.투시 변환 trapazoids이 투시 변환은 3x3행렬(8 DOF)로 표현된다.affine transform의 경우 직사각형이 평행사변형이 되므로 마지막 1점은 추론이 가능하다. 각 점마다 수식이 2개씩 나오게 되므로 총 6개의 수식만 알면 된다.perspective transform의 경우 점이 각각 변하기 때문에 4개 다 계산이 필요하다. 그래서 총 8개의 수식이 필요하다. 어파인 변환 점 행렬 구하기Mat getAffineTransform(const Point2f src[], const Point2f dst[]);Mat getAffineTransform(InputArray src, InputArray dst); src : 3개의 원본 좌표점(point2f src[3]; 또는 vector&amp;lt;Point2f&amp;gt; src;) dst : 3개의 결과 좌표점(point2f dst[3]; 또는 vector&amp;lt;Point2f&amp;gt; dst;) 반환값 : 2x3 크기의 변환 행렬 (CV_64F)원래의 좌표점src와 변환된 결과 좌표점dst를 지정하면 변환 행렬을 출력해준다.getRotationMatrix2D() 함수와 반환값이 같지만 이 함수는 회전 각도를 통해 어떻게 회전을 할지에 대한 함수이다. 영상의 어파인 변환void warpAffine(InputArray src, OutputArray dst, InputArray M, Size dsize, int flags= INTER_LINEAR, int borderMode = BORDER_CONSTANT, const Scalar&amp;amp; borderValue = Scalar()); src,dst : 입력, 출력 영상, 두 개는 같은 타입이어야 함 M : 2x3 어파인 변환 행렬, CV_32F or CV_64F, 위의 회전 변환 행렬의 반환값을 여기에 입력 dsize : 결과 영상의 크기, 입력 영상과 동일하게 하려고 하면 Size()로 지정 flags : 보간법 선택 borderMode : 가장자리 픽셀 처리 방식 borderValue : BORDER_CONSTANT 모드 사용 시 사용할 픽셀 값 투시 변환 행렬 구하기Mat getPerspectiveTransform(const Point2f src[], const Point2f dst[], int solveMethod = DECOMP_LU);Mat getPerspectiveTransform(InputArray src,InputArray dst, int solveMethod = DECOMP_LU); src : 4개의 원본 좌표점(Point2f src[4]; 또는 vector&amp;lt;Point2f&amp;gt; src;) dst : 4개의 원본 좌표점(Point2f dst[4]; 또는 vector&amp;lt;Point2f&amp;gt; dst;) solveMethod : 어떻게 계산할지에 대한 수학적 방법 반환값 : 3x3 크기의 변환 행렬 (CV_64F) 영상의 투시 변환void warpPerspective(InputArray src, OutputArray dst, InputArray M, Size dsize, int flags = INTER_LINEAR, int borderMode = BORDER_CONSTANT, const Scalar&amp;amp; borderValue = Scalar()); src,dst : 입,출력 영상, 둘은 같은 타입 M : 3x3 투시 변환 행렬, CV_32F 또는 CV_64F dsize : 결과 영상의 크기, 입력 영상과 동일하게 하려고 하면 Size()로 지정 flags : 보간법 선택 borderMode : 가장자리 픽셀 처리 방식 borderValue : BORDER_CONSTANT 모드 사용 시 사용할 픽셀 값추가적으로 4개의 대응점으로부터 투시 변환 행렬을 구하는 식에 대해 살펴보고자 한다.차선 영상의 조감도(bird’s eye view) 만들기bird’s eye view : 새가 하늘에서 내려다보듯이 매우 높은 곳에 위치한 카메라가 아래의 피사체를 찍은 화면을 말한다. 투시 변환을 이용하여 전면에서 촬영된 영상을 버드아이뷰처럼 변환할 수 있다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture cap(&quot;../../../data/test_video.mp4&quot;); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Video open failed!&quot; &amp;lt;&amp;lt; endl; return -1; } Mat src; while (true) { cap &amp;gt;&amp;gt; src; if (src.empty()) break; int w = 500, h = 260; vector&amp;lt;Point2f&amp;gt; src_pts(4); // 입력 영상에서의 점들의 좌표 4개 vector&amp;lt;Point2f&amp;gt; dst_pts(4); // 출력 영상에서의 점들의 좌표 4개 src_pts[0] = Point2f(474, 400); src_pts[1] = Point2f(710, 400); // 특정 위치에서 차선을 기준으로 한 사다리꼴의 점 좌표 src_pts[2] = Point2f(866, 530); src_pts[3] = Point2f(366, 530); dst_pts[0] = Point2f(0, 0); dst_pts[1] = Point2f(w - 1, 0); // 출력 영상에서의 좌측 상단, 우측 상단 dst_pts[2] = Point2f(w - 1, h - 1); dst_pts[3] = Point2f(0, h - 1); // 우측 하단, 좌측 하단 Mat per_mat = getPerspectiveTransform(src_pts, dst_pts); // 입력과 출력 좌표를 통해 matrix 획득 Mat dst; warpPerspective(src, dst, per_mat, Size(w, h)); // 투시 변환 실행, 크기는 임의의 크기 500,260#if 1 vector&amp;lt;Point&amp;gt; pts; // 자료형을 변환시키기 위한 변수 for (auto pt : src_pts) { pts.push_back(Point(pt.x, pt.y)); // polylines가 int로 받는데, 위의 point2f는 float타입이르모 point를 통해 int로 변환 } polylines(src, pts, true, Scalar(0, 0, 255), 2, LINE_AA); // 위의 입력 영상에서의 좌표 4개에 대한 사각형 표시#endif imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); if (waitKey(10) == 27) break; }}" }, { "title": "[데브코스] 6주차 - OpenCV Gaussian blurring ", "url": "/posts/opencv2/", "categories": "Classlog, devcourse", "tags": "devcourse, OpenCV", "date": "2022-03-21 14:01:00 +0900", "snippet": "블러링평균 값 필터 (mean filter)영상의 특정 좌표 값을 주변 픽셀 값들의 산술 평균으로 설정하는 것이다. 픽셀 들 간의 그레이스케일 값 변화가 줄어들어 날카로운 엣지가 무뎌지고, 영상에 있는 잡음의 영향이 사라지는 효과가 있다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return -1; } float data[] = {1 / 9.f, 1 / 9.f, 1 / 9.f, 1 / 9.f,1 / 9.f,1 / 9.f, 1 / 9.f,1 / 9.f,1 / 9.f }; Mat kernel(3, 3, CV_32FC1, data); /* Mat kernel = (Mat_&amp;lt;float&amp;gt;(3,3,CV_32FC1, ...) */ Mat dst; filter2D(src, dst, -1, kernel); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}3x3 필터는 각각에 1/9로 다 채우는 것이다. 5x5필터도 있지만 3x3을 가장 많이 사용한다. 이 때, 모든 합이 1이 되어야 평균 밝기가 유지된다.       1/9 1/9 1/9 1/9 1/9 1/9 1/9 1/9 1/9 #include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return -1; }#if 0 float data[] = {1 / 9.f, 1 / 9.f, 1 / 9.f, 1 / 9.f,1 / 9.f,1 / 9.f, 1 / 9.f,1 / 9.f,1 / 9.f }; Mat kernel(3, 3, CV_32FC1, data);#else Mat kernel = Mat::ones(3, 3, CV_32FC1); // 주변의 값들을 다 더해서 산술하기 때문에 255가 다 넘어서 흰색 화면이 나오게 된다. /* Mat kernel = (Mat_&amp;lt;float&amp;gt;(3,3,CV_32FC1, ...) */ Mat dst; filter2D(src, dst, -1, kernel); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}그래서Mat kernel = Mat::ones(3, 3, CV_32FC1) / 9.f;또는blur(src, dst, Size(3,3));blur 함수를 사용해서 블러링을 한다. 여기서 3,3 대신 5,5로 하면 더 심하게 블러 처리가 된다. 즉 사이즈가 커질 수록 블러링이 더 잘된다. 그러나 연산량이 픽셀당 곱셈이 필터의 크기만큼(9,25,49..)로 늘어나기 때문에 주의해야 한다.blur(src, dst2, Size(5, 5)); // 픽셀 당 25blur(src, dst3, Size(3, 3));blur(dst3, dst3, Size(3, 3)); // 픽셀 당 9+9 = 183x3을 두 번하면 픽셀당 총 18번의 연산을 하지만, 5x5를 한 번하면 픽셀당 총 25번의 연산을 한다. 따라서 작은 필터를 여러 번 거는 것이 연산이 더 작아질 수 있다.void blur()블러링 자체를 많이 주려면 필터 크기를 늘리면 되지만, 연산량이 늘어난다.평균값 필터에 의한 블러링의 단점필터링 대상 위치에서 가까이 있는 픽셀과 멀리 있는 픽셀이 모두 같은 가중치를 사용하여 평균을 계산한다. 멀리 있는 픽셀의 영향을 많이 받을 수 있다.다시 말해, 1칸 옆에 있는 것과 3칸 옆에 있는 것의 비중이 같은데, 원래는 둘의 비중이 달라야 한다. 그래서 가우시안을 많이 사용하게 된다.정규 분포와 가우시안정규 분포(Normal distribution)이란? 평균을 중심으로 좌우대칭인 종모양을 갖는 확률 분포 가우시안 분포 자연계에서 일어나는 수많은 일이 설명될 수 있다. 키 몸무게 시험점수 잡음 측정오차 등등 Central limit theoremμ : 평균σ : 표준 편차σ가 클수록 그래프가 펴져 있고, σ가 작을수록 뾰족하다. σ=1인 분포를 표준 정규 분포라 한다. σ=1일 때, 대략적으로 높이가 0.4가 된다. mean(평균) = median(중앙값) = mode(튀어나온 곳) 가우시안 분포의 영역의 넓이는 1이다.가우시안의 예 가우시안 필터 마스크의 크기는 (8σ+1) 또는 (6σ+1)대체로 σ=1로 가정한다. 그래서 가장 중간(0,0)이 가장 큰 값을 가지고, 바깥쪽으로 갈수록 0에 수렴한다.2차원함수를 1차원 2개로 분리가 가능하다. 이를 separable하다고 말한다.또는, mask 9x9 행렬이 있다고 할 때, 이를 전치행렬을 통해 분리하여 1x9 * 9x1 로 계산을 하면 동일하게 9x9로 나오지만, 연산량이 줄어드는 결과를 가져올 수 있다.void GaussianBlur(InputArray src,OutputArray dst,Size ksize,double sigmaX,double sigmaY,int borderType); src : 입력 영상, 각 채녈 별로 처리된다.(CV_8U, CV_16U, CV_16S) dst : 출력 영상, src와 같은 크기와 같은 타입 ksize : 가우시안 커널 크기, Size()를 지정하면 sigma값에 의해 자동 결정된다. truecolor : (8sigma + 1) or gray : (6sigma + 1) Size(9,9) 라고 하면 전체 -9~9 크기 중에서 -5~5 정도의 가우시안 분포만 사용하겠다는 것이다. 그러나 이는 효과가 좋지 않다. 가급적으로 Size()으로 지정하는 것이 좋다. sigmaX : X방향 표준편차 sigmaY : Y방향 표준편차, 0이면 sigmaX와 같게 설정 borderType : 가장자리 픽셀 처리 방식1차원 즉 mean필터를 사용하는 것이 같은 크기의 가우시안 분포를 사용하는 것보다 훨씬 더 많이 블러링이 된다. 그러나 자연스럽게 블러링을 하고 싶다면 가우시안을 사용하는 것이 좋다. 따라서 가우시안의 σ의 크기를 키우면서 블러링 효과를 높이는 것이 좋다.샤프닝언샤프 마스크(unsharp mask) 필터링부드러운 영상을 이용하여 날카로운 영상을 생성한다.변화하는 지점에서 더 큰 변화를 줌으로써 날카롭게 하는 것이다.①번이 원래의 이미지, ②번은 블러링된 이미지다. 즉, 부드럽게 만들어준다. ③번의 경우 ①-② 를 한 것이다. 마지막은 ①*2 - ②를 한 그래프다.Mat = src = imread(&quot;camera.bmp&quot;, IMREAD_GRAYSCALE);Mat blr;blur(src,blr,Size(3,3));Mat dst = 2 * src - blr;이러한 방법으로 구현할 수 있다.위의 식을 마스크의 형태로 판단해봤을 때, f(x,y), 원본에 대한 마스크는 중앙에만 1이 되어야 한다. 그리고 블러링한 g”(x,y)에 대한 마스크는 1/9가 각각 들어가있다. 이를 2*f(x,y)-f”(x,y) 를 마스크의 형태로 결과를 볼 수 있다.따라서 sharpen이라는 마스크의 값을 구현할 수 있다.float sharpen[] = { -1 / 9.f, -1 / 9.f, -1 / 9.f, -1 / 9.f, 17 / 9.f, -1 / 9.f, -1 / 9.f, -1 / 9.f, -1 / 9.f }; Mat filter(3, 3, CV_32F, sharpen); Mat dst; filter2D(src, dst, -1, filter);이렇게 구현해볼 수도 있다.가우시안 언샤프 필터Mat = src = imread(&quot;camera.bmp&quot;, IMREAD_GRAYSCALE);Mat blr;GaussianBlur(src, blr, Size(), 1.0);Mat dst = 2 * src - blr;입력 영상에서 블러링 영상을 빼면, 날카로운 성분만 남는다고 볼 수 있다. 이를 입력에 더해주면 날카로움을 강조한다는 개념이 될 것이다. 이 때, 그냥 더하는 것이 아니라 더할 때 가중치(weight)를 추가해줄 수 있다. 위 그래프의 3번 그래프를 g(x,y)라 하면 unsharp mask f’(x,y) = f(x,y) + alpha * g(x,y)= f(x,y) + alpha(f(x,y) - f”(x,y))= (1+alpha) * f(x,y) - alpha * f”(x,y)float alpha = 1.0f;Mat dst = (1.f + alpha) * src - alpha * blr;어떤 데이터를 0과1로 나눈다는 것, 영상의 경우 0과255로 나누는 것을 이진화라 하는데, 이 g(x,y)를 단순하게 날카로운 성분만이라고는 볼 수 없지만, 단순하게 날카로움이라고 본다는 것이 이진화의 원리다.샤프닝은 사실 잘 사용하지 않는다. 영상 처리를 할 때 샤프닝을 이용하는 일은 잘 없지만, 이 내용과 개념에 대해 설명하기 위해 배웠다.잡음 제거 필터영상의 잡음(Noise)영상의 픽셀 값에 추가되는 원치 않는 형태의 신호를 말한다. 카메라에서 광학 신호를 전기적 신호로 변환하는 과정에서 잡음을 추가할 수 있다. 센서의 발열과 같은 것들에 의해 발생할 수 있다.획득한 영상 = 원본 신호 + 잡음== f(x,y) = s(x,y) + n(x,y)잡음의 종류 가우시안 잡음(Gaussian noise)가우시안의 경우 σ가 클수록 잡음이 많다는 것이고, σ가 작을수록 잡음이 작음을 의미한다. 소금&amp;amp;후추 잡음(salt&amp;amp;pepper)옛날에 많이 나오는 것으로 흰/검정색의 값이 점처럼 찍히는 것을 말한다.프로파일영상에서 특정 경로 상에 있는 픽셀의 밝기 값을 그래프로 나타낸 것 line profile intensity profile흰색 선이 지나가는 곳에서의 profile을 보는 것이다. 중간 부분을 보면 변동이 심하게 나온다.이를 없애기 위해서는 가우시안 노이즈를 잡으면 된다. 저런 지그재그 형태는 가우시안 잡음이기 때문이다.GaussianBlur(src, src, Size(), 2);sigma를 2로 하면 윤곽이 너무 무뎌지기 때문에 1이 적당하다. 1을 하더라도 약간 흐려지지만, 노이즈를 제거하는 것을 통해 영상에서 잘못 판단할 효과를 줄일 수 있다. 차선 인식이나 객체 검출의 경우에도 가우시안 블러를 사용하기도 한다. 블러했다고 해서 찾던 것을 못찾는 일은 거의 없기 때문이다. 영상에 가우시안 잡음 추가하기int main(void){ Mat src = imread(&quot;lenna.bmp&quot;,IMREAD_GRAYSCALE); Mat noise(src,size(), CV_32S); randn(noise,0,10); Mat dst; add(src, noise, dst, noArray(), CV_8U); ...}만약 sigma가 10이라면 그에 대해 randn(noise,0,10)의 범위로 노이즈를 추가하는 것이다. sigma가 20이라면 randn(noise,0,20)미디언 필터주변 픽셀들의 값들을 정렬하여 그 중앙값으로 픽셀 값을 대체하는 것을 말한다.전체 이미지를 스캔하여 정렬시킨다음(std:sort), 중앙값을 중간에 넣어주면 된다. 이 필터는 salt pepper 이미지에 효과적이다. opencv에서 제공하는 미디언 필터void medianBlur( InputArray src, OutputArray dst, int ksize ) src/dst : 입력/출력 영상 ksize : 구멍 크기, 1보다 큰 홀수로 지정해야 함.Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);Mat noise = addNoise(src, 10); // 10% noisemedianBlur(noise, dst1, 3); 직접 만든 미디언 필터void myMedian(const Mat&amp;amp; src, Mat&amp;amp; dst){ CV_Assert(!src.empty()); CV_Assert(src.type() == CV_8UC1); // 런타임에서 조건을 점검한다. // src 영상을 dst로 깊은 복사를 수행 src.copyTo(dst); uchar p[9]; for (int y = 1; y &amp;lt; src.rows - 1; y++) { for (int x = 1; x &amp;lt; src.cols - 1; x++) { // src 영상의 픽셀 값이 0 또는 255인 경우에만 적용 if ( src.at&amp;lt;uchar&amp;gt;(y,x) == 0 || src.at&amp;lt;uchar&amp;gt;(y, x) == 255 ) { p[0] = src.at&amp;lt;uchar&amp;gt;(y - 1, x - 1); p[1] = src.at&amp;lt;uchar&amp;gt;(y - 1, x); p[2] = src.at&amp;lt;uchar&amp;gt;(y - 1, x + 1); p[3] = src.at&amp;lt;uchar&amp;gt;(y, x - 1); p[4] = src.at&amp;lt;uchar&amp;gt;(y, x); p[5] = src.at&amp;lt;uchar&amp;gt;(y, x + 1); p[6] = src.at&amp;lt;uchar&amp;gt;(y + 1, x - 1); p[7] = src.at&amp;lt;uchar&amp;gt;(y + 1, x); p[8] = src.at&amp;lt;uchar&amp;gt;(y + 1, x + 1); sort(p, p + 9); // 정렬된 src 픽셀 값 중에서 중앙값을 dst 픽셀 값으로 설정 dst.at&amp;lt;uchar&amp;gt;(y, x) = p[4]; } } }}이 때 중요한 것은 전체를 다 하는 것이 아니라, 0 또는 255인 경우에만 적용할 것이다. 원래 있는 medianBlur를 하면 모든 픽셀에 대해 수행하는데 이는 불필요하므로 0 or 255에 대해서만 적용하고자 하는 것이다.양방향 필터(Bilateral filter)에지 보전 잡은 제거 필터의 하나로 평균 값 필터 또는 가우시안 필터는 에지 부근에서도 픽셀 값을 평탄하게 만드는 단점이 있다. 기준 픽셀과 이웃 픽셀과의 거리, 그리고 픽셀 값의 차이를 함께 고려하여 스무딩 정도를 조절한다. 즉 값이 조금씩 변화하는 것은 평탄화하고, 크게 변화하는 것은 그대로 둔다. 이 필터는 조금 느리다는 담점이 있다. p,q : p점과 q점의 픽셀 좌표(벡터) Ip, Iq : p점과 q점에서 픽셀 값 Wp : 필터 커널 합이 1이 되도록 만드는 정규화 함수초록색과 노란색 둘다 가우시안(Gσ)의 형태이다. 두 개의 가우시안을 합쳐서 만드는 형태지만, 초록색 부분의 p,q는 픽셀 좌표의 거리, 노란색 부분의 Ip,Iq는 픽셀 값의 차이이다. 표기법 - 굵은 소문자는 벡터 - 볼드체는 행렬 - || || 는 norm이라 하여 거리 계산한다는 뜻 - | |는 절대값가우시안 필터링의 경우 영상 전체에 똑같은 값으로 블러링한다. 그러나 양방향의 경우 어두운 곳은 커널을 0으로 , 밝은 곳은 그대로 블러링한다. 값이 평탄한 곳은 블러링하고, 엣지부분은 그대로 둔다.위의 식을 다시 보면, 앞의 초록색 부분은 space weight, 노란색 부분은 픽셀 값의 차이를 통해 range weight를 준다. 값의 차이가 큰부분에 대해서는 블러링을 안하고, 값의 차이가 작은 부분에서만 블러링을 한다.이를 3차원으로 표시하면 다음과 같다.void bilateralFilter(src, dst, int d, double sigmaColor, double sigmaSpace, int borderType = 4); d : 음수라면 필터 사이즈를 자동으로 결정해준다. sigmaColor : 픽셀 값의 차이가 이 값보다 작은 것에 대해 블러링한다는 기준점, 3σ를 기준으로 σ=10이라면 최대 60(-30~30)의 차이 중에서 10 정도의 차를 기준으로 한다. sigmaSpace : 블러링 정도 borderType : 블러링 타입 방법bilateralFilter(src, dst2, -1, 10, 5); 단순하게 노이즈를 제거할 때는 가우시안 블러를 많이 사용한다." }, { "title": "[데브코스] 5주차 - OpenCV Image Brightness Control and histogram ", "url": "/posts/opencv/", "categories": "Classlog, devcourse", "tags": "devcourse, brightness, histogram", "date": "2022-03-17 14:01:00 +0900", "snippet": "옛날에는 컴퓨터 비전 (영상 처리) 알고리즘이 그레일스케일을 사용했다. 컬러정보가 꼭 필요하지 않을 경우에는 그레이스케일로 변환하기도 한다. 컬러는 그레이스케일의 3배의 용량을 차지하기 때문이다.그래서 여기서는 입력 영상이 Truecolor인 경우 grayscale로 변환하여 사용할 것이다.Mat img1 = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);Mat img2(rows,cols,CV_8UC1);Mat img3(&quot;lenna.bmp&quot;, IMREAD_COLOR);Mat img4;cvtColor(img3,img4,COLOR_BGR2GRAY);화소 처리 (point porcessing)화소 처리 입력 영상의 특정 좌표 픽셀 값을 변경하여 출력 영상의 해당 좌표 픽셀 값으로 설정하는 연산 결과 영상의 픽셀 값이 정해진 범위(그레이스케일 = 0~255)에 있어야 한다. =&amp;gt; 예외 처리 반전, 밝기 조절, 명암비 조절, 이진화 등입력을 출력으로 변환하는 함수를 변환 함수(transfer function)이라 한다. y = x를 가진 함수를 항등함수, 입력과 출력의 밝기는 같다. y = -(x-a)^2 + b 를 가진 함수는 입력보다 출력이 조금더 밝아짐 0 or k 인 값을 가진 함수를 이진화, 이진 함수라 한다.밝기 조절(brightness control)밝기 조절 : 영상 전체 밝기를 일괄적으로 밝게 만들거나 어둡게 만드는 연산이 때 y = x + n 의 형태를 가질 것이다. 여기서 중요한 것은 픽셀이 가질 수 있는 값이 0~255이므로 이것을 처리해줘야 한다. 이것을 처리하는 것을 saturate연산 또는 limit연산 이라 한다.Mat dst1 = src + 50;Mat dst2 = src - 50;이 + 연산을 직접 선언하고자 한다.Mat dst(src.rows,src.cols.CV_8UC1); // at을 사용하기 때문에 선언해줘야 함 for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { dst.at&amp;lt;uchar&amp;gt;(y,x) = src.at&amp;lt;uchar&amp;gt;(y,x) + 100; // unsigned char은 8개 자리만 가져가게 된다. } } assertion failed (data) =&amp;gt; ()안이 참이어야 하는데, 현재는 data가 비어있다 unsigned char : 8bit1,2,3,4,…255,256(0,0,0,0,0,0,0,0)(0,0,0,0,0,0,0,1)(0,0,0,0,0,0,1,0)(0,0,0,0,0,0,1,1)…(1,1,1,1,1,1,1,1), (1,0,0,0,0,0,0,0,0)위의 코드에서 더한 결과가 256이라면 1을 뺀 00000000만 가져오게 되므로 0이 들어가고, 257이면 1이 들어가게 된다.따라서 다음과 같이 포화연산을 추가해야 한다. Mat dst(src.rows,src.cols,CV_8UC1); // at을 사용하기 때문에 선언해줘야 함 for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { int v = src.at&amp;lt;uchar&amp;gt;(y,x) + 100; // 포화 되면 limit를 걸어서 낮춘다. //v = (v &amp;gt; 255) ? 255 : ((v &amp;lt; 0) ? 0 : v); // if (v &amp;gt; 255) v = 255; if (v&amp;lt;0) v=0; //dst.at&amp;lt;uchar&amp;gt;(y, x) = (uchar)v; dst.at&amp;lt;uchar&amp;gt;(y, x) = (uchar)((v &amp;gt; 255) ? 255 : ((v &amp;lt; 0) ? 0 : v)); } } (-) 연산 (max 사용) Mat dst(src.rows,src.cols,CV_8UC1); for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { dst.at&amp;lt;uchar&amp;gt;(y, x) = (uchar)max(0,src.at&amp;lt;uchar&amp;gt;(y,x) - 100); } }그러나 opencv에서 포화 연산 템플릿 함수를 따로 정의해놓았다.for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(src.at&amp;lt;uchar&amp;gt;(y,x) + 100); }} (int)v == int(v) 둘다 가능하다. main 함수만은 마지막에 return을 하지 않아도 된다.dst = src + 50 대신 add 라는 함수를 사용해도 된다.// dst = src + 50;add(src, 50, dst);중요한 것은 add를 사용하면 더하면서 포화처리를 하지만, 그냥 +를 사용하면 다 더한 후 포화처리를 하게 된다. 그래서 다른 결과가 도출될 수도 있다.시간 측정 TickMeter tm; tm.start();#if 1 Mat dst; dst = src + 50;#else Mat dst(src.rows,src.cols,CV_8UC1); for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { dst.at&amp;lt;uchar&amp;gt;(y, x) = saturate_cast&amp;lt;uchar&amp;gt;(src.at&amp;lt;uchar&amp;gt;(y, x) + 100); } }#endif tm.stop(); cout &amp;lt;&amp;lt; &quot;Elapsed time: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; endl;위의 함수를 사용하면 0.2ms~0.3ms , 아래 직접 구현했을 때는 0.5ms~0.6ms 정도 나온다.차이가 나는 이유는 cpu칩은 1개지만, 논리적, 물리적 코어가 여러 개 들어있다.평균 밝기 구하기 및 변경그레이스케일의 중간값이 128로 맞추는 것이 좋다.// 평균 구하기int m = mean(src)[0];// 3. 평균 밝기가 128이 되도록 밝기 보정하기Mat dst = src + (128 - m); 평균이 124 : src + (128 - 124) = src + 4 평균이 211 : src + (128 - 211) = src - 83반전(inverse)영상 내의 모든 픽셀 값을 각각 그레이스케일 최댓값에서 뺀 값으로 설정한다. 즉 밝은 픽셀은 어둡게, 어두운 픽셀은 밝게 변경컬러 영상에 대해 각각의 색상 성분에 대해 반전한다. 영상 처리에서는 대체로 관심있는 부분을 흰색(255), 배경을 검은색(0)으로 두는 것이 일반적이기에 이 때 사용한다.y = 255 - xMat dst = 255 - src;Mat dst = Scalar(255) - src;트랙바를 이용해서 밝기 조절 정도를 지정하는 프로그램밝기 조절 값 -100~100 범위로 지정할 수 있도록 하기 위해 트랙바 최댓값을 200으로 설정하고 트랙바는 초기값이 무조건 0이므로 -100~100으로 범위를 두기 위해 초기값은 100으로 둔다.초기값을 100으로 두기 위해서 setTrackbarPos()를 사용해도 된다. 제출: 파일을 백업할 때 brightness 폴더에서 x64, .vs, .user를 지워라#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;void on_level_change(int pos, void* userdata);int main(void){ Mat img = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); Mat src; img.copyTo(src); namedWindow(&quot;image&quot;); int initvalue = 100; createTrackbar(&quot;level&quot;, &quot;image&quot;, &amp;amp;initvalue, 200, on_level_change, (void*)&amp;amp;img); // setTrackbarPos(&quot;level&quot;, &quot;image&quot;, 100); imshow(&quot;src&quot;, src); waitKey();}void on_level_change(int pos, void* userdata){ Mat img = *(Mat*)userdata; Mat dst; dst = img + pos - 100; // img.convertTo(dst,-1,1,pos - 100); imshow(&quot;image&quot;, dst);}관심 영역의 평균 밝기를보정한 동영상 재생 프로그램조명의 변화가 심한 base_camera_dark.avi 동영상을 재생하면서 차선 위치의 평균 밝기가 균일하게 유지되도록 밝기를 보정하여 재생해라ROI 사각형 꼭짓점 좌표 : [240,280], [400,280],[620,440],[20,440]frame은 트루컬러, gray는 그레이스케일, dst는 적절하게 밝기를 조절해준 화면이어야 함, 평균 밝기가 128 정도로 되도록dst = gray + (128 - m);이나, 저 사각형 안에서의 평균을 구해서 해야 할 것이다. 그에 대한 방법으로는 mask를 생성하여 지정 사각형 크기만큼만 흰색으로 지정하여 mask를 생성 포토샵이나 그림판으로 grayscale로 생성해서 bmp파일로 mask를 만들어서 mean(gray,mask)를 해라.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;Mat calcGrayHist(const Mat&amp;amp; img){ CV_Assert(img.type() == CV_8UC1); Mat hist; int channels[] = { 0 }; int dims = 1; const int histSize[] = { 256 }; float graylevel[] = { 0, 256 }; const float* ranges[] = { graylevel }; calcHist(&amp;amp;img, 1, channels, noArray(), hist, dims, histSize, ranges); return hist;}Mat getGrayHistImage(const Mat&amp;amp; hist){ CV_Assert(hist.type() == CV_32FC1); CV_Assert(hist.size() == Size(1, 256)); double histMax = 0.; minMaxLoc(hist, 0, &amp;amp;histMax); Mat imgHist(100, 256, CV_8UC1, Scalar(255)); for (int i = 0; i &amp;lt; 256; i++) { line(imgHist, Point(i, 100), Point(i, 100 - cvRound(hist.at&amp;lt;float&amp;gt;(i, 0) * 100 / histMax)), Scalar(0)); } return imgHist;}int main(){ VideoCapture cap(&quot;base_camera_dark.avi&quot;); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Camera open failed!&quot; &amp;lt;&amp;lt; endl; return -1; } Mat frame; // frame이란 한장의 정지 영상 /* coordinate vector */ vector&amp;lt;Point&amp;gt; pts; pts.push_back(Point(240, 280)); pts.push_back(Point(400, 280)); pts.push_back(Point(620, 440)); pts.push_back(Point(20, 440)); while (true) { cap &amp;gt;&amp;gt; frame; // == cap.read(frame); if (frame.empty()) { cerr &amp;lt;&amp;lt; &quot;Frame empty!&quot; &amp;lt;&amp;lt; endl; // 동영상 맨 마지막으로 가도 이것이 실행되고 종료 될 것이다. break; } /* grayscale */ Mat gray; cvtColor(frame, gray, COLOR_BGR2GRAY); /* mk mask file */ Mat mask = Mat::zeros(frame.rows, frame.cols, CV_8UC1); fillPoly(mask, pts, Scalar(255, 255, 255),LINE_AA); /* be mask image */ Mat src = Mat::zeros(frame.rows, frame.cols, CV_8UC1); copyTo(frame, src, mask); /* get mean value */ int m = mean(src,mask)[0]; cout &amp;lt;&amp;lt; &quot;mean: &quot; &amp;lt;&amp;lt; m &amp;lt;&amp;lt; endl; /* normalize brightness */ Mat dst; gray.convertTo(dst, -1, 1.5, 128 - m); // Mat dst = gray + (128 - m); /* drawing */ polylines(frame, pts, true, Scalar(0, 0, 255), 2); Mat hist = calcGrayHist(gray); Mat imgHist = getGrayHistImage(hist); imshow(&quot;histogram&quot;, imgHist); imshow(&quot;gray&quot;, gray); imshow(&quot;dst&quot;, dst);// imshow(&quot;src&quot;, src);// imshow(&quot;mask&quot;, mask); imshow(&quot;frame&quot;, frame); if (waitKey(1) == 27) // ESC break; } cap.release(); // 비디오 종료 destroyAllWindows(); // 생성했던 창을 닫아줌}영상의 명암비 조절명암비(constrast) : 밝은 곳과 어두운 곳 사이에 드러나는 밝기 정도의 차이, 대비평균 밝기를 고려한 명암비를 조절하고자 한다. 명암비를 높일 때는 평균 밝기보다 낮으면 더 낮추고, 평균 밝기보다 높으면 더 높이면 된다.dst(x, y) = saturate(s*src(x,y))s는 int일 것이고, s = 0.5라면 출력 크기가 0~255 에서 0~128로 줄어든다. 그러나 s = 2라면 출력 크기가 0~255이지만, 128~255의 입력들은 다 포화가 되므로 255로 통일될 것이다.dst = src * 2;dst = src / 2;그러므로 조금 더 효과적인 방법을 사용해야 한다.y = x + (x-128) * α 의 그래프를 사용하면 α가 1이면 y = 2x -128float alpha = 1.f; // 0.5f,0.8f // 1.0으로 하면 double타입으로 한다.Mat dst = saturate_cast(src+(src-128)*alpha);그러나 사진 자체가 밝으면 포화가 너무 많아진다. 따라서 128 대신 평균을 구해서 (m,m)을 지나는 직선으로 변환하면 된다.y = x + (x-m) * αint m = mean(src)[0];cout &amp;lt;&amp;lt; &quot;mean: &quot; &amp;lt;&amp;lt; m &amp;lt;&amp;lt; endl;float alpha = 1.f;Mat dst1 = src + (src - m) * alpha;Mat dst2 = src + (src - 128) * alpha;tm.stop();cout &amp;lt;&amp;lt; &quot;Elapsed time: &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; endl;imshow(&quot;src&quot;, src);imshow(&quot;dst1&quot;, dst1);imshow(&quot;dst2&quot;, dst2);히스토그램히스토그램(histogram) : 영상의 픽셀 값 분포를 그래프의 형태로 표현한 것이다. 예를 들어 그레이스케일 영상에서 각 그레이스케일 값에 해당하는 픽셀의 개수를 구하고, 이를막대 그래프의 형태로 표현한다.정규화된 히스토그램(normalized histogram) : 히스토그램으로 구한 각 픽셀의 개수를 영상 전체 픽셀 개수로 나누어준 것이다. 해당 스레이스케일 값을 갖는 픽셀이 나타날 확률이다.그냥 히스토그램은 정수로 표현이 가능하고, 정규화되면 0~1사이 값으로 표현되므로 실수가 된다. 이 정규화를 하게되면 확률로 바뀌게 되어 전체를 다 더하면 1이 된다.int hist[256] = { 0, }; // int hist[256]; 으로 선언해도 된다. // 이렇게 하면 전역변수는 0으로 초기화되지만, 지역변수는 값이 다 0이 아닌 가비지 변수로 된다. 그래서 int hist[256] = {}; 라고해야 초기화 된다.// 0, 을 넣어도 되지만, 굳이 안넣어도 된다. 이를 반드시 256으로 지정해야할 필요는 없다. 픽셀 값이 0~1, 2~3, 4~5 로 해서 128개로 할 수도 있다.int hist[256] = {}; for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { hist[src.at&amp;lt;uchar&amp;gt;(y, x)]++; }}이 코드는 히스토그램을 구하는 for문이다. 여기서 중요한 것은 int hist를 사용할 때, 자신이 사용하는 화소의 크기를 알아야 한다. 이 화소가 전체가 int의 크기를 넘어가면 에러가 날 것이다. int가 가지는 범위는 2^31이다. 넘어가면 long long을 사용하면 된다.// 히스토그램 직접 생성 //Mat imgHist(100, 256, CV_8UC1, Scalar(255));for (int i = 0; i &amp;lt; 256; i++) { line(imgHist, Point(i, 100), Point(i, 100 - cvRound(hist[i] * 100 / histMax)), Scalar(0));}이는 히스토그램을 직접 생성하는 것이지만 함수가 따로 있다.void calcHist(const Mat* images, int nimages, const int* channels, InputArray mask, OutputArray hist, int dims, const int* histSize, const float** ranges, bool uniform = true, bool accumulate = false); images : 입력 영상 배열, 입력 영상의 주소, 영상의 배열인 경우 모두 깊이와 크기가 같아야함 nimages : 영상의 개수내부에서 calcHist()함수를 사용하여 반환된 히스토그램 hist는 256x1 크기의 CV_32FC1 타입의 행렬Mat calcGrayHist(const Mat&amp;amp; img){ CV_Assert(img.type() == CV_8U); Mat hist; int channels[] = { 0 }; int dims = 1; const int histSize[] = { 256 }; // 이를 128로 하면 0~1,1~2,, 64로 하면 0~3,4~7,, 로 구할 수 있다 float graylevel[] = { 0, 256 }; const float* ranges[] = { graylevel }; calcHist(&amp;amp;img, 1, channels, noArray(), hist, dims, histSize, ranges); // hist: 256x1, CV_32FC1 ,, float로 저장된다. return hist;}이를 활용해서 그림을 그리는 함수는 다음과 같다.Mat getGrayHistImage(const Mat&amp;amp; hist){ CV_Assert(hist.type() == CV_32F); CV_Assert(hist.size() == Size(1, 256)); double histMax = 0.; minMaxLoc(hist, 0, &amp;amp;histMax); // 최대값만 구함 , 100x256 일 때 최댓값이 100pixel이 되도록 하려고 구함 Mat imgHist(100, 256, CV_8U, Scalar(255)); for (int i = 0; i &amp;lt; 256; i++) { line(imgHist, Point(i, 100), Point(i, 100 - cvRound(hist.at&amp;lt;float&amp;gt;(i, 0) * 100 / histMax)), Scalar(0)); // cvRound()안이 100이 되도록하려고 작성 한 것이다. } return imgHist;}영상과 histogram의 관계이를 봤을 때 위에가 원본인데, 밝게하면 전체적으로 오른쪽으로 이동하고, 어둡게 하면 전체적으로 왼쪽으로 이동한다.명암비를 높게 하면 중간중간 빈칸이 생긴다. 그리고 양 끝이 없었는데, 생기게 된다. 명암비가 낮게 되면 중간중간 솓아오르는게 생기고, 양 끝의 빈공간이 늘어난다.히스토그램 스트레칭영상의 히스토그램이 그레이스케일 전 구간에서 걸쳐 나타나도록 변경하는 선형 변환 기법이다. 특정 구간에 집중되어 나타난 히스토그램을 마치 고무줄 늘이듯이 그레이스케일 범위 전구간에서 히스토그램이 골고루 나타나도록 변환한다. 예를 들어 한 이미지가 픽셀값이 40~220 사이에만 분포한다면 이를 늘려서 0~255로 분포하도록 만든다. 늘리면 중간중간 비어 있는 분포가 만들어진다.가장 낮은 픽셀값을 Gmin, 가장 높은픽셀값을 Gmax라 할 때, 이 둘을 지나는 직선의 방정식을 구하고자 한다. 히스토그램 스트레칭 변환 함수의 기울기 : 255/(Gmax - Gmin) y절편 : -(255xGmin) / (Gmax - Gmin)g(x,y) = 255(Gmax - Gmin) x f(x,y) x 255/(Gmin - Gmax)dst(x,y) = 255(Gmax - Gmin) / (Gmin - Gmax)/*double gmin, gmax;minMaxLoc(src, &amp;amp;gmin, &amp;amp;gmax);Mat dst = (src - gmin) * 255 / (gmax - gmin);*/Mat dst;normalize(src, dst, 0, 255, NORM_MINMAX);두 코드는 같은 형태로 동작한다. 위의 주석 안의 방법이 좀 더 이론적인 방법이다.void normalize(src, dst, alpha=None, beta=None, norm_type=None, dtype=None, mask=None)• src: 입력 영상• dst: 결과 영상• alpha: (노름 정규화인 경우) 목표 노름 값, (원소 값 범위 정규화인 경우) 최솟값• beta: (원소 값 범위 정규화인 경우) 최댓값• norm_type: 정규화 타입. NORM_INF, NORM_L1, NORM_L2, NORM_MINMAX, 히스토그램 스트레칭은 NORM_MINMAX• dtype: 결과 영상의 타입• mask: 마스크 영상https://deep-learning-study.tistory.com/121lenna.bmp의 경우 25픽셀부터 나오기 시작하여 245픽셀까지 존재한다. 따라서 히스토그램으로는 양쪽의 빈공간이 크기가 같아보이지만, 실제로는 다르다. 1~2개인 픽셀이 있다면 이를 무시해야 변화가 보일 것이다. 즉 상위/하위 1%를 무시하도록 설정한다.#include &quot;opencv2/opencv.hpp&quot;#include &amp;lt;iostream&amp;gt;using namespace cv;using namespace std;void histogram_stretching(const Mat&amp;amp; src, Mat&amp;amp; dst);void histogram_stretching_mod(const Mat&amp;amp; src, Mat&amp;amp; dst);int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } Mat dst1, dst2; histogram_stretching(src, dst1); histogram_stretching_mod(src, dst2); imshow(&quot;src&quot;, src); imshow(&quot;dst1&quot;, dst1); imshow(&quot;dst2&quot;, dst2); waitKey();}// 기존의 스트레칭 방식void histogram_stretching(const Mat&amp;amp; src, Mat&amp;amp; dst){ double gmin, gmax; minMaxLoc(src, &amp;amp;gmin, &amp;amp;gmax); dst = (src - gmin) * 255 / (gmax - gmin);}// 개선한 스트레칭 방식void histogram_stretching_mod(const Mat&amp;amp; src, Mat&amp;amp; dst){ int hist[256] = {0,}; // src 영상 전체를 스캔하면서 히스토그램을 hist에 저장하세요. for (int y = 0; y &amp;lt; src.rows; y++) { for (int x = 0; x &amp;lt; src.cols; x++) { hist[src.at&amp;lt;uchar&amp;gt;(y, x)]++; } } int gmin, gmax; int ratio = int(src.cols * src.rows * 0.01); // 전체 픽셀 개수의 1% for (int i = 0, s = 0; i &amp;lt; 255; i++) { s += hist[i]; // 히스토그램 누적 합 s가 ratio보다 커지면, // 해당 인덱스를 gmin에 저장하고 반복문을 빠져나옵니다. if (s &amp;gt; ratio) { gmin = i; break; } } for (int i = 255, s = 0; i &amp;gt;= 0; i--) { s += hist[i]; // 히스토그램 누적 합 s가 ratio보다 커지면, // 해당 인덱스를 gmax에 저장하고 반복문을 빠져나옵니다. if (s &amp;gt; ratio) { gmax = i; break; } } // gmin과 gmax 값을 이용하여 히스토그램 스트레칭을 수행하고, // 그 결과를 dst에 저장합니다. cout &amp;lt;&amp;lt; gmin &amp;lt;&amp;lt; &quot;, &quot; &amp;lt;&amp;lt; gmax &amp;lt;&amp;lt; endl;#if 0 normalize(src, dst, gmin, gmax, NORM_MINMAX);#else dst = (src - gmin) * 255 / (gmax - gmin);#endif}이 변환함수가 직선으로 사용했지만, 꼭 직선이 아니라 곡선이 될 수도 있다.히스토그램 평활화 (histogram equalization)히스토그램이 그레이스케일 전체 구간에서 균일분포로 나타나도록 변경하는 명암비 향상 기법이다. 변환함수는 곡선이 된다. 히스토그램 균등화, 균일화, 평탄화변환 함수 구하기 히스토그램 함수 구하기 : h(g) = Ng 정규화된 히스토그램 함수 구하기 : p(g) = h(g) / (w x h) 누적 분포 함수(cdf) 구하기 : cdf(g) = 시그마(0&amp;lt;=i&amp;lt;=g)p(i) 변환 함수 : dst(x,y) = round(cdf(src(x,y))X Lmax)앞에서부터 현재위치까지 각 픽셀에 대한 p, 확률을 다 더한 것이다. cdf의 맨 마지막은 항상 1이다.                   bin 0 1 2 3 4 5 6 7 h(g) 4 3 2 1 0 2 3 1 p(g) 4/16 3/16 2/16 1/16 0 2/16 3/16 1/16 cdf(g) 4/16 7/16 9/16 10/16 10/16 12/16 15/16 1 cdf(g) x L 28/16 49/16 70/16 70/16 84/16 105/16 7   평등화 히스토그램void equalizeHist( InputArray src, OutputArray dst ); src : 단일 이미지 입력 영상 dst : 출력 영상스트레칭은 그냥 쭉 펴서 되어 있고, 평탄화는 갯수가 많이 있는 부분은 사이의 간격을 넓게, 갯수가 별로 없는 부분은 간격을 좁히는 형식이다. 그래서 전체를 4등분을 했을 때, 갯수가 동일하게 나누어진다.영상의 산술 연산덧셈 연산두 영상의 같은 위치에 존재하는 픽셀 값을 더하여 결과 영상의 픽셀 값으로 설정한다. 덧셈 결과가 255보다 크면 픽셀값을 255로 설정한다.dst(x,y) = saturate(src1(x,y) + src2(x,y))이렇게 하면 포화가 너무 많아져서 좋지않다.가중치 합두 영상의 같은 위치에 존재하는 픽셀 값에 대하여 가중합을 계산하여 결과 영상의 픽셀밧으로 설정한다. 보통 a + b = 1이 되도록 설정한다. 두 입력 영상의 평균 밝기를 유지하기 위해서이다.dst(x,y) = saturate(a * src1(x,y) + b * src2(x,y))평균 연산가중치를 a=b=0.5로 설정한 가중치 합dst(x,y) = saturate(2/1(src1(x,y) + src2(x,y)))평균 연산의 응용 잡음 제거어둡게 여러 장 찍어서 합성을 한다.뺄셈 연산두 영상의 같은 위치에 존재하는 픽셀 값에 대해서 뺄셈 연산을 수행하여 결과 영상의 픽셀값으로 설정한다. 뺄셈 결과가 0보다 작으면 픽셀값을 0으로 설정dst(x,y) = saturate(sr1(x,y) - src2(x,y))차이 연산두 입력 연산에 대해 뺄셈을 하는데, 절댓값을 이용하여 결과 영상을 생성하는 연산이다. 뺄셈과 달리 입력 영상의 순서에 영향을 받지 않는다.dst(x,y) = |sr1(x,y) - src2(x,y)|void add(InputArray src1, InputArray src2, OutputArray dst, InputArray mask = noArray(), int dtype = -1);void subtract(InputArray src1, InputArray src2, OutputArray dst, InputArray mask = noArray(), int dtype = -1); src1 : 첫번째 입력 행렬 또는 스칼라 src2 : 두번째 입력 행렬 또는 스칼라 dst : 출력 행렬, dst의 깊이는 두 개가 같거나덧셈 연산의 경우 두 개의 타입이 같아야 한다. 그러나 add는 출력 영상의 타입을 고를 수 있다. 가중치 추가하여 출력void addWeighted(InputArray src1, double alpha, InputArray src2, double beta, double gamma, OutputArray dst, int dtype = -1); src1 : 첫번째 입력 영상 alpha : 첫번째 배열에 가할 가중치 src2 : 두번째 입력 영상 beta : 두번째 배열에 가할 가중치 gamma : 각 합에 더해질 스칼라 dst : 출력 영상 dtype : optional, 두 입력 영상이 동일한 깊이, 타입을 가졌을 때의 출력 영상의 깊이 1번과 2번의 차이를 출력void absdiff(InputArray src1, InputArray src2, OutputArray dst); src1 : 첫번째 입력 영상 src2 : 두번째 입력 영상 dst : 출력 영상행렬의논리 연산void bitwise_and(InputArray, InputArray, OutputArray)void bitwise_or(InputArray, InputArray, OutputArray)void bitwise_xor(InputArray, InputArray, OutputArray)void bitwise_not(InputArray, InputArray)#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture.cap(&quot;../data/base_camera_dark.avi&quot;); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return -1; } // int height = CAP_PROP_FRAME_HEIGHT;// int width = CAP_PROP_FRAME_WIDTH; Mat mask(480, 640, CV_8UC1, Scalar(0)); vector&amp;lt;Point&amp;gt; pts(4); pts.push_back(Point(240, 200)); pts.push_back(Point(400, 280)); pts.push_back(Point(620, 440)); pts.push_back(Point(20, 440)); fillPoly(mask, pts, Scalar(255)); Mat frame, dst, gray; while (true) { cap &amp;gt;&amp;gt; frame; cvtColor(frame, dst, COLOR_BGR2GRAY); bitwise_and(gray, mask, dst); imshow(&quot;frame&quot;, frame); imshow(&quot;mask&quot;, mask); imshow(&quot;img&quot;, dst); waitKey(); }}필터링필터링 : 영상에서 필요한 정보만 통과시키고 원치 않는 정보는 걸러내는 작업비네팅이라 하여 가운데 중앙부에 대한 빛과 주변부에 대한 빛에 대해 작업하는 것이 있다.주파수 공간에서의 필터링퓨리에 변환(fourier transform)을 이용하여 영상을 주파수 공간으로 변환하여 필터링하는 방법을 말한다.주변부를 고주파(high frequency)라 하고 중앙부를 저주파(low frequency)라 한다. 변화량이 큰 것을 고주파, 변화가 적은 것을 저주파라 한다. 웅앙에 저주파를 제거하고 다시 영상으로 만들면 영상이 조금 더 날카로워진다. band pass filter 이라는 것도 있다.공간적 필터링영상의 픽셀 값을 직접 이용하는 필터링 방법이 있다. 주로 마스크 연산을 이용한다. 3x3,5x5,7x7 정도의 작은 필터링은 공간적 필터링을 많이 사용하고 좀 더 큰 것들은 주파수 필터링을 사용한다. 다양한 모양과 크기의 마스크필터링에 사용되는 마스크는 다양한 크기, 모양을 지정할 수 있지만 대부분 3x3 정방형 필터 사용한다. 마스크는 필터, 커널, window 등으로 불리기도 한다.필터의 중간점을 anchor이라 한다.마스크의 형태와 값에 따라 필터의 역할이 결정된다. 영상 부드럽게 영상 날카롭게 엣지 검출 잡음 제거convolution을 하는 것과 같이 마스크 3x3 과 입력 영상 3x3을 연산해서 출력 영상의 1칸으로 출력된다. 이를 correlation이라 한다. correlation와 convolution은 비슷한 의미지만, 신호 처리의 관점으로 보면 좀 다르다. 그러나 통용적으로 convolution을 많이 사용하기도 하지만, 엄밀히 말하면 correlation이 더 적절하다.좌측 상단의 위치부터 마스크를 이동시키면서 연산을 한다. 0x0과 같이 테두리 부분들은 0x0의 zero padding을 적용해서 필터링하면 외곽과 중앙의 연산을 동일시할 수 있다. 그러나 openCV에서는       a b c d e f g h i 가 있다고 하면           b c a b c e f d e f h i g h i 와 같이 대칭으로 만들어 줄 수 있다.이 때 추가하는 것에는 대칭으로 b,c로 넣어줄수도 있고, a를 반복으로 넣어줄 수 있고, c,b처럼 넣어줄 수도 있다. 중요한 것은 바깥쪽에 중요한 객체가 있더라도 외곽을 신경쓰는 일은 불필요하다. 중앙에 있는 것들에 대해 판별을 해야 하는데 외곽을 신경쓰기보다 zero padding을 적용해놓는 것이 좋다.주변 패딩 생성void copyMakeBorder(InputArray src, OutputArray dst, int top, int bottom, int left, int right, int borderType, const Scalar&amp;amp; value = Scalar() ); src : 입력 영상 dst : 출력 영상 top/bottom/left/right : 위,아래,왼,오른쪽 추가 픽셀 bordertype : 패딩의 타입, 0, aaabc , bcabc , cbabc value : bordertype == BORDER_CONSTANT일 때의 border value필터 생성void filter2D(InputArray src, OutputArray dst, int ddepth, InputArray, Point anchor = Point(-1,-1), double delta = 0, int bordertype = BORDER_DEFAULT); src : 입력 영상 dst : 출력 영상 ddepth : 원하는 결과 영상의 깊이를 지정, -1이면 src와 같은 깊이를 사용 kernel : 필터 마스크 행렬, 1채널 실수형 anchor : 고정점 위치 , (-1,-1)이면 필터 중앙을 고정점으로 사용 delta : 추가적으로 더할 값 bordertype : 가장자리 픽셀값 설정 방법kernel까지만 설정하고, 나머지는 디폴트값쓰면 된다.현재로는 그레이스케일만 하는데, 트루컬러로 할 수도 있고, 트루 컬러중에서도 특정 색깔에 대해서만 필터를 걸 수도 있다.엠보싱(embossing)직물이나 종이, 금속판 등에 올록볼록한 형태로 만든 객체의 윤과 또는 무늬 엠보싱 필터엠보싱 필터는 입력 영상을 엠보싱 느낌이 나도록 변환하는 필터이다.       -1 -1 0 -1 0 1 0 1 1 왼쪽위는 어둡게, 오른쪽 아래는 밝게 하는 마스크#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); if (src.empty()) { cerr &amp;lt;&amp;lt; &quot;Image laod failed!&quot; &amp;lt;&amp;lt; endl; return -1; } float data[] = { -1, -1 ,0 , -1, 0, 1, 0, 1, 1 }; Mat kernel(3, 3, CV_32FC1, data); /* Mat kernel = (Mat_&amp;lt;float&amp;gt;(3,3,CV_32FC1, ...) */ Mat dst; filter2D(src, dst, -1, kernel, Point(-1, -1), 128); imshow(&quot;src&quot;, src); imshow(&quot;dst&quot;, dst); waitKey();}" }, { "title": "[데브코스] 5주차 - OpenCV Function with Mat class and useful feature ", "url": "/posts/opencvfunction/", "categories": "Classlog, devcourse", "tags": "devcourse, event", "date": "2022-03-16 14:01:00 +0900", "snippet": "유용한 OpenCV 함수행렬 합/평균/최대,최소행렬 합 구하기Scalar sum(InputArray src); src : 입력 행렬, 1~4채널 반환값 : 행렬 원소들의 합 예제 코드uchar data[] = {1, 2, 3, 4, 5, 6};Mat mat1(2,3,CV_8UC1, data);/* 1 2 3; 4 5 6 */int sum1 = (int)sum(mat1)[0]; // 21 == val[0]행렬 평균Scalar mean(InputArray src, InputArray mask = noArray()); src : 입력 행렬, 1~4채널 mask : 마스크 영상, noArray()는 mat 클래스를 비어있는 상태와 같음, 마스크라는 것은 전체 행렬에서 원소의 값이 0이 아닌 것만 수행하도록 하는 것, 즉 mask에서 0과 1이 섞여 있을 때 1인 곳의 src 위치인 것만 계산한다는 것, 그렇다면 src와 mask는 크기가 같아야 한다. 반환값 : 행렬의 평균 값 예제 코드Mat img = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);double mean = mean(img)[0]; // 124.0 그레일 스케일이므로 scalar에서 val[0]만 0이 아니고, 1,2,3은 0으로 채워져 있을 것이다. 그래서 val[0]만 불러옴. 만약 IMREAD_COLOR이라면 // Scalar m = mean(img);// m[0] = Blue, m[1] = Green, m[2] = Red// 에 대한 평균행렬 최댓값/최솟값void minMaxLoc(InputArray src, double* minVal, double* maxVal = 0, Point* minLoc = 0, Point* maxLoc = 0, InputArray mask = noArray()); src : 입력 영상, 단일 채널만 가능 minVal, maxVal : 최솟값/최댓값 변수 포인터(필요없으면 NULL 지정) minLoc, maxLoc : 최솟값/최댓값 위치 변수 포인터(필요없으면 NULL 지정) mask : 마스크 영상, mask 행렬 값이 0이 아닌 부분에서만 연산을 수행 예제 코드Mat img = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);double minv, maxv;Point minLoc, maxLoc;minMaxLoc(img, &amp;amp;minv, &amp;amp;maxv, &amp;amp;minLoc, &amp;amp;maxLoc); // 25, 245, [508,71], [116,273]여기서 아래와 같이 작성할 경우double* minv;mixMaxLoc(img, *minv, ...);*의 피연산자가 포인터여야 하는데 double형식으로 되어 있다고 나온다. 또한, 인수 목록이 일치하는 오버로드된 함수 minMaxLoc의 인스턴스가 없다고 나온다.행렬의 자료형(타입) 변환void Mat::convertTo(OutputArray m, int rtype, double alpha=1, double beta=0) const; ::란 Mat의 전역함수가 아닌 멤버함수라는 것, 그래서 ‘Mat.convertTo()’ 형태로 작성해야 한다는 것이다. m : 출력 영상(행렬) rtype : 원하는 출력 행렬 타입, CV_8UC1, CV_32FC1 등등 alpha : 추가적으로 곱할 값 beta : 추가적으로 더할 값 예제 코드Mat img = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE); // CV_8UC1Mat fimg;img.convertTo(fimg, CV_32FC1); // img를 CV_32FC1형태로 바꾸어 fimg로 저장한다.행렬의 정규화void normalize(InputArray src, InputOutputArray dst, double alpha = 1, double beta = 0, int norm_type = NORM_L2, int dtype = -1, InputArray mask = noArray()); src : 입력 행렬(영상) dst : 출력 행렬, src와 같은 크기 alpha : 노름 정규화인 경우 목표 노름(norm)값, norm_minmax인 경우에는 최솟값 beta : norm_minmax인 경우 최댓값 norm_type : 정규화 타입, NORM_INF, NORM_L1, NORM_L2, NORM_MINMAX 중 하나를 지정 dtype : 출력 행렬의 타입 mask : 마스크 영상 예제 코드Mat src = imread(&quot;lenna.bmp&quot;, IMREAD_GRAYSCALE);Mat dst;normalize(src, dst, 0, 255, NORM_MIXMAX); // 모든 픽셀값을 0~255로 픽셀값을 정규화시킨다는 것, 위에서 봤듯이 최솟값이 25, 최댓값이 245였기에 이를 조금 더 0~255로 정규화시킴색 공간 변환 함수void cvtColor(InputArray src, OutputArray dst, int code, int dstCn = 0); src : 입력 영상 dst : 출력 영상 code : 색 변환 코드 COLOR_BGR2GRAY, COLOR_BGR2RGB, COLOR_BGR2HSV, COLOR_BGR2YCrCb 등등, opencv 문서 페이지 참고 dstCn : 결과 영상의 채널 수, 0이면 자동 결정됨 예제 코드Mat src = imread(&quot;lenna.bmp&quot;); // 3channelMat dst;cvtColor(src, dst, COLOR_BGR2GRAY); // 3 -&amp;gt; 1channel채널 분리/병합채널 분리void splic(const Mat&amp;amp; src, Mat* mvbegin); // 참조긴하나 그냥 Mat 타입 변수를 지정하면 될듯void split(InputArray src, OutputArrayOfArrays mv); src : (입력) 다채널 행렬 mvbegin : (출력) Mat 배열의 주소 mv : (출력) 행렬의 벡터, vector 예제 코드Mat src = imread(&quot;lenna.bmp&quot;);vector&amp;lt;Mat&amp;gt; planes;split(src, planes);1개의 다채널 행렬을 여러 개의 단일 채널로 분리해주는 것이다.채널 병합void merge(const Mat* mv, size_t const, OutputArray dst);void merge(InputArrayOfArrays mv, OutputArray dst); mv : (입력) 1채널 Mat 배열 또는 행렬의 벡터 count : (mv가 Mat타입의 배열인 경우) Mat 배열의 크기 dst : (출력) 다채널 행렬합치려는 영상을 배열로 받을 수도 있고, 벡터 Mat타입을 받을 수도 있다. 예제 코드Mat src = imread(&quot;lenna.bmp&quot;);vector&amp;lt;Mat&amp;gt; planes;split(src, planes);swap(planes[0], planes[2]); //planes[0](blue)과 planes[2](red)를 바꿔주는 것Mat dst;merge(planes, dst); // blue와 red가 바뀐 상태로 출력될 것이다.유용한 OpenCV 함수연산 시간 측정 방법연산 시간 측정대부분의 영상 처리 시스템은 대용량 영상 데이터를 다루고 복잡한 알고리즘 연산을 수행한다. 영상 처리 시스템 각 단계에서 소요되는 연산 시간을 측정하고 시간이 오래 걸리는 부분을 찾아 개선하는 시스템 최적화 작업이 필수적이다. 그러므로 OpenCV에서도 연산 시간 측정을 위한 함수를 지원한다. getTickCount, getTickFrequencyint64 t1 = getTickCount();my func();int64 t2 = getTickCount();double ms = (t2 - t1) * 1000 / getTickFrequency(); getTickCount() : tick이라는 발생 횟수를 세는 것, tick은 초의 단위가 아니기 때문에 frequency를 사용해서 구해야 한다. getTickFrequency() : 이를 그대로 나눠주면 상당히 작게 나와서 1000을 곱해서 ms로 구한다. 연산 시간 측정은 릴리즈 모드에서 수행해야 한다. TickMeter 클래스개념적으로 복잡하고, 사용이 번거로워서 새로운 클래스를 사용하면 편하다. 이는 직관적인 인터페이스를 제공하고, getTickCount와 getTickFrequency를 조합해서 시간을 측정한다.TickMeter의 정의는 다음과 같다.class TickMeter{ public: TickMeter(); // 기본 생성자 void start(); // 시간 측정을 시작할 때 사용 void stop(); // 시간 측정을 멈출 때 사용 void reset(); // 시간 측정을 초기화할 때 사용 double getTimeMicro() const; // 연산 시간을 마이크로 초 단위로 반환 double getTimeMilli() const; // 연산 시간을 밀리초 단위로 반환 double getTimeSec() const; // 연산 시간을 초 단위로 반환 ...} 예제 코드TickMeter tm;tm.start():func1();tm.stop();cout &amp;lt;&amp;lt; &quot;func1(): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms. &quot; &amp;lt;&amp;lt; endl;tm.reset(); // reset을 하지 않으면 앞의 시간과 합쳐진다.tm.start();func2();tm.stop();cout &amp;lt;&amp;lt; &quot;func2(): &quot; &amp;lt;&amp;lt; tm.getTimeMilli() &amp;lt;&amp;lt; &quot;ms. &quot; &amp;lt;&amp;lt; endl;ROI 영역과 마스크 연산ROIROI : region of interest 영상에서 특정 연산을 수행하고자 하는 임의의 부분 영역마스크 연산opencv는 일부 함수에 대해 ROI 연산을 지원하며, 이 때 마스크 영상(mask image)를 인자로 함께 전달해야 한다. 마스크 영상으 CV_8UC1타입(grayscale), 마스크 영상의 픽셀 값이 0이 아닌 위치에서만 연산을 수행한다. 보통 마스크 영상으로는 0 또는 255로 구성된 이진 영상(binary image)를 사용한다.void Mat::CopyTo(InputArray m, InputArray mask) const;void copyTo(InputArray src, OutputArray dst, InputArray mask);멤버 함수 m : 출력 영상, 만약 *this(자기 자신)와 크기 및 타입이 같은 m을 입력으로 지정하면 m을 새로 생성하지 않고 연산을 수행하고, 그렇지 않으면 m을 새로 생성하여 연산을 수행한 후 반환한다. mask : 마스크 영상, CV_8U.0이 아닌 픽셀에 대해서만 복사 연산을 수행한다.전역 함수 src : 입력 영상 mask : 마스크 영상 dst : 출력 영상전체 동작은 src는 원본 데이터, dst는 배경 데이터, mask는 물체에 대한 마스크, 즉 물체 이외의 부분은 0으로 되어 있다. 이들을 합치면 영상 합성이 된다.Mat src = imread(&quot;airplane.bmp&quot;, IMREAD_COLOR);Mat mask = imread(&quot;mask_plane.bmp&quot;, IMREAD_GRAYSCALE); // 회색이나 실제로는 컬러이므로 grayscale로 불러와야 한다.Mat dst = imread(&quot;field.bmp&quot;, IMREAD_COLOR);//copyTo(src, dst, mask);src.copyTo(dst, mask); // src의 mask즉 0이 아닌 부분들만 dst에 복사해서 합성imshow(&quot;src&quot;, src);imshow(&quot;dst&quot;, dst);imshow(&quot;mask&quot;, mask);waitKey();이 src와 dst는 같은 크기와 같은 타입이어야 한다. 그리고 위의 코드는 mask 이미지가 있는 경우에 가능하다. 만약 mask가 없다면 png 파일에 알파 채널이 있는지 확인하기 위해 GIMP 프로그램을 통해 열어본다.알파채널이 있다면, [B,G,R,α]에서 앞에 [B,G,R]은 일반적인 컬러 형태의 Mat 클래스 객체로, α채널은 그레이스케일의 Mat 클래스 객체로 만들면 된다. 그리고 중요한 것은 배경 이미지와 물체 이미지의 크기가 다르다면 자신이 물체를 넣을 위치를 잘라서 크기를 맞춰주어야 한다.Mat src = imread(&quot;cat.bmp&quot;, IMREAD_COLOR);Mat logo = imread(&quot;opencv-logo-white.png&quot;, IMREAD_UNCHANGED); // unchanged로 하면 알파가 없어진다.vector&amp;lt;Mat&amp;gt; planes;split(logo, planes);Mat mask = planes[3]; // 알파 채널은 mask로 지정merge(vector&amp;lt;Mat&amp;gt;(planes.begin(), planes.begin() + 3), logo); // 첫번째 인자는 vectorMat을 새로 만드는데, planes 맨 앞부터 3개만 사용해서 vector&amp;lt;Mat&amp;gt; 객체를 새로 만들어서 logo로 출력Mat crop = src(Rect(10, 10, logo.cols, logo.rows));logo.copyTo(crop, mask); // cv 함수로 mask연산이 옵션으로 되어 있다. 즉, 0이 아닌 값들에 대해서만 logo를 copy하여 crop에 추가한다.imshow(&quot;src&quot;, src);waitKey();destroyAllWindows();" }, { "title": "[데브코스] 5주차 - OpenCV Keyboard and Mouse Event Processing ", "url": "/posts/event/", "categories": "Classlog, devcourse", "tags": "devcourse, event", "date": "2022-03-16 01:45:00 +0900", "snippet": "키보드 이벤트 처리하기키보드 입력 대기int waitKey(int delay = 0); delay : 밀리초 단위의 대기 시간, delay &amp;lt;= 0 이면 무한히 기다림 반환값 : 눌린 키 값, 키가 눌리지 않으면 -1 참고 사항 waitkey() 함수는 OpenCV 창이 하나라도 있어야 정상 작동한다. imshow() 함수 호출 후 waitkey() 함수를 호출해야 영상이 화면에 나타남 주요 특수 키 코드 : ESC = 27, ENTER = 13, TAB = 9 화살표키, 함수키 등의 특수키에 대한 이벤트 처리를 수행할 때는 waitkeyEx() 함수 사용 마우스 이벤트 처리하기마우스 이벤트 처리를 위한 콜백 함수 등록void setMouseCallback(const String&amp;amp; winname, MouseCallback onMouse, void* userdata = 0);이를 호출하게 되면 사용자가 지정한 창에서 발생하는 mouse event를 사용자가 다시 처리할 수 있게 해준다. winname : 창이름 onMouse : 마우스 콜백 함수 이름 아래와 같은 형식의 함수를 정의하고, 해당 함수 이름을 지정하면 된다 typedef void (*MouseCallback)(int event, int x, int y, int flags, void* userdata); event : 마우스 이벤트 종류, MouseEventTypes 상수 (더 자세한 것은 MouseEventTypes 정의를 보면 나온다) x,y : 마우스 이벤트 발생 좌표 flags : 마우스 이벤트 플래그 ,MouseEventFlags 상수, 이벤트가 발생할 때의 상태를 나타내는 플래그, (더 자세한 것은 MouseEventFlags 정의를 보면 나온다) userdata : setMouseCallback() 함수에서 지정한 값이 여기로 넘어오는 사용자 지정 데이터 userdata : 콜백 함수에 전달할 사용자 지정 데이터 (optional)void on_mouse(int event, int x, int y, int flags, void*);int main(void) { Mat src = imread(&quot;lenna.bmp&quot;); namedWindow(&quot;src&quot;); setMouseCallback(&quot;src&quot;, on_mouse); imshow(&quot;src&quot;, src); /* imshow(&quot;src&quot;, src); setMouseCallback(&quot;src&quot;, on_mouse); */}void on_mouse(int event, int x, int y, int flags, void*) { cout &amp;lt;&amp;lt; &quot;on_mouse&quot; &amp;lt;&amp;lt; endl;}이처럼 setMouseCallback() 함수는 윈도우가 생성되어 있어야 작동하기 때문에 namedWindow를 통해 먼저 선언을 해주거나 imshow 뒤에 설정해두어야 한다. 그리고 3번째 인자로 void* 이 있어야 하지만, 디폴트값이 있어서 작성하지 않았다.두번째 인자로는 마우스 콜백함수의 이름을 지정해준다. 이는 미리 선언해줘야 한다. 그래서 위에 작성해주고 정의 코드를 작성해준다. 5번째 인자도 필요하지 않으면 작성하지 않아도 된다.마지막으로 정의 부분은 마우스가 창 위에 올라오는 순간 마우스를 인식해서 매번 on_mouse를 출력할 것이다.그래서 특정 마우스 이벤트일 때만 출력할 수 있도록 수정을 해보고자 한다.void on_mouse(int event, int x, int y, int flags, void*) { switch (event) { case EVENT_LBUTTONDOWN: cout &amp;lt;&amp;lt; &quot;EVENT_LBUTTONDOWN: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;, &quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl; break; case EVENT_RBUTTONDOWN: cout &amp;lt;&amp;lt; &quot;EVENT_RBOTTONDOWN: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;, &quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl; case EVENT_MOUSEMOVE: cout &amp;lt;&amp;lt; &quot;EVENT_MOUSEMOVE: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;, &quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl; default: break; }}이렇게만 작성해도 되지만, 내가 마우스 이벤트를 발생시키는 순간 키보드나 마우스의 상태를 flags를 통해서 확인할 수 있다. 그래서 이를 if문에 사용하면 더 정밀한 출력을 만들 수 있다. case EVENT_MOUSEMOVE: if (flags == EVENT_FLAG_LBUTTON) cout &amp;lt;&amp;lt; &quot;EVENT_MOUSEMOVE: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;, &quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;그리고 지난 번에 배웠던 circle을 추가하여 사용해본다. case EVENT_MOUSEMOVE: if (flags &amp;amp; EVENT_FLAG_RBUTTON) { circle(src, Point(x,y), 5, Scalar(200,200,0), 3, LINE_AA); // line(src, Point(x,y), 2 Scalar(0,255,255), -1, LINE_AA); // 누를 때마다 표기하여 그림그리기 ptOld = Point(x, y); // 사용자가 빠르게 움직이면 끊기기 때문에, 이전에 위치한 위치를 기록한 후 이전 위치와 현재 위치 사이를 직선을 만들도록 하는 것이 중요하다. imshow(&quot;src&quot;, src); }여기서 flags는 정의에 보면 정수로 표현되어 있다. 그렇기에 이를 == 이 아니라 &amp;amp;를 통해 설정되어 있는지를 확인해서 조건을 거는 것이 좋다. 왜냐하면 여기서 오른쪽 버튼을 누르면서 &amp;lt;ctrl&amp;gt;키를 함께 누를 경우 출력되지 않는 것을 볼 수 있다.추가로 src라는 것은 메인 함수에 정의된 지역 변수다. 그렇다면 여기서 동작시키려면 에러가 나기 때문에 지역 변수를 전역 변수로 변환해줘야 한다. 따라서 main()안이 아닌 밖으로 Mat src; 를 추가하고, main()함수에서는 src = imread(&quot;lenna.bmp&quot;); 라고 작성해야 한다.트랙바 사용트랙바(trackbar) : 영상 출력 창에 부착되어 프로그램 동작 중에 사용자가 지정도니 범위 안의 값을 선택할 수 있는 GUI, 슬라이더 컨트롤이다.int createTrackbar(const String&amp;amp; trackbarname, const String&amp;amp; winname, int* value, int count, TrackbarCallback onChange = 0, void* userdata = 0); trackbarname : 트랙바 이름, 바 옆에 나오는 이름(창 이름 아님) winname : 트랙바를 생성할 창 이름 value : 트랙바 위치 값을 받을 정수형 변수의 주소, 포인터함수이므로 사용자가 트랙바를 움직일 때마다의 값을 저장 count : 트랙바 최대 위치 (최소 위치는 항상 0) onChange : 트랙바 위치가 변경될 때마다 호출되게 만들 콜백 함수 이름(함수의 포인터) 만약 NULL을 지정하면 콜백 함수는 호출되지 않고 Value로 지정한 변수값만 갱신됨 typedef void (*TrackbarCallback)(int pos, void* userdata); userdata : 트랙바 콜백 함수에 전달할 사용자 데이터의 포인터 (optional) 반환값 : 정상 동작하면 1, 실패하면 0 이 함수를 호출하기 전에 미리 창이 생성되어 있어야 한다. 트랙바 예제#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;void trackbarcallback(int pos, void* userdata);int main(void){ Mat img = Mat::zeros(600, 600, CV_8UC1); namedWindow(&quot;image&quot;); createTrackbar(&quot;level&quot;, &quot;image&quot;, 0, 16, trackbarcallback, (void*)&amp;amp;img); // value가 0==NULL 값으로 할경우 반드시 콜백함수를 지정해야 한다. // (void*)&amp;amp;img : img의 주소값을 전달하는데 이를 void* 타입으로 설정하는데 이는 trackbarcallback에 userdata로 전달이 된다. imshow(&quot;image&quot;, img); waitKey();}void trackbarcallback(int pos, void* userdata) // pos == 현재 트랙바 위치, userdata == (void*)&amp;amp;img{ Mat img = *(Mat*)userdata; // 받은 userdata를 img로 만듬, img.setTo(pos * 16); // 받은 pos * 16을 곱한 값을 img 영상의 모든 픽셀을 설정, pos는 0~16사이의 정수값일 것이다. 255를 넘어가면 255로 설정되어 출력된다. imshow(&quot;image&quot;, img);}" }, { "title": "[데브코스] 5주차 - OpenCV video processing ", "url": "/posts/imgproc/", "categories": "Classlog, devcourse", "tags": "opencv, devcourse", "date": "2022-03-15 18:38:00 +0900", "snippet": "카메라와 동영상 처리VideoCapture 클래스OpenCV에서는 카메라와 동영상으로부터 프레임(frame)을 받아오는 작업을 VideoCapture 클래스 하나로 처리한다. 일단 카메라와 동영상을 여는 작업이 수행되면, 이후에는 매 프레임을 받아오는 공통의 작업을 수행한다.캠과 파일을 open()으로 열게 되면 read()를 통해 영상파일로부터 프레임을 받아온다.추가적으로 isOpened() : 현재 장치나 비디오 파일이 정상적으로 오픈되어 있는지 확인 grab()/retrieve() : read()가 이 두개의 조합으로 구성되는데, grab()은 카메라에게 캡쳐를 시작하라는 명령, retrieve()는 캡쳐된 영상을 프로그램으로 전송받는 명령 get()/set() : get()은 속성을 받아옴, set()은 속성 정보를 설정 release() : 사용이 끝나면 끝났음을 알려주는 것VideoCapture 클래스의 정의는 다음과 같다.class VideoCapture{ public: /* 생성자 */ VideoCapture(); VideoCapture(const String&amp;amp; filename, int apiPreference = CAP_ANY); VideoCapture(int index, int apiPreference = CAP_ANY); virtual ~VideoCapture(); // 소멸자 virtual bool open(const String&amp;amp; filename, int apiPreference = CAP_ANY); // 문자를 받을 때, 동영상 파일을 오픈할 때 사용 virtual bool open(int index, int apiPreference = CAP_ANY); // 정수를 입력으로 받을 때, 카메라 장치를 오픈할 때 사용 virtual void release(); // 현재 사용이 끝날 때 호출 virtual VideoCapture&amp;amp; operator &amp;gt;&amp;gt; (Mat&amp;amp; image); // 프레임 받아오기 virtual bool VideoCapture::read(OutputArray image); // 프레임 받아오기 virtual bool set(int propId, double value); // 속성 설정 virtual double get(int propId) const; // 속성 참조 ...}생성자와 open이 동일하게 인자를 받을 수 있기 때문에 생성과 동시에 오픈을 할 수가 있다.카메라 열기VideoCapture::VideoCapture(int index, int apiPreference = CAP_ANY);bool VideoCapture::open(int index, int apiPreference = CAP_ANY); index : 사용할 캡쳐 장치의 ID, 기본적으로 시스템은 0번부터 시작하기 때문에, 0으로 지정하면 첫번째, 여러 대의 카메라가 연결되어 있으면 0, 1, 순서로 지정하면 된다. apiPreference : 선호하는 카메라 처리 방법을 지정, 대부분 Opencv에서 알아서 지정해준다. e.g. cv::CAP_DSHOW, cv::CAP_MSMF, cv::CAP_V4L, etc 반환값 : VideoCapture 생성자는 VideoCapture 객체를 반환한다. VideoCapture::open() 함수는 작업이 성공하면 true, 실패하면 false동영상 파일 열기VideoCapture::VideoCapture(const String&amp;amp; filename, int apiPreference = CAP_ANY);bool VideoCapture::open(const String&amp;amp; filename, int apiPreference = CAP_ANY); filename : 동영상 파일 이름, 정지 영상 시퀀스(여러 장의 연속 이미지), 비디오 스트림 URL 등 비디오 파일 - ‘video.avi’, ‘./data/video.avi’ 정지 영상 시퀀스 - ‘img_%02d.jpg’ 비디오 스트림 URL - “https://… mp4” apiPreference : 위와 동일 반환값 : 위와 동일현재 프레임 받아오기bool VideoCapture::read(OutputArray image);VideoCapture&amp;amp; VideoCapture::operator &amp;gt;&amp;gt; (Mat&amp;amp; image); image : 현재 프레임, 만약 현재 프레임을 받아오지 못하면 비어 있는 영상으로 설정 반환값 : VideoCapture::read()함수는 작업이 성공하면 true, 실패하면 falseOutputArray 형태이므로 Mat frame 변수에 데이터를 받아 read()안에 넣어도 된다. 참고사항 - ‘»’ 연산자 오버로딩은 내부에서 read()함수를 재호출하는 래퍼(wrapper)함수다. 그래서 ‘»’ 대신 read()함수를 사용해도 된다. 카메라 열어 프레임 출력그렇다면 시스템에 내장되어 있는 카메라를 불러오는 코드를 작성해보았다. 카메라가 오픈되지 않았다면 failed를 출력할 것이다. 카메라를 오픈했다면 아무 메시지도 뜨지 않는다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture cap; cap.open(0); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Camera open failed&quot; &amp;lt;&amp;lt; endl; return -1; }}정상적으로 작동한다면 다음 진행을 작성한다. 먼저 프레임마다 불러올 것이기 때문에 while문을 사용하고, 프레임을 frame 변수에 저장할 것이기에 frame 변수를 선언해서 cap.read(frame)를 사용해서 읽어온다. 그리고 안전성을 위해 불러오지 못한 경우에는 빠져나가도록 하고, imshow를 하여 윈도우를 생성해주면서 창에 이미지를 표현한다. 이 때, waitKey()를 하지 않는다면 이미지가 나오지 않을 것이다. 그래서 waitKey를 설정해준다.#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture cap; cap.open(0); /* 이 두개를 합쳐서 VideoCapture cap(0); 으로 해도 된다. */ if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Camera open failed&quot; &amp;lt;&amp;lt; endl; return -1; } Mat frame, edge; // frame이란 한장의 정지 영상 while (true) { cap &amp;gt;&amp;gt; frame; // == cap.read(frame); if (frame.empty()) { cerr &amp;lt;&amp;lt; &quot;Frame empty!&quot; &amp;lt;&amp;lt; endl; break; } Canny(frame, edge, 50, 150); // frame에서 외각을 따서 만드는 것 imshow(&quot;frame&quot;, frame); imshow(&quot;edge&quot;, edge); if (waitKey(1) == 27) //ESC break; } cap.release(); // 카메라 종료 destroyAllWindows(); // 생성했던 창을 닫아줌}여기서 주의할 점은 waitKey()로 아무 인자없이 실행하면 움직이지 않다가 키를 입력해야 움직이는 것을 볼 수 있다. 그래서 무제한으로 기다리는 것이 아니라 1ms만 기다리고 다음 프레임을 받도록 설정해야 계속 움직인다.여기서 1ms만 기다리라 해서 1ms만 기다리고 다음 프레임을 받아오는 것이 아니라 카메라마다의 FPS를 통해서 만약 30fps라면 33msec 마다 1장씩 날아오므로 loop문 처음으로 가서 프레임이 올 때까지, 즉 32ms를 더 기다리고 받아서 시작할 것이다. 동영상 파일 프레임 출력#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ /* 카메라 VideoCapture cap(0); */ VideoCapture cap(&quot;test_video.mp4&quot;); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Camera open failed&quot; &amp;lt;&amp;lt; endl; return -1; } Mat frame, edge; // frame이란 한장의 정지 영상 while (true) { cap &amp;gt;&amp;gt; frame; // == cap.read(frame); if (frame.empty()) { cerr &amp;lt;&amp;lt; &quot;Frame empty!&quot; &amp;lt;&amp;lt; endl; // 동영상 맨 마지막으로 가도 이것이 실행되고 종료 될 것이다. break; } Canny(frame, edge, 50, 150); // frame에서 외각을 따서 만드는 것 imshow(&quot;frame&quot;, frame); imshow(&quot;edge&quot;, edge); if (waitKey(1) == 27) // ESC break; } cap.release(); // 비디오 종료 destroyAllWindows(); // 생성했던 창을 닫아줌}카메라와 동영상 속성 값 참조 및 설정double VideoCapture::get(int propId) const;bool VideoCapture::set(int propId, double value); probId : 속성 플래스 CAP_PROP_POS_MSEC : 비디오 파일에서 현재 위치를 msec단위로 표기 CAP_PROP_FRAME_WIDTH : 프레임 가로 크기 CAP_PROP_FRAME_HEIGHT : 프레임 세로 크기 CAP_PROP_FPS : 초당 프레임 수 CAP_PROP_FRAME_COUNT : 비디오 파일의 총 프레임 수 CAP_PROP_POS_FRAMES : 현재 프레임 번호 CAP_PROP_BRIGHTNESS : 밝기 CAP_PROP_EXPOSURE : 노출 value : 속성값, 필요한 경우 정수형으로 형변환하여 사용 반환값 : get : 세로나 가로는 정수지만, 통일화하기 위해 double로 추출 set : 성공하면 true, 실패하면 false 현재 카메라 크기 출력int w = cvRound(cap.get(CAP_PROP_FRAME_WIDTH)); // == (int)(cap.get(...)int h = cvRound(cap.get(CAP_PROP_FRAME_HEIGHT)); // == (int)(cap.get(...)cout &amp;lt;&amp;lt; &quot;width: &quot; &amp;lt;&amp;lt; w &amp;lt;&amp;lt; &quot;, Height: &quot; &amp;lt;&amp;lt; h &amp;lt;&amp;lt; endl;이를 실행하면 다음과 같이 출력된다. 크기 조정cap.set(CAP_PROP_FRAME_WIDTH, 640);cap.set(CAP_PROP_FRAME_HEIGHT, 480);int w = cvRound(cap.get(CAP_PROP_FRAME_WIDTH)); int h = cvRound(cap.get(CAP_PROP_FRAME_HEIGHT)); cout &amp;lt;&amp;lt; &quot;width: &quot; &amp;lt;&amp;lt; w &amp;lt;&amp;lt; &quot;, Height: &quot; &amp;lt;&amp;lt; h &amp;lt;&amp;lt; endl;이렇게 하면 다음과 같이 변화한 것을 볼 수 있다.이는 카메라만 가능하고, 동영상의 경우 이미 정해져있기 때문에 동작하지 않을 것이다. fps 출력하기double fps = cap.get(CAP_PROP_FPS);cout &amp;lt;&amp;lt; &quot;fps: &quot; &amp;lt;&amp;lt; fps &amp;lt;&amp;lt; endl;현재 가지고 있는 동영상의 fps는 50이라는 것을 알 수 있다. 이것을 카메라로 돌리면 0으로 나올 것이다.VideoWriter 클래스OpenCV에서는 일련의 정지 영상을 동영상 파일로 저장할 수 있는 VideoWriter 클래스를 제공한다.VideoWriter의 정의는 다음과 같다class VideoWriter{ public: /* 저장할 동영상 파일 열기 */ VideoWriter(); VideoWriter (const String&amp;amp; filename, int fourcc, double fps, Size frameSize, bool isColor = true); virtual ~VideoCapture(); virtual bool open (const String&amp;amp; filename, int fourcc, double fps, Size frameSize, bool isColor = true); virtual void release(); virtual VideoCapture&amp;amp; operator &amp;lt;&amp;lt; (const Mat&amp;amp; image); // 프레임 저장 /* 정보 설정하기(set) &amp;amp; 가져오기(get) */ virtual bool set(int propId, double value); virtual double get(int porpId) const; ...}VideoWriter::VideoWriter (const String&amp;amp; filename, int fourcc, double fps, Size frameSize, bool isColor = true);bool VideoWriter::open(const String&amp;amp; filename, int fourcc, double fps, Size frameSize, bool isColor = true); filename : 저장할 동영상 파일 이름 fourcc : 압축 방식을 나타내는 4개의 문자, VideoWriter::fourcc() 함수로 생성하여 인자로 넣으면 된다. VideoWriter::fourcc(‘D’,’I’,’V’,’X’) : DIVX MPEG-4 코덱 VideoWriter::fourcc(‘X’,’V’,’I’,’D’) : XVID MPEG-4 코덱 VideoWriter::fourcc(‘X’,’2’,’6’,’4’) : H.264/AVC 코덱 VideoWriter::fourcc(‘M’,’J’,’P’,’G’) : Motion-JPEG 코덱 fps : 초당 프레임 수 frameSize : 비디오 크기 isColor : 컬러 동영상 플래그, false로 지정하면 그레이스케일 동영상카메라 동영상으로 저장하기#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture cap(0); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Camera open failed!&quot; &amp;lt;&amp;lt; endl; return -1; } int fourcc = VideoWriter::fourcc(&#39;X&#39;, &#39;V&#39;, &#39;I&#39;, &#39;D&#39;); double fps = 15; // 자신의 카메라에 맞게 설정 Size sz((int)cap.get(CAP_PROP_FRAME_WIDTH), (int)cap.get(CAP_PROP_FRAME_HEIGHT)); // Size sz = Size((int)cap.get(CAP_PROP_FRAME_WIDTH), (int)cap.get(CAP_PROP_FRAME_HEIGHT)); cout &amp;lt;&amp;lt; &quot;FPS = &quot; &amp;lt;&amp;lt; fps &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;Size = &quot; &amp;lt;&amp;lt; sz &amp;lt;&amp;lt; endl; VideoWriter output(&quot;output.avi&quot;, fourcc, fps, sz); if (!output.isOpened()) { cerr &amp;lt;&amp;lt; &quot;output.avi open failed!&quot; &amp;lt;&amp;lt; endl; return -1; } int delay = cvRound(1000 / fps); // 프레임의 시간 간격을 구한 것, ms로 넣어야 하는데, fps란 frame per second이므로 이를 단위 변환 Mat frame, edge; while (true) { cap &amp;gt;&amp;gt; frame; if (frame.empty()) break; Canny(frame, edge, 50, 150); cvtColor(edge, edge, COLOR_GRAY2BGR); // edge는 grayscale이므로 변환해줘야 한다. output &amp;lt;&amp;lt; edge; // 기록을 시작하겠다는 redirection, 즉 edge를 output으로 저장하겠다. imshow(&quot;frame&quot;, frame); imshow(&quot;edge&quot;, edge); if (waitKey(delay) == 27) break; } cout &amp;lt;&amp;lt; &quot;output.avi file is created!!!&quot; &amp;lt;&amp;lt; endl; output.release(); cap.release(); destroyAllWindows();}OpenCV 그리기 함수 그리기 방식 세부 그리기 방식 함수 이름 선 그리기 직선 그리기 line()   화삺표 그리기 arrowedline()   마커 그리기 drawMarker() 도형 그리기 사각형 그리기 rectangle()   원 그리기 circle()   타원 그리기 ellipse()   다각형 그리기 polylines(), fillPoly() 문자열 출력하기 문자열 출력하기 putText()   출력 문자열 크기 계산 getTextSize() fillPoly()는 다각형을 채워서 그린다.직선 그리기void line(InputArray img, Point pt1, Point pt2, const Scalar&amp;amp; color, int thickness=1, int lineType = LINE_8, int shift = 0); img : 입출력 영상 pt1, pt2 : 시작점, 끝점 좌표 color : 선 색상 또는 밝기, Scalar(0,0,255) thickness : 선 두께 lineType : 선 타입, LINE_4, LINE_8, LINE_AA 중 하나를 지정, 이는 직선 그리기 방식이 따로 있다. shift : 그리기 좌표 값의 축소 비율이것을 진행하면 픽셀값이 달라지기 때문에, img를 나중에 다시 사용해야 한다면 복사해놔야 한다.사각형 그리기void rectangle(InputArray img, Rect rec, const Scalar&amp;amp; color, int thickness=1, int lineType = LINE_8, int shift = 0); img : 입출력 영상 rec : 사각형 위치 정보 color : 선 색상 또는 밝기, Scalar(0,0,255) thickness : 선 두께, 음수 (-1)를 지정하면 내부를 채움 lineType : 선 타입, LINE_4, LINE_8, LINE_AA 중 하나를 지정 shift : 그리기 좌표 값의 축소 비율원 그리기void circle(InputArray img, Point center, int radius, const Scalar&amp;amp; color, int thickness=1, int lineType = LINE_8, int shift = 0); img : 입출력 영상 center : 원 중심 좌표 radius : 원 반지름 color : 선 색상 또는 밝기, Scalar(0,0,255) thickness : 선 두께, 음수 (-1)를 지정하면 내부를 채움 lineType : 선 타입, LINE_4, LINE_8, LINE_AA 중 하나를 지정 shift : 그리기 좌표 값의 축소 비율다각형 그리기void polylines(InputArray img, InputArrayOfArrays pts, bool isClosed, const Scalar&amp;amp; color, int thickness=1, int lineType = LINE_8, int shift = 0); img : 입출력 영상 pts : 다각형 꼭짓점(외각선) 점들의 집합 , vector&amp;lt;Point&amp;gt; isClosed : true이면 시작점과 끝점을 서로 이음 (폐곡선) color : 선 색상 또는 밝기, Scalar(0,0,255) thickness : 선 두께, 음수 (-1)를 지정하면 내부를 채움 lineType : 선 타입, LINE_4, LINE_8, LINE_AA 중 하나를 지정 shift : 그리기 좌표 값의 축소 비율문자열 출력하기void putText(InputArray img, const String&amp;amp; text, Point org, int fontFace, double fontScale, Scalar color, int thickness = 1, int lineType = LINE_8, bottomLeftOrigin = false); img : 입출력 영상 text : 출력할 문자열 org : 문자열이 출력될 좌측 하단 시작 좌표 fontFace : 폰트 종류, cv::HersheyFonts 로 시작하는 폰트로 지정해야 함 fontScale : 폰트 크기 지정, 기본 폰트 크기의 배수를 지정 color : 문자열 색상 thickness : 폰트 두께 lineType : 선 타입, LINE_4, LINE_8, LINE_AA 중 하나를 지정#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace std;using namespace cv;int main(){ VideoCapture cap(&quot;test_video.mp4&quot;); if (!cap.isOpened()) { cerr &amp;lt;&amp;lt; &quot;Video open failed!&quot; &amp;lt;&amp;lt; endl; return -1; } Mat frame; while (true) { cap &amp;gt;&amp;gt; frame; if (frame.empty()) { cerr &amp;lt;&amp;lt; &quot;Empty frame!&quot; &amp;lt;&amp;lt; endl; break; } line(frame, Point(570, 280), Point(0, 560), Scalar(255, 0, 0), 2); // 임의의 점을 잡아 차선 인식 해놓음 line(frame, Point(570, 280), Point(1024, 720), Scalar(255, 0, 0), 2); int pos = cvRound(cap.get(CAP_PROP_POS_FRAMES)); String text = format(&quot;frame number: %d&quot;, pos); putText(frame, text, Point(20, 50), FONT_HERSHEY_SIMPLEX, 0.7, Scalar(0, 0, 255), 1, LINE_AA); // frame number 화면에 출력 imshow(&quot;frame&quot;, frame); if (waitKey(10) == 27) break; } cap.release(); destroyAllWindows();}" }, { "title": "[데브코스] 5주차 - OpenCV primary Class ", "url": "/posts/opencvclass/", "categories": "Classlog, devcourse", "tags": "devcourse, opencv", "date": "2022-03-15 02:45:00 +0900", "snippet": "OpenCV 주요 클래스Point_ 클래스point_ 클래스 : 2차원 점의 좌표 표현을 위한 템플릿 클래스 멤버 변수 : x,y ( 점 좌표 ) 멤버 함수 : dot() : 내적 계산 ddot() : 더블 타입으로 내적 계산 cross() : 외적 계산 inside() : 한 점이 어떤 사각형 안에 존재 여부 다양한 사칙 연산에 대한 연산자 오버로딩과 std::cout 출력을 위한 « 연산자(shift 연산자) 오버로딩을 지원아래는 point_ 클래스의 정의를 코드화시킨 것이다. typedef 부분은 생성자 부분으로 어떻게 타입을 넣느냐에 따라 다르다는 것을 보여준다.template&amp;lt;typename _Tp&amp;gt; class Size_{ public: ... _Tp x, y; // the point coordinates}typedef Point_&amp;lt;int&amp;gt; Point2i;typedef Point_&amp;lt;int64&amp;gt; Point2l;typedef Point_&amp;lt;float&amp;gt; Point2f;typedef Point_&amp;lt;double&amp;gt; Point2d;typedef Point2i Point;가장 많이 사용되는 정수형태를 Point2i로 정의한 후 그것을 다시 Point로 정의하여 쉽게 사용할 수 있도록 해놓았다. Point 연산의 예point 객체끼리의 덧셈과 뺄셈, 한 점과 숫자 값과의 곱셈 등의 연산을 지원한다.Point pt1, pt2(4,3), pt3(2,4) // pt1 == [0, 0]int a = 2;pt1 = pt2 + pt3; // pt1 == [6, 7]pt1 = pt2 - pt3; // pt1 == [2, -1]pt1 = pt3 * a; // pt1 == [4, 8]pt1 = a * pt3; // pt1 == [1, 2]pt1 += pt2; // pt1 == [5, 5]pt *= a; // pt1 == [2, 4]double v = norm(pt2); // v = 5.0 원점에서 점까지의 거리bool b1 = pt1 == pt2; // b1 = falsebool b2 = pt1 != pt2; // b2 = truecout &amp;lt;&amp;lt; pt1 &amp;lt;&amp;lt; endl; // &quot;[1, 2]&quot;Size_ 클래스size_ 클래스 : 영상 또는 사각형의 크기 표현을 위한 템플릿 클래스 멤버 변수 : width, height 멤버 함수 : area() - 사각형의 면적 계산 다양한 사칙 연산에 대한 연산자 오버로딩과 std::cout 출력을 위한 « 연산자 오버로딩을 지원template&amp;lt;typename _Tp&amp;gt; class Size_{ public: ... _Tp width, height; // the width and the height};typedef Size_&amp;lt;int&amp;gt; Size2i;typedef Size_&amp;lt;int64&amp;gt; Size2l;typedef Size_&amp;lt;float&amp;gt; Size2f;typedef Size_&amp;lt;double&amp;gt; Size2d;typedef Size2i Size;Rect_ 클래스rect_ 클래스 : 2차원 사각형 표현을 위한 템플릿 클래스 멤버 변수 : x, y, width, height 멤버 함수 : tl() : 좌측 상단 점의 좌표를 반환 br() : 오른쪽 하단의 점의 좌표를 반환 size() : size 객체를 반환 area() : 사각형 크기 계산 contains() : 한 점이 어떤 사각형안에 포함되어 있는지 여부 다양한 사칙 연산에 대한 연산자 오버로딩과 std::cout 출력을 위한 « 연산자 오버로딩을 지원template&amp;lt;typename _Tp&amp;gt; class Rect_{ public: ... _Tp x, y, width, height; // coordinate of upper left point and width and height};typedef Rect_&amp;lt;int&amp;gt; Rect2i;typedef Rect_&amp;lt;float&amp;gt; Rect2f;typedef Rect_&amp;lt;double&amp;gt; Rect2d;typedef Rect2i Rect; Rect 연산의 예Rect 객체와 Size, Point 객체의 덧셈과 뺄셈, Rect 객체끼리의 논리 연산을 지원한다.Rect rc1; // rc1 = [0 x 0 from (0, 0)]Rect rc2(10, 10, 60, 40); // rc2 = [60 x 40 from (10, 10)]Rect rc3 = rc1 + Size(50, 40); // rc3 = [50 x 40 from (0, 0)] 가로세로가 더해짐Rect rc4 = rc2 + Point(10, 10); // rc4 = [60 x 40 from (20, 20)] 좌표가 더해짐Rect rc5 = rc3 &amp;amp; rc4; // rc5 = [30 x 20 from (20, 20)] 두 사각형이 겹치는 최대 크기의 사각형Rect rc6 = rc3 | rc4; // rc6 = [80 x 60 from (0, 0)] 두 사각형을 모두 포함할 수 있는 최소 크기의 사각형Range 클래스Range 클래스 : 정수값의 범위를 나타내기 위한 클래스 멤버 변수 : start, end 멤버 함수 : size() : 범위의 크기 계산 empty() : 해당 범위가 비어있는지에 대한 여부 all() : 범위 안의 모든 수 start는 범위에 포함되고, end는 범위에 포함되지 않음 -&amp;gt; [start, end)class Range{ public: Range(); Range(int _start, int _end); int size() const; bool empty() const; static Range all(); int start, end;}String 클래스string 클래스 : 원래는 OpenCV에서 자체적으로 정의해서 사요하는 문자열이 있엇으나 4.x 버전부터 std::string 클래스로 대체되었다.typedef std::string cv::String; cv::format() 함수를 이용하여 형식 있는 문자열 생성 가능 -&amp;gt; c언어의 printf() 함수와 인자 전달 방식이 유사하다.String str1 = &quot;Hello&quot;;String str2 = std::string(&quot;world&quot;);String str3 = str1 + &quot; &quot; + str2;Mat imgs[3];for (int i = 0; i &amp;lt; 3; i++) { String filename = format(&quot;test%02d.bmp&quot;, i+1); // format 함수의 형태로 생성이 가능하다. 이 때, d에 i+1이 들어가는데, 2자리수 형태로 들어가고, 앞의 자리수가 비어있으면 0으로 채우는 것 -=&amp;gt; test01.bmp imgs[i] = imread(filename);}Vec 클래스벡터(vector) : 같은 자료형 원소 여러 개로 구성된 데이터 형식 (열 벡터) vec클래스는 벡터로 표현하는 탬플릭 클래스다. std:cout 출력을 위해 « 연산자 오버로딩을 지원한다. Matx라는 클래스는 작은 크기의 행렬을 표현하는 클래스로 주로 16개 원소 이하에서 많이 사용한다.template&amp;lt;typename _Tp, int m, int n&amp;gt; class Matx{ public: ... _Tp val[m*n]; // matrix elements};template&amp;lt;typename _Tp, int cn&amp;gt; class Vec : public Matx&amp;lt;_Tp, cn, 1&amp;gt; // 열의 크기를 1로 한정한 클래스{ public: ... const _Tp&amp;amp; operator [](int i) const; _Tp&amp;amp; operator[](int i);} Vec 클래스 이름 재정의자주 사용되는 자료형과 개수에 대한 vec 클래스 템플릿의 이름을 재정의해놓은 것이 있다.typedef Vec&amp;lt;uchar, 2&amp;gt; Vec2b;typedef Vec&amp;lt;uchar, 3&amp;gt; Vec3b; // 컬러 영상의 픽셀값을 참조할 때 많이 사용한다.typedef Vec&amp;lt;uchar, 4&amp;gt; Vec4b;typedef Vec&amp;lt;int, 2&amp;gt; Vec2i;typedef Vec&amp;lt;int, 3&amp;gt; Vec3i;typedef Vec&amp;lt;int, 4&amp;gt; Vec4i;typedef Vec&amp;lt;int, 6&amp;gt; Vec6i;typedef Vec&amp;lt;int, 8&amp;gt; Vec8i;typedef Vec&amp;lt;float, 2&amp;gt; Vec2f;typedef Vec&amp;lt;float, 3&amp;gt; Vec3f;typedef Vec&amp;lt;float, 4&amp;gt; Vec4f;typedef Vec&amp;lt;float, 6&amp;gt; Vec6f;typedef Vec&amp;lt;double, 2&amp;gt; Vec2d;typedef Vec&amp;lt;double, 3&amp;gt; Vec3d;typedef Vec&amp;lt;double, 4&amp;gt; Vec4d;typedef Vec&amp;lt;double, 6&amp;gt; Vec6d;Scalar 클래스scalar 클래스 : 크기가 4인 double 배열(double val[4])을 멤버 변수로 가지고 있는 클래스 4채널 이하의 영상에서 픽셀 값을 표현하는 용도로 자주 사용 []연산자를 통해 원소에 접근 가능template&amp;lt;typename _Tp&amp;gt; class Scalar_ : public Vec&amp;lt;_Tp, 4&amp;gt; // scalar=원소 개수가 4개인 열벡터 (템플릿 클래스){ public: Scalar_(); Scalar_(_Tp v0, _Tp v1, _Tp v2=0, _Tp v3=0); Scalar_(_Tp v0); static Scalar_&amp;lt;_Tp&amp;gt; all(_Tp v0); ...};이렇게 정의되어 있지만 잘 사용하지 않는데 자료형을 double로 한정한 경우만 많이 사용한다.typedef Scalar_&amp;lt;double&amp;gt; Scalar; // 4개의 double형 데이터를 갖는 자료형 (val[0],va[1],val[2],val[3])스칼라 클래스는 double 타입이라 효율이 좋다고는 못하지만, 코드작성의 편의성과 코드의 통일성을 위해 자주 사용된다. Scalar 클래스 객체 생성과 원소 참조 방법원래는 Scalar 클래스는 double val[0:5]를 가지고 있는데 1개만 지정하는 경우 맨 앞, 즉 val[0] 에만 들어가게 된다. 3개를 지정해줄 경우 앞에서부터 val[0~2]가 지정되고 마지막은 0으로 들어간다. yello.val[i]로 출력하는 경우와 yellow[i]로 출력하는 경우가 같다.행렬 기초행렬(matrix) : 수나 기호, 수식 등을 네모꼴로 배열한 것, 괄호로 묶어서 표시한다.이는 4행 4열로 된, 4x4 행렬이다. 행을 row, 열을 column이라 한다.이와 같이 열또는 행만 있는 것을 열벡터, 행벡터라고 한다. 이 두개를 통칭해서 벡터라 한다. 행렬의 덧/곱셈행렬의 덧셈은 같은 자리의 숫자들끼리 더하지만 곱셈의 방식은 다르다.곱셈은 이와 같이 계산되기 때문에 행의 수 == 열의 수 또는 열의 수 == 행의 수 라는 조건이 붙는다. 예를 들어 [2x1] x [2x1] 이면 계산이 되지 않고, [1x2] x [2x1] 이면 계산이 된다. 역행렬A x A^-1 = I, 이 때, I를 단위행렬이라 한다. 전치 행렬Mat 클래스mat 클래스 : n차원 1채널 또는 다채널 행렬 표현을 위한 클래스다 실수 또는 복소수 행렬, grayscale 또는 truecolor 영상, 벡터 필드, 히스토그램, 텐서 등을 표현할 수 있다. 다양한 형태의 행렬 생성, 복사, 행렬 연산 기능을 제공한다. 행렬 생성 시 행렬의 크기, 자료형의 채널 수(타입), 초기값등을 지정할 수잇다. 복사 생성자 &amp;amp; 대입 연산자는 얕은 복사를 수행한다. 즉, 참조 계수로 관리한다. 깊은 복사는 Mat::copyTo() 또는 Mat::clone() 함수를 사용한다. 다양한 사칙 연산에 대한 연산자 오버로딩과 std::cout출력을 위한 « 연산자 오버로딩을 지원한다. 행렬의 원소(픽셀 값) 접근 방법을 제공한다. Mat::data 멤버 변수가 실제 핅셀 데이터 위치를 가리킨다. Mat::at(inty, int x) 또는 Mat::ptr(int y) 함수 사용을 권장한다. class Mat{ public: /* 생성자 &amp;amp; 소멸자 */ Mat(); Mat(int rows, int cols, int type); ... /* 멤버 함수 &amp;amp; 연산자 오버로딩 */ vold create(int rows, int cols, int type); Mat&amp;amp; operator = (const Mat&amp;amp; m); Mat clone() const; void copyTo(OutputArray m) const; template&amp;lt;typename _Tp&amp;gt; _Tp* ptr(int i0 = 0); template&amp;lt;typename _Tp&amp;gt; _Tp&amp;amp; at(int row, int col); /* 멤버 변수 */ int dims; // 행렬의 차원, 영상 데이터의 경우 2차원으로 된다. int rows, cols; // rows = 행 = 세로, cols = 열 = 가로 uchar* data; // new 연산자를 이용해서 행렬의 원소 데이터의 메모리를 동적할당해서 메모리의 시작 주소를 저장할 포인터 변수 MatSize size; // 영상의 크기를 정하는 것 MatStep step; // 영상 데이터 공간의 가로 크기를 정하는 것}Mat 클래스의 깊이 (depth)Mat 클래스의 깊이 : 행렬 원소가 사용하는 자료형 정보를 가리키는 매크로 상수(정보를 나타내는 용어) Mat::depth() 함수를 이용해서 참조한다. 형식 : CV_ &amp;lt;bit-depth&amp;gt; {U|S|F}미리 정의된 매크로 상수에는 다음과 같다.#define CV_8U 0 // uchar, unsigned char#define CV_8S 1 // schar, signed char 일반적인 char 타입#define CV_16U 2 // ushort, unsigned short#define CV_16S 3 // short 2byte signed 부호값#define CV_32S 4 // int 일반적인 int#define CV_32F 5 // float#define CV_64F 6 // double#define CV_16F 7 // float16_tMat 클래스의 채널 (channel)Mat 클래스의 채널 : 원소 하나가 몇 개의 채널로 구성되어 있는가에 대한 것이다. Mat::channels() 함수를 이용하여 참조한다. e.g. grayscale 영상은 픽셀 하나 당 밝기 값 1개, truecolor 영상은 픽셀 하나 당 B,G,R 색상 성분인 3개Mat 클래스의 타입 (type)Mat 클래스의 타입 : 행렬의 깊이와 채널 수를 한꺼번에 나타내는 매크로 상수다. Mat::type() 함수를 이용하여 참조한다. 형식 : CV_8UC1 -&amp;gt; 앞부분에는 깊이 뒷부분에 채널을 붙인 것 C1: 채널 수 U: 정수형 부호 , 정수형 부호(S/U), 실수형(F) 8: 비트 수, 8/16/32/64 Mat 클래스 속성 참조 예제Mat img = imread(&quot;lenna.bmp&quot;);cout &amp;lt;&amp;lt; &quot;width: &quot; &amp;lt;&amp;lt; img.cols &amp;lt;&amp;lt; endl;cout &amp;lt;&amp;lt; &quot;Height: &quot; &amp;lt;&amp;lt; img.rows &amp;lt;&amp;lt; endl;cout &amp;lt;&amp;lt; &quot;channels: &quot; &amp;lt;&amp;lt; img.channels() &amp;lt;&amp;lt; endl;if (img.type() == CV_8UC1) cout &amp;lt;&amp;lt; &quot;img is a grayscale image &quot; &amp;lt;&amp;lt; endl;else if (img.type() == CV_8UC3) cout &amp;lt;&amp;lt; &quot;img is a truecolor image &quot; &amp;lt;&amp;lt; endl;Mat 클래스 사용방법 Mat 클래스 생성Mat 클래스를 생성하는 방법에는 여러 가지가 있다Mat img1; // empty matrix/* img 만들기 */Mat img2(480, 640, CV_8UC1); // unsigned char, 1channel, (rows,cols, type)Mat img3(480, 640, CV_8UC3); // unsigned char, 3channelMat img4(Size(640, 480), CV_8UC1); // Size(width, height), size로 할 때는 (가로, 세로)Mat img5(480, 640, CV_8UC1, Scalar(128)); // unsigned char, 1channel, grayscale인 경우는 밝기 1개의 값만 필요함, (rows, cols, type, initial values)Mat img6(480, 640, CV_8UC3, Scalar(0, 0, 255)); // unsigned char, 1channel, truecolor인 경우는 B,G,R 총 3개의 값이 필요함/* mat클래스에서 제공하는 정적(static) 멤버 함수를 이용해서 행렬 객체를 생성할 수 있다. */Mat mat1 = Mat::zeros(3, 3, CV_32SC1); // 0 matrixMat mat2 = Mat::ones(3, 3, CV_32SC1); // 1 matrixMat mat3 = Mat::eye(3, 3, CV_32SC1); // 단위 행렬 I, identity matrixfloat data[] = {1, 2, 3, 4, 5, 6};Mat mat4(2, 3, CV_32FC1, data); // data라는 배열을 참조하여 행렬 원소를 만들어라, (rows,cols, type, data)/* 1 2 3 4 5 6 */data[0] = 100; // 배열을 참조하고 있다는 것을 주의해야 한다./* 100 2 3 4 5 6 */Mat mat5 = (Mat_&amp;lt;float&amp;gt;(2,3) &amp;lt;&amp;lt; 1,2,3,4,5,6); // opencv에서 제공하는 Mat_ 탬플릿 클래스와 shift 연산자를 사용하여 생성하는 방법Mat mat6 = Mat_&amp;lt;uchar&amp;gt;({2,3}, {1,2,3,4,5,6})); // 이 두가지 방법은 잘 사용하지 않는다.mat4.create(256,256,CV_8UC3); // uchar, 3channels , 기존에 만들어져 있는 Mat객체의 데이터를 지우고 새로 입력하는 방법mat5.create(4,4,CV_32FC1); // float, 1channel , (rows, cols, type), 초기값을 지정하는 것은 없기에 `=` 나 setTo로 입력해야 한다.mat4 = Scalar(255,0,0); // 초기값, 즉 색상을 지정mat5.setTo(1.f); // 모든 요소를 1.f로 지정 Mat클래스 객체의 참조와 복사= 연산자는 참조, 즉 얕은 복사를 수행하고, clone이나 copyTo는 깊은 복사를 수행한다.Mat img1 = imread(&quot;dog.bmp&quot;);Mat img2 = img1; // 생성자 인자를 초기화하는 = 연산자, Mat img2(img1);, 얕은 복사Mat img3;img3 = img1; // 대입하는 = 연산자, 얕은 복사Mat img4 = img1.clone(); // clone은 img1의 복사본을 img4로 복사하는 것Mat img5;img1.copyTo(img5); // img1의 데이터를 img5에 복사되는 것img1.setTo(Scalar(0,255,255)); // yellow, img1만 바꿨을 때 1,2,3이 모두 바뀌는 것을 볼 수 있음imshow(&quot;img1&quot;, img1);imshow(&quot;img2&quot;, img2);imshow(&quot;img3&quot;, img3);imshow(&quot;img4&quot;, img4);imshow(&quot;img5&quot;, img5);waitKey();destroyAllWindows();간단하게 보면 img1 == img2를 하면 같은 메모리 공간을 참조하는 것이다. 그러나 img3 = img1.clone()을 하게되면 메모리 공간을 카피해서 메모리자체를 새로 생성하게 된다. 그래서 img1의 데이터를 변경하게 되면 img2는 바뀐 메모리의 데이터를 참조하기에 함께 바뀌지만, img3은 다른 메모리를 참조하기 때문에 변경되지 않는다. Mat 클래스 객체에서 부분 행렬 추출 Mat 객체에 대해 ()연산자를 이용하여 부분 영상 추출이 가능하다. ()연산자 안에는 Rect 객체를 지정하여 부분 영상의 위치와 크기를 지정 참조를 활용하여 ROI(Region of Interest) 연산 수행 가능 Mat img1 = imread(&quot;cat.bmp&quot;);Mat img2 = img1(Rect(220,120,340,240)); // (x,y,w,h), 저 사각형 크기로 잘라낸 데이터만 img2에 저장Mat img3 = img1(Rect(220,120,340,240)).clone(); // == img2.clone() 저 사각형 크기로 된 부분을 깊은 복사img2 = ~img2; // &#39;~&#39; 는 opencv에서 not 연산을 수행하는 연산자로, 이것을 반전시키는 방법이다.imshow(&quot;img1&quot;,img1);imshow(&quot;img2&quot;,img2);imshow(&quot;img3&quot;,img3);waitKey();destroyAllWindows(); 영상의 픽셀 값 참조이는 openCV에서 제공하는 기능이 아닌 자신만의 새로운 기능을 추가할 때 많이 사용된다. 기본적으로 Mat::data 멤버 변수가 픽셀 데이터 메모리 공간을 가리키지만, Mat 클래스 멤버 함수를 사용하는 방법을 권장한다. 사용 함수 설명 Mat::data 메모리 연산이 잘못될 경우 프로그램이 비정상적으로 종료될 수 있다. 접근 식 : addr(Mi,j) = M.data + M.step[0] * i + M.step[1] * j, 이 때, i,j는 각각 행, 열번호에 해당하고 step[0]는 한 행의 byte크기, step[1]은 원소 하나의 byte크기 Mat::at() 좌표 지정이 직관적이다. 임의 좌표에 접근할 수 있다. Mat::ptr() Mat::at()보다 빠르게 동작한다. 행 단위 연산을 수행할 때 유리하다 MatIterator_ 좌표를 지정하지 않아서 안전하지만 성능이 느리다. Mat::at 함수 사용방법template&amp;lt;typename _Tp&amp;gt; _Tp&amp;amp; Mat::at(int y, int x)y: 참조할 행 번호x: 참조할 열 번호반환값 : (_Tp&amp;amp; 타입으로 캐스팅된) y행 x열 원소 값 참조Mat mat1 = Mat::zeros(3, 4, CV_8UC1);for (int y = 0; y &amp;lt; mat1.rows; y++) { for (int x = 0; x &amp;lt; mat1.cols; x++) { mat1.at&amp;lt;uchar&amp;gt;(y, x)++; // at함수를 사용할 때 현재 mat클래스의 객체가 어떤 타입을 가지고 있는지 지정해야 한다. 이를 통해 참조 뿐만 아니라 설정도 가능하다. if (x==2 &amp;amp;&amp;amp; y==1) mat1.at&amp;lt;uchar&amp;gt;(y, x)++; // 픽셀 값 +1 }}cout &amp;lt;&amp;lt; mat1 &amp;lt;&amp;lt; endl; Mat::ptr() 함수 사용 방법template&amp;lt;typename _Tp&amp;gt; _Tp* Mat::ptr(int y)y : 참조할 행 번호반환값 : (_Tp* 타입으로 캐스팅된) y번 행의 시작 주소for (int y = 0; y &amp;lt; mat1.rows; y++) { uchar* p = mat1.ptr&amp;lt;uchar&amp;gt;(y); // 1개의 인자를 받게 되고, 데이터 타입을 지정해야 하고, 지정한 포인터를 반환한다. for (int x = 0; x &amp;lt; mat1.cols; x++) { p[x]++; }}uchar* p = mat1.ptr(0) 을 하면 p는 mat1의 0번 행을 가리키게 된다. (1)이면 p는 mat1의 1번 행을 가리키게 될 것이다. MatIterator_&amp;lt;T&amp;gt; 반복자 사용 방법openCV는 mat 클래스와 함께 사용할 수 있는 반복자 클래스 템플릿인 MatIterator_를 제공한다. MatIterator_는 클래스 템플릿이므로 사용할 때는 Mat 행렬 타입에 맞는 자료형을 명시해줘야 한다. Mat::begin() 함수는 행렬의 첫번째 원소 위치를 반환, Mat::end() 함수는 행렬의 마지막 원소 바로 다음 위치를 반환한다.for (MatIterator_&amp;lt;uchar&amp;gt; it = mat1.begin&amp;lt;uchar&amp;gt;(); it != mat1.end&amp;lt;uchar&amp;gt;(); ++it) { (*it)++; }하지만 이 방법은 성능이 좋지 않아 잘 사용하지 않는다. 기초 행렬 연산float data[] = {1, 1, 2, 3};Mat mat1(2, 2, CV_32FC1, data); // 2x2 행렬의 원소는 data, type은 float32 1channelcout &amp;lt;&amp;lt; &quot;mat1:\\n&quot; &amp;lt;&amp;lt; mat1 &amp;lt;&amp;lt; endl; // 1 1;2 3Mat mat2 = mat1.inv(); // 역행렬 함수cout &amp;lt;&amp;lt; &quot;mat2:\\n&quot; &amp;lt;&amp;lt; mat2 &amp;lt;&amp;lt; endl; // 3 -1;-2 1cout &amp;lt;&amp;lt; &quot;mat1.t():\\n&quot; &amp;lt;&amp;lt; mat1.t() &amp;lt;&amp;lt; endl; // 전치행렬 함수cout &amp;lt;&amp;lt; &quot;mat1 + 3:\\n&quot; &amp;lt;&amp;lt; mat1 + 3 &amp;lt;&amp;lt; endl; // 전체 +3cout &amp;lt;&amp;lt; &quot;mat1 + mat2:\\n&quot; &amp;lt;&amp;lt; mat1 + mat2 &amp;lt;&amp;lt; endl; // 행렬 덧셈cout &amp;lt;&amp;lt; &quot;mat1 * mat2:\\n&quot; &amp;lt;&amp;lt; mat1 * mat2 &amp;lt;&amp;lt; endl; // 행렬 곱셈InputArray 클래스Inputarray 클래스 : 주로 Mat 클래스를 대체하는 프록시 클래스로 OpenCV 함수에서 입력 인자로 사용된다.사용자가 명시적으로 _InuputArray 클래스의 인스턴스 또는 변수를 생성하여 사용하는 것은 금지되어 있다.imshow나 imwrite를 할 시 2번째 인자의 경우 InputArray 클래스로 정의가 되어 있다. Mat이 아닌 inputarray인 이유는 mat이외에 Mat_,Matx&amp;lt;T,m,n&amp;gt; 등 다양한 것들을 입력 받기 위해서다. 그러나 대체로는 **Mat**을 사용한다.OutputArray 클래스outputarray 클래스 : opencv함수에서 출력 인자로 사용되는 프록시 클래스이다.이 또한, Mat이 아닌 OutputArray인 이유도 다양한 타입을 받기 위해서이나, 대체로 Mat을 주로 사용한다." }, { "title": "[데브코스] 5주차 - OpenCV Print image file and Setting for OpenCV ", "url": "/posts/opencvimread/", "categories": "Classlog, devcourse", "tags": "opencv, devcourse", "date": "2022-03-14 21:15:00 +0900", "snippet": "영상 파일 불러오기 가지고 있던 lenna.bmp 파일을 불러와서 화면에 출력하는 OpenCV 예제 프로그램 편의상 이전 강의에서 작성한 HelloCV 프로젝트에 필요한 소스 코드를 추가아래는 lenna.bmp 파일이다.현재 폴더에 있는 lenna.bmp 파일을 불러와서 화면에 출력하는 OpenCV 예제 코드다.#include &amp;lt;iostream&amp;gt; #include &quot;opencv2/opencv.hpp&quot; // OpenCV 관련 헤더 파일을 includeusing namespace cv; // cv와 std 네임스페이스를 사용하도록 설정using namespace std; // 원래는 std::, cv:: 으로 사용해야 하지만 이를 사용하지 않기 위해int main(){ Mat img = imread(&quot;lenna.bmp&quot;); // lenna.bmp 파일을 불러와서 img에 저장 // mat(matrix)은 행렬을 표현하는 클래스 if (img.empty()) { // 영상 파일 불러오기를 실패하면 에러 메시지를 출력하고 프로그램을 종료 cerr &amp;lt;&amp;lt; &quot;Image load failed!&quot; &amp;lt;&amp;lt; endl; return -1; } namedWindow(&quot;Image&quot;); // image 라는 이름의 새 창을 만듦 imshow(&quot;Image&quot;, img); // 여기에 img 영상을 출력 waitKey(); // 키보드 입력이 있을 때까지 프로그램을 대기, destroyAllWindows(); // 키 입력이 있으면 모든 창을 닫고 종료}이 코드를 실행하면 아래와 같이 나올 것이다.OpenCV 주요 함수 설명1.영상 파일 불러오기Mat imread(const String&amp;amp; filename, int flags = IMREAD_COLOR); filename : 불러올 영상 파일 이름 e.g. “lenna.bmp”, “C:\\lenna.bmp” flags : 영상 파일 불러오기 옵션 플래그 IMREAD_UNCHANGED : 영상 속성 그대로 읽어오기 e.g. 투명한 PNG 파일은 4채널(B,G,R,α) IMREAD_GRAYSCLAE : 1채널 grayscale 영상으로 읽기 IMREAD_COLOR(default) : 3채널 BGR 컬러 영상으로 읽기 반환값 : 불러온 영상 데이터 (Mat 객체)2.비어 있는 Mat 객체 확인bool Mat::empty() const 반환값 : rows, cols, data 멤버 변수가 0이면 true 반환3.영상 파일 저장하기bool imwrite(const String&amp;amp; filename, InputArray img, const std::vector&amp;lt;int&amp;gt;&amp;amp; params = std::vector&amp;lt;int&amp;gt;()); filename : 저장할 영상 파일 이름, 파일 이름에 포함된 확장자를 분석하여 해당 파일 형식으로 저장된다. img : 저장할 영상 데이터 (Mat 객체) params : 파일 저장 옵션 지정 (속성 &amp;amp; 값의 정수 쌍) 예를 들어, JPG 압축율을 90%로 하고자 하면 {IMWRITE_JPEG_QUALITY,90} 을 지정한다. 반환값 : 정상적으로 저장하면 true, 실패하면 falseimwrite(&quot;lenna.png&quot;, img);이를 실행하면 현재 폴더에 lenna.png 파일이 생겨날 것이다.4.새 창 띄우기void namedWindow(const String&amp;amp; winname, int flags = WINDOW_AUTOSIZE); winname : 창 고유 이름, 이 이름으로 창을 구분한다. flags : 창 속성 지정 플래그 WINDOW_NORMAL : 영상 크기가 창 크기에 맞게 지정됨 WINDOW_AUTOSIZE(default) : 창 크기가 영상 크기에 맞게 자동으로 변경됨 WINDOW_OPENGL : OpenGL 이 지원됨 이 namedwindow는 반드시 선언해야 하는 것은 아니다. 저 아래에 imshow라는 코드가 image라는 창을 여는데, 이가 존재하지 않는다면 기본 형태의 창을 생성하고 출력한다.5.창 닫기void destroyWindow(const String&amp;amp; winname);void destroyAllWindows(); winname : 닫고자 하는 창 이름 참고사항 - 일반적인 경우 프로그램 종료 시 운영 체제에 의해 열려 있던 모든 창이 자동으로 닫힌다. 그래서 destroyAllwindows()를 사용하지 않아도 작동한다.6.창 위치 지정void moveWindow(const String&amp;amp; winname, int x, int y); winname : 창 이름 x,y : 이동할 위치 좌표7.창 크기 지정void resizeWindow(const String&amp;amp; winname, int width, int height); winname : 창 이름 width, height : 변경할 창 크기 참고사항 - 윈도우가 WINDOW_NORMAL 속성으로 생성되어야 동작한다.8.영상 출력하기void imshow(const String&amp;amp; winname, InputArray mat); winname : 영상을 출력할 대상 창 이름 mat : 출력할 영상 데이터 (Mat 객체) 영상 출력 방식 8bit unsigned: 픽셀 값 그대로 출력 16bit unsigned or 32bit integer: 픽셀 값을 255로 나눠서 grayscale값으로 출력 32bit(float) or 64bit(double) floating point: 픽셀값에 255를 곱해서 grayscale값으로 출력 ** 따라서 8bit 형태로 출력해야 안전하게 출력할 수 있다. 참고사항 - 만약 winname에 해당하는 창이 없으면 WINDOW_AUTOSIZE 속성의 창을 새로 만들고 영상을 출력한다. 실제로는 waitkey() 함수를 호출해야 화면에 영상이 나타난다. visual studio에서 코드 번호줄 오른쪽에 회색을 클릭하면 해당 줄에 빨간색 점이 생길 것이다. 이를 종단점이라 하는데, 즉, 해당 코드 전까지 실행되다가 멈추겠다는 것이다. 이를 하려면 디버깅 시작을 누르게 되면 해당 줄에서 멈추게 된다. 이 다음부터 디버그 -&amp;gt; 프로시저 단위 실행을 하면 해당 줄을 실행하는 것이다. 이렇게 하면 빨간 점 또는 그 밑에 화살표가 생긴다. 이는 단위 실행을 하면 이 화살표의 줄이 실행된다는 것이다.9.키보드 입력 대기int waitKey(int delay = 0); delay : 밀리초 단위의 대기 시간 delay &amp;lt;= 0 이면 무한히 기다린다. 반환값 : 눌린 키의 아스키 값, 키가 눌리지 않으면 1 참고 사항 - waitkey()함수는 opencv 창이 하나라도 있어야 정상 동작한다. imshow() 함수 호출 후에 waitkey()함수를 호출해야 영상이 화면에 나타난다. 주요 특수 키의 아스키 값 : ESC==27, ENTER==13, TAB==9while (True) { if (waitKey() == 27) // q키 == &#39;q&#39;, spacebar == &#39; &#39; break;}OpenCV API 도움말opcnCV 도움말 사이트도움말 웹페이지에서 우측 상단 검색창을 활용하여 검색하면 된다.이미지 파일 형식 변환 프로그램 제작영상 파일을 다른 형식으로 변환하여 저장하는 프로그램을 작성해보고자 한다.명령행 인자로 입력 받아야 한다.#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot;using namespace cv;using namespace std;int main(int argc, char* argv[]){ // 명령행 인자 개수가 3개보다 작으면 사용법을 출력하고 종료 if (argc &amp;lt; 3) { cout &amp;lt;&amp;lt; &quot;Usage: ocvrt.exe &amp;lt;src_image&amp;gt; &amp;lt;dst_image&amp;gt;&quot; &amp;lt;&amp;lt; endl; return 0; } // 첫번째 이미지 파일을 imread() 함수로 읽어서 img 변수에 저장 Mat img = imread(argv[1]); // 두번째 이미지 파일 이름으로 img 영상을 저장 // 저장이 제대로 되면 ret 변수에 true를, 실패하면 false를 저장 bool ret = imwrite(argv[2], img); if (ret) { cout &amp;lt;&amp;lt; argv[1] &amp;lt;&amp;lt; &quot; is successfully saved as &quot; &amp;lt;&amp;lt; argv[2] &amp;lt;&amp;lt; endl; } else { cout &amp;lt;&amp;lt; &quot;File save failed!&quot; &amp;lt;&amp;lt; endl; }} 참고 블로그Visual Studio에서 OpenCV 편하게 사용하기ImageWatch 확장 프로그램ImageWatch란 OpenCV Mat 데이터를 이미지 형태로 보여주는 visual studio 확장 프로그램이다. OpenCV 프로그램 디버깅시 유용하다.ImageWatch 설치 Visual studio 메뉴에서 [확장] -&amp;gt; [확장 관리] 선택 우측 상단 검색창에 opencv 입력 Image Watch for Visual Studio 항목에서 [다운로드] 클릭 Visual Studiop를 재시작하면 설치된다.사용 방법 HelloCV.cpp 파일에서 imread() 이후 코드에 종단점(F9) 설정 후 디버깅 시작(F5) [보기] -&amp;gt; [다른 창] -&amp;gt; [Image Watch] 메뉴 선택 Image Watch 창에서 Mat 형식의 변수를 이미지 형태로 확인 가능 확대/축소 및 픽셀 값 확인 가능OpenCV 프로젝트 템플릿의 정의프로젝트 템플릿이란? 프로젝트 속성, 기본 소스 코드 등이 미리 설정된 프로젝트를 자동으로 생성하는 기능 Visual Studio의 템플릿 내보내기 마법사를 통해서 ZIP파일로 패키징된 자신만의 템플릿 파일을 생성할 수 있다.OpenCV 프로젝트 템플릿이란? OpenCV 개발을 위한 추가 포함 디렉토리, 추가 라이브러리 디렉토리, 추가 종속성 등이 미리 설정되어 있는 콘솔 응용 프로그램 프로젝트를 생성 OpenCV 기본 소스 코드(main.cpp), 테스트 영상 파일(lenna.bmp)파일도 함께 생성이 가능하다.OpenCV 프로젝트 템플릿 만들기 OpenCVTemplate 이름의 프로젝트 생성 main.cpp 파일 추가 &amp;amp; 코드 작성 lenna.bmp 파일 추가 프로젝트 속성에서 OpenCV 설정(debug, releases) -&amp;gt; 빌드 및 프로그램 동작 확인 [프로젝트] -&amp;gt; [템플릿 내보내기] 메뉴 선택 프로젝트 템플릿 선택 -&amp;gt; 템플릿 이름 및 설명 작성 후 [마침] c:\\Users\\&amp;lt;user_id&amp;gt;\\Documents\\Visual Studio 2022\\Templates\\PorjectTemplates\\ 폴더에 있는 OpenCVTemplates.zip 파일을 수정 main.cpp &amp;amp; lenna.bmp 파일 추가 (안되어 있을 수도 있기에) + MyTemplate.vstemplate 파일 편집 Visual Studio에서 새 프로젝트를 만들 때 해당 프로젝트 템플릿을 선택하여 사용이 때 설명이 너무 단순하기에 더 자세하게 커스터마이징할 수 있다. MyTemplate.vstemplate를 수정해주면 된다. 이를 텍스트 편집기로 들어가면 된다.아래는 원래 우리가 가지고 있던 내용이다.&amp;lt;VSTemplate Version=&quot;3.0.0&quot; xmlns=&quot;http://schemas.microsoft.com/developer/vstemplate/2005&quot; Type=&quot;Project&quot;&amp;gt; &amp;lt;TemplateData&amp;gt; &amp;lt;Name&amp;gt;OpenCV 콘솔 응용 프로그램&amp;lt;/Name&amp;gt; &amp;lt;Description&amp;gt;OpenCV 콘솔 응용 프로그램을 생성합니다.&amp;lt;/Description&amp;gt; ... &amp;lt;ProjectItem ReplaceParameters=&quot;false&quot; TargetFileName=&quot;main.cpp&quot;&amp;gt;main.cpp&amp;lt;/ProjectItem&amp;gt; &amp;lt;ProjectItem ReplaceParameters=&quot;false&quot; TargetFileName=&quot;lenna.bmp&quot;&amp;gt;lenna.bmp&amp;lt;/ProjectItem&amp;gt; &amp;lt;/Project&amp;gt; &amp;lt;/TemplateContent&amp;gt;&amp;lt;/VSTemplate&amp;gt;name태그와 description을 수정해주고, projectitem부분이 잘 생성되어 있는지 확인하고, 잘 되어 있지 않으면 추가해준다.이것들을 수정하고 다시 zip으로 묶어 놓는다. 그 후 visual studio에 들어가서 프로젝트 생성을 보면 아래와 같이 잘 나오는 것을 볼 수 있다." }, { "title": "[데브코스] 5주차 - OpenCV abstract and install ", "url": "/posts/opencvinstall/", "categories": "Classlog, devcourse", "tags": "opencv, devcourse", "date": "2022-03-14 18:27:00 +0900", "snippet": "OpenCV 개요openCV란 Open Source로 개발되고 있는 컴퓨터 비전과 머신러닝에 대한 소프트웨어 라이브러리다.공식 사이트OpenCV를 사용하는 이유 무료 사용 : BSD/Apache 2 라이선스를 따르기 때문에 학교/회사 등으로 상업적 사용도 가능 multiple interface: c, c++, python, java 등 많은 곳에서도 사용가능 multiple platform: windows, linux,mac OS, IOS, Android optimized: 최신 CPU 버전을 지원, 멀티 코어 프로세싱, openCL/CUDA 지원 구글 지도, 침입자 감지, 로봇 네비게이션 등 많은 곳에서 사용OpenCV 역사 1998년: intel 주도로 시작하여 오픈 소스로 개발되었다. 2006년: C로 구현되어 함수, 구조체를 가지고 있었다. 2009년: C++로 전환하여 클래스로 영상 데이터를 다루기 시작했다. 2015년: OpenCV 프로젝트 구조 개선, GPU, IPP 활용 확대 2017년: DNN 모듈 지원 2018년: C++ 11 지원, DNN 지원 강화 2021년: OPENCV 4.5.5 개발OpenCV 구성OpenCV 모듈 OpenCV는 모듈(module)이라고 부르는 다수의 라이브러리들의 집합이다. OpenCV 모듈은 크게 메인 모듈과 추가 모듈로 나눌 수 있다. 메인 모듈 핵심 기능, 널리 사용되는 기능, 기반 기능 https://github.com/opencv/opencv 모듈 이름 설명 core 행렬, 벡터 등의 opencv 핵심 클래스와 연산 함수를 정의 imgcodecs 영상 파일 입출력 imgproc 필터링, 기하학적 변환, 색 공간 변화 등의 영상 처리 기능 objdetect 얼굴, 보행자 검출 등의 객체 검출 highgui 영상의 화면 출력, 마우스 이벤트 처리 등의 사용자 인터페이스 기능 ml 머신러닝 알고리즘 dnn 심층 신경망, DNN 기능 java, js, python java, js, python 인터페이스를 지원 video 옵티컬 플로우, 배경 차분 등의 동영상 처리 기술 videoio 동영상 파일 입출력 world 여러 OpenCV 모듈을 포함하는 하나의 통합 모듈 추가 모듈 최신 기능, 널리 사용되지 않는 기능, 특허가 걸려있는 알고리즘, HW 의존적 기능(CUDA) https://github.com/opencv/opencv_contrib/ 모듈 이름 설명 cudaxxx cuda를 이용해서 컴퓨터 비전을 빠르게 동작 컴퓨터 비전 처리 과정과 필요한 OpenCV 모듈 구성 영상 입출력 : core,videoio, imgcodecs 전처리 : imgproc, photo 특징 추출 : imgproc, features2d 객체 검출,영상 분할 : imgproc, objdetect 분석(객체 인식, 포즈 추정, 움직임 분석, 3D 재구성) : calib3d, video, stitching, ml, dnn 화면 출력, 최종 판단 : highgui, ml ,dnn이 모든 모듈을 통합한 world라는 모듈을 사용하면 된다. 필요하지 않은 것도 포함되어 용량이 커지긴 하나 편리하기 때문에 이를 많이 사용한다. openCV 관련 사이트 opencv 사이트 : http://opencv.org/ opencv 4.x에 대한 도움말 : https://docs.opencv.org/4.x/ opencv에 대한 질문과 답변 사이트 : https://forum.opencv.org/ opencv 메인 모듈 깃허브 : https://github.com/opencv/opencv/ opencv 추가 모듈 깃허브 : https://github.com/opencv/opencv_contrib/ 페이스북 그룹 : https://www.facebook.com/groups/opencvprogrammingOpenCV 설치OpenCV 설치 : opencv 헤더 파일, LIB 파일, DLL 파일을 컴퓨터에 생성하는 작업앞의 두 파일은 OpenCV 프로그램을 빌드할 때 필요한 파일이고, DLL 파일은 OpenCV 프로그램을 실행할 때 필요한 파일이다.  설치 방법 설치 실행 파일 이용 장점 설치가 빠르고 간단 미리 빌드된 DLL, LIB 파일 제공 단점 OpenCV 추가 모듈이 미지원 Windows 64비트 운영 체제만 지원 소스 코드 직접 빌드 장점 자신의 시스템 환경에 최적회된 DLL, LIB 파일을 생성 가능 원하거나 원치 않는 옵션을 선택이 가능 (extra modules, parallel_for backend, etc) 단점 빌드 작업이 복잡하고 시간이 오래 걸린다. 1. 설치 실행 파일 활용1.설치 파일 다운로드 : https://opencv.org/releases 또는 https://github.com/opencv/opencv/releases 로 가서 자신의 운영체제 클릭하면 opencv-4.5.5-vc14_vc15.exe 파일 다운로드이 가능하다.위치를 정하고 extract를 클릭하면 된다.opencv ⊢ build // dll, lib파일, header파일이 저장 ⊢ include ∟ opencv2 ∟ ... // header들 ∟ x64 ⊢ vc14 // visual studio 15 ∟ vc15 // visual studio 17을 이용해서 빌드된 파일들 // 2022에서도 사용 상관 없음 ⊢ bin // dll 파일들 존재 ⊢ ... ⊢ ~55.dll // 릴리즈용으로 빌드된 파일 ∟ ~55d.dll // 디버깅용으로 빌드된 파일 ∟ lib ⊢ ... ⊢ ~55.lib // 릴리즈용으로 빌드된 파일 ∟ ~55d.lib // 디버깅용으로 빌드된 파일 ∟ sources // opencv 메인 모듈 소스 코드2.환경 변수 등록2-1. 환경 변수 새로 만들기opencv를 편리하게 사용하기 위해서는 opencv 폴더 위치를 시스템 환경 변수에 등록해야 한다. 그래야 opencv를 사용하는 프로그램이 dll파일을 찾아서 정상적으로 실행된다.설정 -\\&amp;gt; 시스템 -\\&amp;gt; 정보 -\\&amp;gt; 고급 시스템 설정 -\\&amp;gt; 환경 변수 로 이동해서 &amp;lt;user-id&amp;gt;에 대한 사용자 변수 새로 만들기 선택 변수 이름: OPENCV_DIR 변수 값: C:\\opencv\\build2-2. 환경 변수 PATH 편집이를 하는 이유는 다른 라이브러리를 사용할 때 더 효과적으로 사용할 수 있기 때문이다.이 후 다 확인하면 등록이 된다. 등록 확인을 위해 명령 프롬포트를 켜서 opencv_version을 타이핑하면 버전이 나온다.&amp;gt; opencv_version4.5.5  2. 소스코드로 직접 설치 참고 자료 1.소스코드 다운로드$ mkdir opencv &amp;amp;&amp;amp; cd opencv$ git clone https://github.com/opencv/opencv.git$ git clone https://github.com/opencv/opencv_contrib 2.openCV 빌드를 위한 디렉토리 구조 생성$ mkdir build 3.cmake-gui 실행실행 후 source code에는 opencv.git 을 한 폴더를, build binaries에는 build 폴더를 지정한다. 4.configure 버튼 누르기configure 버튼을 누르면 환경 설정 창이 뜬다. generator for this project : visual studio 17 2022 optional platform for generator : x64 use default native compilers finish 그리고 나와서, 필요한 옵션들을 선택한다. 중요 표시된 부분은 반드시 해줘야 하고, 나머지는 추가로 해주면 된다. (✨중요) BUILD_opencv_world EIGEN_INCLUDE_PATH INSTALL_C_EXAMPLES OPENCV_DNN_CUDA OPENCV_ENABLE_NONFREE여기서 나의 경우 DNN_CUDA가 오류가 발생하여, 체크하지 않았다. 다 선택한 후 configure 버튼을 눌러 설정을 반영시키고, 다 되면, generate 버튼을 누른다. 이 또한 다 수행되면 그 옆에 open project를 누른다. 누르게 되면 visual studio가 열린다.여기서 빌드 -&amp;gt; 솔루션 빌드를 해준다. 다되면, INSTALL 폴더에 옮겨야 하므로, CMakeTargets -&amp;gt; INSTALL 파일을 우클릭하여 빌드해준다. 다하고 나면, 환경 변수 편집과 visual studio를 설정해줘야 한다. 이 부분은 위에서 설명한 것과 동일하다.OPENCV 프로그램 개발 환경 설정HelloCV 프로젝트 만들기1.Visual C++ 새 프로젝트 만들기2.빈 프로젝트 클릭3.프로젝트 이름: HelloCV, 위치는 임의이 폴더, 하단의 솔루션 및 프로젝트를 같은 디렉토리에 배치 를 클릭 - 솔루션은 여러 개의 프로젝트를 관리할 수 있는 컨테이너라고 할 수 있다. - 새 프로젝트를 만드는데, 이 프로젝트를 담기 위한 솔루션이 만들어진다. - 좀 복잡한 프로그램을 만들 경우 솔루션을 다른 곳에 만들기도 한다.4.만들기 클릭5.생성된 파일로 가보면 sln 파일이 있다. 이것이 솔루션 파일, vcxproj 파일이 프로젝트 관리 파일이다. 나머지는 visual studio에서 관리를 위한 파일이다.6.프로젝트 -&amp;gt; 새 항목 추가 -&amp;gt; c++ 파일을 클릭7.아래 소스코드 작성#include &amp;lt;iostream&amp;gt;#include &quot;opencv2/opencv.hpp&quot; // opencv관련 헤더 파일, // 이는 설치한 opencv/bulid/include/opencv2/opencv.hpp 가 있다.int main(){ std::cout &amp;lt;&amp;lt; &quot;Hello OpenCV &quot; &amp;lt;&amp;lt; CV_VERSION &amp;lt;&amp;lt; std::endl;}8.opencv 헤더 파일 위치 지정현재에는 아직 opencv.hpp를 인식하지 못하여 불러오지 못한다. 그래서 직접 지정을 해줘야 한다. 프로젝트 -&amp;gt; HelloCV 속성 -&amp;gt; 구성 속성 -&amp;gt; C/C++ -&amp;gt; 일반 추가 포함 디렉토리 항목에 $(OPENCV_DIR)\\include 입력이 때, 절대적 위치를 작성해줄 수 있지만, 이전에 시스템에 환경 변수를 등록했었다. $(OPENCV_DIR)이란 OPENCV_DIR이라는 이름의 환경변수를 불러오겠다는 것이다. 환경 변수를 지정해주지 않았다면 오류가 날 것이다.여기서 상단에 구성을 보면 활성(debug) 이외에도 releases가 있을 것이다. 디버그 모드는 개발하는 동안 디버깅하면서 버그를 찾고 에러를 수정하는 모드이고, 릴리즈모드는 개발이 끝나서 프로그램을 배포할 때 사용하는 모드다. dedug를 클릭해서 수정한다.또한, 구성 옆에 그 옆에 플랫폼도 x64가 맞는지 잘 확인한다.9.opencv lib 파일 위치 지정 프로젝트 -&amp;gt; HelloCV 속성 -&amp;gt; 구성 속성 -&amp;gt; 링커 -&amp;gt; 일반 추가 라이브러리 디렉토리 항목에 $(OPENCV_DIR)\\x64\\vc15\\lib 입력10.추가 종속성 프로젝트 -&amp;gt; HelloCV 속성 -&amp;gt; 구성 속성 -&amp;gt; 링커 -&amp;gt; 입력 추가 종속성 항목에 화살표를 클릭하고 편집을 클릭한 후 opencv_world455d.lib를 입력한다. 이 때, 구성이 debug이어서 455d.lib을 지정해줬다. 만약 릴리즈 모드라면 d를 뺀 455.lib을 입력해야 한다.릴리즈모드도 따로 동일하게 해줘야 한다.이렇게 설정을 마치고 솔루션 빌드를 누르면 아래 오류가 있는지 확인할 수 있다. 오류가 없으면 C:\\coding\\opencv\\HelloCV\\x64\\Debug에 HelloCV.exe 파일이 생성되었다고 나올 것이다. 실제로 파일 탐색기로 들어가보면 파일이 존재하고 이를 더블클릭하면 바로 꺼지면서 확인이 어렵다.따라서 파일 주소창에 cmd를 입력하면 프롬포트에 현재 주소 기준으로 켜질 것이다. 여기서 HelloCV.exe를 입력하면 버전이 나온다.&amp;gt; HelloCV.exeHello OpenCV 4.5.5또는 편리하게 상단에 디버그 -&amp;gt; 디버깅 시작 또는 디버깅하지 않고 시작 을 누르면 바로 나온다. 그러나 현재는 종단점을 설정하지 않아서 바로 프로세스가 종료된다." }, { "title": "[데브코스] 5주차 - Computer Vision abstract and image analysis about memory allocation ", "url": "/posts/comvision/", "categories": "Classlog, devcourse", "tags": "computer-vision, devcourse", "date": "2022-03-14 14:34:00 +0900", "snippet": "개요컴퓨터 비전(computer vision)이란?컴퓨터를 이용하여 정지 영상 또는 동영상으로부터 의미 있는 정보를 추출하는 방법을 연구하는 학문이다.컴퓨터 비전과 영상 처리(image processing)영상(image)란 사진이나 비디오 등을 다 포함한 것을 말한다. 동영상을 말할 때는 video라고 한다. 영상 처리는 영상을 입력으로 받아 화질을 개선하겆나 크롭핑하여 다시 영상을 출력하는 것을 말하고, 거기서 객체를 인식하는 등의 추가 작업을 컴퓨터 비전이라 한다. 그러나 뜻이 거의 동일하기에 동일시 하는 곳도 많다.컴퓨터 비전의 역사1960년 MIT The summer vision project 연구가 컴퓨터 비전의 시초이다. 이 때, 위성으로부터 전송 받은 달 표면 사진의 화질을 복원했다. 현재는 2012년에 딥러닝 모델인 AlexNet이 나오게 되면서 딥러닝이 기하급수적으로 발전되었다. 컴퓨터 비전 vs 휴먼 비전사람은 주변의 영향을 많이 받기 떄문에 정확한 그곳의 색상을 보기 보다는 주변의 상황과 함께 판단해서 어디가 어두운지, 밝은지를 판단한다. 그러나 컴퓨터는 해당 위치의 픽셀을 추출해서 판단하기 때문에 객관적으로 판단할 수 있을 것이다.컴퓨터 비전의 응용 분야영상의 화질 개선카메라로 찍은 사진을 더욱 선명하게 만들거나 색상을 원하는 형태로 변경하는 등의 작업을 한다. RAW 영상의 변환, 사진앱의 필터, 잡은 제거, HDR, 초해상도 등을 할 수 있다.내용 기반 영상 검색 (content-based image/video retrieval)영상에 존재하는 사람, 사물, 색상 정보 등을 인식하여 유사한 영상을 자동으로 찾아주는 시스템이다. 이를 비주얼 검색(visual search)라고도 한다.얼굴 검출 및 인식얼굴 검출(face detection): 영상에서 얼굴의 위치와 크기를 찾는 기법이다.얼굴 인식(face recognition): 검출된 얼굴이 누구인지를 판단하는 기술이다. 미세한 표정 변화도 감지한다. 조명 변화, 안경 착용, 헤어 스타일 변화에 의해 정확도가 낮아질 수는 있다.의료 영상 처리x-ray, CT에 사용되는 영상처리이다. 영상의 화질 개선, 영상의 자동 분석을 할 수 있다.광학 문자 인식영상에 있는 텍스트를 인식한다. 이를 OCR(optical character recognition)이라 한다. 번역, 자동차 번호판 인식 등을 할 수 있다.마커 인식정해진 형태의 마커를 인식하여 숨겨진 정보를 추출할 수 있다. 2D 바코드, QR 코드를 인식할 수 있다.영상 기반 증강 현실카메라로 특정 사진을 가리키면 관련된 정보가 증강되어 나타나는 기술이다. 마커 기반과 비 마커 기반으로 나뉜다.머신 비전(machine vision)주로 산업계에서 제품의 위치 확인, 측정, 불량 검사 등을 위해 사용되는 영상 기반 기술이다. 공장의 자동화를 촉진시킬 수 있다. 빠른 처리 시간, 높은 정확도, 객관성이 장점이다. 카메라, 렍, 조명, 필터, 영상 보드, 영상 처리 소프트웨어 등으로 구성되어 있다.인공지능 서비스입력 영상을 객체와 배경으로 분할하여 객체와 배경을 인식하고 그 후 상황을 인식할 수 있다. 이를 바탕으로 로봇과 자동차의 행동을 지시할 수 있다.computer vision + sensor fusion + deep learning 이라 할 수 있다. 이를 상용화한 것이 amazon go\\/ 구글,테슬라의 자율 주행 자동차가 있다.영상 데이터의 구조와 특징영상(image)이란?픽셀이 바둑판 모양의 격자에 나열되어 있는 형태로 2차원 행렬 형태를 가지고 있다. 이 때 픽셀이랑 영상의 기본 단위이다.Grayscale Image영상 데이터에는 크게 2가지 종류가 있는데, 하나는 grayscale, 즉 흑백 사진처럼 색상 정보가없이 오직 밝기 정보만으로 구성된 영상을 말한다. 밝기 정보를 256단계로 표현한다. 그레이스케일 영상의 픽셀값 표현그레일스케일 영상에서 하나의 픽셀은 0~255 사이의 정수 값을 가진다. 0이 검정색, 255가 흰색을 나타낸다.grayscale level: 그레일 스케일의 픽셀이 가질 수 있는 범위를 나타내는 것으로 [0,255] 또는 [0,256) 이라 표현할 수 있다. 즉 []는 포함, ()는 미포함이라 할 수 있다.이를 c/c++에서는 unsigned char로 표현할 수 있다. 이는 1byte 공간을 사용한다.typedef unsigned char BYTE; // windowstypedef unsigned char uint8_t; // linuxtypedef unsigned char uchar; // opencv이 unsigned char을 그냥 사용하지 않고, 편의성과 통일성을 위해 typedef를 사용하여 쓴다. 각각 사용하는 곳이 다르다. 픽셀 값 분포의 예이는 영상 처리에서 유명한 사진인 camera man 사진이다. 이 때, 픽셀 값을 보면 어두운 값이 0에 가깝고, 밝을수록 255에 가까울 것이다. 그리고 자세히 보게 되면 우리는 구분하지 못하지만, 값이 10~20정도 차이가 나는 것을 볼 수 있다.truecolor image컬러 사진처럼 다양한 색상을 표현할 수 있는 영상을 말한다. R/G/B 색 성분을 각 256 단계로 표현한다. 트루컬러 영상의 픽셀값 표현R/G/B 색 성분의 크기를 각각 0.~255 범위의 정수로 표현한다. 0은 없는 상태, 255는 가득있는 상태를 말한다.이를 c/c++에서는 unsigned char로 자료형 3개 있는 배열 또는 구조체로 표현한다. 따라서 3byte를 차지하게 된다.class RGB{ unsigned char R; unsigned char G; unsigned char B;} 픽셀 값 분포의 예이 사진을 보면, 각각의 픽셀이 3개의 값을 가지고 있다. 코의 부분에서는 빨간색이 높게 나오고 있고, 오른쪽부분은 파란색이 높게 나오고 있다.영상에서 사용되는 좌표계영상에서 사용되는 좌표계는 좌측 상단을 0,0으로 기준을 잡는다. 그리고 0,0부터 시작하므로 크기가 (w,h)의 이미지, 즉 w-by-h image라 할 때, 마지막은 (w,h)가 아닌 (w-1,h-1)의 좌표를 가진다. 이를 2차원 행렬로 표현하기도 한다. M행, N열을 가진 M x N 행렬이 있다고 할 때, 이를 m-by-n matrix라 한다.여기서 주의해야 할 것은 영상에서는 w, 즉 가로를 먼저 작성하지만, 행렬로 표현할 때는 행, 즉 세로를 먼저 작성한다. 그래서 640x480 이미지라 하면 영상으로 표현되어 있는지 행렬로 표현되어 있는지를 알아야 한다. 이것이 틀릴 경우 출력이 이상하게 나오거나 오류가 날 것이다.영상 데이터의 표현 방법이미지는 대체로 2차원 행렬로 이루어져 있기에 2차원 배열을 생성하면 될 것이다.정적 2차원 배열 생성unsigned char a[480][640] {}; unsigned char: 1바이트 사용(0~255 사이의 정수 표현) 2차원 배열 전체 크기만큼의 메모리 공간이 연속적으로 할당된다. (640x480 = 307200 bytes) 단점: 배열의 크기를 미리 알고 있어야 한다. -&amp;gt; 다양한 크기의 영상을 표현하기에 부적절하다. stack 영역에 메모리를 할당하기에 대략 1MB까지만 할당이 가능하다. =&amp;gt; 그래서 방법은 잘 사용하지 않고, 동적 메모리 할당을 사용한다. 동적 2차원 배열 생성int w = 640;int h = 480;unsigned char** p;p = new unsigned char*[h];for (int i = 0; i &amp;lt; h; i++) { p[i] = new unsigned char[w] {};} 행 단위로만 연속된 메모리 공간이 보장된다. 프로그램 동작 중 다양한 크기의 영상을 생성할 수 있다. heap 영역에 메모리를 할당하므로서 x86의 경우 2GB까지 할당이 가능하고, x64의 경우 8TB까지 가능하다.코드 자세히 분석1. unsigned char** p;2. p = new unsigned char*[h];3. for (int i = 0; i &amp;lt; h; i++) {4. p[i] = new unsigned char[w] {};}1.unsigned char 2차원 포인터 p를 선언 -&amp;gt; local 영역에 로컬 변수 형태로 포인터 변수가 생성된다.2.new 연산자를 통해서 unsigned char 포인터 타입을 h개 만큼 할당 -&amp;gt; h개만큼의 포인터 변수가 생성되고 그 위치를 p가 가리키도록 할당된다. 그렇게 되면 p[0]~p[h-1]까지 접근할 수 있게 된다.3.w개만큼의 unsigned char 메모리 공간을 h만큼 할당하게 된다. 동적 2차원 배열 원소 접근 방법할당을 한 후에는 2중 for문을 사용해서 각각의 픽셀값에 접근할 수 있다.// 2차원 배열 p의 모든 원소 값을 10씩 증가for (int y=0;y &amp;lt; h; y++) { // y 좌표 for (int x=0; x &amp;lt; w; x++) { // x 좌표 p[y][x] = p[y][x] + 10; }}좌측 상단부터 오른쪽으로 간 후 다음 줄 -&amp;gt; 오른쪽 -&amp;gt; 다음줄 … 순서로 된다.동적 2차원 배열 메모리 해제동적으로 할당한 2차원 배열은 사용이 끝난 후에는 반드시 해제를 해야한다. 이때 delete를 사용한다. 동적 2차원 배열 생성의 역순으로 해제해야 한다.for (int y = 0; y &amp;lt; h; y++) { delete[] p[i];delete[] p;}이 때, delete에 []를 반드시 같이 사용해야 한다. 그리고 2차원 각각의 행에 대한 메모리를 해제하고, p의 포인터에 대해 한 번 더 해제해야 한다.대용량 1차원 메모리 할당 후 영상 데이터 저장그러나 영상 데이터를 2차원이 아닌 1차원으로 메모리를 할당하여 저장할 수도 있다.int w = 10, h = 10;unsigned char * data = new unsigned char[w * h] {};...delete[] data; // 메모리 해제10x10 영상데이터를 저장한다고 했을 때, 10*10=100개의 unsigned char 메모리 공간을 할당하고 모든 픽셀 데이터를 저장한다. 순서는 위와 같이 좌측 상단부터 다음 줄 -&amp;gt; 오른쪽 -&amp;gt; 다음줄 … 순서로 저장한다. 특정 좌표 (x,y) 위치 픽셀 값 참조unsigned char&amp;amp; p1 = *(data + y*w + x);포인터 연산을 사용해서 w는 영상의 가로 크기, data는 시작 좌표를 의미한다.간단한 형태의 영상 데이터 저장 클래스 생성class MyImage{public: MyImage() : w(0), h(0), data(0) {} // default, 즉 init MyImage(int _w, int _h) : w(_w), h(_h) { // w,h 두 개의 정수 값을 받는 생성자에서는 w,h를 초기화 data = new unsigned char[w * h] {}; // new 연산자를 이용해서 데이터 공간을 메모리 할당, 그 시작 주소를 data가 가르키도록 했다. } ~MyImage() { // 소멸자 if (data) delete[] data; // 메모리 할당이 있었으면 그것을 삭제하도록 } unsigned char&amp;amp; at(int x, int y) { // at을 이용해서 (x,y) 좌표에 있는 픽셀값을 반환하도록 하는데, 참조(&amp;amp;)로 반환하도록 해서 읽어올 뿐만 아니라 픽셀값을 설정할 수도 있도록 만듦 return *(data + y * w + x); }public: int w, h; // w: 영상 가로 크기, h: 영상 세로 크기 unsigned char* data; // 픽셀 데이터를 저장하기 위해 동적 할당한 메모리 공간의 시작 주소를 가리킬 포인터 변수}영상 파일의 형식과 특징BMP 파일 구조비트맵(bitmap) 비트(bit)들의 집합(map) -&amp;gt; 픽셀의 집합 영상의 전체 크기에 해당하는 픽셀 정보를 그대로 저장한다. 장점: 표현이 직관적이고 분석이 용이하다. 단점: 메모리 용량을 많이 차지한다. 영상의 확대/축소시 화질 손상이 심하다. 사진, 포토샵 비트맵의 종류 장치 의존 비트맵(DDB): 출력 장치(화면, 프린터 등)의 설정에 따라 다르게 표현된다. 장치 독립 비트맵(DIB): 출력 장치가 달라져도 항상 동일하게 출력된다. BMP 파일은 Windows 환경에서 비트맵을 DIB형태로 저장한 파일 포맷이다. 벡터 그래픽스(vector graphics) 점과 점을 연결해 수학적 원리로 그림을 그려 표현하는 방식 이미지 크기를 확대/축소해도 화질이 손상되지 않음 폰트, 일러스트레이터아래는 BMP 파일 구조를 나타낸 것이다.1.비트맵 파일 헤더이는 비트맵 파일에 대한 정보를 담고 있다.파일 헤더의 구조체는 다음과 같다. WORD의 경우 2byte, DWORD는 4byte크기의 자료형이다. 그러므로 이는 전체 14 bytes로 표현되는 구조체다.typedef struct tagBITMAPFILEHEADER { WORD bfType; // 이 파일이 bmp파일인지 아닌지를 나타내는 지시자, &#39;B&#39;, &#39;M&#39;, 0x42/0x4D 와 같이 16진수로 기록이 된다. DWORD bfsize; // BMP 파일 크기를 4byte크기로 저장된다 WORD bfReserved1;// 현재 사용되지 않는 플래그 WORD bfReserved2;// 현재 사용되지 않는 플래그 DWORD bf0ffBits; // 비트맵 비트까지의 오프셋으로 비트맵 파일 헤더로부터 픽셀 데이터가 있는 위치까지의 거리를 나타낸다.} BITMAPFILEHEADER;2.비트맵 정보 헤더여기서는 비트맵 영상에 대한 정보를 담고 있다.typedef struct tagBITMAPINFOHEADER { DWORD biSize; // BITMAPINFOHEADER 크기 LONG biWidth; // 비트맵 가로 크기, 4bytes LONG biHeight; // 비트맵 세로 크기 WORD bitPlanes; // 항상 1을 저장 WORD bitBitCount; // 한 픽셀의 컬러를 표현하기 위해 사용되는 비트 수를 나타낸다. truecolor의 경우 24, grayscale의 경우 8을 가진다. DWORD biCompression; // BI_RGB, 대부분의 경우 0 DWORD biSizeImage; // 대부분의 경우 0 LONG biXPelsPerMeter; // 대부분의 경우 0 LONG biYPelsPerMeter; // 대부분의 경우 0 DWORD biClrUsed; // 대부분의 경우 0 DWORD biClrImportant; // 대부분의 경우 0} BITMAPINFOHEADER;3.색상 테이블/팔레트여기서는 비트맵에서 사용되는 색상 정보를 담고 있다.RGBQUAD는 4byte로 구성되어 있다.typedef struct tagRGBQUAD { BYTE rgbBlue; // Blue BYTE rgbGreen; // Green BYTE rgbRed; // Red BYTE rgbReserved; // 사용되지 않는 공간인 1, 이를 사용하는 이유는 4의 배수로 맞춰주기 위함이다. 이는 메모리를 조금 더 빠르게 사용할 수 있기 때문에 사용한다.} RGBQUAD;이는 256컬러 이하의 비트맵에서만 존재한다. 그래서 grayscale 비트맵에서는 존재하고, truecolor 비트맵에서는 존재하지 않는다. 그레일스케일 비트맵 (0,0,0,0),(1,1,1,0)…,(255,255,255,0) 의 총 256개의 색상을 가진다. 전체 4 * 256 = 1024bytes의 색상 테이블을 가지고 있게 된다. 트루컬러 비트맵 이 색상 테이블을 가지지 않고 마지막 픽셀 데이터에 색상 정보를 저장한다. 4.픽셀 데이터 그레이스케일 비트맵: RGBQUAD 배열에 이미 정의되어 있기 때문에 배열의 인덱스 값을 저장 트루컬러 비트맵: 색상 테이블이 없기 때문에 (B,G,R) 순서로 픽셀 값을 저장일반적으로 상하가 뒤집힌 상태로 저장된다. (bottom-up)또한, 효율적인 데이터 관리를 위해 영상의 가로 크기를 4의 배수로 맞춰서 저장한다.그레이스케일의 경우4bytes x 3bytes         idx idx idx 0 idx idx idx 0 idx idx idx 0 트루컬러의 경우12bytes x 3bytes                         B G R B G R B G R 0 0 0 B G R B G R B G R 0 0 0 B G R B G R B G R 0 0 0 이러한 그레이스케일 파일이 있다고 하자. 이를 바이너리 편집기로 열면 아래와 같이 출력된다.맨 앞 줄의 숫자는 빼고, 그 다음부터 확인하면 된다. 오른쪽이 숫자들을 분석한 사진이다. bfType: bmp파일이므로 42, 4D bfSize: bmp파일의 크기 biSize: 28 00 00 00, 16진수이므로 32+8이므로 bitmapinfoheader의 크기는 40byte biWidth, biHeight: 가로,세로가 4x4 biBitCount: 1픽셀을 표현하기 위해 8bit 즉, 2^8=256가지의 색상을 나타낼 수 있다. 그레이스케일이므로 biClrImportant에서 infoheader가 끝나고 rgbquad를 4x256 = 1024byte만큼의 색상 테이블이 나타나게 된다. 0부터 FF(255)까지 나타나있다. 파란색: 실제 픽셀값에 대한 정보를 역순으로 나타내게 됨. 즉 FF FF FF FF는 맨 아래, 흰색을 나타낸다. 근데 중요한 것은 FF가 스케일값 자체를 나타내는 것이 아니라 색상 테이블 중에서 255번째에 있는 색상을 가지고 있다는 것을 나타낸다. bfType: bmp파일이므로 42, 4D biSize: 32+8=40byte biWidth,biHeight: 4x4 biBitCount: 1=16 + 8 == 24bit, 즉 1픽셀당 3bytes 트루컬러이므로 색상 테이블이 없다. [80(B) 80(G) 80(R)] 는 16진수이므로 [128, 128, 128] 즉 회색 FF 00 00: Blue 가 4개 00 FF 00: Green 가 4개 00 00 FF: Red 가 4개이 bmp파일을 구체화시켜서 보고자 한다. visual studio 에서 새 프로젝트 생성 -&amp;gt; window 데스크톱 애플리케이션 이름은 각자 작성하고, 솔루션 및 프로젝트~ 를 체크한다.생성하게 되면 기본적인 소스코드를 만들어준다. 여기서 간단하게 코드를 살펴보면 mwinmain : 클래스를 등록 initinstance: createwindow라는 함수를 이용해서 실제 윈도우를 만들고, showwindow를 사용해서 화면에 보여준다. myregisterclass: 기본적인 윈도우 스타일에 대한 정보를 기록하고 있다. 이를 registerclassexw라는 것으로 보낸다. 이는 운영체제에 이러한 윈도우를 만들 것이라는 것을 등록한다. winproc: 윈도우 메시지를 처리하는 함수다. wm_command: 어떤 메뉴를 선택했을 때 처리하는 케이스 wm_paint: 화면에 어떤 그림을 그릴 때 사용하는 케이스 wm_destroy: 프로그램이 종료될 때 실행되는 케이스 about: 도움말 대화상자솔루션 빌드를 하고, 디버깅하지 않고 시작을 누르면 창이 하나 생성되는 것을 볼 수 있다.그래서 이 WM_COMMAND가 끝나는 부분에 왼쪽 마우스가 눌렸을 때에 대한 메시지를 처리하도록 하고자 한다.# include &amp;lt;stdio.h&amp;gt; // 파일에 대한 처리를 위해 맨 위에 # include &amp;lt;stdio.h&amp;gt; 를 추가case WM_LBUTTONDOWN:{ FILE* fp = NULL; fopen_s(&amp;amp;fp, &quot;cat.bmp&quot;, &quot;rb&quot;); // 현재 폴더에 있는 cat.bmp 열기 if (!fp) break; BITMAPFILEHEADER bmfh; // fileheader에 대한 정보를 담을 구조체 선언 BITMAPINFOHEADER bmih; // infoheader에 대한 정보를 담을 구조체 선언 fread(&amp;amp;bmfh, sizeof(BITMAPFILEHEADER), 1, fp); // 불러온 파일에서 bitmapfileheader 크기만큼을 불러와서 bmfh에 저장 fread(&amp;amp;bmih, sizeof(BITMAPINFOHEADER), 1, fp); // 불러온 파일에서 bitmapfileheader 크기만큼을 불러와서 bifh에 저장 LONG nWidth = bmih.biWidth; // bitmapinfoheader중에서 가로 추출 LONG nHeight = bmih.biHeight; // bitmapinfoheader중에서 세로 추출 WORD nBitCount = bmih.biBitCount; // bitmapinfoheader중에서 각각의 픽셀이 몇 비트로 표현되고 있는가에 대한 정보 추출 DWORD dwWidthStep = (DWORD)((nWidth * nBitCount / 8 + 3) &amp;amp; ~3); // 하나의 행을 표현하기 위해 필요한 메모리 크기를 계산하는 것, 가로 크기 * 각 byte 크기를 한 후, `+ 3) &amp;amp; ~3)은 이 값보다 같거나 큰 4의 배수값을 구하는 방법이다. DWORD dwSizeImage = nHeight * dwWidthStep; // 영상의 세로크기와 위의 값을 곱하면 전체 픽셀 데이터를 저장하기 위해 필요한 메모리 공간의 크기를 구할 수 있다. DWORD dwDibSize; if (nBitCount == 24) // 트루 컬러의 경우 dwDibSize = sizeof(BITMAPINFOHEADER) + dwSizeImage; // 전체 Dibsize라고 해서 infoheader와 전체 영상 데이터의 크기를 더한다. else // 그레이스케일의 경우 dwDibSize = sizeof(BITMAPINFOHEADER) + sizeof(RGBQUAD) * (1 &amp;lt;&amp;lt; nBitCount) + dwSizeImage; // 색상 테이블의 크기까지 추가해서 전체 Dibsize를 구할 수 있다. BYTE* pDib = new BYTE[dwDibSize]; // 메모리 공간을 동적 할당 // fseek 함수를 이용해서 파일의 맨 처음에서 비트맵 파일 헤더 크기만큼 이동하므로 비트맵 정보 헤더 위치로부터 DIB크기(dwDibSize)만큼을 파일로부터 읽는다. fseek(fp, sizeof(BITMAPFILEHEADER), SEEK_SET); // pDib 메모리에는 비트맵 정보 헤더와 색상테이블(팔레트), 픽셀 데이터 정보가 저장된다. fread(pDib, sizeof(BYTE), dwDibSize, fp); LPVOID lpvBits; if (nBitCount == 24) // 두 가지를 따로 계산하는 이유는 이 두개의 포인터 값을 이용해서 setDIBitsToDevice라는 win32함수를 사용해서 bitmap을 나타낼 때 사용되기 때문이다. lpvBits = pDib + sizeof(BITMAPINFOHEADER); // 실제 픽셀 데이터가 나타나는 주소값을 계산 else lpvBits = pDib + sizeof(BITMAPINFOHEADER) + sizeof(RGBQUAD) * (1 &amp;lt;&amp;lt; nBitCount); HDC hdc = GetDC(hWnd); int x = LOWORD(lParam); // winRroc에 있는 인자값에 있는 것으로 실제 마우스가 클릭된 x좌표를 파싱하는 코드 int y = HIWORD(lParam); // winRroc에 있는 인자값에 있는 것으로 실제 마우스가 클릭된 y좌표를 파싱하는 코드 ::SetDIBitsToDevice(hdc, x, y, nWidth, nHeight, 0, 0, 0, nHeight, lpvBits, (BITMAPINFO*)pDib, DIB_RGB_COLORS); // 출력하는데 필요한 정보들을 전달 ReleaseDC(hWnd, hdc); delete[] pDib; fclose(fp);}break;BMP/JPG/GIF/PNG 파일 형식의 특징 BMP 픽셀 데이터를 압축하지 않고 그대로 저장 -&amp;gt; 파일 용량이 큰 편 파일 구조가 단순해서 별도의 라이브러리 도움 없이 파일 입출력 프로그래밍이 가능 파일의 크기가 중요하지 않은 연산의 경우 이를 사용한다. JPG/JPEG 주로 사진과 같은 컬러 영상을 저장 손실 압축(lossy compression) 압축률이 좋아서 파일 용량이 크게 감소 -&amp;gt; 디지털 카메라 사진 포맷으로 주로 사용됨 정밀한 영상 처리나 컴퓨터 비전에서는 픽셀 값이 조금만 바뀌어도 성능이 차이가 나서 선호는 하지 않는다. GIF 256 색상 이하의 영상을 저장 -&amp;gt; 일반 사진을 저장 시 화질 열화가 심하다. 무손실 압축(lossless compression) 움직히는 GIF 지원 영상 처리에서는 잘 사용하지 않는다. PNG Portable Network Graphics 무손실 압축 (컬러 영상도 무손실 압축) 알파 채널(투명도/불투명도)를 지원한다. 파일의 크기가 중요하지 않을 때 사용한다. 영상 데이터 크기 분석 그레이스케일 영상: (가로 크기) x (세로 크기) bytes (512x512) 의 경우 512x512=26211bytes 트루컬러 영상: (가로 크기) x (세로 크기) x 3 bytes FHD(1820x1080) 의 경우 1920x1080x3 = 6220800bytes 이를 30fps로 1분 재생하려면 6MB x 30fps = 180MB x 60sec = 1GB   속성 521x512 Grayscale 1920x1080 Truecolor 1920x1080 Truecolor BMP 263,222 6,220,854 6,220,854 PNG 167,488 2,730,645 4,081,084 JPG(95%) 90.965 512,220 1,098,200 JPG(80%) 37,923 213,879 542,790 bytes 변환하는 방법으로는 cv2::imwrite()함수를 사용하면 된다. 나는 그냥 python으로 구해보았다.from pathlib import Pathname = &#39;lenna_gray&#39;filenames = [&#39;./&#39; + name+&#39;.bmp&#39;, &#39;./&#39;+ name+&#39;.png&#39;,&#39;./&#39;+name+&#39;_95p.jpg&#39;,&#39;./&#39;+name+&#39;_80p.jpg&#39;]for f in filenames: file_size =Path(f).stat().st_size print(f.split(&#39;/&#39;)[-1],&quot; size is:&quot;, file_size,&quot;bytes&quot;)lenna_gray.bmp size is: 263222 byteslenna_gray.png size is: 167488 byteslenna_gray_95p.jpg size is: 90965 byteslenna_gray_80p.jpg size is: 37923 byteslenna_gray.bmp 파일의 크기는 비트맵 크기 14 + infoheader크기 40 + 팔레트 크기 1024 + 가로x세로 512x512 = 263,222 로 구할 수 있다.sky.bmp나 tree.bmp의 경우는 14 + 40 + 1920x1080 = 6,220,854bytes 가 된다.jpg의 경우 압축률을 정할 수 있다. 그래서 위의 경우 95%, 80%로 지정했다. 80%가 더 큰 압축률을 의미한다.sky와 tree의 크기가 다른 이유는 픽셀값의 변화가 다소 작은 경우 압축이 더 많이 되기 때문에 sky가 크기가 더 작다. 이처럼 픽셀값의 변화가 작은 경우 저주파 성분이 강하다고 하고, 변화가 큰 경우 고주파 성분이 강하다고 할 수 있다." }, { "title": "[데브코스] 4주차 - ROS Self-driving using sensors ", "url": "/posts/sensordriving/", "categories": "Classlog, devcourse", "tags": "ros, urdf, rviz, devcourse", "date": "2022-03-11 16:14:00 +0900", "snippet": "라이다를 통한 장애물 회피 lidar_drive 패키지를 사용 lidar_gostop.py , lidar_gostop.launch&amp;lt;!-- lidar_gostop.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;include file=&quot;$(find xycar_motor)/launch/xycar_motor.launch&quot;/&amp;gt; &amp;lt;include file=&quot;$(find xycar_lidar)/launch/lidar_noviewer.launch&quot;/&amp;gt; &amp;lt;node name=&quot;lidar_driver&quot; pkg=&quot;lidar_drive&quot; type=&quot;lidar_gostop.py&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt;#!/usr/bin/env python# import msg fileimport rospy, timefrom sensor_msgs.msg import LaserScan from xycar_msgs.msg import xycar_motor motor_msg = xycar_motor()distance = [] # prepare storage to save the distance value for lidar# if topic about lidar enter, define the callback function to implementdef callback(data): global distance, motor_msg distance = data.ranges# define function for goingdef drive_go(): global motor_msg motor_msg.speed = 5 motor_msg.angle = 0 pub.publish(motor_msg)# define function for stopingdef drive_stop(): global motor_msg motor_msg.speed = 0 motor_msg.angle = 0 pub.publish(motor_msg)# make node and define sub and pub noderospy.init_node(&#39;lidar_driver&#39;)rospy.Subscriber(&#39;/scan&#39;, LaserScan, callback, queue_size = 1)pub = rospy.Publisher(&#39;xycar_motor&#39;, xycar_motor, queue_size = 1)# ready to connect lidartime.sleep(3)# scan from 60 degree to 120 degree, because front side is 90 degreewhile not rospy.is_shutdown(): ok = 0 for degree in range(60, 120): if distance[degree] &amp;lt;= 0.3: ok += 1 if ok &amp;gt; 3: # if more than three are lower 30cm, do stop drive_stop() break if ok &amp;lt;= 3: # else go drive_go() 실행$ roslaunch lidar_drive lidar_gostop.launch초음파를 통한 장애물 회피 ultra_drive 패키지 ultra_drive.launch, ultra_drive.py&amp;lt;!-- ultra_drive.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;include file=&quot;$(find xycar_motor)/launch/xycar_motor.launch&quot;/&amp;gt; &amp;lt;node name=&quot;xycar_ultra&quot; pkg=&quot;xycar_ultrasonic&quot; type=&quot;ultra_ultrasonic.py&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;node name=&quot;ultra_driver&quot; pkg=&quot;ultra_drive&quot; type=&quot;ultra_gostop.py&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt;#!/usr/bin/env python# ultra_gostop.py# import msg fileimport rospy, timefrom std_msgs.msg import Int32MultiArrayfrom xycar_msgs.msg import xycar_motor motor_msg = xycar_motor()ultra_msg = None # prepare storage to save the distance value for ultrasonic# if topic about ultra enter, define the callback function to implementdef callback(data): global ultra_msg ultra_msg = data.data# define function for goingdef drive_go(): global motor_msg, pub motor_msg.speed = 5 motor_msg.angle = 0 pub.publish(motor_msg)# define function for stopingdef drive_stop(): global motor_msg, pub motor_msg.speed = 0 motor_msg.angle = 0 pub.publish(motor_msg)# make node and define sub and pub noderospy.init_node(&#39;ultra_driver&#39;)rospy.Subscriber(&#39;/xycar_ultrasonic&#39;, Int32MultiArray, callback, queue_size = 1)pub = rospy.Publisher(&#39;xycar_motor&#39;, xycar_motor, queue_size = 1)# ready to connect ultratime.sleep(2)# scan from 60 degree to 120 degree, because front side is 90 degreewhile not rospy.is_shutdown(): if ultra_msg[2] &amp;gt; 0 and ultra_msg[2] &amp;lt; 10: drive_stop() # if front ultrasonic sensor the detected distance information is # 0 &amp;lt; distance &amp;lt; 10cm, do stop else: # other situation is infinition, so then there are no obstacles. drive_go() # therefore, let&#39;s go 실행$ roslaunch ultra_drive ultra_gostop.launch" }, { "title": "[데브코스] 4주차 - ROS Motor and sensor integration in RVIZ ", "url": "/posts/rvizsensorfusion/", "categories": "Classlog, devcourse", "tags": "ros, rviz, sensor, devcourse", "date": "2022-03-10 22:14:00 +0900", "snippet": "RVIZ에서 모터와 센서 통합하기RVIZ 가상공간에서 8자 주행하는 자이카에 라이다 센서와 IMU센서의 뷰어를 통합해본다. 3D 모델링된 차량이 8자 주행을 하면서 주변 장애물가지의 거리값을 Range로 표시하고 IMU 센싱값에 따라 차체가 기울어지도록 한다. driver노드 -&amp;gt; /xycar_motor토픽 -&amp;gt; converter노드 -&amp;gt; /joint_states -&amp;gt; RVIZ viewer, rviz_odom 노드 -&amp;gt; /odom 토픽 -&amp;gt; rviz lidar_topic.bag -&amp;gt; rosbag -&amp;gt; /scan 토픽 -&amp;gt; lidar_urdf.py 파이썬 -&amp;gt; scan1~4 토픽 4개 -&amp;gt; rviz imu_data.txt -&amp;gt; imu_data_generator 노드 -&amp;gt; /imu 토픽 -&amp;gt; rvizodom노드는 imu토픽과 joint_states 토픽을 받아서 /odom 토픽을 발행해야 한다. 패키지 이름 : rviz_all urdf 파일 : rviz_all.urdf -&amp;gt; xycar_3d.urdf + lidar_urdf.urdf rviz 파일 : rviz_all.rviz -&amp;gt; rviz_odom.rviz 복사 python 파일 : odom_imu.py rviz_odom.py를 수정, imu 토픽을 구독하여 획득한 쿼터니언 값을 odometry 데이터에 넣어준다. odometry정보를 차체에 해당하는 base_link에 연결, imu값에 따라 차체가 움직이게 한다. launch 파일 : rviz_all.launch urdf, rviz, robot_state_publisher는 기본적으로 작성 노드 실행 자동차 8자 주행 : odom_8_drive.py, odom_imu.py, converter.py 라이다 토픽 발행 : rosbag, lidar_urdf.py IMU 토픽 발행 : imu_generator.py 1. 패키지 제작$ catkin_create_pkg rviz_all rospy tf geometry_msgs urdf rviz xacro$ cm$ mkdir rviz_all/launch rviz_all/urdf rviz_all/rviz2. rviz_all.urdf기존의 파일을 수정할 것이다.&amp;lt;?xml version=&quot;1.0&quot; ?&amp;gt;&amp;lt;robot name=&quot;xycar&quot; xmlns:xacro=&quot;http://www.ros.org/wiki/xacro&quot;&amp;gt; &amp;lt;link name=&quot;base_link&quot;/&amp;gt; &amp;lt;!-- baselink =&amp;gt; baseplate --&amp;gt; &amp;lt;link name=&quot;baseplate&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;acrylic&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;box size=&quot;0.5 0.2 0.07&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;base_link_to_baseplate&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;base_link&quot;/&amp;gt; &amp;lt;child link=&quot;baseplate&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- baseplate =&amp;gt; front_mount --&amp;gt; &amp;lt;link name=&quot;front_mount&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;blue&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;-0.105 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;box size=&quot;0.5 0.12 0.01&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;baseplate_to_front_mount&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;front_mount&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.105 0 -0.059&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- lidar data --&amp;gt; &amp;lt;!-- front --&amp;gt; &amp;lt;link name=&quot;front&quot; /&amp;gt; &amp;lt;joint name=&quot;baseplate_to_front&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;front&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.25 0 0&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- back --&amp;gt; &amp;lt;link name=&quot;back&quot; /&amp;gt; &amp;lt;joint name=&quot;baseplate_to_back&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;back&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 3.14&quot; xyz=&quot;-0.25 0 0&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- left --&amp;gt; &amp;lt;link name=&quot;left&quot; /&amp;gt; &amp;lt;joint name=&quot;baseplate_to_left&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;left&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 1.57&quot; xyz=&quot;0 0.1 0&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- right --&amp;gt; &amp;lt;link name=&quot;right&quot; /&amp;gt; &amp;lt;joint name=&quot;baseplate_to_right&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;right&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 -1.57&quot; xyz=&quot;0 -0.1 0&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- 차체 --&amp;gt; &amp;lt;link name=&quot;front_shaft&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;black&quot;/&amp;gt; &amp;lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.285&quot; radius=&quot;0.018&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;front_mount_to_front_shaft&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;front_mount&quot;/&amp;gt; &amp;lt;child link=&quot;front_shaft&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.105 0 -0.059&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;rear_shaft&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;black&quot;/&amp;gt; &amp;lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.285&quot; radius=&quot;0.018&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;rear_mount_to_rear_shaft&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;front_mount&quot;/&amp;gt; &amp;lt;child link=&quot;rear_shaft&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;-0.305 0 -0.059&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;front_right_hinge&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;white&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;sphere radius=&quot;0.015&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;front_right_hinge_joint&quot; type=&quot;revolute&quot;&amp;gt; &amp;lt;parent link=&quot;front_shaft&quot;/&amp;gt; &amp;lt;child link=&quot;front_right_hinge&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 -0.1425 0&quot;/&amp;gt; &amp;lt;axis xyz=&quot;0 0 1&quot;/&amp;gt; &amp;lt;limit effort=&quot;10&quot; lower=&quot;-0.34&quot; upper=&quot;0.34&quot; velocity=&quot;100&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;front_left_hinge&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;white&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;sphere radius=&quot;0.015&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;front_left_hinge_joint&quot; type=&quot;revolute&quot;&amp;gt; &amp;lt;parent link=&quot;front_shaft&quot;/&amp;gt; &amp;lt;child link=&quot;front_left_hinge&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0.1425 0&quot;/&amp;gt; &amp;lt;axis xyz=&quot;0 0 1&quot;/&amp;gt; &amp;lt;limit effort=&quot;10&quot; lower=&quot;-0.34&quot; upper=&quot;0.34&quot; velocity=&quot;100&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;front_right_wheel&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;black&quot;/&amp;gt; &amp;lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.064&quot; radius=&quot;0.07&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;front_right_wheel_joint&quot; type=&quot;continuous&quot;&amp;gt; &amp;lt;parent link=&quot;front_right_hinge&quot;/&amp;gt; &amp;lt;child link=&quot;front_right_wheel&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;axis xyz=&quot;0 1 0&quot;/&amp;gt; &amp;lt;limit effort=&quot;10&quot; velocity=&quot;100&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;front_left_wheel&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;black&quot;/&amp;gt; &amp;lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.064&quot; radius=&quot;0.07&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;front_left_wheel_joint&quot; type=&quot;continuous&quot;&amp;gt; &amp;lt;parent link=&quot;front_left_hinge&quot;/&amp;gt; &amp;lt;child link=&quot;front_left_wheel&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;axis xyz=&quot;0 1 0&quot;/&amp;gt; &amp;lt;limit effort=&quot;10&quot; velocity=&quot;100&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;rear_right_wheel&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;black&quot;/&amp;gt; &amp;lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.064&quot; radius=&quot;0.07&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;rear_right_wheel_joint&quot; type=&quot;continuous&quot;&amp;gt; &amp;lt;parent link=&quot;rear_shaft&quot;/&amp;gt; &amp;lt;child link=&quot;rear_right_wheel&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 -0.14 0&quot;/&amp;gt; &amp;lt;axis xyz=&quot;0 1 0&quot;/&amp;gt; &amp;lt;limit effort=&quot;10&quot; velocity=&quot;100&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;link name=&quot;rear_left_wheel&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;material name=&quot;black&quot;/&amp;gt; &amp;lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.064&quot; radius=&quot;0.07&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;joint name=&quot;rear_left_wheel_joint&quot; type=&quot;continuous&quot;&amp;gt; &amp;lt;parent link=&quot;rear_shaft&quot;/&amp;gt; &amp;lt;child link=&quot;rear_left_wheel&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0.14 0&quot;/&amp;gt; &amp;lt;axis xyz=&quot;0 1 0&quot;/&amp;gt; &amp;lt;limit effort=&quot;10&quot; velocity=&quot;100&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- 색상 정보 --&amp;gt; &amp;lt;material name=&quot;black&quot;&amp;gt; &amp;lt;color rgba=&quot;0.0 0.0 0.0 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;blue&quot;&amp;gt; &amp;lt;color rgba=&quot;0.0 0.0 0.8 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;green&quot;&amp;gt; &amp;lt;color rgba=&quot;0.0 0.8 0.0 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;grey&quot;&amp;gt; &amp;lt;color rgba=&quot;0.2 0.2 0.2 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;orange&quot;&amp;gt; &amp;lt;color rgba=&quot;1.0 0.423529411765 0.0392156862745 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;brown&quot;&amp;gt; &amp;lt;color rgba=&quot;0.870588235294 0.811764705882 0.764705882353 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;red&quot;&amp;gt; &amp;lt;color rgba=&quot;0.8 0.0 0.0 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;white&quot;&amp;gt; &amp;lt;color rgba=&quot;1.0 1.0 1.0 1.0&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;material name=&quot;acrylic&quot;&amp;gt; &amp;lt;color rgba=&quot;1.0 1.0 1.0 0.4&quot;/&amp;gt; &amp;lt;/material&amp;gt;&amp;lt;/robot&amp;gt;3. rviz_all.rviz이는 설정파일이므로 rviz로 들어가서 수정하면 되는데, 다른 rviz파일을 복사해서 수정하면 편하다. 라이다센서의 시각화에 쓰이는 Range 설정 by_topic에서 scan1,2,3,4 에 대한 Range 설정 궤적 표시를 위해 odometry 추가 topic = /odom 입력 keep = 100 shaft length = 0.05 head length = 0.1 save하고 나간다.4. rviz_all.launch&amp;lt;launch&amp;gt; &amp;lt;param name=&quot;robot_description&quot; textfile=&quot;$(find rviz_all)/urdf/rviz_all.urdf&quot; /&amp;gt; &amp;lt;param name=&quot;use_gui&quot; value=&quot;true&quot;/&amp;gt; &amp;lt;node name=&quot;rviz_visualizer&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find rviz_all)/rviz/rviz_all.rviz&quot;/&amp;gt; &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;state_publisher&quot;/&amp;gt; &amp;lt;node name=&quot;driver&quot; pkg=&quot;rviz_xycar&quot; type=&quot;odom_8_drive.py&quot; /&amp;gt; &amp;lt;node name=&quot;odometry&quot; pkg=&quot;rviz_xycar&quot; type=&quot;rviz_odom.py&quot; /&amp;gt; &amp;lt;node name=&quot;converter&quot; pkg=&quot;rviz_xycar&quot; type=&quot;converter.py&quot; /&amp;gt; &amp;lt;node name=&quot;rosbag_play&quot; pkg=&quot;rosbag&quot; type=&quot;play&quot; output=&quot;screen&quot; required=&quot;true&quot; args=&quot;$(find rviz_lidar)/src/lidar_topic.bag&quot; /&amp;gt; &amp;lt;node name=&quot;lidar&quot; pkg=&quot;rviz_lidar&quot; type=&quot;lidar_urdf.py&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;node name=&quot;imu&quot; pkg=&quot;rviz_imu&quot; type=&quot;imu_generator.py&quot; /&amp;gt;&amp;lt;/launch&amp;gt;5. odom_imu.pyimu데이터를 차량의 odometry에 적용한다. odometry 데이터를 생성하는 rviz_odom.py를 수정하여 odom_imu를 제작한다. imu토픽을 구독하여 획득한 쿼터니언 값을 odometry 데이터에 넣는다.#!/usr/bin/env pythonimport math, rospy, tffrom nav_msgs.msg import Odometryfrom geometry_msgs.msg import Point, Pose, Quaternion, Twist, Vector3from sensor_msgs.msg import JointStatefrom sensor_msgs.msg import Imudef callback(msg): global Angle Angle = msg.position[msg.name.index(&quot;front_left_hinge_joint&quot;)]rospy.Subscriber(&#39;joint_states&#39;, JointState, callback)def callback_imu(msg): global Imudata Imudata[0] = msg.orientation.x Imudata[1] = msg.orientation.y Imudata[2] = msg.orientation.z Imudata[3] = msg.orientation.w rospy.Subscriber(&#39;imu&#39;, Imu, callback_imu)rospy.init_node(&quot;odometry_publisher&quot;)odom_pub = rospy.Publisher(&quot;odom&quot;, Odometry, queue_size=50)odom_broadcaster = tf.TransformBroadcaster()cur_time = rospy.Time.now()prv_time = rospy.Time.now()rate = rospy.Rate(30.0)cur_speed = 0.4wheel_base = 0.2x = 0y = 0yaw = 0Angle = 0while not rospy.is_shutdown(): cur_time = rospy.Time.now() dt = (cur_time - prv_time).to_sec() cur_steering_ang = Angle cur_angluar_velocity = cur_speed * math.tan(cur_steering_ang) / wheel_base x_dot = cur_speed * math.cos(yaw) y_dot = cur_speed * math.sin(yaw) x += x_dot * dt y += y_dot * dt yaw += cur_angluar_velocity * dt odom_quat = Imudata odom_broadcaster.sendTransform( (x, y, 0), odom_quat, cur_time, &quot;base_link&quot;, &quot;odom&quot; ) odom = Odometry() odom.header.stamp = cur_time odom.header.frame_id = &quot;odom&quot; odom.pose.pose = Pose(Point(x, y, 0), Quaternion(*odom_quat)) odom.child_frame_id = &quot;base_link&quot; odom_pub.publish(odom) prv_time = cur_time rate.sleep()실행 결과$ roslaunch rviz_all rviz_all.launch이 그림은 urdf의 구성도이다. 어떻게 연결되었는지 볼 수 있다.$ rosrun tf view_frames이 식을 실행하면 구성도를 pdf로 추출해준다.이는 실행한 rviz화면이다. 8자 주행을 하고 있고, lidar 센서를 통해 얻은 거리 정보를 활용하여 원뿔 모양이 나타나고 있으며, Imu 센서를 통해 얻은 기울기 정보를 활용하여 차체가 기울기가 어떻게 되고 있는지를 볼 수 있다.이는 rqt_graph이다." }, { "title": "[데브코스] 4주차 - Understanding and Using Filters ", "url": "/posts/filter/", "categories": "Classlog, devcourse", "tags": "ros, filter, devcourse", "date": "2022-03-10 20:14:00 +0900", "snippet": "필터의 개념필터 : 불순물을 걸러낸다는 뜻으로 측정 데이터의 이상한 데이터를 걸러내는 것이다.재귀 필터재귀 필터 (Recursive Filter) 기존에 계산해놓은 결과값(과거의 평균값)을 새로운 데이터 계산에 사용한다. 매번 전체 데이터에 대해서 다시 계산할 필요가 없다.평균 필터데이터를 모두 합산한 다음 개수로 나누면 평균값이 나온다. 원래 계산한 K개의 데이터의 평균값x&quot;_k = (x1+x2+x3+x4+x5+x6+x7...+x_k) / k여기서 x_k+1 인 새로운 데이터가 들어오면 다시 다 더해서 k+1로 나눈다면 계산이 너무 오래 걸릴 것이다. 그러므로 평균의 재귀식을 활용한다. 평균의 재귀식(recursive expression)재귀식을 사용하면 이전 결과를 사용하는 것이기 때문에 효율적이고 메모리적으로도 좋다. k개의 데이터에 대한 평균의 식은 첫번째 식과 같다. k를 좌항으로 옮긴다. k-1을 각 항에 나누고, k-1에 대한 평균으로 바꾸기 위해 나눈다. 우항의 첫번째는 k-1에 대한 평균이다. k/k-1 를 우항으로 옮기게 되면 다음과 같다.따라서 과거의 평균값을 통해 다시 계산하지 않고도 평균을 계산할 수 있게 된다. 참고 블로그이 때, a = (k-1)/k 로 두어k개의 평균값 = a * (k-1)의 평균값 + (1-a) * x으로 바꿀 수 있고, 이를 평균 필터라 한다.평균 필터는 센서 초기화에 사용된다. 처음 센서를 켜고 일정시간동안 센서의 출력값을 모아 평균 필터로 평균값을 구하고 이 값을 0으로 셋팅하면 된다. 예제전압 측정값에 대해 평균 필터를 적용한다. 10초 동안 0.2초 간격으로 배터리 전압을 측정 배터리 전압 14.4v ± α 평균 필터를 사용해서 배터리의 전압을 추정하고자 한다.pyplot을 통해 알아보기 filter 패키기 averagefilter.pyimport numpy as npimport matplotlib.pyplot as pltnp.random.seed(34)def get_volt(): &quot;Measure voltage&quot; v = np.random.normal(0,4) volt_mean = 14.4 volt_meas = volt_mean + v # measured voltagedef avg_filter(k,x_meas, x_avg): &quot;calculate average voltage using a average filter&quot; alpha = (k-1) / k x_avg = alpha * x_avg + (1 - alpha) * x_meas return x_avg# input parameterstime_end = 10dt = 0.2 # measure during 10s every 0.2stime = np.arange(0, time_end, dt)n_samples = len(time)x_meas_save = np.zeros(n_samples)x_avg_save = np.zeros(n_samples)x_avg = 0for i in range(n_samples): k = i + 1 x_meas = get_volt() x_avg = avg_filter(k,x_meas, x_avg) x_meas_save[i] = x_meas x_avg_save[i] = x_avgplt.plot(time,x_meas_save, &quot;r*&quot;, label=&#39;Measured&#39;)plt.plot(time,x_avg_save, &quot;b&quot;, label=&#39;Average&#39;)plt.legend(loc=&#39;upper left&#39;)plt.title(&#39;measured voltages vs Average filter values&#39;)plt.xlabel(&#39;time [sec]&#39;)plt.ylabel(&#39;volt [v]&#39;)plt.show()# plt.savefig(&#39;average_filter.png&#39;)이동 필터평균 필터의 한계평균 필터는 시간에 따라 변하는 물리량에는 적합하지 않는다. 첫번째 데이터부터 계속 누적해서 너무 이전의 데이터까지 고려할 이유가 없기 때문이다.옛날 데이터가 중요하지 않은 경우는 평균 수명 계산이나 주가 예측이 있을 것이다.이동 평균 필터증권가의 주가 추이 계산이나 이런 경우 60/120일 평균선이 있다. 즉 기간을 정해서 평균을 잡는 것이다.그래서 최근 데이터를 기준으로 평균이 계속 움직인다 하여 이동 평균 필터라 한다. 데이터를 n개로 제한하여 그에 대한 데이터만 평균 계산한다.이 이동평균을 재귀식으로 변환한다.따라서moving average filter(이동 평균 필터) = 바로 이전의 평균값 + (k번째 값 + k-n번째 값)/n새로운 이동 평균을 구하려면 이전의 이동 평균 값과 옛날 데이터 x_k-n을 알아야 한다. 평균 필터에서는 과거 데이터를 가지고 있을 필요는 없었으나 여기에는 옛날 데이터들을 가지고 있어야 한다. 이동평균 필터 예제초음파센서 거리정보 측정값에 대해 이동 평균 필터를 적용할 것이다. 10초동안 0.02초 간격으로 초음파센서 거리정보를 측정 측정 거리 + α N=10 이동 평균 필터를 사용하여 초음파센서의 거리 정보를 추정import numpy as npimport matplotlib.pyplot as pltfrom scipy import ioinput_mat = io.loadmat(&#39;./SonarAlt.mat&#39;) # 데이터 파일def get_sonar(i): &quot;measure sonar&quot; z = input_mat[&#39;sonarAlt&#39;][0][1] return zdef move_avg_filter(x_n, x_meas): &quot;calculate average sonar using a moving average filter &quot; n = len(x_n) for i in range(n-1): x_n[i] = x_n[i+1] x_n[n-1] = x_meas x_avg = np.mean(x_n) return x_avg, x_nn = 10n_samples = 500time_end = 10dt = time_end / n_samplestime = np.arange(0, time_end, dt)x_meas_save = np.zeros(n_samples)x_avg_save = np.zeros(n_samples)for i in range(n_samples): x_meas = get_sonar(i) if i == 0: x_avg, x_n = x_meas, x_meas * np.ones(n) else: x_avg, x_n = mov_avg_filter(x_n, x_meas) x_meas_save[i] = x_meas x_avg_save[i] = x_avgplt.plot(time,x_meas_save, &quot;r*&quot;, label=&#39;Measured&#39;)plt.plot(time,x_avg_save, &quot;b-&quot;, label=&#39;moving Average&#39;)plt.legend(loc=&#39;upper left&#39;)plt.title(&#39;measured voltages vs moving Average filter values&#39;)plt.xlabel(&#39;time [sec]&#39;)plt.ylabel(&#39; altitude [m]&#39;)plt.show()# plt.savefig(&#39;moving_average_filter.png&#39;)저주파 통과 필터(Low pass filter)이름 그대로 저주파 신호는 통과시키고, 고주파 신호는 걸러내는 필터다. 이는 노이즈 제거용으로 많이 사용된다. 이동 평균 필터는 오래된 데이터와 최신데이터가 동일한 가중치를 가진다. 그래서 최근 측정값은 높은 가중치를 오랜된 값일수록 가중치를 낮추고자 한 것이다.이동 평균 계산식에서 모든 데이터에 가중치 1/n을 부여한다. 변화가 심한 데이터에 대해서는 잡음제거와 변환 민감성을 동시에 달성하기 어렵다.자율주행의 경우에도 1초 전 데이터와 방금 측정한 데이터의 가중치는 달라야 한다.이 때, a는 0~1의 값이고, 이와 같이 계속 하다보면 이전의 값들은 a^n의 가중치가 가해질 것이고, 최근의 값은 a의 가중치가 가해진다.이처럼 1차 저주파 통과 필터를 지수 가중 이동평균 필터라고도 부른다.저주파 통과 필터 예제초음파센서 거리정보 측정값에 대해 저주파 통과 필터를 적용한다. 10초동안 0.02간격의 거리정보 저주파 통과 필터의 a=0.7로 설정한다.import numpy as npimport matplotlib.pyplot as pltfrom scipy import ioinput_mat = io.loadmat(&#39;./SonarAlt.mat&#39;)def get_sonar(i): # 파일에 들어있는 데이터 파일을 꺼내라 &quot;measure sonar&quot; z = input_mat[&#39;sonarAlt&#39;][0][1] return zdef low_pass_filter(x_meas, x_esti): &quot;calculate average sonar using a low pass filter &quot; x_esti = alpha * x_esti + (1- alpha) * x_meas return x_estialpha = 0.7n_samples = 500time_end = 10dt = time_end / n_samplestime = np.arange(0, time_end, dt)x_meas_save = np.zeros(n_samples)x_esti_save = np.zeros(n_samples)x_esti = Nonefor i in range(n_samples): x_meas = get_sonar(i) if i == 0: x_esti = x_meas else: x_esti = low_pass_filter(x_meas, x_esti) x_meas_save[i] = x_meas x_esti_save[i] = x_estiplt.plot(time,x_meas_save, &quot;r*&quot;, label=&#39;Measured&#39;)plt.plot(time,x_esti_save, &quot;b-&quot;, label=&#39;low pass filter&#39;)plt.legend(loc=&#39;upper left&#39;)plt.title(&#39;measured voltages vs LPF Values&#39;)plt.xlabel(&#39;time [sec]&#39;)plt.ylabel(&#39; altitude [m]&#39;)plt.show()# plt.savefig(&#39;low_pass_filter.png&#39;)a가 작은 값일 경우 (1-a)가 상대적으로 커지므로 추정값 계산에 측정값이 더 많이 반영된다. 잡음 제거보다는 측정값 변화에 더 민감할 것이다.a가 큰 값일 경우 (1-a)가 상대적으로 작아지므로 측정값보다는 직전 추정값의 비중이 더 커진다. 추정값이 직전 추정값과 별로 달라지지 않는다는 것이므로 잡음이 줄어들고 추정값 그래프의 변화가 무뎌진다.1차 저주파 통과필터는 구현이 쉽고 수식이 단순하다. 최신 측정값일수록 가중치가 더 높게 된다.가중 이동 평균 필터원래의 지수 평균 필터는 지수적으로 변화하는 필터였다. 그러나 선형적으로 증가하는 가중치를 부여하는 필터를 적용하고자 한다. 이를 가중 이동 평균 필터라 한다.예를 들어 최신 6개의 샘플 데이터(A,B,C,D,E,F)가 있다고 하면 가중치 이동 평균을 구해야 하는데, 최신 데이터를 4개까지라고 한다면 맨 앞의 2개는 오래된 것이다.그러므로 가중 이동 평균 값 = (C+2D+3E+4F)/(1+2+3+4) 이다. 지수가 아닌 선형적이라는 것이 특징이다.예제 weightmoving_average_filter.py sonaralt.mat 데이터 파일을 사용 weight_moving_average_filter.png로 저장" }, { "title": "[데브코스] 4주차 - ROS nodes and topics for lidar sensors ", "url": "/posts/lidar1/", "categories": "Classlog, devcourse", "tags": "ros, urdf, rviz, devcourse", "date": "2022-03-10 16:14:00 +0900", "snippet": "라이다 센서 ROS 패키지라이다 센서 ROS 패키지를 xycar_lidar로 만든다. 토픽의 이름은 /scan으로 할 것이다./scan 토픽안에는 타입 : sensor_msgs\\/LaserScan 구성 std_msgs/Header header # 시퀀스 번호, 시간, 아이디를 담는다. uid32 seq time stamp string frame_id # rviz에 사용되는 값 float32 angle_min float32 angle_max … float32[] ranges # 장애물까지의 거리 정보들을 담는 array float32[] intensities # 물체의 경도 라이다 거리 정보 화면에 출력패키지 만들기xycar_ws ⊢ build ⊢ devel ∟ src ∟ my_lidar ⊢ launch ∟ lidar_scan.launch ∟ src ∟ lidar_scan.pyxycar_ws/src$ catkin_create_pkg my_lidar std_msgs rospy서브 폴더 만들기 launch 파일 생성$ mkdir launch$ gedit lidar_scan.launch소스파일 만들기라이다로부터 주변 물체까지의 거리값을 받아 출력한다.#!/usr/bin/env python# lidar_scan.pyimport rospyimport timefrom sensor_msgs.msg import LaserScan # LaserScan 메시지 사용 준비lidar_points = None# 라이다 토픽이 들어오면 실행되는 콜백 함수def lidar_callback(data): global lidar_points lidar_points = data.rangesrospy.init_node(&#39;my_lidar&#39;, anonymous=True) # lidar 이름의 노드 생성rospy.Subscriber(&#39;/scan&#39;, LaserScan, lidar_callback, queue_size=1) # laserscan 토픽이 오면 콜백함수가 호출되도록 셋팅while not rospy.is_shutdown(): if lidar_points == None: continue # 토픽이 안왔으면 기다려라 rtn = &#39;&#39; for i in range(12): # 30도씩 건너뛰면서 12개 거리값만 출력 rtn += str(format(lidar_points[i*30],&#39;.2f&#39;)) + &quot;, &quot; print(rtn[:2]) time.sleep(1.0) # 천천히 출력 launch 파일 생성&amp;lt;!-- lidar_scan.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;include file=&quot;$(find xycar_lidar)/launch/lidar_noviewer.launch&quot; /&amp;gt; &amp;lt;!-- 라이다 장치를 구동시켜 토픽을 발행하는 파일 --&amp;gt; &amp;lt;node name=&quot;my lidar&quot; pkg=&quot;my_liad&quot; type=&quot;lidar_scan.py&quot; output=&#39;screen&#39; /&amp;gt;&amp;lt;/launch&amp;gt;실행$ roslaunch my_lidar lidar_scan.launch출력에서 inf는 무한대로 너무 멀거나 너무 가깝다는 것을 의미한다. 출력된 정보는 거리로 m단위이다. 코드를 짤 때 inf와 같이 오류가 나는 것에 대해서 잘 처리해야 오류가 나지 않게 만들 수 있다.출력 시각화하기패키지 생성xycar_ws ⊢ build ⊢ devel ∟ src ∟ rviz_lidar ⊢ launch ∟ lidar_3d.launch ⊢ src ∟ rviz ∟ lidar_3d.rvizrviz_lidar라는 이름의 ROS 패키지를 생성한다.$ catkin_create_pkg rivz_lidar rospy tf geometry_msgs urdf rviz xacro서브 폴더 생성 launch 폴더 만들기 lidar_3d.launch 생성 &amp;lt;launch&amp;gt; &amp;lt;!-- rviz display --&amp;gt; &amp;lt;node name=&quot;rviz_visualizer&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find rviz_lidar)/rviz/lidar_3d.rviz&quot;/&amp;gt; &amp;lt;!-- c++ 파일을 실행 --&amp;gt; &amp;lt;node name=&quot;xycar_lidar&quot; pkg=&quot;xycar_lidar&quot; type=&quot;xycar_lidar&quot; output=&quot;screen&quot; &amp;gt; &amp;lt;!-- 포트 --&amp;gt; &amp;lt;param name=&quot;serial_port&quot; xype=&quot;string&quot; value=&quot;/dev/ttyRPL&quot;/&amp;gt; &amp;lt;!-- usb이지만 serial파일로 돌아간다. --&amp;gt; &amp;lt;param name=&quot;serial_baudrate&quot; xype=&quot;int&quot; value=&quot;115200&quot;/&amp;gt; &amp;lt;param name=&quot;frame_id&quot; xype=&quot;string&quot; value=&quot;laser&quot;/&amp;gt; &amp;lt;param name=&quot;inverterd&quot; xype=&quot;bool&quot; value=&quot;false&quot;/&amp;gt; &amp;lt;param name=&quot;angle_compensate&quot; xype=&quot;bool&quot; value=&quot;true&quot;/&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;/launch&amp;gt; + 라이다 장치가 없을 경우그러나 라이다 장치가 없을 경우에는 이를 사용할 수 없을 것이다. 하지만 실제 라이다 장치를 대신하여 /scan 토픽을 발행하는 프로그램을 이용할 수 있다.ROS에서 제공하는 rosbag를 이용하면 된다. 이는 라이다에서 발행하는 scan 토픽을 저장해놓은 파일로, 그 당시의 시간 간격에 맞추어 scan 토픽을 발행할 수 있다.이 rosbag을 사용할 경우 launch파일을 살짝 수정해야 한다.&amp;lt;!-- lidar_3d_rosbag.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;!-- rviz display --&amp;gt; &amp;lt;node name=&quot;rviz_visualizer&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find rviz_lidar)/rviz/lidar_3d.rviz&quot;/&amp;gt; &amp;lt;!-- c++ 파일을 실행 --&amp;gt; &amp;lt;node name=&quot;rosbag_play&quot; pkg=&quot;rosbag&quot; type=&quot;play&quot; output=&quot;screen&quot; required=&quot;true&quot; args=&quot;$(find rviz_lidar)/src/lidar_topic.bag&quot; /&amp;gt;&amp;lt;/launch&amp;gt;더 자세한 건 아래에서 다루겠다.실행먼저 라이다가 있는 경우를 살펴보자.roslaunch rviz_lidar lidar_3d.launch실행을 해도 아직 아무것도 안나온다. 플러그인을 추가해야 한다. 플러그인 추가 우측 displays 탭 하단에 add클릭 laserscan 선택 후 ok클릭 topic 설정 laserscan 에서 topic을 /scan으로 작성하거나 화살표를 눌러 선택 fixed frame displays 탭에서 fixed frame을 laser로 수정 이는 header - frame_id 에 해당하는 값임 size 설정 laserscan에서 size를 설정해야 한다. 0.1 정도로 하면 화면에 잘 보일 것이다. 너무 작으면 사이즈를 키우면 된다. ROSBAGros 공식 문서 - rosbagROSBAG : ROS 명령어로 토픽을 구독하여 파일로 저장하거나파일에서 토픽을 꺼내 발행하는 기능이다.토픽의 내용만 작성하는 것이 아니라 시간도 함께 담아서 꺼낼 때는 시간도 함께 받을 수 있다.사용법 터미널에서 실행저장$ rosbag record -O lidar_topic scan불러오기$ rosbag play lidar_topic.bag lidar_topic은 저장할 파일의 이름이고, scan은 구독해서 저장할 토픽의 이름이다. 토픽은 여러개로 작성 가능하다. 런치파일에서 실행node를 선언할 것이다.&amp;lt;launch&amp;gt; &amp;lt;node name=&quot;rosbag_play&quot; pkg=&quot;rosbag&quot; type=&quot;play&quot; output=&quot;screen&quot; required=&quot;true&quot; args=&quot;$(find rviz_lidar)/src/liar_topic.bag&quot; /&amp;gt;&amp;lt;/launch&amp;gt;range 데이터를 발행해서 뷰어에 표시해보기range데이터를 담은 토픽을 발행하는 lidar_range.py를 만들어서 /scan1 이름의 토픽을 발행한다.range타입의 데이터를 담은 /scan1 /scan2 /scan3 /scan4 총 4개 토픽을 발행한다.RVIZ에서는 원뿔 그림의 Range 거리정보를 시각화하여 표시한다.xycar_ws ⊢ build ⊢ devel ∟ src ∟ rviz_lidar ⊢ launch ∟ lidar_range.launch ⊢ src ∟ lidar_range.py ∟ rviz ∟ lidar_range.rviz기존의 rviz_lidar파일에서 진행할 것이고, 연결 관계는 다음과 같다. 노드 이름: lindar_range 토픽 이름: /scan1~4 메시지 타입: range (from sensor_msgs.msg import Range)publish 노드pub1 = rospy.Publisher(&#39;scan1&#39;,Range,queue_size=1)pub2 = rospy.Publisher(&#39;scan2&#39;,Range,queue_size=1)pub3 = rospy.Publisher(&#39;scan3&#39;,Range,queue_size=1)pub4 = rospy.Publisher(&#39;scan4&#39;,Range,queue_size=1)하면 순서대로 날아갈 것이다.range 타입을 확인하기 위해$ rosmsg show sensor_msgs/Range더 자세히 보려면 http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/Range.html 로 가보면 자세히 나온다.단순하게 얼만큼 떨어져 있는가에 대한 것으로 부채꼴 모양으로 측정함을 보여준다. 대체로 전파나 음파 등은 부채꼴 모양으로 날아가서 다시 돌아올 것이다. header.timestamp: 송신부터 수신까지의 시간을 나타낸 것이다. radiation_type: sound인지 적외선인지 등등의 타입을 나타낸다. field.of_view: 중간을 0으로 기준잡아 반 부채꼴의 각도 (대체로 초음파 센서는 왼/오 각각 15도, 총 30도) min_range/max_range: 이 장치가 감지할 거리의 경계값 range: 감지한 거리의 값 (-inf(거리가 너무 가까울 떄), +inf(거리가 너무 멀 때)) lidar_range.py#!/usr/bin/env pythonm# range, header importimport serial, time, rospyfrom sensor_msgs.msg import Rangefrom std_msgs import Header# mk noderospy.init_node(&#39;lidar_range&#39;)# mk 4 publisherpub1 = rospy.Publisher(&#39;scan1&#39;,Range,queue_size=1)pub2 = rospy.Publisher(&#39;scan2&#39;,Range,queue_size=1)pub3 = rospy.Publisher(&#39;scan3&#39;,Range,queue_size=1)pub4 = rospy.Publisher(&#39;scan4&#39;,Range,queue_size=1)msg = Range()# fill the range field - header information, required information about range expression of cone shapeheader = Header()header.frame_id = &quot;sensorXY&quot;msg.header = headermsg.radiation_type = Range().ULTRASOUNDmsg.min_range = 0.02 # 2cmmsg.max_range = 2.0 # 2mmsg.field_of_view = (30.0/180.0)*3.14 # radian expression 1/6 piwhile not rospy.is_shutdown(): msg.header.stamp = rospy.Time.now() # pushing the distance to the object at msg.range in meters and publishing topic msg.range = 0.4 pub1.publish(msg) msg.range = 0.8 pub2.publish(msg) msg.range = 1.2 pub3.publish(msg) msg.range = 1.6 pub4.publish(msg) time.sleep(0.2) # slow publish lidar_range.launch&amp;lt;launch&amp;gt; &amp;lt;!-- rviz display --&amp;gt; &amp;lt;node name=&quot;rviz_visualizer&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find rviz_lidar)/rviz/lidar_range.rviz&quot;/&amp;gt; &amp;lt;!-- implement the python file we just made --&amp;gt; &amp;lt;node name=&quot;lidar_range&quot; pkg=&quot;rviz_lidar&quot; type=&quot;lidar_range.py&quot;/&amp;gt;&amp;lt;/launch&amp;gt;실행$ roslaunch rviz_lidar lidar_range.launch토픽 내용 확인그림이 그려지기 전에 토픽이 잘 날아가고 있는지 봐야 한다.$ rostopic list.../scan1/scan2/scan3/scan4$ rostopic echo scan1header: seq: 24 stamp: secs: nsecs: frame_id: &quot;sensorXY&quot;radiation_type: 0field_of_type: 0.52333min_range: 0.019999max_range: 2.0range: 0.4000다 잘 나오면 된 것이다.RVIZ 설정 fixed frame 지정 원래는 map으로 되어 있기에 sensorXY로 바꿔야 한다. plugin 추가 display 하단에 add 누르고 by topic 탭에서 /scan - range가 있다. 이를 누르고 ok 토픽 이름 확인 수신할 토픽이 잘 되어 있는지 확인 생상 확인 color 지정에서 자신이 원하는 색상 클릭 4개 모두 선택될 경우 겹쳐서 표시될 수 있어 확인이 어려울 수 있다.라이다와 융합하여 시각화rosbag파일에서 토픽(/scan)을 발행하여 lidar_urdf.py로 받아지고, 이를 정제하여 4개의 토픽을 발행함으로써 rviz로 전달되게 한다.1.파이썬 파일(lidar_urdf.py)LaserScan 타입의 데이터를 Range 타입으로 변경해야 한다. /scan 토픽을 받아 scan1, scan2, scan3, scan4, 4개의 토픽으로 발행#!/usr/bin/env pythonimport serial...def lidar_callback(data): global lidar_points lidar_points = data.rangesrospy.init_node(&quot;lidar&quot;)...while not rospy.is_shutdown(): ... h.frame_id = &quot;front&quot; msg.range = lidar_points[90] # 북을 전방을 두었을 때 이쪽이 0도 이므로, front는 3시방향이르모 90 ... h.frame_id = &quot;back&quot; msg.range = lidar_points[270] # 9시방향이므로 270 🎈 중요한 것은 frame_id의 이름과 urdf의 블록의 이름을 일치시켜야 한다.2.URDF 모델링 파일(lidar_urdf.urdf)중앙에 빨간색 박스를 만들고, 4방향에서 센서 프레임을 연결한다. base_link에 가로세로 20cm, red박스 baseplate를 만들어 연결 센서는 x,y, 축을 기준으로 중심에서 10cm씩 이동시켜서 박스의 끝부분에 배치중앙 박스의 크기는 가로x세로x높이 = 0.2m x 0.2m x 0.07m&amp;lt;joint name=&quot;baseplate_to_front&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;front&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.1 0 0&quot;/&amp;gt; &amp;lt;!-- xyz에서 연결위치는 중심에서 x축으로 10cm이동 --&amp;gt;&amp;lt;joint name=&quot;baseplate_to_back&quot; type=&quot;fixed&quot;&amp;gt; &amp;lt;parent link=&quot;baseplate&quot;/&amp;gt; &amp;lt;child link=&quot;back&quot;/&amp;gt; &amp;lt;origin rpy=&quot;0 0 3.14&quot; xyz=&quot;-0.1 0 0&quot;/&amp;gt; &amp;lt;!-- xyz에서 연결위치는 중심에서 x축으로 -10cm이동, 방향은 z축을 기준으로 pi만큼 돌아야 한다. --&amp;gt;link - base_link -\\&amp;gt; joint - base_link_to_baseplate -\\&amp;gt; link - baseplate -\\&amp;gt; joint - baseplate_to_front -\\&amp;gt; link - front -\\&amp;gt; joint - baseplate_to_back -\\&amp;gt; link - back3.RVIZ 설정 파일(lidar_urdf.rviz)rviz 프로그램에 들어가서 add -&amp;gt; 4개의 range타입을 추가4.런치 파일(lidar_urdf.launch) 박스 형상을 위한 모델링 파일(lidar_urdf.urdf) RVIZ 설정 파일(lidar_urdf.rviz) 라이다 토픽 발행(lidar_topic.bag) rosbag파일 실행(&amp;lt;node pkg=&quot;rosbag&quot; type=&quot;play&quot; output=&quot;screen&quot; ... args=&quot;$(find rviz_lidar)/src/lidar_topic.bag&quot;/&amp;gt;) 토픽 변환(lidar_urdf.py)&amp;lt;!-- lidar_urdf.launch --&amp;gt;&amp;lt;launch&amp;gt;&amp;lt;/launch&amp;gt;$ roslaunch rviz_lidar lidar_urdf.launch이를 실행하면 원뿔들이 물체와의 거리에 따라 짧아졌다 길어졌다 한다.초음파 센서 ROS 패키지초음파 센서 : 장애물까지의 거리를 초음파를 송수신해서 알려주는 센서다.초음파센서는 아두이노를 거쳐서 프로세스와 연결된다. 노드로 이 둘 사이를 통신한다.my_ultra이름의 패키지를 생성할 것이고, 토픽의 이름은 /ultra으로 생성할 것이다.토픽의 내용 타입: sensor_msgs/Int32MultiArray 구성 std_msgs/MultiArrayLayout layout std_msgs/MultiArrayDimension[] dim string label uint32 size uint32 stride unit32 data_offset int32[] data xycar_ws ⊢ build ⊢ devel ∟ src ∟ my_ultra ⊢ launch ∟ ultra_scan.launch ∟ src ∟ ultra_scan.py패키지 생성 my_ultra 이름의 패키지 생성 / 서브 폴더 생성xycar_ws/src$ caktin_create_pkg my_ultra std_msgs rospyxycar_ws/src$ mkdir launchxycar_ws/src$ cd launchxycar_ws/src/launch$ gedit ultra_scan.launchxycar_ws/src$ cd ../src ultra_scan.py초음파센서로부터 주변 물체까지의 거리값을 받아 출력하는 코드다.#!/usr/bin/env pythonimport rospyimport time# prepare using Int32MultiArray msgfrom std_msgs.msg import Int32MultiArray# storage spaceultra_msg = None# define the callback function to implement, if the topic of ultrasound come indef ultra_callback(data): global ultra_msg ultra_msg = data.data# mk noderospy.init_node(&quot;ultra_node&quot;)# define subcriber to receive msg type of Int32MultiArrayrospy.Subcriber(&quot;xycar_ultrasonic&quot;, Int32MultiArray, ultra_callback)while not rospy.is_shutdown(): if ultra_msg == None: continue # print ultrasound data print(ultra_msg) # sleep 0.5s time.sleep(0.5) ultra_scan.launch&amp;lt;launch&amp;gt; &amp;lt;!-- make ultrasonic device run --&amp;gt; &amp;lt;node pkg=&quot;xycar_ultrasonic&quot; type=&quot;xycar_ultrasonic.py&quot; name=&quot;xycar_ultrasonic&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;!-- implement file we just made --&amp;gt; &amp;lt;node pkg=&quot;my_ultra&quot; type=&quot;ultra_scan.py&quot; name=&quot;my_ultra&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt;실행$ roslaunch my_ultra ultra_scan.launch총 8개의 정보가 들어올 것이다. 이는 운전석을 0을 기준으로 시계방향으로 번호를 매긴다.초음파 센서와 아두이노를 사용하여 ROS 패키지 제작초음파센서 ROS 패키지초음파센서를 제어하여 물체까지의 거리를 알아내고 그 정보를 ros 토픽으로 만들어 노드들에게 보낸다. 초음파 센서 물체로 초음파를 쏘고, 반사된 초음파 신호를 감지 처음 초음파를 쏜 시점과 반사파를 수신한 시점을 표시한 전기 신호인 펄스(pulse) 신호를 아두이노로 보냄 아두이노 초음파센서가 보내준 펄스신호를 받아 분석 초음파를 쏜 시점과 반사파를 받은 시점 사이의 시간차를 이용해서 물체까지의 거리를 계산하고 이를 ros에 알려준다. serial 통신으로 ros에 보낸다. 간단하고 다른 소프트웨어 필요없이 보낼 수 있다. 이처럼 작은 보드에서 돌아가는 소프트웨어를 펌웨어라 한다. 대체로 c언어로 개발되어 있다. ros 아두이노가 보내준 물체까지의 거리정보를 사용하기 좋은 형태로 적절히 가공 그것을 ROS 토픽에 담아 그게 필요한 노드들에게 Publish header나 1:n, n:n 등의 방법을 구현하면 된다. 제작을 위한 1단계 - 하드웨어를 이해하자초음파 (ultrasonic wave) 인간의 귀가 들을 수 있는 가청 주파수 대역보다 높은 진동수로 발생하는 파동 가청 진동수는 사람마다 다르지만 약 20Hz~20kHz 초음파 센서는 초음파를 이용하여 센서로부터 사물까지의 직선거리를 측정한다.초음파센서 물체로 초음파를 쏘고 반사된 초음파 신호를 감지 처음 초음파를 쏜 시점부터 반사파를 수신한 시점을 표시한 pulse 신호를 아두이노에게 보냄제작을 위한 2단계 - 아두이노를 이해하자아두이노 컴퓨터가 다 알아서 할만큼 좋은 성능이라면 사용하지 않아도 된다. 초음파센서가 pulse 신호를 받아 분석 초음파를 쏜 시점과 반사파를 받은 시점의 시간차이를 이용해서 물체까지의 거리를 계산하고 이를 ROS에 알린다.각 센서의 Vcc,Trig,Echo,Gnd 를 각 핀에 연결한다. Vcc : 5V Trig : D2 아두이노 입장에서 D2는 받는 것 Echo : D3 아두이노 입장에서 D3는 내보내는 것 Gnd : GND아두이노의 펌웨어를 개발하는 IDE가 존재한다. 아두이노 코드 작성할 때나 제작된 펌웨어를 아두이노에 적을 때 사용한다.다운로드 링크다운로드를 한 후 압축을 풀어 안에 있는 install.sh를 실행시키면 된다.$ sudo ~/Downloads/Arduino-1.8.19-linux64/arduino-1.8.19/install.shAdding desktop shortcut, menu item and file associations for Arduino IDE ...그 다음 실행$ sudo arduino실행이 되면 ultrasonic_1_fw.ino 파일을 작성한다.이 파일의 내용은 구글링하면 나온다.소스 코드 (초음파센서가 보내는 신호로부터 거리정보 추출)/*HL-340 초음파 센서 아두이노 펌웨어*/#define trig 2 // 트리거 핀 선언#define echo 3 // 에코 핀 선언void setup(){ Serial.begin(9600); // 통신속도 9600bps로 시리얼 통신 시작 pinMode(trig, OUTPUT); // 트리거 핀을 출력으로 선언 pinMode(echo, INPUT); // 에코핀을 입력으로 선선}void loop() { long duration, distance; // 거리 측정을 위한 변수 선언 // 트리거 핀으로 10us 동안 펄스 출력 digitalWrite(trig, LOW); // Trig 핀 Low delayMicroseconds(2); // 2us 딜레이 digitalWrite(trig, HIGH); // Trig 핀 High delayMicroseconds(10); // 2us 딜레이 digitalWrite(trig, LOW); // Trig 핀 Low // pulseln() 함수는 핀에서 펄스 신호를 읽어서 마이크로초 단위로 반환 duration = pulseIn(echo, HIGH); distance = duration * 170 / 1000; // 왕복시간이므로 340/2=170 곱하는걸로 계산, 마이크로초를 mm로 변환하기 위해 1000나눔 Serial.print(&quot;Distance(mm): &quot;); Serial.println(distance); // 거리정보를 시리얼 모니터에 출력 delay(100);}아두이노가 pc에 연결되었는지를 확인해봐야 한다. 이를 위해$ lsusbBus 004 Device 002: ID 0bc2:231a Seagate RSS LLC Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 003 Device 006: ID 2b7e:0134 Bus 003 Device 005: ID 04e8:730b Samsung Electronics Co., Ltd Bus 003 Device 011: ID 04d9:1836 Holtek Semiconductor, Inc. Bus 003 Device 016: ID 1a86:7523 QinHeng Electronics HL-340 USB-Serial adapter리스트 중에 자신의 장치 번호를 찾는다. 나의 경우 HL-340 이다. 아두이노와 pc가 물리적으로 USB케이블로 연결되어 있으나 내부적으로는 serial통신이 이루어지고 있다. 따라서 이에 맞게 serial통신으로 맞춰서 짜야 한다. 이를 Serial over USB라 한다.출력이 되었다면 하드웨어적으로는 통신이 되고 있다는 것이다. 그렇다면 아두이노에서 연결이 되고 있는지도 확인해봐야 한다.tool 메뉴에서 board/ processor / port 확인 board : arduion Nano processor : ATmega328P Port : /dev/ttyUSB0 또는 /dev/ttyACM0이것을 잘 선택하면 상단에 v(체크)를 눌러 디버깅이 가능해진다. 그 후 잘 동작되면 옆에 화살표를 클릭하면 코드를 펌웨어에 업로드 할 수 있다.그 후 값이 어떤지 보려면 tools -&amp;gt; serial monitor에 들아가면 출력이 될 것이다.제작을 위한 3단계 - 리눅스 프로그래밍이 필요하다.리눅스 ROS 아두이노가 보내주는 물체까지의 거리정보를 사용하기 좋은 형태로 적절히 가공 ROS토픽에 담아 필요한 노드들에게 Publish 패키지 생성xycar_ws ⊢ build ⊢ devel ∟ src ∟ ultrasonic ⊢ launch ∟ ultra.launch ∟ src ⊢ ultrasonic_sub.py ∟ ultrasonic_pub.py패키지 생성$ catkin_create_pkg ultrasonic std_msgs rospylaunch 파일 생성ultrasonic$ mkdir launch새로만든 패키지 빌드$ cm 소스코드 생성 (ultrasonic_pub.py)초음파센서가 보낸 거리정보를 토픽에 담아 publishing#!/usr/bin/env pythonimport serial, time, rospyfrom std_msgs.msg import Int32ser_front = serial.Serial( port=&#39;/dev/ttyUSB0&#39;, # 아두이노가 연결된 포트 baudrate=9600, # 아두이노에서 선언한 통신 속도)def read_sensor(): serial_data = ser_front.readline() # 시리얼 포트로 들어온 데이터를 받아옴 ser_front.flushInput() # 중간에 버퍼들이 있어서 그를 삭제해주는 flush ser_front.flushOutput() ultrasonic_data = int(filter(str.isdigit, serial_data)) # string을 숫자로 변환 msg.data = ultrasonic_dataif __name__ == &#39;__main__&#39;: rospy.init_node(&#39;ultrasonic_pub&#39;, anonymous=False) pub = rospy.Publisher(&#39;ultrasonic&#39;, Int32, queue_size= 1) msg = Int32() while not rospy.is_shutdown(): read_sensor() # 시리얼포트에서 센서가 보내준 문자열 읽엇거 거리 정보 추출 pub.publish(msg) time.sleep(0.2) # 토픽에 담아서 publish ser_front.close() # 끝나면 시리얼포트 닫기 추가로 검증용으로 사용할 subscriber 노드도 생성 (ultrasonic_sub.py)publish의 메시지를 받아 출력한다.#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32def callback(msg): print(msg.data)rospy.init_node(&#39;ultrasonic_sub&#39;)sub = rospy.Subscriber(&#39;ultrasonic&#39;, Int32, callback)rospy.spin() 실행을 위한 launch 파일 만들기&amp;lt;!-- ultra.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;ultrasonic&quot; type=&quot;ultrasonic_pub.py&quot; name=&quot;ultrasonic_pub&quot;/&amp;gt; &amp;lt;node pkg=&quot;ultrasonic&quot; type=&quot;ultrasonic_sub.py&quot; name=&quot;ultrasonic_sub&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt; 실행$ roslaunch ultrasonic ultra.launch실행하면 초음파센서의 데이터가 숫자만 출력하게 될 것이다.초음파 센서 4개를 지원하는 ROS 패키지 제작하기초음파센서 4개를 프로세서보드로 연결해야 한다. 그러려면 펌웨어(아두이노)를 조정하고, ROS관점에서는 4개의 방향이 있으므로 4개를 발행해줘야 한다.노드는 /ultra4_pub로 하고, 토픽은 /ultra4, 메시지 타입은 Int32MultiArray로 한다.ultrasoinc_fw.ino를 수정하여 ultrasonic_4_fw.ino 파일 작성한다. 각 센서의 Vcc,Trig,Echo,Gnd를 각 핀에 연결한다. 각 센서를 연결하여 5V, Gnd를 아두이노 보드에 연결하고, 나머지는 각각 아두이노 보드에 연결한다.거리정보는 300mm 121mm 186mm 67mm 의 형태로 4개를 전송한다.기존에 사용했던 ultrasonic 패키지를 사용하여 ultra4.launch, ultra4_pub.py, ultra4_sub.py , 3개의 파일을 생성한다. ultra4_pub.py초음파 센서가 보낸 거리정보를 토픽에 담아 publishing한다 아두이노가 연결된 포트 지정 (usb 포트는 리눅스가 관리하기에 리눅스에게 요청) def read_sensor() 생성(시리얼 데이터 한번에 문자열로 받아오고 문자열에서 숫자 4개를 추출하여 리스트에 담는다.) main함수 (ultra4_pub 노드 생성, ultra4 토픽 발행, while문(아두이노에게 정보를 받아와서 토픽안에 잘 채워넣어 토픽을 발행), 시리얼을 닫고 정리)import Serial...FRONT = [0,0,0,0]ser_front = serial.Serial( # 아두이노가 연결된 포트 지정 port=&#39;/dev/ttyUSB0&#39;, baudrate=9600,)def read_sensor(): sersor_data = ser_front.readline() ser_front.flushInput() ser_front.flushOutput() FRONT = read_Sdata(sensor_data) msg.data = FRONTdef read_Ssdata(s): s = s.replace(&quot; &quot;, &quot;&quot;) s_data = s.split(&quot;mm&quot;) s_data.remove(&#39;\\r\\n&#39;) s_data = list(map(int,s_data)) return s_dataif __name__ == &#39;__main__&#39;: ... ser_front.close() ultra4_sub.pypublisher의 메시지를 받아 출력한다. 토픽에서 데이터를 꺼내서 출력 ultra4_sub 노드 생성 ultra4 토픽 구독 준비 ultra4 토픽을 받으면 callback 호출 무한 루프#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32def callback(msg): print(msg.data)rospy.init_node(&#39;ultrasonic_sub&#39;)sub = rospy.Subscriber(&#39;ultrasonic&#39;, Int32, callback)rospy.spin() ultra4.launch 토픽의 발행자 실행 토픽의 구독자 실행&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;ultrasonic&quot; type=&quot;ultrasonic_pub.py&quot; name=&quot;ultrasonic_pub&quot;/&amp;gt; &amp;lt;node pkg=&quot;ultrasonic&quot; type=&quot;ultrasonic_sub.py&quot; name=&quot;ultrasonic_sub&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt; 실행 연결 확인$ lsusb~~~~~~ HL-340 USB~~~ 펌웨어 프로그래밍$ sudo arduino 아두이노가 연결된 리눅스 포트 확인tools메뉴에서 Board/Processor Port를 체크해야 한다. board : Arduino Nano Processor : Atmega328P Port : /dev/ttyUSB0 or /dev/ttyACM0 아두이노 펌웨어 소스코드초음파센서가 보내는 신호로부터 거리정보를 추출하는 소스코드다.int trig[4] = {2,4,6,8}; // 센서를 꼽았던 포트 번호int echo[4] = {3,5,7,9}; //int i=0;void setup() { Serial.begin(9600); for(i=0;i&amp;lt;4;i++) { pinMode(trig[i],OUTPUT); // TRIG 핀을 출력모드로 세팅 pinMode(echo[i], INPUT); // ECHO 핀을 입력모드로 세팅 }}void loop() { long dur[4]={0.0,}; long dist[4]={0.0,}; for(i=0; i&amp;lt;4; i++) { digitalWrite(trig[i],LOW); delayMicroseconds(2); digitalWrite(trig[i],HIGH); // trig 신호를 high 상태로 변경 delayMicroseconds(10); // 10마이크로초 대기 digitalWrite(trig[i],LOW); // Trig 신호를 low 상태로 변경 dur[i] = pulseIn(echo[i],HIGH); // Echo 펄스에서 왕복시간을 계산해서 저장 dist[i] = dur[i]*170/1000; // 시간을 이동거리로 환산해서 저장 if(dist[i]&amp;gt;=2000 || dist[i]&amp;lt;0) { // 거리가 200cm이상이거나 0보다 작으면 0으로 설정 dist[i]=0; } } Serial.print(dist[0]); //dist[0] 출력 Serial.print(&quot;mm &quot;); Serial.print(dist[1]); Serial.print(&quot;mm &quot;); Serial.print(dist[2]); Serial.print(&quot;mm &quot;); Serial.print(dist[3]); Serial.print(&quot;mm &quot;); delay(50);} 컴파일 &amp;amp; 업로드컴파일 및 업로드를 진행한다. 결과 확인시리얼 모니터를 이용해서 아두이노의 출력값을 확인할 수 있다. tools -&amp;gt; serial Monitor 실행$ roslaunch ultrasonic ultra4.launch" }, { "title": "[데브코스] 4주차 - ROS 3d modeling of Car based on urdf", "url": "/posts/urdf/", "categories": "Classlog, devcourse", "tags": "ros, urdf, devcourse", "date": "2022-03-10 01:47:00 +0900", "snippet": "URDF란?URDF (Unified Robot Description Format) 란 로봇의 3D 형상 및 외관, 관성 등 물리적 특성 등을 XML 언어로 정의하는 것을 말한다. URDF로 정의된 로봇 모델은 RVIZ에서 3차원으로 보거나 Gazebo에서 물리 시뮬레이션이 가능하다.URDF 좌표계 및 단위좌표계 위치나 크기를 표현하기 위해 데카르트 좌표계 x,y,z를 사용한다. 회전을 표현하기 위해 오일러각도 roll, pitch, yaw를 사용한다.단위 길이 : meter 각도 : radian 1라디안 = 180도/π = 57.3도 π라디안 = 1라디안 x π = 180도/π x π = 180도 질량 : kg 속도 : m/s (병진/직진) 각속도 : radian/s (회전)URDF 형상 표현형상 표현 cylinder 원통 box 상자 sphere 공색상 표현 RGB 3원색과 투과율 A(0~1)의 숫자로 정의한다.URDF 기구 표현기구 표현 base : 고정 부분 (땅에 붙은 곳) link : 관절에 연결되는 로봇 팔의 부분 joint : 링크를 연결하는 부위로 보통 모터의 회전을 통해 움직임을 만듦 joint의 동작 표현 fixed : 고정 revolute : 작동 범위 제한 continuous : 연속 회전 예시robot 태그 안에 link가 3개, joint가 2개로 이루어져 있고, 맨 밑에 있는 link를 base_link라 하는 것 처럼 각각의 이름을 지정해줄 수 있다. type을 통해 동작을 표현해줄 수 있다.연결되는 2개의 link에서 각각을 parent, child link라고 할 수 있다.&amp;lt;visual&amp;gt; : 시각화를 위해 형상과 위치를 정의 &amp;lt;geometry&amp;gt; : 형상 및 크기 정의 (원통, 상자, 공) &amp;lt;origin&amp;gt; : 고정축을 기준으로 얼마나 회전되어 있는지에 대한 link 형상의 roll, pitch, yaw 값을 라디안으로 나타내고 x,y,z 좌표 위치를 미터 단위로 지정 &amp;lt;material&amp;gt; : 형상의 컬러값을 지정&amp;lt;parent&amp;gt; : child frame과 연결하기 위해 parent frame의 이름을 지정&amp;lt;child&amp;gt; : parent frame과 연결하기 위해 child frame의 이름을 지정&amp;lt;origin&amp;gt; : 고정축을 기준으로 얼마나 회전되어 있는지에 대한 형상의 roll,pitch,yaw 값을 라디안으로 나타내고, x,y,z 좌표 위치를 미터 단위로 지정, joint는 parent의 origin을, child는 joint의 origin을 고정축으로 정한다.&amp;lt;limit&amp;gt; : joint의 운동범위에 대한 제한값을 지정한다. 이를 지정하지 않으면 제한없이 360도 돌아가는 것을 말한다. lower: revolute type의 joint에서 최저 각을 라디안으로 지정 upper: revolute type의 joint에서 최대 각을 라디안으로 지정 effort: N 단위로 힘의 최대값을 지정 velocity: radian/s 단위로 최대속도를 지정RVIZ에서 회전막대 모델 시각화하기 URDF 파일(xml 형식)로 회전막대 모델링 ROS 패키지 생성 : 패키지 생성, 패키지 이름은 ex_urdf /urdf 폴더 생성 : .urdf파일 만들기 /launch 폴더 생성 : .launch파일 만들기 RVIZ 실행패키지 생성xycar_ws ⊢ build ⊢ devel ∟ src ∟ ex_urdf ⊢ launch ∟ view_pan_tilt_urdf.launch ∟ urdf ∟ pan_tilt.urdf ex_urdf 패키지 만들기 ~/xycar_ws/src$ catkin_create_pkg ex_urdf roscpp tf geometry_msgs urdf rviz xacro 서브 폴더 생성 /urdf 폴더 생성 pan_tilt.urdf 파일 작성 /launch 폴더 생성 view_pan_tilt_urdf.launch 파일 작성 pan_tilt.urdf base link&amp;lt;?xml version=&quot;1.-&quot;?&amp;gt;&amp;lt;robot name=&quot;ex_urdf_pan_tilt&quot;&amp;gt; &amp;lt;!-- base link --&amp;gt; &amp;lt;link name=&quot;base_link&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;!-- 원판 모양의 base link, 높이 1cm, 반지름 20cm 의 원통--&amp;gt; &amp;lt;cylinder length=&quot;0.01&quot; radius=&quot;0.2&quot;/&amp;gt; &amp;lt;!-- 시작 위치는 0센치 --&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;material name=&quot;yellow&quot;&amp;gt; &amp;lt;color rgba=&quot;1 1 0 1&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;collision&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.03&quot; radius=&quot;0.2&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;/collision&amp;gt; &amp;lt;inertial&amp;gt; &amp;lt;mass value=&quot;1&quot;/&amp;gt; &amp;lt;inertia ixx=&quot;1.0&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;1.0&quot; iyz=&quot;0.0&quot; izz=&quot;1.0&quot;/&amp;gt; &amp;lt;/inertial&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;!-- pan_joint --&amp;gt; &amp;lt;joint name=&quot;pan_joint&quot; type=&quot;revolute&quot;&amp;gt; &amp;lt;parent link=&quot;base_link&quot;/&amp;gt; &amp;lt;child link=&quot;pan_link&quot;/&amp;gt; &amp;lt;origin xyz=&quot;0 0 0.1&quot;/&amp;gt; &amp;lt;!-- joint위치는 base_link 0에서 10을 더해 10cm --&amp;gt; &amp;lt;axis xyz=&quot;0 0 1&quot;/&amp;gt; &amp;lt;limit effort=&quot;300&quot; velocity=&quot;0.1&quot; lower=&quot;-3.14&quot; upper=&quot;3.14&quot;/&amp;gt; &amp;lt;dynamics damping=&quot;50&quot; friction=&quot;1&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- pan link --&amp;gt; &amp;lt;link name=&quot;pan_link&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;!-- 원판 모양의 pan link, 높이 40cm, 반지름 4cm 의 원통--&amp;gt; &amp;lt;cylinder length=&quot;0.4&quot; radius=&quot;0.04&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0.09&quot;/&amp;gt; &amp;lt;!-- 실린더 중앙점의 위치는 pan_joint 10 + 9 = 19cm --&amp;gt; &amp;lt;material name=&quot;red&quot;&amp;gt; &amp;lt;color rgba=&quot;0 0 1 1&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;collision&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.4&quot; radius=&quot;0.06&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0.09&quot;/&amp;gt; &amp;lt;/collision&amp;gt; &amp;lt;inertial&amp;gt; &amp;lt;mass value=&quot;1&quot;/&amp;gt; &amp;lt;inertia ixx=&quot;1.0&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;1.0&quot; iyz=&quot;0.0&quot; izz=&quot;1.0&quot;/&amp;gt; &amp;lt;/inertial&amp;gt; &amp;lt;/link&amp;gt; &amp;lt;!-- tilt joint --&amp;gt; &amp;lt;joint name=&quot;tilt_joint&quot; type=&quot;revolute&quot;&amp;gt; &amp;lt;parent link=&quot;pan_link&quot;/&amp;gt; &amp;lt;child link=&quot;tilt_link&quot;/&amp;gt; &amp;lt;origin xyz=&quot;0 0 0.2&quot;/&amp;gt; &amp;lt;!-- joint위치는 pan_joint 10 + 20 = 30cm --&amp;gt; &amp;lt;axis xyz=&quot;0 1 0&quot;/&amp;gt; &amp;lt;limit effort=&quot;300&quot; velocity=&quot;0.1&quot; lower=&quot;-4.71239&quot; upper=&quot;-1.570796&quot;/&amp;gt; &amp;lt;!-- -270도 ~ -90도 == 90 ~ 90 --&amp;gt; &amp;lt;dynamics damping=&quot;50&quot; friction=&quot;1&quot;/&amp;gt; &amp;lt;/joint&amp;gt; &amp;lt;!-- tilt link --&amp;gt; &amp;lt;link name=&quot;tilt_link&quot;&amp;gt; &amp;lt;visual&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;!-- 원판 모양의 tilt link, 높이 40cm, 반지름 4cm 의 원통--&amp;gt; &amp;lt;cylinder length=&quot;0.4&quot; radius=&quot;0.04&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;origin rpy=&quot;0 1.570796 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;!-- 실린더 중앙점 위치는 tilt_joint 30 + 0 = 30cm, 가로로 되어 있기 때문에 0 --&amp;gt; &amp;lt;material name=&quot;green&quot;&amp;gt; &amp;lt;color rgba=&quot;1 0 0 1&quot;/&amp;gt; &amp;lt;/material&amp;gt; &amp;lt;/visual&amp;gt; &amp;lt;collision&amp;gt; &amp;lt;geometry&amp;gt; &amp;lt;cylinder length=&quot;0.4&quot; radius=&quot;0.06&quot;/&amp;gt; &amp;lt;/geometry&amp;gt; &amp;lt;origin rpy=&quot;0 1.570796 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt; &amp;lt;/collision&amp;gt; &amp;lt;inertial&amp;gt; &amp;lt;mass value=&quot;1&quot;/&amp;gt; &amp;lt;inertia ixx=&quot;1.0&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;1.0&quot; iyz=&quot;0.0&quot; izz=&quot;1.0&quot;/&amp;gt; &amp;lt;/inertial&amp;gt; &amp;lt;/link&amp;gt; pan_tilt_urdf.launch&amp;lt;launch&amp;gt; &amp;lt;arg name=&quot;model&quot; /&amp;gt; &amp;lt;param name=&quot;robot_description&quot; textfile=&quot;$(find ex_urdf)/urdf/pan_tilt.urdf&quot; /&amp;gt; &amp;lt;!-- setting gui parameter to true for display joint slider --&amp;gt; &amp;lt;param name=&quot;use_gui&quot; value=&quot;true&quot; /&amp;gt; &amp;lt;!-- starting joint state publisher node which will publish the joint values --&amp;gt; &amp;lt;node name=&quot;joint_state_publisher&quot; pkg=&quot;joint_state_publisher&quot; type=&quot;joint_state_publisher&quot; /&amp;gt; &amp;lt;!-- starting robot state publish which will publish tf --&amp;gt; &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;state_publisher&quot; /&amp;gt; &amp;lt;!-- Launch visualization in rviz --&amp;gt; &amp;lt;node name=&quot;rviz&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; args=&quot;-d $(find ex_urdf)/urdf.rviz&quot; required=&quot;True&quot; /&amp;gt; &amp;lt;/launch&amp;gt;실행$ cm$ cd src/ex_urdf/urdf작성된 urdf 파일에 문법적인 오류가 있는지 확인해주는 코드$ check_urdf pan_tilt.urdflink와 joint 관계도를 pdf 파일로 만들어줌$ urdf_to_graphiz pan_tilt.urdf패키지 설치$ sudo apt install ros-kinetic-joint-state-publisher-gui이 패키지는 urdf 파일로 모델링된 로봇의 joint부분을 움직이기 위한 윈도우 기반의 GUI 도구이다.실행$ roslaunch ex_urdf view_pan_tilt_urdf.launch그러나 실행해도 아무 표시가 되지 않는다. 이유는 rviz설정이 아직 안된 것이다. add를 눌러 robotmodel을 눌러 추가하고, fixed frame에 map이 아닌 base_link를 넣어야 한다.모델이 나오면 마우스를 통해 회전, 이동할 수 있다. 실행하고나면 그리드화면 이외에 위의 publisher_gui 창이 하나 더 나올 것이다. 이를 사용하여 joint를 움직일 수 있다.또, 중요한 것은 종료할 때 save를 해야 원래 불러오려던 rviz파일안에 저장이 되는 것이다. 이를 저장하면 ex_urdf 폴더에 urdf.rviz 파일이 생성될 것이다.참고 자료 참고 블로그 참고 블로그" }, { "title": "[데브코스] 4주차 - ROS Sensor devices ", "url": "/posts/sensor2/", "categories": "Classlog, devcourse", "tags": "sensor, ros, devcourse", "date": "2022-03-09 16:00:00 +0900", "snippet": " 센서장치 카메라 센서 IMU 센서 라이다 센서 초음파 센서 Depth 카메라 센서 카메라 센서1080P USB 카메라 170도 어안렌즈 -&amp;gt; 화면이 휘어지지만 openCV 툴을 사용하여 펼 수 있다. CMOS OV2710 Sensor (CCD 카메라가 더 좋긴 하나 비싸고, 전기를 많이 먹는다) 120fps (640x480), 30fps (1920x1080) (일반 영화관같은 곳이 24~30fps) 빛의 3원색인 RGB를 통해 색을 표현 그림/동영상 데이터는 각 1byte(8bit)로 표현 3개 bytes를 통해 색을 표현 가능 full-HD(1920x1080) 화면을 가득 채우는 그림의 크기는 1920x1080 x 3bytes = 6Mytes 흔히 색이 있는 사진을 볼 때, 데이터적으로 보면 흑백으로 표현된 사진이 R/G/B 색을 각가 입혀 이를 다 융합한 것이다. 해상도, PPI, 화소수 해상도 이름 : 640x480=SD, 1280x720=HD, 1920x1080=FHD, 2560x1640=QHD, 3840x2160=UHD PPI: 같은 크기안에 몇개의 화소가 들어있는지 화소: 해상도의 가로x세로를 곱한 것으로 rgb 한 묶음의 갯수를 말한다. 카메라 센서의 기능 영상 촬영 연속된 사진을 찍어 이미지파일로 전송하는 것 최대 해상도 : 1920x1080 추가 정보 640x480 사진을 초당 30장이라 가정할 때 1. 파일 크기 : 화소수 (640x480 = 30만 화소) x 3bytes = 사진 1장의 파일 크기는 90만 = 0.9Mbytes = 7.2Mbits 2. 전송속도: 초당 30장 사진찍으므로 1초 동안의 전송 데이터는 0.9M x 30장 = 27Mbytes 3. 전송 속도는 216Mbps, USB 2.0의 최대 속도는 480Mbps, 실제는 그 절반인 240Mbps가 된다. 따라서 전송에 과부하는 없다. 하지만 실제로는 사진을 압축해서 보내기 때문에 더 높은 해상도와 더 많은 프레임 전송이 가능하다. &amp;gt; 그러나 요즘에는 압축 시에는 압축을 또 풀어야 하기 때문에 압축을 하지 않는 추세라고 한다. 테슬라와 같은 회사가 이를 연구하고 있다. 노출도 (exposure) 노출도: 주변 밝기에 따른 이미지 밝기 변화 환경에 따라 노출도를 조절하여 사진을 적절하게 찍을 수 있다. 자율주행의 경우 사람이 적절하다고 하는 것보다 더 낮게, 어둡게 하는 것이 더 잘 인식한다. 자동초점조정 (auto-focus) 자율주행 차량에서는 이 자동초점을 하는 시간이 걸리면 위험하므로 안쓰는 것이 좋다. 광폭으로 사용한다. IMU 센서 (lnertial Measurement Unit sensor)관성 측정 장치: 가속도계와 회전속도계, 자력계의 조합을 사용하여 어떤 물체(질량)에 가해지는 힘, 회전 각속도 등을 측정하는 장치다.센서 종류 6축 IMU 센서 가속도 센서 (Accelerometer) : 직선방향 가속도 감지, 중력가속도 감지 MEMS(Micro-Electro-Mechanical Systems) 기술로 만들어지는 센서 반도체 칩 안에 입체적인 구조물을 만듦 이 구조물이 외부의 힘에 따라 움직이는 것을 전기적 신호로 바꾸어 출력 X,Y,Z축 방향의 직선 움직임과 기울어짐의 자세 파악 자이로 센서 (Guroscope) : 회전속도인 각속도를 감지 MEMS 기술 기반으로 만들어짐 X,Y,Z축을 기준으로 한 회전 움직임 감지 x축을 기준으로 회전 -&amp;gt; rolling 이동방향에 대해 평행한 축 주위의 회전각을 나타낸다. 차량이 코너링할 때 옆으로 기울어지는 것을 감지 y축 기준 회전 -&amp;gt; pitching 이동 방향에 대해 수직을 이루는 축 주위의 회전각을 나타낸다. 차량이 언덕을 올라가거나 내려가오는 것을 감지할 수 있다. z축 기준 회전 -&amp;gt; yawing 이동방향에 대해 수직을 이루는 축 주위의 회전각을 나타낸다. 차량의 좌회전/우회전 회전하는 것을 알 수 있다. 9축 IMU 센서 가속도 센서 (Accelerometer) 자이로 센서 (Guroscope) 지자계 센서 (Magnetometer) : N,S극 방향 감지, 동서남북 방위각 감지 3축 나침반과 같이 방위를 알려주는 디지털 나침반 기능을 한다. 라이다 센서라이다 센서 종류1채널 2D 라이다 (0.9 degree, 4000 sampling,0.15m~18m) 1채널 : 광선이 1개 나가는 것, 1채널이라는 것은 높이가 1개로만 된다는 것, 위나 아래에 대해서는 인식이 안될 수 있다. 0.9 degree : 0.9도씩 광선을 쏜다. 0.15m~18m 범위 : 광선의 거리가 0.15cm부터 18m까지 인식이 된다.1채널 2D 라이다 (0.504 degree, 5000 sampling, 0.12m~12m) 1채널 0.504 degree : 0.504도씩 광선을 쏜다. 0.12~18m : 5000Hz : 초당 5000개의 데이터를 받는다. motor frequency : 1초에 몇 바퀴 도는지 resolution : 회전속도에 따라 각도를 정하고, 이 두 개를 곱해서 1초당 얼마나 데이터를 나오는지 알 수 있다.멀티 채널 라이다 위아래로 스캔 가능 사람, 차량, 벽, 기둥 인식 가능라이다와 레이다의 차이 레이다(RADAR) 전파 신호의 반사파를 이용 속도 감지 가능 구름 많은 날씨 환경 및 야간에도 손쉽게 작동 LIDAR보다 더 긴 작동거리를 제공 라이다(LIDAR) 레이저 신호의 반사파를 이용 짧은 주파수로 작은 물체도 감지 가능 정확한 3D 단색 이미지 구성 라이다로 장애물을 감지할 수 있다. x,y 좌표로 변환하여 장애물 위치를 판단하고 진행방향에 장애물이 있으면 정지한다. 360도를 1도씩 한다고 하면 360개의 거리값을 array데이터로 만들어 사용하는 것이다. 라이다는 극좌표로 값을 받기 때문에 이를 직교좌표계로 바꿔야 한다. 초음파 센서초음파 (Ultrasonic Wave) 인간의 귀가 들을 수 있는 주파수 대역을 가청이라 하는데, 이 가청 주파수 대역보다 높은 진동수로 발생하는 파동을 말한다. 가청 주파수는 사람마다 다르지만, 대체로 20Hz ~ 20kHz 초음파 센서는 초음파를 이용하여 센서로부터 사물까지의 직선거리를 측정한다. 초음파센서는 차량에서 전방은 라이다 센서를 사용하기 때문에 전방이 아닌 측/후방에 장착된다.초음파 센서 동작 시그널 Vcc : 센서 부품에 전력을 공급 (DC 5V) (전기의 +극과 같음) GND : 회로의 그라운드에 연결 (전기의 -극과 같음) Trig(신호값) : 센서를 동작시키기 위한 트리거 시그널 (입력) Echo(신호값) : 거리 츩정 결과를 전달하기 위한 시그널 (출력) 동작 순서 송신부에서 초음파를 발사한다. 초음파가 물체에서 반사된다. 반사된 초음파가 수신부에서 감지된다. 송신과 수신 사이의 시간 간격을 기준으로 물체까지의 거리를 계산한다.구체적인 동작 시나리오 (시작 -&amp;gt; 초음파 발사 및 수신 -&amp;gt; 시간차 출력) TRIG 핀에 10us(1x10^-6)동안 5V의 전기 신호를 보낸다. 초음파센서가 40kHz 초음파를 여덟 개 만들어서 물체로 보낸다. 물체에 반사된 초음파가 되돌아와서 초음파센서에 수신된다. 센서는 송신과 수신의 시간차에 비례하는 길이를 echo핀으로 출력한다.거리 측정 소리의 속도는 초당 340m -&amp;gt; 1cm 이동하는데 약 29us 소요 송신과 수신의 시간차 = 초음파가 왕복이므로 2를 나누어야 한다. 물체까지의 거리 = [(송신과 수신의 시간차 / 2) / 29us ]cm초음파 센서의 문제점 쏜 신호가 흠수되면 수신이 불가능해진다. 비스듬하게 쏴서 다른 곳으로 튕겨 나가면 수신이 블가능하다. 물체가 너무 작을 때 물체가 너무 부드러울 때DEPTH 카메라depth 카메라는 stereo 카메라고도 한다.구성 : 왼쪽 적외선 카메라, 적외선 방출기, 오른쪽 적외선 카메라, RGB컬러 카메라적외선은 흑백으로 출력되기 때문에 RGB컬러를 입힐 카메라가 더 있다.거리 측정적외선 카메라를 2개 사용해서 깊이 즉 거리를 측정한다.적외선을 통해 얻은 depth 정보와 RGB 정보와 융합해서 3차원 공간에 칼라 이미지를 표시할 수 있다." }, { "title": "[데브코스] 4주차 - ROS auto driving using Sensor ", "url": "/posts/rviz/", "categories": "Classlog, devcourse", "tags": "rviz, ros, devcourse", "date": "2022-03-08 19:00:00 +0900", "snippet": "RVIZ 기반 오도메트리 활용odometry오도미터(odometry): 차량이나 로봇이 주행하며 이동한 거리를 측정하는 기기오도메트리(odomerty): 오도미터 등의 기기의 측정값으로 움직이는 사물의 위치를 측정하는 방법자동차의 이동거리는 바퀴의 회전수로 계산한다.핸들을 꺽을 때 앞 바퀴 두개가 서로 다르다. 이유는 원을 그렸다고 생각했을 때 안쪽 바퀴는 바깥 바퀴보다 꺽여야 할 각도가 더 많이 꺽여야 한다. 계산하는 방법은 뒷바퀴를 기준으로 연장선을 긋고, 앞바퀴 두개의 각각 연장선이 동심원에서 만나야 한다.이를 간단하게 구하는 방법을 정의한 것이 Ackermann Steering이다.스티어링을 하나 만들어서 그 각도는 뒷바퀴의 중심에만 가도록 만들게 되면 두 개의 바퀴는 항상 동심원을 가진다는 것이다. 그러나 두개의 각도를 각각 구하는 것은 복잡하기 때문에 두 바퀴의 중간점에서의 각도로 정의한다.자동차 위치 정보 현재 위치 : (x,y) 좌표 + theta (x,y): 현재 뒷바퀴의 위치 theta: 직교좌표계의 x축과 차량의 수직선이 이루는 각도 이동 속도 : 선속도 v + 각속도 w 조향각 델타 델타: 앞바퀴와 도착하고자 하는 뒷바퀴의 위치와, 현재 차량의 수직선이 이루는 각도 odometry 토픽/odom이라는 토픽을 발행하는 예제 코드가 있다.파이썬 코드 : ros_odometry_publisher_example.pyhttps://gist.github.com/atotto/f2754f75bedb6ea56e3e0264ec405dcf파일이름은 각자 변경가능하다.아래 파일을 ex_urdf 파일에 추가한다.#!/usr/bin/env python# name change ros_odometry_publisher_example.py -&amp;gt; odom_publisher_ex.pyimport mathfrom math import sin, cos, piimport rospyimport tffrom nav_msgs.msg import Odometryfrom geometry_msgs.msg import Point, Pose, Quaternion, Twist, Vector3rospy.init_node(&#39;odometry_publisher&#39;) # odomety_publisher 노드 생성odom_pub = rospy.Publisher(&quot;odom&quot;, Odometry, queue_size=50) # odom 토픽 발행 준비odom_broadcaster = tf.TransformBroadcaster()# 초기 위치x = 0.0y = 0.0th = 0.0# 초기 속도 x축 속도는 10cm/s, y축 속도는 -10cm/s, 주행 방향은 0.1라디안(5.7도)vx = 0.1vy = -0.1vth = 0.1# 시간 정보 계산용 변수current_time = rospy.Time.now() last_time = rospy.Time.now()r = rospy.Rate(1.0) # 1초에 한번씩 돌기while not rospy.is_shutdown(): current_time = rospy.Time.now() # compute odometry in a typical way given the velocities of the robot # 그림1 참고 dt = (current_time - last_time).to_sec() # 델타 시간 delta_x = (vx * cos(th) - vy * sin(th)) * dt # 속도 x 시간 = 거리 delta_y = (vx * sin(th) + vy * cos(th)) * dt delta_th = vth * dt x += delta_x y += delta_y th += delta_th # since all odometry is 6DOF we&#39;ll need a quaternion created from yaw # 오일러 좌표계 회전을 쿼터니언 값으로 계산함 odom_quat = tf.transformations.quaternion_from_euler(0, 0, th) # 아래 좌표계 회전 설명 참고 # first, we&#39;ll publish the transform over tf odom_broadcaster.sendTransform( (x, y, 0.), # 위치 정보애 대한 발행 준비 odom_quat, current_time, &quot;base_link&quot;, # odom과 base_link를 연결하는 코드 &quot;odom&quot; ) # next, we&#39;ll publish the odometry message over ROS odom = Odometry() odom.header.stamp = current_time odom.header.frame_id = &quot;odom&quot; # set the position odom.pose.pose = Pose(Point(x, y, 0.), Quaternion(*odom_quat)) # set the velocity odom.child_frame_id = &quot;base_link&quot; odom.twist.twist = Twist(Vector3(vx, vy, 0), Vector3(0, 0, vth)) # publish the message odom_pub.publish(odom) last_time = current_time r.sleep()이 코드가 하는 일 odometry_publisher노드를 생성 거기서 /odom 토픽을 발행 1초에 1번씩 발행이동 속도이동 속도는 Vx, Vy의 두 벡터의 합성이다.좌표계 회전좌표축이 회전하는 것이 3가지가 있다. x축 기준으로 회전하는 것을 roll, y축 기준으로 회전하는 것을 pitch, z축 기준으로 회전하는 것을 yaw이라 한다.이와 같이 나타내는 것을 오일러 방식이라 한다. 그러나 여기서난 쿼터니언 방식을 사용하는데, 이유는 계산하기 쉽기 때문이다.실행$ roscore$ rosrun ex_urdf odom_publisher_ex.py$ rostopic list$ rostopic info odom$ rqt_graph odomety 토픽 : /odom odom 메시지 타입 : nav_msgs/Odometry$ rosmsg show nav_msgs/Odometry$ rostopic echo odomposition: 뒷바퀴 기준 점Quaternion orientation: 롤 요키치를 가상의 좌표계로 나타낸 값vector linear: 선속도vector angular: 각속도RVIZ 가상공간에서 물체 이동시키기기존의 odom_publisher_ex.py가 있는 패키지를 이용한다. launch 파일도 생성한다.&amp;lt;!-- odom_pub.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;arg name=&quot;model&quot; /&amp;gt; ... &amp;lt;!-- add python file --&amp;gt; &amp;lt;node name=&quot;odom_publisher&quot; pkg=&quot;ex_urdf&quot; type=&quot;odom_publisher_ex.py&quot; /&amp;gt;&amp;lt;/launch&amp;gt;$ roslaunch ex_urdf odom_pub.launch/tf는 어떤 물체의 자세나 이것저것 계산해서 rviz에 쏘는 시각화 노드이다.그래서 직접 RVIZ에서 주행을 시키기 위한 과정으로는 8자 주행 프로그램이 /motor 토픽인 모터 제어 메시지를 보낸다. 변환 프로그램이 받아서 변환한 후 /joint_states 토픽으로 만들어 발행 토픽을 오도메트리 프로그램이 받아서 변환해서 /odom 토픽으로 만들어 발행" }, { "title": "[데브코스] 4주차 - ROS Develop Pygame ", "url": "/posts/pygame/", "categories": "Classlog, devcourse", "tags": "ros, pygame, devcourse", "date": "2022-03-08 18:29:00 +0900", "snippet": "개발 환경 구축pygame은 python을 통해 게임을 만드는 도구이다. 이를 설치해서 차량 시뮬레이터를 제작할 것이다.차량의 움직임을 제어하려면 구동 메커니즘을 이해해야 한다. 이를 위해 ackermann steering을 이해해야 할 것이다. 안쪽 바퀴는 많이, 바깥 바퀴는 덜 꺽일 것이다.키보드로 차량을 조종할 수도 있다. 차량은 진짜 차처럼 ackermann steering 방식으로 회전하도록 해야 할 것이고, 진짜 차량의 엑셀/브레이크/핸들처럼 동작시켜야 한다.이를 window + visual studio를 설치하겠다.라이브러리 설치라이브러리 python==3.7.0 pygame==1.9.6 pip install pygame==1.9.6 pip install pillow pip install pillow numpy numpy사이트로 가서 파이썬 3.7로 다운 python -m pip install .\\numpy-1.20.2-cp37m-win_amd64.whl visual studio 설치자신에 맞는 버전을 다운 받으면 된다.커뮤니티버전으로 설치하고, c++를 사용한 데스크톱 개발, MSVC v141 - VS 2017 C++ ~ 클릭Pygame 예제간단한 집 그리기https://kkamikoon.tistory.com/129 pygame 선언 (import) pygame 초기화 (pygame.init()) pygame에서 사용할 전역 변수 선언 size : x,y 크기 screen : pygame.display.set_mode(size) clock : pygame.time.Clock() pygame 메인 루프(while) pygame event 설정 pygame 화면 설정 사용자 행동 # pygame_draw_house.py&#39;&#39;&#39; 1. pygame 선언 &#39;&#39;&#39;# Import a library of functions called &#39;pygame&#39; as pgimport pygame as pg&#39;&#39;&#39; 2. pygame 초기화 &#39;&#39;&#39;# Initialize the game enginepygame.init() &#39;&#39;&#39; 3. pygame 전역 번수 선언 &#39;&#39;&#39;# Define the colors we will use in RGB formatBLACK= ( 0, 0, 0)WHITE= (255,255,255)BLUE = ( 0, 0,255)GREEN= ( 0,255, 0)RED = (255, 0, 0) # Set the height and width of the screensize = [400,300]screen= pygame.display.set_mode(size) pygame.display.set_caption(&quot;Game Title&quot;) #Loop until the user clicks the close button.done= False # while 루프를 빠져나오기 위한 플래그 변수clock= pygame.time.Clock() # fps 초당 화면 갱신 회수 지정 용도&#39;&#39;&#39; 4. pygame 메인 루프 &#39;&#39;&#39;while not done: # This limits the while loop to a max of 10 times per second. # Leave this out and we will use all CPU we can. clock.tick(10) # 초당 10번 루프를 돌도록 즉, 초당 10번 갱신, fps가 10 # Main Event Loop for eventin pygame.event.get():# User did something if event.type == pygame.QUIT:# If user clicked close done=True # Flag that we are done so we exit this loop # All drawing code happens after the for loop and but # inside the main while done==False loop. # Clear the screen and set the screen background screen.fill(WHITE) &#39;&#39;&#39; Your Work..... &#39;&#39;&#39; pygame.draw.polygon(screen, GREEN, [[30,150], [125,100], [220,150]],5) # (window, color, coordinates, thickness) pygame.draw.polygon(screen, GREEN, [[30,150], [125,100], [220,150]],0) pygame.draw.lines(screen, RED,False, [[50,150], [50,250], [200,250], [200,150]],5) pygame.draw.rect(screen, BLACK, [75,175,75,50],5) pygame.draw.rect(screen, BLUE, [75,175,75,50],0) # 0 이면 안을 채운다. pygame.draw.line(screen, BLACK, [112,175], [112,225],5) pygame.draw.line(screen, BLACK, [75,200], [150,200],5) # Go ahead and update the screen with what we&#39;ve drawn. # This MUST happen after all the other drawing commands. pygame.display.flip()# Be IDLE friendlypygame.quit()코드 실행python pygame_draw_house.pyPygame 예제 2키보드 입력을 사용하는 예제import pygmaepygame.init()size = [400,300]screen = pygame.display.set_mode(size)pygaem.display.set_caption(&quot;Game Titie&quot;)done = Falseclock = pygame.time.Clock()player_location= [200, 150] # 초기값speed = 10 #키를 눌렀을 때 몇 픽셀 가는지while not done: clock.tick(30) # fps for event in pygame.event.get(): if event.type == pygame.QUIT: done = True # QUIT 이면 루프를 빠져나감 pressed = pygame.key.get_pressed() # 사용자가 무엇을 눌렀나 if pressed[pygame.K_UP]: player_location[1] -= speed elif pressed[pygame.K_DOWN]: player_location[1] += speed if pressed[pygame.K_RIGHT]: player_location[0] += speed elif pressed[pygame.K_LEFT]: player_location[0] -= speed screen.fill((255,255,255)) # 흰색 pygame.draw.circle(screen, (0,0,255), player_location, 40) # RGB, 파란색 pygame.display.filp()pygame.quit()차랑주행 시뮬레이터 설계시뮬레이터 UI스크린에 차량을 먼저 표기하고 키보드로 움직이도록 하고자 한다.처음 셋팅 저장된 차량 이미지를 표시 처음 위치 셋팅 차량 바퀴는 x 스크린 크기 1280x720 바탕은 검정행동 감지 위방향 -&amp;gt; 전진할 때는 가속, 후진할 때는 브레이크 아래방향 -&amp;gt; 전진할 때는 브레이크, 후진할 때는 가속 오른방향 -&amp;gt; 바퀴를 오른쪽으로 꺽음, 최대 꺽임 각도 30도 왼방향 -&amp;gt; 바퀴를 왼쪽으로 꺽음, 최대 꺽임 각도는 -30도 스페이스바는 방향 상관없이 브레이크좌우 핸들링 좌우 키를 누르고 있으면 바퀴가 조금씩 좌/우로 돌아감 키에서 손을 떼면 바퀴가 즉시 정면으로 되돌아와 정렬된다.전진 위쪽 키를 누르고 있으면 엑셀을 계속 밟는 것처럼 속도가 점점 빨라진다. 키에서 손을 떼면 엑셀에서 발을 뗀 것처럼 속도가 점차 줄어서 멈춘다 아래 방향을 누르면 브레이크가 걸린 것처럼 속도가 빨리 줄어서 멈춘다.(멈추고도 계속 누르면 후진 가속, 정차시키려면 스페이스바를 누른다)후진 전과 동 키만 반대HLD(High Level Design) 설계 Car 클래스 정의 (위치/자세 정보) pygame 초기화 윈도우 타이틀 지정 / 크기 설정 전역 변수 설정 / 클래스 객체 생성 while not exit_flags: for 루프(pygame.QUIT 이벤트 처리) 키 입력값 읽어들여서 if/elif/else 블록에서 처리 7-1. if pressed[pygame.K_UP] : if # 선속도가 음수면 , else # 선속도가 양수면 차량의 새로운 위치/자세 계산하고 그림 새로 그리기 화면 업데이트LLD(Low Level Design) 설계class car:car클래스가 가지고 있어야 할 값들 self.x/y : 현재 x,y좌표 self.yaw : 진행방향 self.brake_deceleration : 브레이크로 인한 감속 가속도 self.free_deceleration : 정지마찰력으로 인한 감속 가속도 self.linear_accelation : 선가속도 self.linear_velocity : 선속도 self.steering_angle : 조향각 self.wheel_base : 차량의 휠베이스 길이 self.car_img_x : 차량 이미지를 둘러싼 사각형의 좌상단점의 x좌표 # 비스듬히 있을 때는 차량에 딱 맞게 비스듬히가 아닌 x,y 좌표와 평행한 큰 사각형으로 정해야 한다. self.car_img_y : 차량 이미지를 둘러싼 사각형의 좌상단점의 y좌표 self.car_x_ori : 차량 이미지 4개 꼭지점의 x좌표들 self.car_y_ori : 차량 이미지 4개 꼭지점의 y좌표들 def update(self, dt): 선속도 계산 with 선가속도 , 시간 각속도 계산 with 선속도 , 휠베이스, 조향각 차량진행방향 계산 with 각속도 , 시간 이동거리 계산 with 선속도 , 시간 새로운 차량위치 계산 with 이동거리 , 차량진행방향 차량 이미지 4개 꼭지점 새로운 위치 계산 with 차량진행방향 회전 변환선속도 = 이동거리 / 이동시간각속도 = 이동각도 / 이동시간선속도 = 회전반지름 x 각속도회전 반경 r = 휠베이스 L / tanϴ이 때, 휠베이스는 앞바퀴축과 뒷바퀴축간의 거리, ϴ는 조향각이다. 휩베이스는 항상 정해져있고, ϴ는 키를 계속 누르고 있는 만큼 계속 일정하게 추가되면 된다.회전 차량을 그릴 때 회전 없이 이동시킨 후 회전시키는 것이 좋다.각도가 ϴ만큼 회전할 때의 좌표가 (x,y) -&amp;gt; (x’,y’) 로 변한다고 생각을 하면, 이를 X,Y 축으로 분할을 해서 생각을 해본다.이를 행렬로 나타내면 다음과 같다.x’ = x * cosϴ - y * sinϴy’ = x * sinϴ + y * cosϴ그림 작도이처럼 비스듬히 되면 x,y축과 평행한 사각형으로 생성해야 한다. 따라서, (x1”,y1”) 반드시 알아야 한다.#!/usr/bin/env pythonimport osimport pygameimport numpy as npfrom math import sin, radians, degrees, copysignclass car: # 생성자 함수 def __init__(self,x,y,yaw=0.0,max_steering=30, max_acceleration=1000.0): # initial point(x,y) self.x = x self.y = y # yaw value self.yaw = yaw # max acceleration self.max_acceleration = max_acceleration # max steering self.max_steering = max_steering # down acceleration due to brake (spacebar) self.brake_deceleration = 300 # down acceleration due to static friction force (no press key and only pulling excel self.free_deceleration = 50 # linear acceleration self.linear_acceleration = 10.0 # linear velocity self.linear_velocity = 0.0 # max velocity self.max_velocity = 1000 # steering self.steering_angle = 0.0 # wheel base (축거 : 앞바퀴축과 뒷바퀴축 사이의 거리) self.wheel_base = 84 # car image coordinate (widthxheight = 128x64, car.png) self.car_img_x = 0 self.car_img_y = 0 self.car_x_ori = [-64,-64,64,64] # 1 3 self.car_y_ori = [-32,32,-32,32] # 2 4 def update(self, dt): # fps를 주는 것, dt를 이용 # calculate linear velocity (linear velocity = linear acceleration x dt self.linear_velocity += (self.linear_acceleration * dt) # limit range of linear velocity between -100 and 100 self.linear_velocity = min(max(-self.max_velocity, self.linear_velocity), self.max_velocity) self.angular_velocity = 0.0 # steering is not zero if self.steering_angle != 0.0: # calculate the angular velocity, angular velocity = (linear velocity / radius) # = (linear velocity / wheel base) * tan(thata) self.angular_velocity = (self.linear_velocity / self.wheel_base) * np.tan(np.radians(self.steering_angle)) # calculate angular distance and add to the angle value (angular velocity x time = angular distance) # angle of movement = angular velocity * time of movement self.yaw += (np.degrees(self.angular_velocity) * dt) # distance = linear velocity * time of movement self.spatium = self.linear_velocity * dt # get the x,y coordinate using rotational transformation matrix # distance of movement = linear velocity * time of movement self.x += (self.spatium * np.cos(np.radians(-self.yaw))) self.y += (self.spatium * np.sin(np.radians(-self.yaw))) # storage space for coordinate of upper left rectangle car_x = [0,0,0,0] car_y = [0,0,0,0] for i in range(4): # x&#39; = x * cosϴ - y * sinϴ # y&#39; = x * sinϴ - y * cosϴ # 회전행렬의 각도는 반시계 방향이 +인데, 자동차는 우회전이 +, 좌회전이 -이기 때문에 -를 붙인 것이다. car_x[i] = self.car_x_ori[i] * np.cos(-radians(self.yaw)) - self.car_y_ori[i] * np.sin(-radians(self.yaw)) + self.x car_y[i] = self.car_x_ori[i] * np.sin(-radians(self.yaw)) - self.car_y_ori[i] * np.cos(-radians(self.yaw)) + self.y self.car_img_x = int(round(min(car_x))) # x1&quot; self.car_img_y = int(round(min(car_y))) # y1&quot;pygame.init()pygame.display.set_caption(&quot;Pygame Car Simulator #1&quot;)width, height = 1280, 720screen = pygame.display.set_mode((width, height))clock = pygame.time.Clock()current_dir = os.path.dirname(os.path.abspath(__file__))image_path = os.path.join(current_dir, &quot;car.png&quot;)car_image = pygame.image.load(image_path)image_scale = pygame.transform.scale(car_image, (128,64))pygame.image.save(image_scale, &quot;car.png&quot;)car = car(100,100) # 초기 위치가 (100,100)exit_flags = Falsewhile not exit_flags: clock.tick(60) # 60fps dt = clock.get_time() / 1000 # fps는 밀리초로 반환되는데 이걸 1000으로 나누어 초로 변환 for event in pygame.event.get(): if event.type == pygame.QUIT: exit_flags = True pressed = pygame.key.get_pressed() # if up_key press if pressed[pygame.K_UP]: # linear velocity is negative (now backward) if car.linear_velocity &amp;lt; 0: # apply the brake deceleration car.linear_acceleration = car.brake_deceleration # linear velocity is positive (now advance) else: # increase the linear acceleration 10 in the + direction car.linear_acceleration += 20 * dt # if down_key press elif pressed[pygame.K_DOWN]: # linear velocity is positive (now advance) if car.linear_velocity &amp;gt; 0: # apply the brake deceleration car.linear_acceleration = -car.brake_deceleration # linear velocity is negative (now backward) else: # increase the linear acceleration by 10 in the - direction car.linear_acceleration -= 20 * dt # if spacebar press elif pressed[pygame.K_SPACE]: # it is because if only give subtraction, we make backward acceleration increase # linear velocity is more than brake_acceleration * dt if abs(car.linear_velocity) &amp;gt; dt * car.brake_deceleration: # copysign(double x, double y) ==&amp;gt; use the y sign as the abs(x) sign # subtract the brake acceleration at linear acceleration to decrease the linear acceleration car.linear_acceleration = -copysign(car.brake_deceleration, car.linear_velocity) # linear velocity is less than brake_acceleration * dt else: # simply, subtract [(linear velocity/dt) = linear acceleration] to make linear velocity be zero car.linear_acceleration = -car.linear_velocity / dt # if another key press, apply the free friction force at car else: # linear velocity is more than free_acceleration * dt if abs(car.linear_velocity) &amp;gt; dt * car.free_deceleration: # apply the free acceleration, so stop car.linear_acceleration = -copysign(car.free_deceleration, car.linear_velocity) # linear velocity is less than free_acceleration * dt else: # linear velocity is more than (free deceleration x dt) if dt != 0: # subtract [(linear velocity / dt) = linear acceleration] from linear acceleration, so acceleration is zero car.linear_acceleration = -car.linear_velocity / dt # limit the value of range of linear acceleration between -1000.0 and 1000.0 car.linear_acceleration = max(min(car.linear_acceleration, car.max_acceleration),-car.max_acceleration) # if right_key press if pressed[pygame.K_RIGHT]: # turn right, subtract (30 x dt) car.steering_angle -= 30 * dt # if left_key press elif pressed[pygame.K_LEFT]: # turn right, add (30 x dt) car.steering_angle += 30 * dt # if anything is not pressed else: # set the steering angle to 0 car.steering_angle = 0 # limit the value of range of steering angle between -30 and 30 car.steering_angle = max(min(car.steering_angle, car.max_steering), -car. max_steering) # update the state of car every unit of time car.update(dt) screen.fill((0,0,0)) # rotate the car image rotated = pygame.transform.rotate(car_image, car.yaw) # draw the rotated car image at calculated point screen.blit(rotated, [car.car_img_x, car.car_img_y]) pygame.display.flip()pygame.quit()" }, { "title": "[데브코스] 4주차 - ROS RVIZ 8 RACE Driving", "url": "/posts/package/", "categories": "Classlog, devcourse", "tags": "xycar, ros, devcourse", "date": "2022-03-07 13:00:00 +0900", "snippet": "RVIZ 8자 주행시키기ros프로그래밍을 통해 차량 움직이기차량의 속도는 고정하고, 핸들만 조종하여 차량을 8자로 주행시켜보고자 한다.작업 공간 확보 my_motor 패키지 생성 $ catkin_create_pkg my_motor std_msgs rospy 서브 폴더 생성 /launch -&amp;gt; 8_drive.launch 소스코드 src 폴더에 8_drive.py 만들기 8_drive.py는 조향각과 속도 값을 토익에 담아 발행해주는 코드이다.#!/usr/bin/env python# 필요한 모듈 가져오기import rospyfrom my_motor.msg import direct8# 노드 새로 만들기rospy.init_node(&#39;auto_drive&#39;)# 토픽 발행 준비pub = rospy.Publisher(&#39;xycar_motor&#39;, msg)# 토픽 발행 함수 생성 - # angle값과 speed 값을 인자로 받아 그걸 토픽에 담아 발행def motor_pub(angle, speed):# 차량 속도 고정speed = 3# 차량의 조향각을 바꿔가면서 8자로 주행시킨다.while : pub.publist(msg)&amp;lt;!-- 8_drive.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;!-- motor node --&amp;gt; &amp;lt;include file&quot;$(find xycar_motor)/launch/xycar_motor.launch&quot; /&amp;gt; &amp;lt;!-- 8 driver --&amp;gt; &amp;lt;!-- 새로 작성한 노드 파일 실행 --&amp;gt; &amp;lt;node name=&quot;auto_drive&quot; pkg=&quot;my_motor&quot; type=&quot;8_drive.py&quot;&amp;gt;모터에 대해서는 include를 넣으면 motor node를 실행시킬 수 있다. 이미 만들어져 있는 파일을 가지고 오는 것이다. xycar_motor은 xycar_motor 패키지 안에 xycar_motor을 실행시키는 파일이다. 8 driver 파이썬 코드가 토픽을 발행하고, 그 토픽이 그 위에 모터노드로 구독이 되어 실행이 되는 구조이다.&amp;lt;!-- xycar_motor.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;arg name=&quot;motor_type&quot; default=&quot;$(env motor_version)&quot; /&amp;gt; &amp;lt;arg name=&quot;angle_offset&quot; default=&quot;0&quot; /&amp;gt; &amp;lt;group unless=&quot;$(arg motor_type)&quot;&amp;gt; &amp;lt;include file=&quot;$(find vesc_driver)/launch/vesc_drive_xycar_motor.launch&quot; /&amp;gt; &amp;lt;/group&amp;gt; &amp;lt;node name=&quot;xycar_motor&quot; pkg=&quot;xycar_motor&quot; type=&quot;xycar_motor.py&quot; output=&quot;screen&quot; &amp;gt; &amp;lt;param name=&quot;motor_type&quot; value=&quot;$(arg motor_type)&quot; /&amp;gt; &amp;lt;param name=&quot;angle_offset&quot; value=&quot;$(arg angle_offset)&quot; /&amp;gt; &amp;lt;/node&amp;gt;&amp;lt;/launch&amp;gt;실행$ roslaunch my_motor 8_drive.launchRVIZ 기반 3D 자동차~ - xycar_ws ⊢ build ⊢ devel ⊢ src ∟ rviz_xycar ⊢ launch ∟ xycar_3d.launch ⊢ urdf ∟ xtcae_3d.urdf ⊢ src ∟ move_joint.py ∟ rviz ∟ xycar_3d.rviz실행$ roslaunch rviz_xycar xycar_3d.launch이것을 실행시키면 창이 열린다. 여기서 joint_state_publisher UI를 사용하여 슬라이드바를 움직여서 바퀴를 움직일 수 있다.또한, 파이썬 코드로도 이 슬라이드바를 움직일 수 있다. 그를 위해서는 토픽을 확인해야 한다.$ rqt_graph/joint_states 이라는 이름의 토픽을 통해 바퀴를 움직일 수 있다는 것을 알 수 있다.더 자세히 보기 위해$ rostopic info joint_statesType: sensor_msg/JointStatePublisher:Subscriber:$ rosmsg show sensor_msgs/JointStatestd_msgs/Header header...string[] namefloat64[] positionfloat64[] velocityfloat64[] effort$ rostopic echo joint_statesheader:...name: [front_right_hinge_joint, front_left_hinge_joint, front_right_wheel_joint, front_left_wheel_joint ,rear_right_wheel_joint, rear_left_wheel_joint]position: [0.0, 0.231231231, 0.0, 0.12311532, 0.0, 0.0]velocity: []effort: []name에 hinge는 좌/우회전할 때 얼마나 꺽인지, wheel은 바퀴가 얼마나 돌아가는지를 나타내는 것이다. 그래서 0.231231231 정도 left로 돌아갔다는 것이고, 0.12311532만큼 바퀴가 회전되어 있다는 것이다.직접 발행을 위해 move_joint.py 라는 소스파일을 만들어 joint_state_publisher 제어창을 대신해보고자 한다.&amp;lt;!-- move_joint.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;param name=&quot;robot_description&quot; textfile=&quot;$(find rviz_xycar)/urdf/xycar_3d.urdf&quot;/&amp;gt; &amp;lt;param name=&quot;use_gui&quot; value=&quot;true&quot;/&amp;gt; &amp;lt;!-- rviz display --&amp;gt; &amp;lt;node name=&quot;rvize_visualizer&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find rviz_xycar)/rviz/xycar_3d.rviz&quot;/&amp;gt; &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;state_publisher&quot;/&amp;gt; &amp;lt;node name=&quot;move_joint&quot; pkg=&quot;rviz_xycar&quot; type=&quot;move_joint.py&quot;/&amp;gt;&amp;lt;/launch&amp;gt;urdf/xycar_3d.urdf: 차량을 모델링한 파일을 가져온다.args: 파일 위치를 나타내는 것으로 xycar_3d.rviz는 설정 파일이다.위의 두 개의 노드는 그림을 보여주는데 필요한 것들을 실행시키는 것이고, 마지막 노드는 원래는 스크롤바를 통해 토픽을 발행하는 노드가 있었는데, 이를 대신해 파이썬 코드를 넣는 것이다. 파이썬 코드 #!/usr/bin/env pythonimport rospyfrom sensor_msgs.msg import JointStatefrom std_msgs.msg import Headerrospy.init_node(&#39;move_joint&#39;)pub = rospy.Publisher(&#39;joint_states&#39;, JointState, queue_size=10)hello_xycar = JointState()hello_xycar.header = Header()hello_xycar.name = [&#39;front_right_hinge_joint&#39;,&#39;front_left_hinge_joint&#39;, &#39;front_right_wheel_joint&#39;,&#39;front_left_wheel_joint&#39;, &#39;rear_right_wheel_joint&#39;,&#39;rear_left_wheel_joint&#39;]hello_xycar.velocity = []hello_xycar.effort = []a = -3.14b = -3.14rate = rospy.Rate(50)while not rospy.is_shutdown():hello_xycar.header.stamp = rospy.Time.now()if a &amp;gt;= 3.14: a = -3.14 b = -3.14else: a += 0.01 b += 0.01 # 0.01라디안 == 약 6도hello_xycar.position = [0,0,a,b,0,0]pub.pulish(hello_xycar) rate.sleep() 실행 권한 추가, 실행$ chmod +x move_joint.py$ cm$ roslaunch rviz_xycar move_joint.launchrviz안에 TF에서 FRONT_RIGHT_WHEEL을 누르면 회전하는 것을 쉽게 볼 수 있다.rviz안에서 8자 주행실제로 움직이지는 못하지만, 바퀴를 움직일 수는 있기에 바퀴가 8자 주행하도록 만들고자 한다.~ - xycar_ws ⊢ build ⊢ devel ⊢ src ∟ rviz_xycar ⊢ launch ∟ rviz_drive.launch ⊢ urdf ∟ xtcae_3d.urdf ⊢ src ⊢ converter.py ∟ rviz_8_drive.py ∟ rviz ∟ xycar_drive.rviz원래의 8_drive 파일을 가져와서 사용하려고 하는데, 문제가 토픽의 종류가 다르다는 것이다.원래는 /xycar_motor이라는 토픽을 발행하는데, viewer는 /joint_states라는 토픽을 받는다. 따라서 이를 그대로 사용하기 위해 converter.py라는 토픽의 이름을 변경해주는 코드를 함께 만들었다.xycar_motor의 메시지타입에서 angle과 speed를 name, position, velocity, effort로 변경해야 한다. 기존의 angle을 position에 넣으면 될 것이다.# converter.py&amp;lt;!-- rviz_drive.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;param name=&quot;robot_description&quot; textfile=&quot;$(find rviz_xycar)/urdf/xycar_3d.urdf&quot;/&amp;gt; &amp;lt;param name=&quot;use_gui&quot; value=&quot;true&quot;/&amp;gt; &amp;lt;!-- rviz display --&amp;gt; &amp;lt;node name=&quot;rviz_visualizer&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find rviz_xycar)/rviz/rviz_drive.rviz&quot;/&amp;gt; &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;state_publisher&quot;/&amp;gt; &amp;lt;node name=&quot;driver&quot; pkg=&quot;rviz_xycar&quot; type=&quot;rviz_8_drive.py&quot; /&amp;gt; &amp;lt;node name=&quot;converter&quot; pkg=&quot;rviz_xycar&quot; type=&quot;converter.py&quot; /&amp;gt;&amp;lt;/launch&amp;gt;실행$ roslaunch rviz_xycar rviz_drive.launch" }, { "title": "[데브코스] 3주차 - Self driving Process ", "url": "/posts/selfdrivingprocess/", "categories": "Classlog, devcourse", "tags": "ros, devcourse", "date": "2022-03-04 13:00:00 +0900", "snippet": "자율주행 자동차 기술 개요자율주행 자동차는 어떤 기능을 제공해야 하는가4차 산업혁명이랑 인공지능, 빅데이터, 초연결 등으로 촉발되는 지능화 혁명 또는 그 이상자율주행 = 인공지능 + 빅데이터 + 초연결자율주행 기술을 구분하는 6단계 구분법미국 자동차기술학회(SAE)에서 구분한 자율주행 기술 6단계자율주행 프로세스자율주행 프로세스 단계별 요소 기술 인지 주변상황을 파악한다는 것, 경로 탐색, 차량간 통신, ADAS 센서 → 고정지물(차로,차선,횡단보도,터널) 인식 및 경로탐색, 변동지물(차량,보행자,신호등,사고차량) 및 이동물체 인식 판단 주변상황 판단 및 주행전략 결정 → 차선변경, 추월, 좌/우 회전, 정차 주행 경로 생성 → 목표궤적, 목표속도, 전방타겟 등 제어 차량 제어 → 목표 조향각, 토크, 목표 가감속 엔진 가감속, 조향 사람의 경우 네비게이션의 도움을 받아 지도 제공 받음, 목적지까지 경로 찾기, 좌회전 우회전 경로 변경 안내를 받는다. 사람은 차선과 지형지물을 보고 주행, 주변 차량 살펴서 주행하고, 교통 신호등을 인식해서 주행하고, 돌발 상황에 대응한다. 그러나 자율주행의 경우 보다 정확한 지도가 필요할 것 지도에서 자기의 현재 위치를 알아내야 함 → GPS, 영상 매칭, 라이다 매칭 목적지까지 경로 찾기 주변 살피기 → 각종 센서 상황에 맞게 속도 조정, 핸들 꺽기 경로 그대로 따라가기 (차선 준수) 교통신호 따르기 예외상황에 대처 (추월 ,정차) 돌발상황에 대처 (급브레이크, 장애물 피하기) 아래 1~8은 인지, 9,10은 판단, 11은 제어에 해당한다.전체 프로세스1.지도 생성먼저 지도는 HD map(High Definition Map)이어야 한다. 기존 네비게이션, 티맵보다 더 정밀한 10~20센티 정밀도를 제공해야 한다. 다양한 부가정보 포함 차선 정보 가드레일 도로 곡률, 경사 신호등 위치 교통 표식 고정밀 지도의 종류로는 벡터맵, 포인트맵 등이 있다.아래 사진이 벡터맵에 대한 것이다. 벡터맵은 차선 정보, 인식 정보 등 모든 정보가 담긴 맵핑된 지도다.포인트맵은 라이다를 이용해서 만든 지도이다. 벡터맵은 카메라로 한 것이기에 2차원으로 보이지만, 이는 라이다를 통해 만든 것이므로 3차원으로 보인다. 포인트맵은 미리 주행을 하면서 얻은 라이다 정보를 지도 제작 알고리즘을 통해 미리 제작한 것이다.3차원의 좌표축의 점들을 표시한 것이고, 사진으로는 거리정보는 잘 알기 힘들다. 그러나 라이다는 거리를 알 수 있다. 따라서 벡터맵은 2차원, 포인트맵은 3차원으로 보이는 것이다.이 두개를 합쳐서 사용하게 되면 아래의 그림이 된다.맵을 제작할 때는 MMS(Mobile Mapping System)을 사용한다. 이는 데이터를 수집한 다음 후처리 작업을 통해 지도를 제작하는 것을 말한다.2.위치 조정정밀 지도와 연동하여 차량의 현재 위치를 파악하는 기술을 Localization이라 한다. 이는 라이다와 카메라를 이용하여 지도의 어느 위치에 있는지 파악한다.카메라가 기후 악화 등으로 잘 인식하지 못할 때는 라이다와 가지고 있는 지도를 통해 인식한다. 라이다와 지도를 비교하여 표지판이나 고정지물의 위치를 대조해보고 일치하도록 위치를 조정한다.이렇게 라이다와 비교하는 이유는 GPS가 오차가 존재할 수 있기 때문이다.3.목적지 루트 설정 (Route Planning)목적지까지 경로를 찾는 것을 Route Planning이라 한다. 중간 목적지 또는 최종 목적지까지의 경로를 찾을 수 있고, 경로에 맞게 각 교차로에서의 행동도 결정해야 한다.3-1. local path planning (trajectory planning)다음 이동할 곳으로의 경로를 찾는 것이다. 이는 충돌을 회피할 때 고려되는 것으로 여러 개의 후보 경로를 확보해놓고, 끊임없이 후보를 삭제, 신규 후보를 등록하는 작업을 반복한다.이 주황색 영역으로 이동을 해야하는데, 수많은 경로 후보를 생성하고, 추가/삭제를 반복한다. 짧은 시간안에 수행되어야 하므로 다시 경로를 계산하는 것이 아닌 10개정도의 경로 후보를 생성해 놓는 것이다.4.객체 검출 (object detection)주행 중 주변 차량, 보행자, 오토바이, 자전거, 신호등 등을 인식해야 한다. 이 객체를 인식하는 기술을 object detection이라 한다.이 그림은 Monocular 3D Object Detection for Autonomous Driving 논문에서 발췌한 사진이다.5.객체 추적 (object tracking)각 객체를 인식만 하는 것이 아니라 추적하고, 카메라 밖을 벗어나면 추적을 멈추어야 한다. 그럴 때 사용하는 기술을 object tracking이라 한다.이 때는 각 객체에 ID를 부여하고, 예상되는 주행경로도 예측해야 한다.6.Predictionperception중에서 prediction이라는 기술도 존재한다. 이는 tracking에 기반한 객체의 현재까지의 움직임과 객체의 고유한 특징 등 다양한 정보를 바탕으로 객체의 미래 움직임을 추정한다. Multimodal Trajectory Prediction이라는 키워드로 연구가 되고 있다. 예를 들어 특정 자동차가 좌회전 깜박이를 킨다면 왼쪽으로 갈 확률이 높으므로 왼쪽으로 움직인다고 예측할 수 있다. 사람의 움직임도 예측해야 하는데, 차에 비해 사람은 전후좌우 즉각적으로 다 가능하므로, 차인지 사람인지에 따라서 예측이 또 달라질 수 있다.7.3D Pose Estimation객체를 인식하는 것 뿐만 아니라 객체의 정확한 위치를 추정하는 것이 중요하다. 대체로 이미지 안에서는 객체를 잘 찾는데, 그 객체에 대해 실제적인 위치를 알아야 사고가 나지 않으므로 인식 뿐만 아니라 객체의 위치까지 추정해야 한다. 그를 위해 bird’s eye view로 변환하여 추정할수도 있고, 이를 Multiple View Geometry 분야라 말하기도 하지만, lidar 데이터를 통해 위치 정보를 통해 객체 위치를 찾는 것이 훨씬 더 쉽다. 이 lidar 정보와 카메라 정보를 융합하는데, 카메라에서 찾은 객체 정보와 lidar에서의 점 분포를 관계를 만들어서 3차원 객체 탐지를 한다.lidar는 객체를 구분할 수 없지만, 정확한 위치 정보를 얻을 수 있고, 카메라는 객체의 위치 정보를 파악하기 어렵지만, 대상이 무엇인지는 구별할 수 있다.8.Sensor fusionlidar와 카메라의 각각의 장단점을 융합하기 위해 sensor fusion을 수행한다. lidar 데이터를 이미지 데이터에 그대로 적용을 시키고, 객체를 탐지한 bbox안에 있는 값들만 골라서 lidar의 x,y,z를 추정해서 3D object detection을 수행한다.이외에 GPS와 IMU를 함께 사용해도 sensor fusion에 해당한다.위의 모든 것들이 결합되어 하나의 맵을 만든다.9.행동 결정 (behavior selector)지도가 만들어지고 나면 다음 행동을 결정해야 한다. 이를 행동 결정이라 하고, 운전 방법 및 성향을 반영할수도 있다.10.경로 따라 운전 (trajectory following)경로를 따라 차량을 운전하는 것을 말한다. 차량의 현재 위치를 결정 차량에서 가장 가까운 경로상의 점을 찾는다 목표점을 찾는다 곡률을 계산, 해당 곡률로 차량의 방향을 업데이트 차량의 위치를 업데이트추가로 경로를 따라가기 위한 알고리즘으로 Pure pursuit 알고리즘이 있다.11.vehicle control다 결정이 되었으니 주행을 제어해야 한다. 이 때는 차량 운동학, 관성, 마찰력 등을 고려해야 한다.12.Acceleration이 외에도, 장비의 성능과 속도에 대해서도 중요하다. 최근에는 perception에 딥러닝이 완전히 자리잡았다. 그로 인해 데이터도 많아지고 처리해야 하는 양도 많아졌다. 그러나 자율주행은 빠른 속도로 움직이는 환경이므로 검출 성능 뿐만 아니라 검출 속도도 중요하다. 그래서 최적화 방법으로 Model Quantization, Pruning, Hardware Optimizaion 등이 있다. Model quantization은 모델 양자화로, 데이터 타입들을 속도를 빠르게 만들기 위해 변경한다는 의미이다. 즉, float32를 float16으로 변경하거나 int로 변경하여 연산하는 것을 말한다. Pruning은 가지치기로 네트워크에서 출력에 영향이 별로 없는 가지(노드)를 삭제해버리는 기술이다. hardware optimization은 자신의 하드웨어에 맞게 최대한 빠르게 끌어 올리는 방법이다.전체 프로세스 global planner: 지도가 global planning으로 연결되어 있는데, 넓은 단위의 자동차 내비게이션 역할을 한다. sensors : 위치와 가속도값을 결합하고, encoder는 자동차의 회전 카운트를 의미한다. perception : od(object detection)이나 global panner과 결합하여 지도에서의 내 위치를 결정짓는 용도로 사용 local panner : 차선 변경이나 물체 회피 등의 용도로 사용되는 것으로 어디로 이동을 할지를 결정detection에서도 lane detection, traffic light detection, traffic sign detection, object detection &amp;amp; tracking으로 나뉠 수 있다. 카메라를 통해 lane detection ,traffic detection, object detection을 수행할 수 있고, OD의 경우 RADAR, LIDAR와 융합하여 센서 퓨전을 할 수 있다. OD를 하다가 가려지거나 추출되지 않을 경우 이전 프레임의 정보를 사용하는 tracking을 활용할 수도 있다.오픈소스 자율주행 통합 플랫폼Autoware오토웨어는 자율주행 통합 플랫폼이다. 주소는 https://www.autoware.org/이고, 레벨2 정도에 해당하는 기술력을 가지고 있으며, 실차에 적용이 가능한 솔루션이다. 현재 100개 이상의 회사들에 의해 사용되고 있다. autoware 버전이 AI, IO, AUTO가 있는데, autoware.AI는 ROS1을 기반으로 사용되고 있고, AUTO는 ROS2를 기반으로 구성되어 있다.아래는 전체 프로세스에 대한 그림이다.이는 조금더 보기 편하게 만들어진 그림으로 sensing, computing, actuation으로 구성되어 있다.그리고 아래 그림은 오토웨어 소프트웨어의 스택(아키텍처)에 대한 사진이다.하드웨어로는 여러 장치가 있고, OS는 Linux를 사용하고 있다. 런타임 라이브러리로는 ros, cuda, caff, opencv 등이 있다.SVL라는 자율주행차 개발자를 위한 unity 기반 multi-robot 시뮬레이터가 있다. Autoware.Auto에 대한 센서 입력을 만들 수 있도록 제공한다. 차량 운전자나 사람의 위치 등 설정이 가능하다.Apolloapollo github V2X adapter : vehicle to everything, 자동차가 현재 어떤 신호를 내보내고 있는지 어딘가로 보낼 수 있는데, 이렇게 서로 통신하는 것을 V2X라 한다. Simulation : 자동차를 실험하기 위해서는 비용이 너무 많이 들엇허 simulation을 많이 사용한다. HD Map : 정밀 지도 OTA : over-the-air, 무선으로 소프트웨어를 관리하고, 업데이트하는 방법을 말한다. apollo software overviewapollo는 전체 프레임워크를 제공하기 때문에, monitor와 HMI를 제공한다. 이는 자동차에 탑승한 사람에게 자동차가 어떻게 움직이는지 알려주기 위한 용도이다. CANBus : 내부 네트워크를 말하는 용어 Guardian : 보안과 관련된 것으로, 내부에서 내리던 명령이 외부에서 해킹을 해서 명령을 내리면 너무 위험하므로 보안을 추가LiDAR vs Vision자율주행의 현재 상태는 크게 lidar + 정밀 지도 기반의 자율주행과 camera + 비정밀 지도 기반의 자율주행으로 나뉠 수 있다. 정밀 지도(HD map)은 x,y,z,d 의 정보를 점 형태로 다 저장하기 때문에 용량이 너무 크지만, 이름 그대로 정밀한 정보를 얻을 수 있다. 전자의 경우 waymo, 후자의 경우 tesla, mobileye가 있다.waymo의 경우 3개의 lidar과 전방 주시하는 1개의 카메라가 존재한다. waymo는 HD map을 사용하고 있다. 그에 반해 tesla는 매우 많은 카메라만을 사용하고 있다. 여기에 radar는 최근에 사용하지 않고도 잘 주행이 가능하다고 한다. 그리고 tesla는 정밀 지도가 아닌 다른 지도를 만들어서 사용한다.HD map vs Navigation map정밀 지도(HD map)은 자율주행에 필요한 많은 사전 정보를 가지고 있고, 이를 통해 지도로 만든 것이다. HD map은 차선 단위의 지도로 높은 정밀도를 가지지만, 지도 제작이 어렵고 비싸다. 그에 반해 navigation map을 일반적인 내비게이션에서 사용하는 맵이다.정밀 지도를 사용하지 않는 경우라도 네비게이션 맵은 반드시 사용할 수밖에 없다. 최종 목표로 도착에 필요한 global planning을 하기 위한 맵이 필요하다. local planning은 센서가 탐지가능한 거리에서의 planning이다.REMHD map이 많은 정보를 가지지만, 이에 대한 지도 제작과 유지 보수가 어려워서 다른 방법을 채택한 회사도 있다. 그 대표적인 회사가 Mobileye, Tesla이다. REM(road experiment management)라 하여 도로에서 발생하는 다양한 경험들을 관리하겠다는 의미이다. 이는 크게 3가지 단계로 나뉠 수 있다. harvesting ADAS기능이나 카메라를 이용한 다양한 기술이 들어가 있다. 차선에 대한 곡률과 위치를 통해 주행해야 하는 가이드 path도 뽑을 수 있다. alignment 위의 harvesting은 1대에 대한 주행 정보이지만, 이를 n대의 차량을 주행하여 모든 데이터를 모아놓으면 매우 많은 정보를 가질 수 있다. 사람마다 빠르게 달리는 사람이 있고, 느리게 달리는 사람이 있기 때문에, 그에 대해 누적 데이터를 만들고, 그에 대한 알맞는 경계값들을 조정한다. modeling &amp;amp; semantic 이 누적된 데이터를 통해 정밀지도와 유사한 선 형태의 지도를 만들 수 있게 된다. 이 방법은 사람이 차량을 구매한 후에도 업데이트가 지속적으로 가능하다. 운전자가 직접 데이터를 모아서 컴퓨터가 alignment를 수행하여 지도를 계속 조정하게 되면 사람마다 알맞는 차량의 기능이 만들어진다.Vector spacetesla는 5~6대의 카메라가 존재한다. 이 카메라들이 서로 다른 화면을 보고 있지만, 같은 객체임을 인지하는 것이 중요하다. 그러나 카메라를 활용해서 위치 정보를 특정하는 것은 매우 어렵다. 그래서 tesla는 5~6대를 가지고 가상의 카메라 하나를 만들어서 각각의 카메라마다의 이미지를 1개의 이미지로 융합한다. 이를 통해 vector space라고 하는 공간을 찾게 되었다.자율주행 자동차의 구현에 필요한 기술 자율주행 알고리즘 센싱, 인지, 의사 결정 자율주행 클라이언트 시스템 소프트웨어, 하드웨어 자율주행 클라우드 플랫폼 분산 컴퓨팅, 분산 스포티지 자율주행 알고리즘 센싱 주변 정보 획득 GPS IMU 라이다 카메라 초음파 인지 Localization 자기 위치 파악 GPS와 IMU를 조합해서 위치 측정 (위치 예측과 업데이트 반복) Stereo 카메라 영상으로 위치 측정 라이다, 포인트 클라우드, 파이클 필터로 위치 측정 여러 센서를 융합하여 정확도 개선 - Object detection 딥러닝 기반의 인식모델 사용 - Object tracking 객체 이동 궤적 추적 차량, 보행자와의 충돌 회피 의사 결정 동작 예측 다른 차량의 동작을 예측 확률 모델을 만들어 확률 분포 구하기 - 경로 계획 cost function으로 최적 경로 탐색 계산량 줄이기 위해 확률 기법 적용 - 장애물 회피 1단계 능동형: 충돌까지의 시간과 최소거리 추정치 뽑아서 경로를 다시 계획 2단계 반응형: 이동경로상에 장애물이 감지되면 주행제어 시스템에 개입 자율주행 클라이언트 시스템 소프트웨어 실시간성과 신뢰성 확보 ROS 문제점 해결 필요 master가 죽으면 전체시스템이 다운되며 복구용도의 모니터가 없음 메시지를 브로드캐스팅하면서 성능이 저하됨. 멀티캐스팅 메커니즘 적용하면 좋다. 노드가 해킹되기 쉬움, 리눅스 컨테이너, 샌드박스로 보안 기능 강화 가능 로컬에서는 TCP/IP 통신 대신에 공유메모리 통신방식을 적용하면 좋다. 하드웨어 성능 향상 필요 파이프라인 병렬 프로세싱 기능 필요 HW 가성비 좋게 차량의 배터리 문제가 있어 전력 소요량 최소화 노력이 필요 차량이라는 환경에서는 발열문제가 심각하기에 발열을 최소화하거나 열을 쉽게 배출할 수 있는 방법이 필요 자율주행 클라우드 플랫폼 분산 컴퓨팅 시뮬레이션 ROS Bag/Replay 분산 환경으로 처리 - HD맵 생성 원본데이터 처리 포인트 클라우드 생성 포인트 클라우드 정렬 2D 반사맵 생성 HD맵 레이블링 분산 스토리지 딥러닝 모델 학습 학습데이터 준비 학습 진행 모델의 유효성과 효율성을 지속적으로 업데이트 " }, { "title": "[데브코스] 3주차 - ROS Node Communication programming", "url": "/posts/ros_node/", "categories": "Classlog, devcourse", "tags": "ros, devcourse", "date": "2022-03-02 13:00:00 +0900", "snippet": "다양한 상황에서의 노드 통신 누락 없이 모두 잘 도착하는지 데이터 크기에 따른 전송속도 도착하는 데이터를 미처 처리하지 못하면? 주기적 발송에서 타임슬롯을 오버하면 어떻게 되나 협업해야 하는 노드를 순서대로 가동시킬 수 있나?과제 1. 노드간 동기화문제 설명퍼블리셔와 서브스크라이버간의 데이터를 빠짐없이 잘 보내고 받는지를 확인해보고자 한다.과제 설명 sender_serial.py receiver_serial.py sr_serial.launch 숫자를 보내서 받는 쪽에서 누락된 게 있는지 알아보자. 그래서 보내는 쪽이 안 보낸 것인지 받는 쪽이 못 받는 건지 구분할 수 있는가 숫자가 빠진 것에 대해 파이썬 코드를 사용하여 판단해보자 특히 처음과 끝 받는 쪽을 먼저 실행시켜놓고 그 다음에 보내는 쪽을 실행시켜야 다 받을텐데, roslaunch로는 노드를 순서대로 실행시킬 수 없으니 rosrun을 사용해야 하는데 더 좋은 방법이 있는가코드 구현 sender_serial.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32rospy.init_node(&#39;sender_serial&#39;)pub = rospy.Publisher(&#39;my_topic&#39;,Int32)rate = rospy.Rate(10)count = 1while not rospy.is_shutdown(): if count == 1 or 100: print(&quot;start or end : &quot;,count) pub.publish(count) if count == 100: print(&quot;count complete&quot;) break count += 1 rate.sleep() receiver_serial.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32def callback(msg): if msg.data == 1: # 1이 도착하면 문장을 출력하도록 했다. print(&quot;1 is arrive&quot;) print msg.datarospy.init_node(&#39;receiver_serial&#39;)sub = rospy.Subscriber(&#39;my_topic&#39;,Int32, callback)rospy.spin()실행 결과문제점publisher와 subscriber간의 실질적인 통신 구축에 지연문제가 존재한다. 받을 사람이 준비가 되지 않았는데 물건을 던지면 당연히 받지 못한다.해결 방법 노드가 등록되어 있는지 확인하는 함수를 이용while ( pub.get_num_connections() == 0): count = 1이 루프를 빠져나오면 이제부터 메시지를 전송하도록 한다.과제 2. ROS 전송속도데이터 크기에 따른 전송속도가 어떻게 되는지 알아보고자 한다.과제 설명 sender_speed.py receiver_speed.py sr_speed.launch 정해진 크기의 데이터를 반복해서 왕창 보내기 보내는 쪽은 10분동안 시간을 정해놓고 쉴새 없이 보내고 10분동안 몇 바이트 보냈는지 체크해서 송신속도 계산 받는 쪽도 10분 동안 시간 정해놓고 모두 얼마나 받았는지 체크해서 수신속도를 계산 사이즈를 바꿔서 1Mbyte, 5M, 10M, 20M, 50M 전송한 송수신 속도에 대한 그래프도 작성, 어떨 때 가장 빠른지 단위는 bps(bytes/sec) 받는 쪽이 없으면 더 빨라지는가 sender_speed.py sender이라는 이름으로 노드 생성 발행하는 토픽 이름은 long_string, 타입은 string 1초에 1번씩 다양한 용량의 long string을 발행, 문자열은 #으로 가득 채워라 사이즈를 바꿔서 1Mbyte, 5M, 10M, 20M, 50M 전송해보며 최대 전송을 알아보는 것도 좋다. 코드에 사이즈를 나열해서 안쓰는 사이즈는 주석 처리하면 편하다. receiver_speed.py receiver이름으로 노드 생성 다양한 용량의 long_string을 수신해서 long_string 1개를 받으면 소요시간을 화면에 출력 가능하면 속도도 출력, 단위는 bps 코드 구현 msg_speed.launch&amp;lt;launch&amp;gt; &amp;lt;node pkg=&#39;msg_send&#39; type=&#39;sender_speed.py&#39; name=&#39;sender&#39;&amp;gt; &amp;lt;param name=&#39;size&#39; value=&quot;50&quot; /&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node pkg=&#39;msg_send&#39; type=&#39;receiver_speed.py&#39; name=&#39;receiver&#39; output=&#39;screen&#39; /&amp;gt;&amp;lt;/launch&amp;gt; sender_speed.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringdic = {&quot;1M_str&quot; : 1028000, # == 0.98Mbytes &quot;5M_str&quot; : 1028000 * 5, &quot;10M_str&quot; : 1028000 * 10, &quot;20M_str&quot; : 1028000 * 20, &quot;50M_str&quot; : 1028000 * 50,} rospy.init_node(&#39;sender&#39;)pub = rospy.Publisher(&#39;long_string&#39;,String)long_str = String()size_dir = str(rospy.get_param(&#39;~size&#39;)) + &quot;M_str&quot;hashs = &#39;#&#39; * dic[size_dir]rate = rospy.Rate(1)while not rospy.is_shutdown(): gettime = str(rospy.get_time()) long_str.data = hashs + &quot;:&quot; + gettime pub.publish(long_str) rospy.loginfo(gettime) rate.sleep() receiver_speed.py#!/usr/bin/env pythonimport rospyimport sysfrom std_msgs.msg import Stringdef callback(msg): data = msg.data.split(&quot;:&quot;) start_time = float(data[1]) current_time = rospy.get_time() str_size = sys.getsizeof(data[0]) rospy.loginfo(str(str_size) + &quot; byte : &quot; + str(current_time - start_time) + &quot; second&quot;) rospy.loginfo(&quot;speed: &quot; + str(float(str_size)/(current_time-start_time)) + &quot; byte/s&quot;) rospy.init_node(&#39;receiver&#39;)sub = rospy.Subscriber(&#39;long_string&#39;,String, callback)rospy.spin()실행 결과 1Mbyte일 때: 약 0.01초, 속도는 280 Mbps 5Mbyte일 때: 약 0.02초, 속도는 240 Mbps 10Mbyte일 때: 약 0.05초, 속도는 200 Mbps 20Mbyte일 때: 약 0.08초, 속도는 280 Mbps 50Mbyte일 때: 약 0.2초, 속도는 180 Mbps통계적으로 봤을 때, 용량이 커지게 되면 속도는 느려져야 맞지만, 그닥 그렇지는 않은 것 같다. 시간이 오래 걸리는 것은 맞으나 속도는 대체로 일정했다.과제 3. ROS 처리 지연 문제도착하는 데이터를 미처 처리하지 못하면 어떻게 할지에 대해 알아보고자 한다.과제 설명 sender_overflow.py receiver_overflow.py sr_overflow.launch 도착하는 데이터를 미처 처리하지 못하면 어떻게 되는지 알아본다. 늦더라도 다 처리하는지, 순서가 뒤섞이는지, 몇몇은 버리는지 확인한다. 받는쪽이 버벅되게 만들어놓고 데이터를 왕창보낸다. 구독자의 콜백함수 안에 시간 많이 걸리는 코드 넣어서 토픽 처리에 시간이 걸리도록 만들어라 콜백함수가 끝나지 않았는데 토픽이 새로 도착하면 어떻게 되나. 도착한 토픽은 임시로 어딘가에 쌓이는가? 나중에 꺼내서 처리할 수 있는가? 그냥 없어지는가? 한 번 못받은 토픽은 영영 못받는 것인가 발행자는 이 사실을 아는가? 알려줄 수 있는 방법이 있나 sender_overflow.py sender이라는 이름으로 노드 생성 발행하는 토픽 이름은 my_topic, 타입은 int32 1초에 1000번씩 숫자를 1씩 증가해서 토픽을 발행 receiver_overflow.py receiver이름으로 노드 생성 sender로부터 my_topic을 화면에 출력하여 토픽의 누락 여부를 확인 1씩 숫자가 증가하지 않으면 뭔가 문제가 있다는 것을 확인할 수 있다. 받는 큐사이즈를 늘렸을 때 누락없이 잘 추출되는 것을 볼 수 있다.코드 구현 msg_overflow.launch&amp;lt;launch&amp;gt; &amp;lt;node pkg=&#39;msg_send&#39; type=&#39;sender_overflow.py&#39; name=&#39;sender&#39; /&amp;gt; &amp;lt;node pkg=&#39;msg_send&#39; type=&#39;receiver_overflow.py&#39; name=&#39;receiver&#39; output=&#39;screen&#39; /&amp;gt;&amp;lt;/launch&amp;gt; sender_overflow.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32rospy.init_node(&#39;sender&#39;,anonymous=True)pub = rospy.Publisher(&#39;my_topic&#39;, Int32)rate = rospy.Rate(100000)count = 1while (pub.get_num_connections() == 0): continuewhile not rospy.is_shutdown(): pub.publish(count) count = count + 1 rate.sleep() receiver_overflow.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32def callback(msg): rospy.loginfo(&quot;callback is being processed&quot;) rospy.sleep(5) print msg.datarospy.init_node(&#39;receiver&#39;)sub = rospy.Subscriber(&#39;my_topic&#39;,Int32, callback,queue_size = 1)# Queue Size는 발행되는 메세지를 얼마나 가지고 있을지에 관련된 변수이며 신규 데이터가 들어오는 경우 오래된 데이터부터 삭제하게 된다.# 출처 : https://changun516.tistory.com/127rospy.spin()실행 결과중간에 누락이 되는 토픽들이 많았다.과제 4. ROS 타임 슬롯 문제주기적 발송에서 타임슬롯을 오버하면 어떻게 되는지 알아보고자 한다과제 설명 주기적 발송에서 타임슬롯을 오버하면 어떻게 되는가 1초에 5번, rate(5)로 셋팅하고 작업시간이 0.2초가 넘도록 만들자. sleep앞에 들어간 작업코드의 수행시간을 늘려본다. 늘렸다 줄였다를 반복해서 해보자. 입력값을 받아서 이걸 조정할 수 있게 만들면 된다. input을 통해 사용자가 직접 느리게 하든지, 양을 크게 해서 시간을 늘리든지 그 후 중요한 것은 걸린 시간을 출력하는 것이다. 1초에 5번을 지킬 수 없으면 어떻게 작동하는지 보아라 앞에서부터 쭉 밀리는 식으로 일하는지 쉬는 시간을 조정하는지 3번만하고 다음으로 넘어가는지 입력받은 카운트만큼 반복을 진행해서 시간을 계속 늘리도록 한다. 각각의 시작, 끝, 쉬는 시간을 리스트로 묶는다.코드 구현 msg_timeslot.launch&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;msg_timeslot.py&quot; name=&quot;teacher&quot;&amp;gt; &amp;lt;!-- &amp;lt;param name=&#39;epoch&#39; value=&quot;100&quot; /&amp;gt; --&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt; sender_timeslot.py#!/usr/bin/env pythonimport rospyimport timeitfrom std_msgs.msg import Int32rospy.init_node(&#39;teacher&#39;)pub = rospy.Publisher(&#39;msg_to_students&#39;, Int32, queue_size = 0)rate = rospy.Rate(5)time = input(&#39;input epoch : &#39;)def do_job(time): for i in range(0,time): i += 1 pub.publish(i)while not rospy.is_shutdown(): start_time = timeit.default_timer() do_job(time) end_time = timeit.default_timer() print &#39;send time : %.4f sec&#39;%(end_time - start_time) rate.sleep() end_sleeptime = timeit.default_timer() print &#39;sleep time : %.4f sec&#39;%(end_sleeptime-end_time) total = end_sleeptime - start_time print &#39;total time : %.4f sec&#39;%(total) print &#39;\\n&#39; student_int.py실행 결과epoch, 반복을 100으로 했을 때는 대체로 0.2초는 잘 지켜지고 있는 것을 볼 수 있다. 반복을 10000으로 올려봐도 아직도 0.2초는 잘 지켜지고 있다.크게 올려보기 위해 백만으로 반복해본다.0.2초를 넘어서는 것을 볼 수 있다. 즉 아무리 오래 걸리더라도 모든 데이터는 다 보내는 방식으로 되어 있는 듯하다. 결국 매 타임마다 순차적으로 delay가 생긴다.과제 5. ROS 노드의 순차 실행협업해야 하는 노드를 순서대로 가동시키는 방법에 대해 알아보고자 한다.과제 설명 협업해야 하는 노드를 순서대로 가동시킬 수 있나 roslaunch로 구현해보고 정 안되면 rosrun으로 진행해보아라 first.py second.py third.py fourth.py receiver.py sr_order.launch 순서대로 receiver에 메시지를 보내도록 만든다. receiver는 도착한 순서대로 출력해야 하는데, 이때 first→second→third→fourth가 되어야 한다. 앞에 노드가 움직이기 전에는 절대 토픽을 먼저 보내면 안된다. 어떻게 동기를 맞추고 순서를 맞추는가 Launch파일을 이용해서 할 수 있는가, 이게 가장 편리할 듯하다. 이것을 먼저 고민해보라 ros의 도움으로 할 수 있나 아니면 내가 프로그래밍 해야 하는가 receiver.py 작성 구독해야할 토픽의 이름은 “msg_to_receiver”, 내용은 string, my name is first, my name is second, my name is third, my name is fourth 테스트를 위해 받은 토픽이 순차적으로 오는지 화면에 출력 first.py/second.py/third.py/fourth.py 자기 이름에 맞춰서 first, second, third, fourth first노드가 receiver 노드로 최소한 첫 토픽을 보내는 시점 이후에 전송을 시작해야 한다. 코드 구현과제 5에서는 새로운 패키지를 생성하려고 한다.$ cd ~/xycar_ws/src$ catkin_create_pkg order_test std_msgs rospy sr_order.launch&amp;lt;!-- sr_order.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node name=&quot;receiver&quot; pkg=&quot;order_test&quot; type=&quot;receiver.py&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;node name=&quot;first&quot; pkg=&quot;order_test&quot; type=&quot;first.py&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;node name=&quot;second&quot; pkg=&quot;order_test&quot; type=&quot;second.py&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;node name=&quot;third&quot; pkg=&quot;order_test&quot; type=&quot;third.py&quot; output=&quot;screen&quot; /&amp;gt; &amp;lt;node name=&quot;fourth&quot; pkg=&quot;order_test&quot; type=&quot;fourth.py&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt;한 개마다 command 명령 퍼블리셔를 하나 더 만들었고, 그것을 받는 서브스크라이버도 만들 것이다. first.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringname = &#39;first&#39;OK = Nonedef ctl_callback(data): global OK OK = str(data.data)rospy.init_node(name)rospy.Subscriber(&quot;start_ctl&quot;, String, ctl_callback)while True: if not OK: continue d = OK.split(&quot;:&quot;) if (len(d) == 2) and (d[0] == name) and (d[1] == &quot;go&quot;): breakpub = rospy.Publisher(&quot;msg_to_receiver&quot;, String, queue_size=1)rate = rospy.Rate(2)hello_str = String()while not rospy.is_shutdown(): hello_str.data = &quot;my name is &quot; + name pub.publish(hello_str) rate.sleep() second.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringname = &#39;second&#39;OK = Nonedef ctl_callback(data): global OK OK = str(data.data)rospy.init_node(name)rospy.Subscriber(&quot;start_ctl&quot;, String, ctl_callback)while True: if not OK: continue d = OK.split(&quot;:&quot;) if (len(d) == 2) and (d[0] == name) and (d[1] == &quot;go&quot;): breakpub = rospy.Publisher(&quot;msg_to_receiver&quot;, String, queue_size=1)rate = rospy.Rate(2)hello_str = String()while not rospy.is_shutdown(): hello_str.data = &quot;my name is &quot; + name pub.publish(hello_str) rate.sleep() third.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringname = &#39;third&#39;OK = Nonedef ctl_callback(data): global OK OK = str(data.data)rospy.init_node(name)rospy.Subscriber(&quot;start_ctl&quot;, String, ctl_callback)while True: if not OK: continue d = OK.split(&quot;:&quot;) if (len(d) == 2) and (d[0] == name) and (d[1] == &quot;go&quot;): breakpub = rospy.Publisher(&quot;msg_to_receiver&quot;, String, queue_size=1)rate = rospy.Rate(2)hello_str = String()while not rospy.is_shutdown(): hello_str.data = &quot;my name is &quot; + name pub.publish(hello_str) rate.sleep() fourth.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringname = &#39;first&#39;OK = Nonedef ctl_callback(data): global OK OK = str(data.data)rospy.init_node(name)rospy.Subscriber(&quot;start_ctl&quot;, String, ctl_callback)while True: if not OK: continue d = OK.split(&quot;:&quot;) if (len(d) == 2) and (d[0] == name) and (d[1] == &quot;go&quot;): breakpub = rospy.Publisher(&quot;msg_to_receiver&quot;, String, queue_size=1)rate = rospy.Rate(2)hello_str = String()while not rospy.is_shutdown(): hello_str.data = &quot;my name is &quot; + name pub.publish(hello_str) rate.sleep() receiver.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringdef callback(msg): rospy.loginfo(&quot;I heard %s&quot;, msg.data)rospy.init_node(&#39;receiver&#39;)rospy.Subscriber(&#39;msg_to_receiver&#39;,String, callback)pub = rospy.Publisher(&#39;start_ctl&#39;,String,queue_size=1)rate = rospy.Rate(10)hello_str = String()rospy.sleep(1)sq = [&#39;first&#39;,&#39;second&#39;,&#39;third&#39;,&#39;fourth&#39;]pub_msg = String()for i in sq: pub_msg.data = i+&quot;:go&quot; pub.publish(pub_msg) rospy.sleep(3)rospy.spin()" }, { "title": "[데브코스] 3주차 - ROS Turtlesim 8 RACE and MSG Package ", "url": "/posts/rosturtlesim8/", "categories": "Classlog, devcourse", "tags": "ros, devcourse", "date": "2022-03-01 13:00:00 +0900", "snippet": "ROS turtlesim 8자 주행# pub8.py#!/usr/bin/env pythonimport rospyfrom geometry_msgs.msg import Twistturn = 0rospy.init_node(&#39;my_node&#39;, anonymous=True)pub = rospy.Publisher(&#39;/turtle1/cmd_vel&#39;,Twist,queue_size=10)msg = Twist()msg.linear.x = 2.0msg.linear.y = 0.0msg.linear.z = 0.0msg.angular.x = 0.0msg.angular.y = 0.0msg.angular.z = 1.8rate = rospy.Rate(1)while not rospy.is_shutdown(): pub.publish(msg) rate.sleep() turn = turn + 1.8 # 얼마나 이동했는지 보기 위해서 if turn &amp;gt;= 12: msg.angular.z *= -1 turn = 0msg 패키지 전체 구성노드1 ( teacher ) ——토픽 전송 ( my_topic )——&amp;gt; 노드2 ( student )xycar_ws ⊢ src ⊢ my_pkg1 ∟ msg_send ⊢ launch ∟ m_send.launch ∟ src ⊢ teacher.py ∟ student.py ⊢ build ∟ devel1.먼저 패키지 폴더 생성$ catkin_create_pkg msg_send std_msgs rospy$ cd msg_send$ mkdir launch$ cm2.파일 내용teacher.py: publisher → 토픽에 call me please를 담아 전송student.py: subscriber → 토픽 받아서 내용을 꺼내서 화면에 출력주고 받는 토픽 이름은 my_topic teacher.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringrospy.init_node(&#39;teacher&#39;)pub = rospy.Publisher(&#39;my_topic&#39;, String)rate = rospy.Rate(2)while not rospy.is_shutdown(): pub.publish(&#39;call me please&#39;) rate.sleep() student.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringdef callback(msg): print msg.datarospy.init_node(&#39;student&#39;)sub = rospy.Subscriber(&#39;my_topic&#39;, String, Callback)rospy.spin() m_send.launch$ gedit m_send.launch&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher.py&quot; name=&quot;teacher&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student.py&quot; name=&quot;student&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt;코드 분석teacher.py1.인터프리터 선언파일의 첫줄에 #!로 시작되는 라인을 shebang 라인이라 한다. 스크립트 파일의 첫 줄에 사용되고, 해당 파일의 실행에 어던 인터프리터를 사용할지 지정한다.python2.6 ,python3 등으로 파이썬 버전을 구분해줄 수도 있다. 이 shebang라인을 선언해줄 경우 $./teacher.py로 실행할 수 있다.#!/usr/bin/env python2.임포트남이 만든 소프트웨어나 라이브러리를 가져와 사용할 때 사용한다. 이 경우 rospy라는 라이브러리를 import하여 사용하겠다는 것, 그리고 std_msgs.msg에서 string이라는 라이브러리르 사용하겠다는 것이다.import rospyfrom std_msgs.msg import String3.’teacher’ 이름의 노드 생성해당 노드를 초기화하고 노드의 이름을 정하는 코드다. 노드를 관리하고 통합하는 ros 프레임워크를 python으로 만든 것이 rospy이고, 이를 통해 노드를 초기화한다.rospy.init_node(&#39;teacher&#39;) def init_node를 자세히 보면 엄청 많은 인자들이 존재한다. name: 노드의 이름으로 타입은 string argv: 사용자가 지정한 argument를 넘겨받을 때 사용한다. 타입은 list&amp;lt;string&amp;gt; 이다. anonymous: 노드의 이름을 자동으로 생성한다는 것으로 보통은 name뒤에 임의의 숫자를 붙인다. 같은 노드로 여러 instance를 사용할 때 사용한다. log_level: 타입은 int, rospy.DEBUG, rospy.INFO,rospy.ERROR 등을 사용할 수 있다. disable_rostime: 내부적인 테스트시에만 사용 disable_rosout: 내부적인 테스트시에만 사용 disable_signal: true라면 rospy는 사용자의 signal handler를 등록하지 않는다. 사용자가 main thread로부터 init_node를 콜하지 않을 때나 사용자가 자신만의 signal handling을 설정해야하는 환경에서 rospy를 사용할 때 사용한다. xmlrpc_port: client XMLRPC node에 대한 포트번호 tcpros_port: TCPROS는 이 포트를 통해 통신하게 된다.4.퍼블리셔 선언my tipic이라는 이름의 토픽을 발행하는 노드라는 것을 말해주는 코드다. 그 뒤에는 토픽의 타입을 말해주는 것으로 지금은 string이다. 메시지 타입으로도 넣을 수 있다.pub = rospy.Publisher(&#39;my_topic&#39;, String) # 퍼블리셔를 생성 토픽이름이 my topic이고 메시지 타입은 string인 퍼블리셔5.반복1초에 2번 loop를 반복할 수 있도록 rate라는 객체를 생성하는 것이다. 1초안에 2번이므로 약 0.5초에 1번씩 돈다. 0.5초 안에 작업을 마친다면 나머지 시간은 휴식한다.rate = rospy.Rate(2) # 1초에 2번 루프를 돈다는 것6.루프 시작shutdown 즉, ros시스템이 끝나지 않을 때동안 계속 진행하라는 것이다.while not rospy.is_shutdown():7.퍼블리셔토픽의 내용을 발행한다. pub.publish(&#39;call me please&#39;) # call me please라는 메시지를 전달해라 8.sleep작업을 마친 다음 남는 시간은 멈춰있으라는 말이다. rate.sleep()student.py1.인터프리터 선언#!/usr/bin/env python2.임포트import rospyfrom std_msgs.msg import String3.콜백함수 선언callback함수 선언하는 코드로, 이는 토픽이 도착했을 때마다 실행되는 함수이다. 이 함수는 msg.data 를 화면에 출력한다. 토픽을 보낼 때 string 타입으로 보냈다. 이를 자세히 보면 data라는 곳에 담아져 날아오기 때문에 이를 불러오기 위해서는 msg가 아니라 data로 불러와야 한다. 만약 다른 타입에서 array라는 공간에 담겨져 온다면 msg.array라고 해야 한다.def Callback(msg): print msg.data4.초기화rospy.init_node(&#39;student&#39;) # student이름의 node 생성5.서브스크라이버 선언subscriber 노드인데 받을 토픽 이름이 my topic이고 string 메시지 타입을 받을 것이다. 이 토픽이 도착하면 callback 함수를 불러달라는 코드다.sub = rospy.Subscriber(&#39;my_topic&#39;, String, Callback) 6.무한 반복ros 시스템이 끝날 때까지 계속 반복rospy.spin()1:N, N:1, N:N통신통신 구성 1:N 통신 e.g. 카메라 → 인공지능 , 영상처리, 하드웨어, 화면 N:1 통신 e.g. 머신러닝, 알고리즘, 인지판단 → 모터 N:N 통신앞서 사용한 토픽에서 string이 아닌 int32를 사용N:N통신 teacher_int.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32rospy.init_node(&#39;teacher&#39;)pub = rospy.Publisher(&#39;my_topic&#39;, Int32)rate = rospy.Rate(2)count = 1while not rospy.is_shutdown(): pub.publish(count) count = count + 1 rate.sleep() student_int.py#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Int32def callback(msg): print msg.datarospy.init_node(&#39;student&#39;)sub = rospy.Subscriber(&#39;my_topic&#39;, Int32, callback)rospy.spin()노드를 여러 개 띄울 때 하나의 코드로 여러 개의 노드를 연결하려면 각 도드의 이름을 달리해야 한다. 그러나 노드의 init함수에서 anonymous=True 값을 넣어주면 노드 이름이 자동 설정된다.$ rosrun msg_send teacher_int-1.py$ rosrun msg_send teacher_int-2.py$ rosrun msg_send teacher_int-3.py위의 방법은 비효율적이다. 따라서 node 설정시 anonymous=True로 설정한 후 실행하면 각자 다른 이름이 설정된다. 이 때 중요한 것은 subscriber 쪽에도 anonymous를 설정해야 동일하게 작동된다.student_int.py$ rospy.init_node(&#39;student&#39;,anonymous=True)teacher_int.py$ rospy.init_node(&#39;teacher&#39;,anonymous=True)------$ rosrun msg_send teacher_int1.py$ rosrun msg_send student_int1.py$ rosrun msg_send teacher_int2.py$ rosrun msg_send student_int2.py$ rosrun msg_send teacher_int3.py$ rosrun msg_send student_int3.py&amp;lt;!-- m_send_nn.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher1&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher2&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher3&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student1&quot; output=&quot;screen&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student2&quot; output=&quot;screen&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student3&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt;이로써 소스파일은 건들이지 않고 이름만 바꿔줌으로써 N:N 통신이 가능해진다.1:N 통신m_send_nn.launch파일을 살짝 바꿔주면 된다.&amp;lt;!-- m_send_1n.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student1&quot; output=&quot;screen&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student2&quot; output=&quot;screen&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student3&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt;N:1 통신m_send_nn.launch 파일을 수정&amp;lt;!-- m_send_n1.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher1&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher2&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;teacher_int.py&quot; name=&quot;teacher3&quot;/&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;student_int.py&quot; name=&quot;student&quot; output=&quot;screen&quot;/&amp;gt;&amp;lt;/launch&amp;gt;나만의 메시지 만들기xycar_ws ⊢ src ∟ msg_send ⊢ launch ∟ m_send.launch ⊢ msg ∟ my_msg.msg ⊢ src ⊢ teacher.py ∟ student.py ⊢ CMakeLists.txt ∟ package.xml ⊢ build ∟ devel$ cd ~/xycar_ws/src/msg_send$ mkdir msg &amp;amp;&amp;amp; cd msg$ gedit my_msg.msg my_msg.msgstring first_namestring last_nameint32 ageint32 scorestring phone_numberint32 id_number그 다음 package.xml 파일을 수정해야 한다. 다음 코드를 맨 아래에 추가한다. ... &amp;lt;exec_depend&amp;gt;std_msgs&amp;lt;/exec_depend&amp;gt; &amp;lt;!-- 추가 --&amp;gt; &amp;lt;build_depend&amp;gt;message_generagtion&amp;lt;/build_depend&amp;gt; &amp;lt;exec_depend&amp;gt;message_runtime&amp;lt;/exec_depend&amp;gt;또한, CMakeLists.txt도 수정해야 한다.find_package(catkin REQUIRED COMPONENTS rospy std_msgs message_generation)## Generate messages in the &#39;msg&#39; folder , 코멘트를 풀고 수정add_message_files( FILES my_msg.msg)## Generate added messages and services with any dependencies listed here, 코멘트 풀기generate_messages( DEPENDENCIES std_msgs)## 1줄 추가catkin_package( CATKIN_DEPENDS message_runtime) 실행 결과$ rosmsg show msg_send/my_msgstring first_namestring last_nameint32 ageint32 scorestring phone_numberint32 id_numbercustom message 사용하여 코드 작성참고 사이트: ros 공식 사이트# from 패키지이름.msg import 메시지 파일 이름from msg_send.msg import my_msg다른 패키지의 custom msg도 사용할 수 있다. msg_sender.py#!/usr/bin/env pythonimport rospy# from 패키지이름.msg import 메시지 파일 이름from msg_send.msg import my_msg # my_msg라는 파일을 쓸 것이다.# msg_sender 노드 생성rospy.init_node(&#39;msg_sender&#39;, anonymous=True)# pub 노드, 토픽 이름은 msg_to_xycar 메시지 타입은 my_msgpub = rospy.Publisher(&#39;msg_to_xycar&#39;, my_msg)# 데이터 채우기msg = my_msg()msg.first_name = &quot;JaeHo&quot;msg.last_name = &quot;Yoon&quot;msg.id_number = &quot;12345678&quot;msg.phone_number = &quot;010-1234-5678&quot;# 1초에 1번 쉬기rate = rospy.Rate(1)while not rospy.is_shutdown(): # 메시지를 발행 pub.publish(msg) # 발행하면 sending message 출력 print(&quot;sending message&quot;) rate.sleep() msg_receiver.py#!/usr/bin/env pythonimport rospyfrom msg_send.msg import my_msgdef callback(msg): print(&quot;1. name : &quot;, msg.last_name + msg.first_name) print(&quot;2. id : &quot;, msg.id_number) print(&quot;3. phone number : &quot;, msg.phone_number)rospy.init_node(&quot;msg_receiver&quot;, anonymous=True)sub = rospy.Subscriber(&#39;msg_to_xycar&#39;, my_msg, callback)rospy.spin() 실행$ cm$ roscore$ rosrun msg_send msg_receiver.py$ rosrun msg_send msg_sender.py launch 파일 만들어서 실행&amp;lt;!-- my_sender.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;msg_sender.py&quot; name=&quot;sender1&quot; /&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;msg_sender.py&quot; name=&quot;sender2&quot; /&amp;gt; &amp;lt;node pkg=&quot;msg_send&quot; type=&quot;msg_receiver.py&quot; name=&quot;receiver&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt;$ roslaunch msg_send my_sender.launch" }, { "title": "[데브코스] 3주차 - ROS Abstract and Turtlesim ", "url": "/posts/rostutorial/", "categories": "Classlog, devcourse", "tags": "ros, devcourse", "date": "2022-02-28 13:00:00 +0900", "snippet": "ROS 기초ROS 기능 소개https://www.ros.orgros : robot operating system 오픈소스 로봇 운영체제 소스 무료 공개 개방형 구조 로봇 소프트웨어를 개발하는 데 필요한 소프트웨어의 집합체 소프트웨어 프레임워크 메타 운영체제 (meta os), 미들웨어 (middleware) 소프트웨어 모듈 + 라이브러리 집합 + 도구 집합 ROS는 자동차 제어를 위한 미들웨어라고 할 수 있다. -&amp;gt; 각종 센서와 모터를 프로그래머가 편하게 사용할 수 있도록 지원ros는 실제 차에는 적용하기 힘드나 자이카와 같은 차에는 적용하기 적합하다.ROS 특징 로봇 sw를 만들기 위한 코드의 재사용이 용이한 환경제공이 목표 다양한 프로그래밍 언어를 지원 : C++, python 표준화된 ros인터페이스를 따르는 hw 와 sw를 편하게 엮을 수 있음 하드웨어 부품과 소프트웨어 부품을 조립하여 여러 응용 구성 가능 대규모 실행 시스템 및 프로세스에도 적용 가능 다양한 도구들을 함께 제공 RVIZ, RQT, Gazebo 다양한 os 환경에서 통일된 방법으로 상호작용을 구현하는 것이 가능 linux, os x, windows, android, ios 표준화된 통신 프로트콜을 따르는 이기종간의 메시지 교환이 가능 ROS 도구RVIZ 시각화 도구 센서데이터를 비롯한 주변환경 변화를 시각화RQT Qt 기반의 GUI 응용 개발 도구 노드 연결 정보를 그래프로 표현 사용자 상호작용을 UI를 갖춘 응용 개발에 이용 rqt_graph 노드와 토픽의 관계 정보를 그래프로 출력 $ rqt_graphGAZEBO 물리 엔진 기반의 3차원 시뮬레이터 시뮬레이터 제작 및 모델링에 이용ros버전 ros indigo ros kinetic ros melodic ros에서의 통신 - 노드(node) → 토픽(topic) → 노드(node) - 프로세스 → 메시지 → 프로세스ros 핵심 기능노드간 통신을 기반으로 전체 시스템을 구동시킬 수 있다. 노드란 하드웨어 부품 또는 소프트웨어 모듈에 하나씩 할당되는 프로세스이다. 노드는 os의 도움을 받아 하드웨어 장치들을 제어한다. 노드들은 마스터의 도움을 받아 서로 메시지를 주고 받는다. 마스터는 노드간의 만남을 주선해주는 주선책과 같은 역할을 한다.서로 분리된 하드웨어 장치 안에 있는 노드들이 네트워크 연결을 통해 서로 통신하면서 하나의 단일 시스템으로서 동작하는 것이 가능하다.ros 구현 사례 라이다 + 카메라 + 모터 + sw모듈1 + sw모듈2 이는 하드웨어 장치 3개와 소프트웨어 모듈 2개를 함께 엮어서 원하는 기능을 구현 라이다와 카메라 정보를 상황인지 sw가 분석한 후 결과를 운전판단 sw로 보내고, 운전판단 sw가 제어명령을 생성해서 모터로 보내 차량을 움직이게 함 급제동 sw노드를 한 개 더 만들어서 라이다를 통해 거리가 너무 가까워지면 또는 급박한 상황일 때 빠르게 모터를 제어한다.기본 용어마스터(master) 서로 다른 노드들 사이의 통신을 총괄 관리 통상 ROS Core이라 부른다.노드(node) 실행가능한 최소의 단위, 프로세스로 이해할 수 있다. ROS에서 발생하는 통신(메시지 송/수신)의 주체 HW장치에 대해 하나씩의 노드, SW모듈에 대해 하나씩의 노드 할당토픽(Topics) 노드와 노드가 주고받는 데이터 토픽 안에 들어 있는 실제 데이터를 메시지라 부른다. 예: 센서데이터, 카메라 이미지 ,액츄에이터 제어명령…발행자(publisher) 특정 토픽에 메시지를 담아 외부로 송신하는 노드 예 : 센서, 카메라, 모터제어 알고리즘…구독자(subscribers) 특정 토픽에 담겨진 메시지를 수신하는 노드 예 : 액츄에이터 제어기, 데이터 시각화 도구 …패키지 packages 하나 이상의 노드와 노드의 실행을 위한 정보 등을 묶어 놓은 단위 노드, 라이브러리, 데이터, 파라미터 등을 포함ROS 응용 예 센서각각 어느 토픽에 어떤 형태의 메시지를 발행하는지가 정해져 있다. 액츄에이터어느 토픽에 어떤 메시지를 발행하면 어떻게 동작하는지가 정해져 있다.ROS 노드간 통신 기본 과정통신이 이루어지기 이전에 통신을 원하는 노드는 마스터에 의뢰하여 연결해야 하는 노드의 정보(주소)를 얻어오고, 서로 접속정보를 교환한다.통신환경 구축이 완료되고 나면 노드간 통신은 마스터를 거치지 않고 직접 이루어진다. 두 노드에는 이름과 번호, 역할이 지정되어 있다.ros노드간 통신의 두 가지 방식1. 토픽 방식의 통신 일방적이고 지속적인 메시지 전송 1:1 뿐만 아니라 1:N , N:N 통신도 가능 자율주행시 이 방식을 사용함2. 서비스 방식의 통신 서버가 제공하는 서비스에 클라이언트가 요청을 보내고 응답을 받는 방식 양방향 통신, 일회성 메시지 송수신ROS 시나리오 마스터 (roscore) 시동 통신이 이루어지려면 우선 roscore가 실행되고 있어야 한다. $ roscore 구독자(subscriber) 노드 구동 특정 토픽(topic)에 발행되는 메시지를 수신하기를 요청 발행자(publisher) 노드 구동 특정 토픽(topic) 메시지를 발행하겠다는 의사를 전달 토픽 이름의 특정 타입을 원하는 사람이 있으면 발행하라는 것을 마스터에게 전달 노드 정보 전달 마스터가 발행자 정보를 구독자에게 전달 노드간 접속 요청: 마스터로부터 받은 정보를 통해 구독자 노드가 발행자 노드에게 TCPROS 접속을 요청 노드간 접속 요청 발행자 노드가 자신의 TCPROS URI 주소와 포트번호를 전송 TCPROS 접속 TCPROS를 이용하여 퍼블리셔 노드와 서브스크라이버 노드 사이에 소켓(socket) 연결이 이루어짐 메시지 전송 발행자 노드가 구독자 노드에게 메시지 전송(토픽) 메시지 전송 반복 접속이 한번 이루어진 뒤에는 별도 절차없이 지속적으로 메시지 송수신 ROS 명령어 ros 셀 명령어 roscd: 지정한 ros패키지 폴더로 이동 rosls: ros 패키지 파일 목록 확인 rosed: ros 패키지 파일 편집 roscp: ros 패키지 파일 복사 ros 실행 명령어 roscore: master+rosout+parameter server rosrun: 패키지 노드 실행 roslaunch: 패키지 노드를 여러 개 실행 rosclean: ros 로그 파일 검사 및 삭제 ros 정보 명령어 rostopic: 토픽 정보 확인 rosnode: 노드 정보 확인 rosbag: 메시지 기록, 재생 ros catkin 명령어 catkin_create_pkg: catkin 빌드 시스템으로 패키지 생성 catkin_make: catkin 빌드 시스템으로 빌드 ros package 명령어 rospack: 패키지와 관련된 정보 보기 rosinstall: 추가 패키지 설치 rosdep: 해당 패키지의 의존성 파일 설치 🎈주요 명령어ros 기본 시스템이 구동되기 위해 필요한 프로그램들 실행$ roscorerosrun [package name] [node_name]: 패키지에 있는 노드를 선택 실행$ rosrun turtlesim turtlesim_noderosnode [info...]: 노드의 정보를 표시$ rosnode info node_namerostopic [option]: 토픽의 정보를 출력 (메시지 타입, 노드 이름 등)$ rostopic info /imu roslaunch [package_name] [file.launch]: 파라미터 값과 함께 노드를 실행$ roslaunch usb_cam usb_cam-test.launchROS 설치 ubuntu 버전 : ubuntu 18.041.ros를 제공하는 software repository 등록$ sudo sh -c &#39;echo &quot;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main&quot; &amp;gt; /etc/apt/sources.list.d/ros-latest.list&#39;$ cat /etc/apt/sources.list.d/ros-latest.list2.apt key 셋업$ sudo apt-key adv --keyserver &#39;hkp://keyserver.ubuntu.com:80&#39; --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C6543.패키지 설치$ sudo apt update필요한 추가적인 것들 다 설치$ sudo apt install ros-melodic-desktop-full 4.쉘 환경 설정$ echo &quot;source /opt/ros/melodic/setup.bash&quot; &amp;gt;&amp;gt; ~/.bashrc$ source ~/.bashrc5.추가로 필요한 도구 설치$ sudo apt install python-rosdep python-rosinstall python-rosinstall-generator python-wstool build-essential6.rosdep 초기화$ sudo rosdep init$ sudo rosdep update참고 자료 - ROS 공식 사이트roscore 실습1.roscore (나중에 &amp;lt;ctrl+c&amp;gt;를 누르면 종료$ roscore2.(다른 터미널에서) resnode list$ rosnode list오류가 난다면$ apt update업그레이드할 것이 있다면 업그레이드 진행$ apt upgrade$ rosnode listROS 작업 공간 생성$ mkdir -p ~/xycar_ws/src$ cd xycar_ws$ catkin_make이 작업 공간을 ros 프로그래밍 공간으로 사용할 것이다./src: 소스코드는 여기에 넣음catkin_make ros의 workspace에서 새로운 소스코드 파일이나 패키지가 만들어지면 catkin_make를 명령한다. 이 명령을 통해 빌드(build)작업을 진행한다. ROS 프로그래밍 작업과 관련 있는 모든 것들을 깔끔하게 정리해서 최신 상태로 만드는 작업 환경 정리와 같음 ros 작업환경 설정ros 작업에 필요한 환경변수 설정을 위해 .bashrc 파일 수정$ sudo gedit ~/.bashrc아래 내용 작성# 자주 쓰는 명령어를 정의해놓는 것alias cm=&#39;cd ~/xycar_ws &amp;amp;&amp;amp; catkin_make&#39; alias cs=’cd ~/xycar_ws/src’source /opt/ros/melodic/setup.bashsource ~/xycar_ws/devel/setup.bash# 환경 변수 설정export ROS_MASTER_URI=http://localhost:11311export ROS_HOSTNAME=localhost그다음 수정한 내용을 시스템에 반영하기 위해 다음 코드를 실행한다.$ source ~/.bashrc환경 변수가 잘 적용되었는지 확인$ printenv | grep ROSROS_ETC_DIR=/opt/ros/melodic/etc/rosROS_ROOT=/opt/ros/melodic/share/ros...ROS예제 프로그램 구동 실습publisher 이름: teleop_turtle 보내는 토픽: turtle1/cmd_vel 담고 있는 메시지: geometry_msgs/Twistsubscriber 이름: turtlesim 받고자하는 토픽: turtle1/cmd_vel 토픽이 담고 있어야 하는 메시지: geometry_msgs/Twist실행총 4개의 터미널을 열어야 한다.1.마스터$ roscore2.ros node의 확인$ rosnode list/rosout아무것도 구동하지 않았기에 나오지 않는다. rosrun $ rosrun turtlesim turtlesim_node roscore와 turtlesim_node가 실행중인 상태일 때 $ rosrun turtlesim turtle_teleop_key 4번 터미널에서 키보드 방향키를 누르면 거북이가 움직인다.rosnode list를 확인하면 3개의 노드가 보인다.$ rosnode list/rosout/teleop_turtle/turtlesim또는 rqt_graph를 통해 시각화 할 수 있다.$ rqt_graph어떤 토픽이 이동되는지 보려면 , 또는 그 토픽의 메시지를 보려면$ rostopic list/rosout/rosout_agg/turtle1/cmd_vel/turtle1/color_sensor/turtle1/pose$ rostopic echo /turtle1/cmd_vellinear: x: 0.0 y: 0.0 z: 0.0angular: x: 0.0 y: 0.0 z: 2.0---linear: x: 0.0 y: 0.0 z: 0.0angular: x: 0.0 y: 0.0 z: -2.0---날아다니는 topic list에 대해 알아보기 위해 rostopic을 실행한다. 토픽 - 메시지 내용 - 보내거나 받는 노드개수 - pub/sub$ rostopic list -vPublished topics: * /turtle1/color_sensor [turtlesim/Color] 1 publisher * **/turtle1/cmd_vel [geometry_msgs/Twist] 1 publisher** * /rosout [rosgraph_msgs/Log] 2 publishers * /rosout_agg [rosgraph_msgs/Log] 1 publisher * /turtle1/pose [turtlesim/Pose] 1 publisherSubscribed topics: * **/turtle1/cmd_vel [geometry_msgs/Twist] 1 subscriber** * /rosout [rosgraph_msgs/Log] 1 subscriberrostopic type을 통해 메시지의 타입과 구성을 볼 수 있다.$ rostopic type /turtle1/cmd_velgeometry_msgs/Twist$ rosmsg show geometry_msgs/Twistgeometry_msgs/Vector3 linear float64 x float64 y float64 zgeometry_msgs/Vector3 angular float64 x float64 y float64 z토픽 직접 발행하기토픽 1번만 보내기 rostopic pub [-발행 횟수] [토픽 이름] [메시지 타입] – ‘메시지 내용’1번만 보내기$ rostopic pub -1 /turtle1/cmd_vel geometry_msgs/Twist -- &#39;[2.0,0.0,0.0]&#39; &#39;[0.0,0.0,1.8]&#39;publishing and latching message for 3.0 seconds실행 전실행 후토픽 계속 보내기 rostopic pub [토픽 이름] [메시지 타입] [-r #Hz] — ‘메시지 내용’one time per 1 second$ rostopic pub /turtle1/cmd_vel geometry_msgs/Twist -r 1 -- &#39;[2.0,0.0,0.0]&#39; &#39;[0.0,0.0,1.8]&#39;&amp;lt;ctrl+C&amp;gt; 누르면 종료된다.ROS package패키지 : ROS에서 개발되는 소프트웨어를 논리적 묶음으로 만든 것 e.g. 보행자 추적 패키지, 차선인식 패키지 등..ROS 패키지 만들기코드를 작성하는 명령어로는 gedit(메모장)을 사용하면 된다. 이를 실행하면 해당 디렉토리의 파일이 존재하면 그 파일을 열고, 없으면 새로 생성하여 열어준다.$ gedit pub.py$ gedit sub.py1.패키지 담을 디렉토리로 이동$ cd ~/xycar_ws/src2.패키지 새로 만들기 catkin_create_pkg [패키지 이름] [이 패키지가 의존하고 있는 다른 패키지들 나열]$ catkin_create_pkg my_pkg1 std_msgs rospy3.빌드(갱신)$ catkin_make4.만들어진 패키지 확인$ rospack find my_pkg1/home/jaehoyoon/xycar_ws/src/my_pkg1 의존하고 있는 패키지 출력$ rospack depends1 my_pkg1rospystd_msgs패키지로 이동$ roscd my_pkg1~/xycar_ws/src/my_pkg1$5.소스코드를 추가한 후 그것을 실행하려고 할 때, 파일 타입이 실행 권한이 없으면 실행이 불가능하다. 따라서 타입을 바꿔줘야 한다.$ ls -l합계 8-rw-rw-r-- 1 jaehoyoon jaehoyoon 397 2월 28 20:52 pub.py-rw-rw-r-- 1 jaehoyoon jaehoyoon 285 2월 28 20:54 sub.py $ chmod +x pub.py sub.py$ ls -l합계 8-rwxrwxr-x 1 jaehoyoon jaehoyoon 397 2월 28 20:52 pub.py-rwxrwxr-x 1 jaehoyoon jaehoyoon 285 2월 28 20:54 sub.pyROS 패키지 실행publisher터미널 4개로 해서 roscore pub.py 실행 : rosrun my_pkg1 pub.py (my_pkg1안에 있는 pub.py를 실행) turtlesim 노드 실행 : rosrun turtlesim turtlesim_node rqt_graph 실행$ roscore$ rosrun turtlesim turtlesim_node$ rosrun my_pkg1 pub.py 노드 상태 확인$ rqt_graph$ rosnode list/my_node_59758_1646054585073/rosout/turtlesimsubscriber우선, turtle이 어떤 토픽에 어떤 메시지를 발행하고 있는지 알아봐야 한다.이동되고 있는 전체 토픽$ rostopic list/rosout/rosout_agg/turtle1/cmd_vel/turtle1/color_sensor/turtle1/pose어떤 타입이고 어떤 값을 가지는지 보기$ rostopic type /turtle1/poseturtlesim/Pose어떤 메시지를 가지는가$ rosmsg show turtlesim/Posefloat32 xfloat32 yfloat32 thetafloat32 linear_velocityfloat32 angular_velocity토픽에 어떤 메시지가 발행되고 있는지 실제 값 보기$ rostopic echo /turtle1/posex: 4.42990398407y: 6.82239103317theta: -1.73598301411linear_velocity: 2.0angular_velocity: 1.79999995232---x: 4.425552845y: 6.79068851471theta: -1.70718300343linear_velocity: 2.0angular_velocity: 1.79999995232---x: 4.42211675644y: 6.75887346268theta: -1.67838299274linear_velocity: 2.0angular_velocity: 1.79999995232---...터미널 4개 실행 roscore turtlesim 노드 실행 pub.py sub.py$ roscore$ rosrun turtlesim turtlesim_node$ rosrun my_pkg1 pub.py$ rosrun my_pkg1 sub.py[INFO] [1646055414.320049]: /my_listener_59983_1646055411189Location: 4.47,6.996734[INFO] [1646055414.336577]: /my_listener_59983_1646055411189Location: 4.46,6.966142[INFO] [1646055414.351566]: /my_listener_59983_1646055411189Location: 4.45,6.935292[INFO] [1646055414.370935]: /my_listener_59983_1646055411189Location: 4.45,6.904211[INFO] [1646055414.383501]: /my_listener_59983_1646055411189Location: 4.44,6.872923[INFO] [1646055414.400245]: /my_listener_59983_1646055411189Location: 4.43,6.841455[INFO] [1646055414.415747]: /my_listener_59983_1646055411189Location: 4.43,6.809832[INFO] [1646055414.431543]: /my_listener_59983_1646055411189Location: 4.42,6.778082[INFO] [1646055414.448686]: /my_listener_59983_1646055411189Location: 4.42,6.746230[INFO] [1646055414.463205]: /my_listener_59983_1646055411189Location: 4.42,6.714302[INFO] [1646055414.480053]: /my_listener_59983_1646055411189Location: 4.42,6.682326...$ rqt_graphrostopic echo를 실행하면 노드가 1개 더 생성된다.roslaunch*.launch 파일 내용에 따라 여러 노드들을 한꺼번에 실행시킬 수 있다. 파라미터 값을 노드에 전달할 수도 있다.roslaunch [패키지이름] [실행시킬 launch 파일 이름] roslaunch my_pkg1 aaa.launch*.launch 파일실행시킬 노드들의 정보가 XML 형식으로 기록되어 있다. launch 파일은 my_pkg1/launch/*.launch 경로로 생성해야 한다. node 태그: 실행할 노드 정보를 입력할 때 사용되는 태그 &amp;lt;node pkg=”패키지 명” type=”노드가 포함된 소스파일 명” name=”노드 이름” /&amp;gt; 속성 pkg: 실행시킬 노드의 패키지 이름을 입력하는 속성 → 반드시 빌드된 패키지의 이름을 입력해야 함 type: 노드의 소스코드가 담긴 파이썬 파일의 이름을 입력하는 속성 → 이때 파이썬 py파일은 반드시 실행권한이 있어야 함 실행권한이 없으면 ERROR: cannot launch node of type ~ 이라 나옴 name: 노드의 이름을 입력하는 속성 → 소스코드에서 지정된 노드의 이름을 무시하고 launch파일에 기록된 노드의 이름으로 노드가 실행된다. include 태그: 다른 launch 파일을 불러오고 싶을 때 사용하는 태그 &amp;lt;include file=”같이 실행할 *.launch 파일 경로” /&amp;gt; 이 때 &amp;lt;include file=”$(find usb_cam)/src/launch/aaa.launch” /&amp;gt; 라고 작성할 수 있는데, $()는 함수를 실행하는 것으로 find usb_cam을 실행한 결과를 가져오는 것 속성 file: 함께 실행시킬 *.launch 파일의 경로를 입력하는 속성 launch 파일 실행launch 파일 생성my_pkg1$ mkdir launchlaunch 파일 작성$ cd launch$ gedit pub-sub.launch&amp;lt;!-- pub-sub.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;turtlesim&quot; type=&quot;turtlesim_node&quot; name=&quot;turtlesim_node&quot; /&amp;gt; &amp;lt;node pkg=&quot;my_pkg1&quot; type=&quot;pub.py&quot; name=&quot;pub_node&quot; /&amp;gt; &amp;lt;node pkg=&quot;my_pkg1&quot; type=&quot;sub.py&quot; name=&quot;sub_node&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt; 💡 roslaunch를 할 때는 roscore를 할 필요가 없다. 스스로 roscore가 작동되기 때문이다.$ roslaunch my_pkg1 pub-sub.launch ... logging to /home/jaehoyoon/.ros/log/2089d226-989f-11ec-8c28-000c2983f177/roslaunch-jaeho-vm-60582.logChecking log directory for disk usage. This may take a while.Press Ctrl-C to interruptDone checking log file disk usage. Usage is &amp;lt;1GB.started roslaunch server http://localhost:44745/SUMMARY========PARAMETERS * /rosdistro: melodic * /rosversion: 1.14.12NODES / pub_node (my_pkg1/pub.py) sub_node (my_pkg1/sub.py) turtlesim_node (turtlesim/turtlesim_node)auto-starting new masterprocess[master]: started with pid [60592]ROS_MASTER_URI=http://localhost:11311setting /run_id to 2089d226-989f-11ec-8c28-000c2983f177process[rosout-1]: started with pid [60603]started core service [/rosout]process[turtlesim_node-2]: started with pid [60606]process[pub_node-3]: started with pid [60611]process[sub_node-4]: started with pid [60612][INFO] [1646056978.076219]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.092649]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.107839]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.123153]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.140403]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.156125]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.172139]: /sub_nodeLocation: 5.54,5.544445[INFO] [1646056978.187652]: /sub_nodeLocation: 5.54,5.544445...$ rqt_graphlaunch 파일의 유용한 tag USB 카메라 구동과 파라미터 세팅을 위한 launch 파일launch를 통해 파라미터의 값을 셋팅할 수 있다. 패키지 이름: usb_cam&amp;lt;launch&amp;gt; &amp;lt;node name=&quot;usb_cam&quot; pkg=&quot;turtlesim&quot; type=&quot;cam_node&quot; output=&quot;screen&quot;&amp;gt; &amp;lt;param name=&quot;autoexposure&quot; value=&quot;false&quot;/&amp;gt; &amp;lt;param name=&quot;exposure&quot; value=&quot;180&quot;/&amp;gt; &amp;lt;param name=&quot;image_width&quot; value=&quot;640&quot;/&amp;gt; &amp;lt;param name=&quot;image_height&quot; value=&quot;480&quot;/&amp;gt; &amp;lt;param name=&quot;camera_frame_id&quot; value=&quot;usb_cam&quot;/&amp;gt; &amp;lt;/node&amp;gt;&amp;lt;/launch&amp;gt;type=”cam_node” py가 없는 걸로 보아 c나 c++로 되어 있음을 알 수 있다.autoexposure이라는 파라미터의 값을 false, exposure 파라미터를 180, 이미지 크기를 [640,480]으로 정하는 것이다.paramROS 파라미터 서버에 변수를 등록하고 그 변수에 값을 설정하기 위한 태그다.&amp;lt;param name=”변수 이름” type=”변수 타입” value=”변수 값” /&amp;gt; name: 등록할 변수의 이름 type(선택): 등록할 변수의 타입, 사용할 수 있는 타입의 종류는 str, int, double, bool, yaml value: 등록할 변수의 값param을 통해 ROS파라미터 서버에 등록된 변수는 노드 코드(소스코드)에서 불러와 사용할 수 있다. *.launch ⇒ &amp;lt;param name=”age” type=”int” value=”11” /&amp;gt; *.py ⇒ import rospy; rospy.init_node(’노드’); print(rospy.get_param(’~age’))private parameter은 앞에 ~를 붙여야 한다.실습1.launch 파일을 새로 만든다.$ cp pub-sub.launch pub-sub-param.launch 패키지 이름 = my_pkg1 타입 (소스코드 파일) = pub_param.py 노드 이름 = node_param 파라미터 이름 = circle_size 파라미터 값 = 2&amp;lt;!-- pub-sub-param.launch --&amp;gt;&amp;lt;launch&amp;gt; &amp;lt;node pkg=&quot;turtlesim&quot; type=&quot;turtlesim_node&quot; name=&quot;turtlesim_node&quot; /&amp;gt; &amp;lt;node pkg=&quot;my_pkg1&quot; type=&quot;pub_param.py&quot; name=&#39;node_param&#39;&amp;gt; &amp;lt;param name=&quot;circle_size&quot; value=&quot;2&quot; /&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node pkg=&quot;my_pkg1&quot; type=&quot;sub.py&quot; name=&quot;sub_node&quot; output=&quot;screen&quot; /&amp;gt;&amp;lt;/launch&amp;gt;2.pub_param.py 만들기원래 만들었던 pub.py를 복사해서 수정한다.$ cp pub.py pub_param.py#!/usr/bin/env pythonimport rospyfrom geometry_msgs.msg import Twistrospy.init_node(&#39;my_node&#39;, anonymous=True)pub = rospy.Publisher(&#39;/turtle1/cmd_vel&#39;,Twist,queue_size=10)msg = Twist()# msg.linear.x = 2.0# --- 추가 사항 --- #linear_X = rospy.get_param(&#39;~circle_size&#39;)msg.linear.x = linear_X# ----------------- #msg.linear.y = 0.0msg.linear.z = 0.0msg.angular.x = 0.0msg.angular.y = 0.0msg.angular.z = 1.8rate = rospy.Rate(1)while not rospy.is_shutdown(): pub.publish(msg) rate.sleep()msg.linear.x 대신 linear_X를 통해 즉, circle_size를 통해 linear.x를 받는다. value = 3 value = 1rqt_graph참고 자료 https://goldenboylife.com/elementor-2890/ ROS 공식 사이트" }, { "title": "[데브코스] 2주차 - linux 기초(File System)", "url": "/posts/mkfs/", "categories": "Classlog, devcourse", "tags": "linux, file-system, devcourse", "date": "2022-02-25 13:00:00 +0900", "snippet": "오답노트file systemfs(file system)는 os에서 매우 큰 부분을 차지한다. 디렉토리도 파일 시스템의 일종이다.파일 시스템 타입 linux: xfs, ext4 windows: ntfs, exfat, fat32새로운 FS 만드는 법 partitioning command: fdisk, parted file sysyem command: mkfs(==format), 만든 후 checking command: fsck or xfs_* mount command: mount / umount mount를 자동으로 하기 위해 설정하는 파일인 /etc/fstab partition(구획) 물리적, 논리적 파티션이 존재한다. 예전에는 물리적, 즉 4TB를 둘로 나눴는데 1개를 거의 안써서 1개를 1/3로 나누고 싶다면 logical로 묶어야 했다. 최근에는 logical partition을 선호한다. 대표적으로 logical volume(LVM)을 사용하는 방법이 있다. 그러나 여기서는 physical만 다룰 것이다.physical disk 나누는 방법디스크를 나누기 위해서는 labeling 작업을 해야하는데 label type이 2가지가 있다. DOS 방식: 고전 2tb의 제한 tool: fdisk를 사용 GPT 방식: DOS레이블 문제를 개선하기 위해 나온 새로운 방식 용량 제한이 없음 tool: parted / gparted or gdisk를 사용 dos가 고전이긴 하나 2tb가 넘어가지 않는다면 dos도 많이 사용하긴 한다.DOS labeled disk (MBR)DOS partition의 종류 primary partition(주 파티션) 최대 4 partitions per disk extended partition(확장 파티션) 4개 이상의 파티션이 필요할 때 주 파티션 대신에 1개를 만들 수 있다. extended partition은 다시 여러 개의 logical drive로 나눌 수 있다. logical drive(논리 드라이브)파티션이 4개 이하여도 확장, 논리 드라이브를 만들 수 있다.fdisk# fdisk -lfdisk -l를 대체하는 요즘 용어# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...sda 8:0 0 50G 0 disk `-sda1 8:1 0 50G 0 part /sr0 11:0 1 2.3G 0 rom /media/jaehoyoon/Ubuntu 18.04.6 LTS amd64+ block device저장 장치를 이르는 용어이다. 입출력 장치는 character device모든 디바이스는 /dev/ 에 존재한다. 그 안에 sd가 붙으면 SCSI(=scuzzy) disk 디바이스들은 순서대로 a,b,c,d… e.g. 두개가 있다면 sda, sdb가 있는 것 NVME SSD사용하면 NVME로 뜸fdiskcommand 옵션 d: 파티션 삭제 l: 알려진 파티션 ID(다른 파티션 타입) n: 새로운 파티션 생성 p: 현재 파티션 상태 출력 t: 파티션 ID 를 변경 q: 변경된 상태를 저장하지 않고 종료 w: 변경된 상태로 저장하고 종료실습 중에는 저장하지 않고 나가야 한다. 저장하지만 않으면 실제로 삭제되는 것도 아니라서, 문제가 안생긴다.# fdisk /dev/sda...# 현재 파티션 상태 출력command(m for help) : p...Device Boot Start End Sectors Size Id Type/dev/sda1 * 2048 104855551 104853504 50G 83 Linux# 삭제command(m for help) : dSelected partition 1Partition 1 has been deleted.# 생성Command (m for help): nPartition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions)Select (default p): pPartition number (1-4, default 1): First sector (2048-104857599, default 2048): Last sector, +sectors or +size{K,M,G,T,P} (2048-104857599, default 104857599): +4GCommand (m for help): pDevice Boot Start End Sectors Size Id Type/dev/sda1 2048 8390655 8388608 4G 83 Linux# extended를 만들 때는 대체로 4번을 지목해주는 것이 좋다.Command (m for help): nPartition type p primary (1 primary, 0 extended, 3 free) e extended (container for logical partitions)Select (default p): ePartition number (2-4, default 2): 4First sector (8390656-104857599, default 8390656): Last sector, +sectors or +size{K,M,G,T,P} (8390656-104857599, default 104857599):Command (m for help): nAll space for primary partitions is in use.Adding logical partition 5First sector (8392704-104857599, default 8392704): Last sector, +sectors or +size{K,M,G,T,P} (8392704-104857599, default 104857599):Command (m for help): pDevice Boot Start End Sectors Size Id Type/dev/sda1 2048 8390655 8388608 4G 83 Linux/dev/sda4 8390656 104857599 96466944 46G 5 Extended/dev/sda5 8392704 104857599 96464896 46G 83 Linux# 타입 변경Command (m for help): tSelected partition 1Hex code (type L to list all codes): l0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden or c6 DRDOS/sec (FAT- 4 FAT16 &amp;lt;32M 40 Venix 80286 85 Linux extended c7 Syrinx 5 Extended 41 PPC PReP Boot 86 NTFS volume set da Non-FS data 6 FAT16 42 SFS 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS/exFAT 4d QNX4.x 88 Linux plaintext de Dell Utility 8 AIX 4e QNX4.x 2nd part 8e Linux LVM df BootIt 9 AIX bootable 4f QNX4.x 3rd part 93 Amoeba e1 DOS access a OS/2 Boot Manag 50 OnTrack DM 94 Amoeba BBT e3 DOS R/O b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad hi ea Rufus alignment e W95 FAT16 (LBA) 53 OnTrack DM6 Aux a5 FreeBSD eb BeOS fs f W95 Ext&#39;d (LBA) 54 OnTrackDM6 a6 OpenBSD ee GPT 10 OPUS 55 EZ-Drive a7 NeXTSTEP ef EFI (FAT-12/16/11 Hidden FAT12 56 Golden Bow a8 Darwin UFS f0 Linux/PA-RISC b12 Compaq diagnost 5c Priam Edisk a9 NetBSD f1 SpeedStor 14 Hidden FAT16 &amp;lt;3 61 SpeedStor ab Darwin boot f4 SpeedStor 16 Hidden FAT16 63 GNU HURD or Sys af HFS / HFS+ f2 DOS secondary 17 Hidden HPFS/NTF 64 Novell Netware b7 BSDI fs fb VMware VMFS 18 AST SmartSleep 65 Novell Netware b8 BSDI swap fc VMware VMKCORE 1b Hidden W95 FAT3 70 DiskSecure Mult bb Boot Wizard hid fd Linux raid auto1c Hidden W95 FAT3 75 PC/IX bc Acronis FAT32 L fe LANstep 1e Hidden W95 FAT1 80 Old Minix be Solaris boot ff BBTHex code (type L to list all codes): ^CHex code (type L to list all codes): 82Command (m for help): pDevice Boot Start End Sectors Size Id Type/dev/sda1 * 2048 104855551 104853504 50G 82 Linux swap / SolarisCommand (m for help): tSelected partition 1Hex code (type L to list all codes): 7Changed type of partition &#39;Linux swap / Solaris&#39; to &#39;HPFS/NTFS/exFAT&#39;.Command (m for help): p...Device Boot Start End Sectors Size Id Type/dev/sda1 * 2048 104855551 104853504 50G 7 HPFS/NTFS/exFAT+ Mask fs저장 장치를 아주 커다란 종이로 생각했을 때, 이를 통째로 쓰기 힘들 것이다. partitioning : 종이를 몇개로 자르는 작업 make FS: 파티션된 종이를 A4 같은 규격화된 형태로 잘라서 분철하는 것과 비슷하다. 분철하면 권수와 페이지 번호가 만들어짐 = 인덱싱 가능 방식, 밑줄, 칸의 간격 혹은 자물쇠를 설치하는 등의 세부 방식에 대한 것이 FS type ext4,xfs,ntfs,fat32 등의 타입이 주로 사용됨 그러나 IOT기기에는 ext4를 주로 사용하고, 고성능 SSD를 사용하는 경우 xfs를 사용함 + ext4 (extended file system 4)ext4 : 대부분의 linux에서 사용하는 FS특징: 저널링 지원 : 무엇을 적는, 파일이 깨지는 것 등을 쓰고 지움 연속된 파일의 접근, 작은 파일들의 접근이 빠른 장점 저성능의 I/O에서 효율이 높음, IoT, HD 삭제된 파일을 복구+ xfsxfs : 저널링 기반의 대용량 파일 시스템특징: online 상태(사용중인 상태)에서 확장이 가능 대용량 파일 처리 시 성능 좋음 삭제된 파일을 복구할 수 없음 깨진 파일 시스템 복구를 위한 명령어로 xfs_repair를 제공mkfs (make file system)추가한 디스크 확인 방법 virtual machines을 사용하는 경우 virtual machines setting에서 cd/dvd에서 hard-disk, add 누르고 csci 등 선택 lsblk 추가한 디스크가 안보이면 코드를 쳐야 하는데, 공백 지켜서 쳐야 한다.# ls /sys/class/scsi_host/ | while read hostdev ; do echo &quot;- - -&quot; &amp;gt; /sys/class/scsi_host/$hostdev/scan ; done새로 추가한 디스크에 fdisk로 partition을 만들어보자.1.fdisk 로 디스크 상태 확인 fdisk /dev/sdb```bash fdisk /dev/sdb Command (m for help): nPartition type p primary (1 primary, 0 extended, 3 free) e extended (container for logical partitions)Select (default p): pPartition number (2-4, default 2): First sector (2099200-20971519, default 2099200): Last sector, +sectors or +size{K,M,G,T,P} (2099200-20971519, default 20971519): +1G Created a new partition 2 of type ‘Linux’ and of size 1 GiB.Command (m for help): nPartition type p primary (1 primary, 0 extended, 3 free) e extended (container for logical partitions)Select (default p): pPartition number (2-4, default 2): First sector (2099200-20971519, default 2099200): Last sector, +sectors or +size{K,M,G,T,P} (2099200-20971519, default 20971519): +2GCreated a new partition 2 of type ‘Linux’ and of size 2 GiB.Command (m for help): p…Device Boot Start End Sectors Size Id Type/dev/sdb1 2048 2099199 2097152 1G 83 Linux/dev/sdb2 2099200 6293503 4194304 2G 83 LinuxCommand (m for help): w&amp;lt;br&amp;gt;2.특정 디스크 파티션 파일 시스템(타입) 지정- mkfs [-t fstype] [options] &amp;lt;device&amp;gt;```bash# mkfs -t ext4 /dev/sdb2mke2fs 1.44.1 (24-Mar-2018)Creating filesystem with 524288 4k blocks and 131072 inodesFilesystem UUID: 67d17692-1557-4b47-806f-8276a9f5eac0Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done3.FS checking fsck [-y] [options] [-t fstype] 처음 파일 시스템을 만들 때는 그 파일 시스템을 켜주기 위해 체크해야 한다. 일반적으로 offline된 fs를 체크해야 한다. online 상태의 fs를 검사하는 경우에는 안전을 보장할 수 없다.fsck의 종료를 확인하기 위해 종료코드를 쳐봐야 한다. `$?`는 바로 이전에 했던 명령어의 결과값 , 0이 아니면 실패# fsck -y -t ext4 /dev/sdb2fsck from util-linux 2.31.1e2fsck 1.44.1 (24-Mar-2018)/dev/sdb2: clean, 11/131072 files, 26156/524288 blocks# echo $?0이 때, sum 값으로 출력되기 때문에 3이 나온다면 1+2# man fsckThe exit code returned by fsck is the sum of the following conditions: 0 No errors 1 Filesystem errors corrected 2 System should be rebooted 4 Filesystem errors left uncorrected 8 Operational error 16 Usage or syntax error 32 Checking canceled by user request 128 Shared-library errorSWAP스왑 공간은 메모리가 부족할 때 디스크에다가 메모리의 일부분을 바꿔치기하는 것이다.(ms windows에는 pagefile이라 부름) swap in/out mkswap(스왑 공간 작성), swapon(활성), swapoff(비활성) mkswap [-L label] swapon &amp;lt;device swapfile -L label&amp;gt; swapoff &amp;lt;device swapfile -L label&amp;gt; 스왑 공간 작성 스왑공간으로 해주기 위해 fdisk를 통해 83을 82로 바꿔야 하지만, 안바꿔줘도 쓸수는 있다.# mkswap -L swapfs2 /dev/sdb1Setting up swapspace version 1, size = 1024 MiB (1073737728 bytes)LABEL=swapfs2, UUID=52154162-328f-4596-b6b8-d721e438a0cb일반 파일을 스왑 공간으로 사용 (일시적 스왑파일로서)# dd if=/dev/zero of =./swapfile1 bs=1024 count=262144이 때, UUID는 겹치지 않기 위해 만든 ID이다.if= input file of = output file : if를 읽어서 of를 만들어라, bs=block size, count = 개수 bs * count = 용량스왑 공간 활성 / 비활성# swapon /dev/sdb1파일 올리기# swapon ./swapfile1스왑공간을 레이블로 선언# swapon -L swapfs2스왑 공간 상태 확인# cat /proc/swapsFilename Type Size Used Priority/swapfile file 2097148 524 -2/dev/sdb1 partition 1048572 0 -3특정 1개 공간 중지# swapoff /dev/sdb1전체 중지# swapoff -apriority는 2가 우선 순위로 2-&amp;gt;3-&amp;gt;4…, 우선 순위를 먼저 사용하고 다 사용하면 다음꺼를 사용한다. fstab에 등록된 스왑 공간 확인grep으로 /etc/fstab에서 swap이란 문자가 들어간 행을 추출# grep swap /etc/fstab/swapfile none swap sw 0 0 tip: 간혹 몇몇 linux 설치 프로그램이 한국어를 사용하여 설치할 때 파티션 레이블이 한글로 되어 있기도 함, 한글을 다시 영문으로 바꿔주는 것이 좋다. # cat /etc/fstab | grep swapLABLE=?3???^^???Ldevice 확인# lsblkmkswap [-L label] &amp;lt;device&amp;gt;# mkswap -L swapfs1 /dev/sda8fatab에서 swap 파티션의 레이블을 swapfs1으로 수정# vi /etc/fstab Mount란?마운트란 파일 시스템을 탑재하는 것을 의미한다. 마운트는 root directory를 기점으로 시작된다.명령어 mount: 마운트하는 명령 umount: 언마운트 findmnt: 마운트 리스트mount, umountmount [-t fstype] [-o option] [device] fstype은 알아서 판단하므로 안적어줘도 됨 option: rw, ro 등이 있지만 아직 사용하지 않을 예정 device: /dev/sda, /dev/sdb directory: 마운트 대상 지점 (mount point) 즉, device가 붙을 지점 umount &amp;lt;directory device&amp;gt; == or 연산자로 둘 중 아무거나 적어줘도 상관없음 디바이스 이름 sda1 ⇒ usb의 1번째 장치의 첫번째 파티션 sdb5 ⇒ usb의 2번째 장치의 다섯번째 파티션 sde2 ⇒ usb의 5번째 장치의 두번째 파티션실습sdb1은 스왑공간으로 했으니 2로 마운트 할 것이다.순서 mkfs -t ext4 /dev/sdb2 : /dev/sdb2 파티션을 ext4 파일시스템으로 만듦 mkdir /media/backup : media/backup 파일 생성 mount -t ext4 /dev/sdb2 /media/backup : sdb2 파티션을 backup 파일에 ext4 파일시스템 타입으로 마운트# mkfs -t ext4 /dev/sdb2Creating filesystem with 524288 4k blocks and 131072 inodesFilesystem UUID: 6989fcda-bfb6-47a8-9ba8-69fb972b7e1aSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done# mkdir /media/backup# mount -t ext4 /dev/sdb2 /media/backup확인# lsblk...sdb 8:16 0 10G 0 disk ├─sdb1 8:17 0 1G 0 part [SWAP]└─sdb2 8:18 0 2G 0 part /media/backup압축, c는 create, f는 파일명 지정, a 는 auto, /etc 파일을 backup에 etc_backup.tar.gz로 압축# tar cfa /media/backup/etc_backup.tar.gz /etc파일 확인# ls -l /media/backup합계 1820-rw-r--r-- 1 root root 1844734 2월 25 18:13 etc_backup.tar.gzdrwx------ 2 root root 16384 2월 25 18:08 lost+found umount# cd /media/backup~/media/backup# pwd/media/backup~/media/backup# umount /media/backupumount: /media/backup: target is busy해당 디바이스에 들어가 있으면 작업중이라 umount할 수 없다. 따라서 작동되고 있는 프로세스를 죽이던지, 다른데로 옮겨가던지 해야 한다.이럴 때 범인, 즉 작업중인 프로세스를 찾기 위해 fuser &amp;lt;dir&amp;gt;로 PID를 알 수 있다.해당 파일 시스템 아래에서 작동하는 프로세스를 체크, 이를 실행하면 여기서 작동중인 PID를 추출해줌# fuser -c /media/찾아내서 죽이기까지 하려면 # fuser -ck /media/디바이스로도 umount 가능# umount /dev/sdb2# lsblk...sdb 8:16 0 10G 0 disk ├─sdb1 8:17 0 1G 0 part [SWAP]└─sdb2 8:18 0 2G 0 part여기서 주의해야 할 점이 있다.# ls -l /media/backup합계 0새 파일 생성# touch /media/backup/hey.txt리스트를 보면 파일이 존재한다.# ls /media/backuphey.txt마운트# mount /dev/sdb2 /media/backup이 때는 보이지 않는다. 삭제된 것이 아니라 보이지 않을 뿐 umount하면 다시 보임# ls /media/backupetc_backup.tar.gz lost+found # umount /dev/sdb2# ls /media/backupetc_backup.tar.gz hey.txt lost+found 마운트 옵션 사용마운트 테이블 보기# findmnt...├─/media/backup /dev/sdb2 ext4 rw,relatimerw(readwrit)를 ro(readonly)로 리마운트# mount -o remount,rw /dev/sdb2...├─/media/backup /dev/sdb2 ext4 ro,relatime# mount -o remount,ro /dev/sdb2├─/media/backup /dev/sdb2 ext4 rw,relatimeUSB memoryusb 메모리를 장착하면 lsblk에서 장치명이 인식된다. e.g. 현재는 sda, sdb까지 있으면 usb 장치의 이름은 sdcUSB 장치는 mount 명령으로 인식시키면 된다. 그러나 fstype을 알고 있어야 한다. 대부분은 vfat, fat32, exfat등이지만, ntfs이기도 한다. VMware은 사용하는 usb장치가 2.0인지 3.x인지에 따라 설정이 다르다. virtual machine setting에서 usb 누르면 connection이 어떻게 되어 있는지 확인이 가능하다. 3.0 3.1, 3.2 다 같다.주의해야 할 것은 x windows는 usb연결시 자동 마운트를 해주기 때문에 실습을 위해 x windows를 사용하지 않도록 콘솔창으로 이동 ctrl+alt+f3을 눌러 console로 이동 usb연결: vmware팝업창을 볼 수 있음 connect to a virtual machine을 선택 root로 로그인 후 lsblk를 쳐서 마운트 상태 확인 마운트할 위치를 먼저 만들어야 함 mkdir /media/usbstick mount /dev/sdc1 &amp;lt;ALT+.&amp;gt; 더 자세하게 디바이스를 보고자 한다면 # blkid /dev/sdc1 /dev/sdc1: LABEL = &quot;USB32&quot; UUID= ... TYPE=&quot;vfat&quot; ... usb 메모리에 데이터를 써보기 # cp ~/.bashrc /media/usbstick/ umount umount를 하지 않으면 데이터가 꼬일 수도 있음 disconnect host로 다시 연결하기 위해서는 우측 상단에 disconnect 누르면 댐 host(window)에서 usb를 들어가 .bashrc를 열어보자mount binding많이 쓰는 기능으로 디렉토리를 다른 위치의 디렉토리에 붙이는 기능이다.cp(copy)를 하게 되면 용량이 2배로 먹기 때문에 비효율적이다. 그래서 mount —bind 옵션을 사용한다. 접근할 수 있는 가짜 디렉토리를 만들어준다.# mount --bind /usr/src/redhat/RPMS /Var/www/html/rpms이 기능은 ftp나 웹서비스에서 다른 위치의 디렉토리를 노출시킬 때 편리하다. 노출시키려는 디렉토리가 다른 상위 디렉토리에 있을 때도 편리하고, binding을 해제하기만 하면 안보이게 할 수 있다.Fstab (filesystem table)디렉토리는 /etc/fstab에 위치하고 있고, 부팅시 파일 시스템을 자동 마운트하기 위한 정보를 담고 있다. 대체로 6개의 필드로 구성된다. device, mount point, fstype, options, dump 장치파일, 마운트할 곳, 파일 시스템 타입, 옵션, 덤프 여부, 부팅시 체크 순서&amp;lt;file system&amp;gt; &amp;lt;mount point&amp;gt; &amp;lt;type&amp;gt; &amp;lt;options&amp;gt; &amp;lt;dump&amp;gt; &amp;lt;pass&amp;gt;UUID=08364a41-d369-4d72-bd50-6081da820786 / ext4 errors=remount-ro 0 1/swapfile none swap sw 0 0ext4를 이용할 때만 부팅하면서 검증해야 하기에 1로 지정되어 있다.여기서 디바이스 장치(file system)에 다양한 형태가 존재한다. /dev/sd* : 실제 디바이스 장치로 설정하는 방식 단점: 가장 오래된 방식으로 port를 순서대로 연결하지 않는 경우 장치명의 변경되거나 역전될 가능성이 있다. e.g. port 0,3,4번에 장치가 연결되어 있다면 0(sda),3(sdb),4(sdc)인데, 2번포트에 1개를 더 연결하면 1칸씩 밀려야 한다. 그러면 3:sdc, 4:sdd가 되어야 하는데, 이름이 바뀌어 있지 않으므로 오류가 난다. LABEL=name : 레이블 이름으로 검색하여 장치를 찾아내는 방식 LABEL : 식별 가능한 문자열을 FS의 label부분에 넣어서 찾음 UUID=uuid : UUID값으로 검색해서 장치를 찾아내는 방식 가장 세련된 방식으로 UUID(universally unique identidfie)를 FS에 넣어 찾는 방법 UUID를 변경하려면 uuidgen으로 생성한 다음 e2label, xfs_admin을 통해 수행한다. fstab 옵션 defaults : 기본값 사용 noauto : 읽기/스기 가능 rw : 읽기/쓰기 가능 ro : 읽기만 가능 user : 일반 유저도 마운트 가능 nouser: 일반 유저는 마운트 할 수 없음 noexec : 실행 파일 사용 금지 linux fs 튜닝 목적의 옵션 - relatime : atime을 특정 경우에만 업데이트 (자주 업데이트하면 성능이 떨어질 수 있음 - noatime: atime 자체를 사용하지 않음 (log및 매우 많은 깊이를 가지는 디렉토리 및 파일 시스템에서 유리e2label / findfs / tune2fslabel이나 uuid를 찾고 변경하는 방법이다. e2label : ext4 리눅스 파일 시스템의 레이블을 확인 및 편집 e2label [new-label] tune2fs : ext4 파일 시스템의 다양한 설정 조정 xfs_admin : XFS 파일 시스템의 레이블 / UUID 확인 및 편집 findfs : 레이블을 검색대체로 e2label,tune2fs는 설정하는 용도, findfs는 찾는 용도로 많이 사용된다.tune2fs tune2fs -U &amp;lt;clear random time uuid_value&amp;gt; clear: 삭제 random, time: 무작위로 또는 시간 기반으로 생성 uuid_value: uuid를 직접 넣어줄 때 uuidgen : UUID 생성 유틸디바이스를 fstab에 등록하면 부팅시 자동 마운트가 가능하다.실습id, type 확인# blkid /dev/sdb2/dev/sdb2: UUID=&quot;6989fcda-bfb6-47a8-9ba8-69fb972b7e1a&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;cde1097b-02&quot;sdb2에 random UUID 설정# tune2fs -U random /dev/sdb2tune2fs 1.44.1 (24-Mar-2018)This operation requires a freshly checked filesystem.Please run e2fsck -f on the filesystem.# blkid /dev/sdb2/dev/sdb2: UUID=&quot;6989fcda-bfb6-47a8-9ba8-69fb972b7e1a&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;cde1097b-02&quot;# mount /media/backup# findmnt ...├─/media/backup /dev/sdb2 ext4 rw,noexec,reludisks2D-bus 기반의 block device manager로 daemon service로 작동하고 다양한 block device에 대한 관리를 함, 중요한 것은 udiskctl CLI를 제공하는데 일반 유저로 명령 가능하다. 그러나 sudo를 쓰면 더 안좋게 발생할 수도 있다.서비스 확인$ systemctl status udisks2● udisks2.service - Disk Manager Loaded: loaded (/lib/systemd/system/udisks2.service; enabled; vendor preset: Active: active (running) since Sun 2022-02-27 15:11:53 KST; 4h 5min ago Docs: man:udisks(8) Main PID: 773 (udisksd) Tasks: 5 (limit: 2292) CGroup: /system.slice/udisks2.service └─773 /usr/lib/udisks2/udisksd...udisksctl# lsblk# mount --block-device /dev/sdc1# lsblksdc1의 옆에 추가로 /media/root/ESD-USB가 추가되었다.언마운트는 다음과 같다.# udisksctl unmount --block-device /dev/sdc1여기서 —block-device 는-b로 줄여서 쓸 수 있다.loopback deviceloopback device를 설정해줄 수 있다. 이는 파일을 디바이스처럼 마운트한다는 것으로 주로 ISO, DVD, image를 마운트, loopback device는 read-only이다.$ ls -l *.iso$ udisksctl loop-setup --file ubuntu-18.04.7-server-amd64.isodumpdump명령으로 device를 살펴볼 수 있다. 전체 장치를 다 볼 수 있음# udisksctl dump개별 장치 정보는 $ udisksctl info -b /dev/sdc1을 사용$ udisksctl info -b /dev/sdb1/org/freedesktop/UDisks2/block_devices/sdb1: org.freedesktop.UDisks2.Block: Configuration: [] CryptoBackingDevice: &#39;/&#39; Device: /dev/sdb1 DeviceNumber: 2065 Drive: &#39;/org/freedesktop/UDisks2/drives/VMware_2c_VMware_Virtual_S_1&#39; HintAuto: false HintIconName: HintIgnore: false HintName: ...status현재 시스템에 어떤 디스크들이 어디에 맵핑되어 있는지 확인할 수 있다.$ udisksctl statusMODEL REVISION SERIAL DEVICE--------------------------------------------------------------------------VMware, VMware Virtual S 1.0 sda VMware, VMware Virtual S 1.0 sdb VMware Virtual SATA CDRW Drive 00000001 01000000000000000001 sr0USB나 flash 메모리를 리더기를 통해 사용할 때 udisk를 많이 사용한다. mount -t를 사용해서도 할 수 있지만 옵션도 길어지고, 쳐야할 것도 많아지는 등 귀찮아지기 때문에 잘 사용 안한다.LINUX 디렉토리 구조 / : root 디렉토리 ( root 유저의 홈디렉토리와는 다름 - root 유저의 홈 디렉토리는 /root) /dev : 장치 파일들 (device file) /tmp : 임시 파일 저장용 디렉토리 (temporary directory) /dev/null : null device /dev/console : 시스템 콘솔 장치 /boot : 부팅을 위한 커널 관련 이미지 /bin : 기본 실행 명령어 파일들 (binary == 실행파일) 요즘에는 대체로 /bin → usr/bin을 syslink로 연결하기도 함 /sbin : 수퍼유저용 시스템 관리용 명령어 파일들 /etc : 시스템 설정 파일 및 관련 스크립트 /lib : 시스템 라이브러리 /usr : 응용 프로그램 관련 파일들 /usr/bin : 응용 프로그램 실행 파일 /usr/sbin : 관리자용 응용 프로그램 실행 파일 /usr/lib : 라이브러리 /usr/include : C언어용 헤더 파일 /usr/share : 응용 프로그램이 공유하는 파일 /usr/src : 소스코드 (커널,패키지) /usr/local : 패키지가 아닌 임의로 설치되는 응용 프로그램 파일 직접 소스코드를 쳐서 다운 받는 것들 bin,lib, apt 등 /var : 각종 잡다한 파일 /var/log : 로그 파일 /media : 마운트용 디렉토리 /exp : 외부 저장 장치 ( 사용자가 자주 만들어서 쓰는 디렉토리)" }, { "title": "[데브코스] 2주차 - linux 기초(REGEX)", "url": "/posts/regex/", "categories": "Classlog, devcourse", "tags": "linux, vim, devcourse", "date": "2022-02-24 13:00:00 +0900", "snippet": "regexPOSIX, PCREregex에는 여러 변종이 있지만, 2가지가 가장 유명하다. POSIX REGEX UNIX 계열 표준 정규표현식 BRE(basic RE), ERE(Extended RE) BRE : basic REGEX - grep이 작동되는 기본값 ERE : extended REGEX - 좀더 많은 표현식과 편의성을 제공 meta character중에 ERE라고 적힌 것을 의미 이것이 공통이고, 표준이므로 이를 먼저 배워야 한다. PCRE Perl 정규표현식 호환으로 확장된 기능을 가짐 C언어 기반으로 시작하는데, POSIX REGEX보다 성능이 좋은 실무에서도 이를 많이 쓰고, 대부분의 다른 python이나 등등의 언어는 이를 기준으로 작성됨 현재는 PCRE2 버전을 사용 + EBNF*,+,?,[…] 등은 EBNF의 영향이 크다. EBNF를 알고 있다면 학습이 쉬워지기 때문에 EBNF는 필수로 알아야 한다. XML같은 마크업 언어 및 대부분의 설정 포맷은 EBNF를 기준으로 쓰인다.command line utilitygrep (global regular expression print) 유닉스에서 가장 기본적인 REGEX평가 유틸리티 정규식을 평가할 수 있는 유틸리티sed (stream editor) REGEX 기능을 일부 탑재하고 있다. 스트림 에디터awk REGEX뿜나 아니라 문자열 관련 방대한 기능을 가진 프로그래밍 언어 패턴식을 다룰 수 있는 언어툴 제일 많은 기능을 가짐 실무에서 실제 많이 사용되는 것 grep → sed → awk 순으로 공부하는 것이 좋다.grepmatcher: -G : BRE를 사용하여 작동 (기본값) (기본이라 잘 안쓰임) -E : ERE를 사용하여 작동, egrep으로 작동시킨 것과 같다. -P : PCRE를 사용하여 작동, pcre2grep으로 작동시킨 것과 같다. -F : 고정길이 문자열을 탐색하는 모드, fgrep과 같음, 속도가 빠른 것이 장점 (잘안씀)option —color : matched 된 부분을 강조하여 색깔 -o : 매칭에 성공한 부분만 잘라서 보여줌 -e PATTERN : 패턴 여러개 연결했을 때 사용하긴 함 (잘안씀) -v, —invert-match : 검색에 실패한 부분만 봄 meta character 번호 이름 기호 설명 ⑴ 문자 지정 . 임의의 문자 한 개를 의미 ⑵ 반복 지정 ? 선행문자패턴이 0 or 1개 나타난다. (ERE) ⑶ 반복 지정 + 선행문자패턴이 1개 이상 반복된다. (ERE) ⑷ 반복 지정 * 선행문자패턴이 0개 이상 반복된다. ⑸ 반복 지정 {m,n} interval, 반복수를 직접 지정할 수 있다. (ERE) ⑹ 위치 지정 ^ 라인의 앞부분을 의미한다. ⑺ 위치 지정 $ 라인의 끝부분을 의미한다. ⑻ 그룹 지정 […] 안에 지정된 문자들 그룹 중에 한 문자를 지정 ⑼ 그룹 지정 [^…] 안에 지정된 그룹의 문자를 제외한 나머지를 지정 ⑽ 기타 \\ escape, 메타의 의미를 없앤다. ⑾ 기타 | alternation, choice, or연산을 한다 (ERE) ⑿ 기타 () 괄호는 패턴을 그룹화 및 백레퍼런스 작동을 한다. (ERE) escape란 메타의 의미를 잃는다는 것으로 실제 ., ?, + 등이 포함된 것만 추출한다는 것이다. 반복 지정 (2~4) 수량자는 선행문자패턴(atom이라 부름)을 수식하는 기능을 가진다. ? : x?ml ⇒ xml, ml * : can* ⇒ ca, can, cann, cannn, … * 은 곱셈이라서 0부터 C* : C * 0 = null, C * 1 = C + : can+ ⇒ can, cann, cannn, … + 는 덧셈이라서 1부터 C+ : C + 0 = C, C+ 1개의 C = CC {}(interval) : abc{2,5} : abcc, abccc,abcccc,abccccc m과 n은 생략 가능하다. {n}, {m,}, {,n} 그러나 이 *(kleene star)은 최소 null과 매칭되므로 backtracking으로 느려지는 원인이 될 수 있다. 위치 지정 (8 ~ 9)^, $ 는 패턴의 위치를 지정하는 패턴이다. ^asd : asd로 시작하는 행 ^$ : 비어있는 행 &amp;lt;BR&amp;gt;$ : &amp;lt;BR&amp;gt;로 끝나는 경우 그룹 지정 (10 ~ 11)[] , [^ ] 는 character class [abcd] : a,b,c,d 중에 하나 [0-9] : 0~9 [a-zA-Z0-9] : 대소문자 알파벳과 숫자 [^0-9] : [0-9]를 제외한 나머지 ^ 자체를 그룹에 넣으려면 [ 이것 바로 뒤에 오지만 않으면 된다. \\⇒ [0-9^] 또는 escape 시켜도 된다. p[abcd]\\+ous : p 바로 뒤부터 abcd 중에서만 추출하는데 1개 이상인 것, 그리고 ous로 마무리 -&amp;gt; opacous, opacousness이 때, grep에서 ERE문법을 사용하기 위해서는 \\를 추가해서 작성해야 한다. 그러나 egrep에서는 그냥 작성하면 된다. ERE문법이 아닌 것에 \\를 추가하면 이는 문자 자체로 인식하게 만드는 역할을 한다.실습log data와 grep을 묶어서 사용할 수 있다.# grep --color pam_systemd exjournal.log# grep --color -A 1 &quot;pam_systemd&quot; exjournal.log여기서 -A 1는 after 즉, 찾은 것 다음에 1줄을 더 출력하겠다. -B는 before ,-C는 이 둘을 더한 것-C 1 == -A 1 -B 1greedy matchingpattern은 최대한 많은 수의 매칭을 하려고 하는 성질이 있다.$ var2=&quot;It&#39;s a gonna be &amp;lt;b&amp;gt;real&amp;lt;/b&amp;gt;It&#39;s gonna &amp;lt;i&amp;gt;change everything&amp;lt;/i&amp;gt; I feel&quot;$ echo $var2 | egrep -o &quot;&amp;lt;.+&amp;gt;&quot;&amp;lt;b&amp;gt;real&amp;lt;/b&amp;gt;It&#39;s gonna &amp;lt;i&amp;gt;change everything&amp;lt;/i&amp;gt;여기서 ., 즉 문자 1개이상이 포함되어 있는 것을 추출하는데, 최대한 많은 수의 매칭을 하려고 하기 때문에 &amp;lt;b&amp;gt;&amp;lt;/b&amp;gt;처럼 작은 단위가 아닌 &amp;lt;b&amp;gt;~&amp;lt;/i&amp;gt;까지 모두를 추출한 것이다.그래서 이 greedy matching 성질 때문에 처음에는 큰 범위로 불러온 후 점점 좁혀나가는 것이 좋은 방법이다.방금 조건처럼 크게 된 이유는 .일 것이다. 그래서 &amp;lt;b&amp;gt;,&amp;lt;/b&amp;gt;만 추출하려면 이것을 다른 것으로 바꾸어 표현하게 된다면 &amp;lt;&amp;gt;를 제외하고 &amp;lt;+&amp;gt;를 보면$ echo $var2 | egrep -o &quot;&amp;lt;[^&amp;lt;&amp;gt;]+&amp;gt;&quot;&amp;lt;b&amp;gt;&amp;lt;/b&amp;gt;&amp;lt;i&amp;gt;&amp;lt;/i&amp;gt;non-greedy matchinggreedy matching과 반대대는 개념으로 &amp;lt;[^&amp;lt;&amp;gt;]+&amp;gt;를 사용하면 최소 매칭이 되는데이를 좀더 쉽게 푸는 것은 기본 버전인 POSIX BRE에는 존재하지 않는다. 그래서 기본버전에서 위와 같이 다소 어렵게 표현된 것이다.그러나 특이하게도 vim에서는 POSIX BRE를 사용하지만 non-greedy 수량자인 \\{-}를 제공한다. vim 에디터에 It&#39;s a gonna be &amp;lt;b&amp;gt;real&amp;lt;/b&amp;gt;It&#39;s gonna &amp;lt;i&amp;gt;change everything&amp;lt;/i&amp;gt; I feel 를 작성 타이핑 후 /&amp;lt;.+&amp;gt; 으로 검색 (greedy matching&amp;gt; /&amp;lt;.{-}&amp;gt; 로 다시 검색 (non-greedy matching&amp;gt;$ vim hi.txtIt&#39;s a gonna be &amp;lt;b&amp;gt;real&amp;lt;/b&amp;gt;It&#39;s gonna &amp;lt;i&amp;gt;change everything&amp;lt;/i&amp;gt; I feel$ cat hi.txtIt&#39;s a gonna be &amp;lt;b&amp;gt;real&amp;lt;/b&amp;gt;It&#39;s gonna &amp;lt;i&amp;gt;change everything&amp;lt;/i&amp;gt; I feel$ cat hi.txt | grep /&amp;lt;.\\+&amp;gt;$ cat hi.txt | grep /&amp;lt;.\\{-}&amp;gt;PCRE에서 사용하는 non-greedy matching 기능은 Lazy quantifier이라고 부르는데, ?를 더하면 된다.즉$ echo $var2 | grep -P -o &quot;&amp;lt;.+?&amp;gt;&quot;&amp;lt;b&amp;gt;&amp;lt;/b&amp;gt;&amp;lt;i&amp;gt;&amp;lt;/i&amp;gt;여기서 -P는 PCRE에 대해서 실행한다는 것이다. 왜냐하면 lazy quantifier는 PCRE에서만 지원되서 POSIX REGEX모드에서는 사용할 수가 없다. 그래서 -P matcher를 붙여서 PCRE모드를 실행한 것이다. /usr/share/dict/words 라는 디렉토리에는 단어들이 많이 있다.실습$ var4=&#39;URLS : http://asdf.com/en/ , https://asdf.com/en/&#39;$ echo $var4 | grep --color &#39;http://[A-Za-z./]*&#39;http://asdf.com/en/$ echo $var4 | grep -o &#39;http://[A-Za-z./]+&#39;$ echo $var4 | grep -o &#39;http://[A-Za-z./]\\+&#39;http://asdf.com/en/$ echo $var4 | egrep -o &#39;http://[A-Za-z./]+&#39;http://asdf.com/en/ERE문자를 BRE에서 \\ 없이 사용하면 실제 문자로 인식하게 되어 아무것도 출력되지 않았다.위처럼 인식하게 만들기 위해 \\를 추가했지만, grep 대신 egrep을 사용하면 \\를 추가하지 않아도 된다.실습2curl : 터미널에서 웹 페이지를 접속하는 유틸 을 사용하여 url링크를 추출$ curl -s https://www.naver.com | egrep -o &#39;https://[0-9a-zA-Z.]+/&#39;https://www.naver.com/https://s.pstatic.net/https://www.naver.com/https://s.pstatic.net/...중복된 값 제거$ curl -s https://www.naver.com | egrep -o &#39;https://[0-9a-zA-Z.]+/&#39; | sort -uhttps://book.naver.com/https://business.naver.com/https://campaign.naver.com/https://comic.naver.com/https://contents.premium.naver.com/https://developers.naver.com/https://dict.naver.com/https://expert.naver.com/https://finance.naver.com/https://help.naver.com/https://in.naver.com/....대신 [^&amp;lt;&amp;gt;]를 사용한 이유는 greedy matching 성질 때문에 &amp;lt;&amp;lt;&amp;lt;&amp;gt;&amp;gt;&amp;lt;&amp;gt;&amp;lt;&amp;gt;&amp;gt; 처럼 제일 끝에 걸로 추출하기 위해$ curl -s https://www.naver.com | egrep -o &quot;&amp;lt;img [^&amp;lt;&amp;gt;]+&amp;gt;&quot;&amp;lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAMAAADVRocKAAAC91BMVEUAAACE1dAbf6jE1Nq/z9e82drQ3OG7ytOzws+6x9QNs7MeLpQSx60QrLAfL50gMakTxKyzwc8fMacSr6yywc8fMKIdLYzJ1d4KrrW6ydMJrrUbr6QgMKW2xNEcr6W7ydUfL5cIr7XBztgMtbMeL50fMaQQzrI0tK4cLIQUvq...PCRE에서의 non-greedy matching인 ? 사용$ curl -s https://www.naver.com | grep -P -o &quot;&amp;lt;img .+?&amp;gt;&quot;&amp;lt;img src=&quot;https://s.pstatic.net/dthumb.phinf/?src&amp;amp;#x3D;%22https%3A%2F%2Fsports-phinf.pstatic.net%2F20220225_38%2F1645792775360TpuI0_JPEG%2F15_20220225018F202.jpg%22&amp;amp;amp;type&amp;amp;#x3D;nf728_360&quot; data-src=&quot;https://s.pstatic.net/dthumb.phinf/?src&amp;amp;#x3D;%22https%3A%2F%2Fsports-phinf.pstatic.net%2F20220225_38%2F1645792775360TpuI0_JPEG%2F15_20220225018F202.jpg%22&amp;amp;amp;type&amp;amp;#x3D;nf728_360&quot; alt=&quot;&amp;amp;#x27;풀세트 대역전승&amp;amp;#x27; 현대건설에 시즌 첫 연패를 안겨준 KGC인삼공사&quot; BRE vs ERE - BRE 사용하는 유틸 : grep, vim, sed … - ERE 사용하는 유틸 : egrep, awk - PCRE는 별개지만 기본적으로 ERE를 베이스로 함back - reference소괄호 () : back-reference, group매칭된 결과를 다시 사용하는 패턴이다. ()로 묶인 패턴 매칭 부분을 “#”의 형태로 재사용etc/passwd 라는 곳은 유저의 정보를 담은 곳으로 총 7개로 구분되어 있다. 구분자는 :이다username:x:(패스워드였지만 지금은 안씀):uid:gid:정보(생략가능): 홈디렉토리 : shell이 때, 홈디렉토리와 username이 같은 경우를 일반유저라고 한다. 유저명이 달라질 때마다 홈디렉토리도 달라진다.$ egrep &quot;^(.+):x:[0-9]+:[0-9]+:.*:/home/\\1:&quot; /etc/passwdjaehoyoon:x:1000:1000:jaehoyoon,,,:/home/jaehoyoon:/bin/bash앞에 (.+)는 username을 찾는 것이고, \\1 이라는 것은 앞에 불러온 매칭인 (.+)에 대한 결과값을 저장하는 것이다. 즉 (.+)을 통해 jaehoyoon이 매칭되었다면 \\1도 jaehoyoon이 매칭된다.이 때 괄호가 1개였기에 \\1이고, 두개 세개가 쓰인다면 \\2,\\3 이 된다.이것을 활용하여 태그와 태그안에 적힌 부분을 함께 추출할 수 있다.$ echo $var2 | egrep -o &#39;&amp;lt;([a-zA-Z0-9]+)&amp;gt;.*&amp;lt;/\\1&amp;gt;&#39;&amp;lt;b&amp;gt;real&amp;lt;/b&amp;gt;&amp;lt;i&amp;gt;change everthing&amp;lt;/i&amp;gt;()를 통해 \\1 과 매칭해준 것이다.alternation or에 해당하는 것으로 (cat dog) 이면 cat 또는 dog 를 추출, 주의할 것은 대/소문자를 가려서 걸러낸다. $ echo &quot;cat and dog&quot; | egrep -o &quot;(cat|dog)&quot;catdog실습$ curl -s https://www.naver.com | egrep -o &quot;&amp;lt;(a|A) [^&amp;lt;&amp;gt;]+&amp;gt;.+&amp;lt;/\\1&amp;gt;&quot;$ curl -s https://www.naver.com | grep -o &quot;&amp;lt;\\(a\\|A\\) [^&amp;lt;&amp;gt;]\\+&amp;gt;.\\+&amp;lt;/\\1&amp;gt;&quot;&amp;lt;a href=&quot;#newsstand&quot;&amp;gt;&amp;lt;span&amp;gt;뉴스스탠드 바로가기&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt; &amp;lt;a href=&quot;#themecast&quot;&amp;gt;&amp;lt;span&amp;gt;주제별캐스트 바로가기&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt; &amp;lt;a href=&quot;#timesquare&quot;&amp;gt;&amp;lt;span&amp;gt;타임스퀘어 바로가기&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt; &amp;lt;a href=&quot;#shopcast&quot;&amp;gt;&amp;lt;span&amp;gt;쇼핑캐스트 바로가기&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt; &amp;lt;a href=&quot;#account&quot;&amp;gt;&amp;lt;span&amp;gt;로그인 바로가기&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;...$ curl -s https://www.naver.com | egrep -o &#39;http://[0-9A-Za-z./_]+\\.(jpg|JPG|png|PNG)&#39; ?,+,{},(), 다 ERE 문법이므로 escape 시켜줘야 한다. 대문자도 신경써야 할 것이고, 여기서 .는 진짜 . 을 검색해야 하기 때문에 escape 시킨 것substitution교체하는 것은 sed, awk을 많이 사용한다. sed$ echo $var2 | sed -e &quot;s/&amp;lt;[^&amp;lt;&amp;gt;]\\+&amp;gt;/ /g&quot;It`s gonna be real It`s gonna change everthing I feel$ echo $var2 | sed -re &quot;s,&amp;lt;[^&amp;lt;&amp;gt;]+&amp;gt;, ,g&quot;It`s gonna be real It`s gonna change everthing I feelsed는 구분자로 /를 많이 쓰지만, , 도 쓰기도 한다. 기본적으로 작성할 때는 BRE로 쓰지만, ERE로 바꿔서 사용할 줄도 알아야 한다. awk$ echo $var2 | awk &#39;{ gsub(/[ ] *&amp;lt;[^&amp;lt;&amp;gt;]+&amp;gt;[ ]*/, &quot; &quot;); print }&#39;It`s gonna be real&amp;lt;/b&amp;gt;It`s gonna change everthing&amp;lt;/i&amp;gt; I feelawk 는 ERE를 사용해서 \\를 붙이지 않았다.실습$ curl -s https://www.naver.com | sed -n &quot;s,&amp;lt;\\(a\\|A\\) [^&amp;lt;&amp;gt;]\\+&amp;gt;\\(.\\+\\)&amp;lt;/\\1&amp;gt;,\\2,gp&quot;&amp;lt;div&amp;gt;[지금 우크라 국경에선] &quot;피란길 아이들 울음 끊이지 않았다&quot;&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;한국리서치 &quot;李·尹 39.8% 동률&quot;…D-10 오차범위내 초접전&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;이재명 &quot;선거때 누구 눌러 포기압박 안돼…통합정부로 정치교체&quot;&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;尹, 단일화 전말 작심 공개…安책임론 부각·지지층 결집 승부수&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;安측 &quot;尹측 진의 확인하려 만나…전권협상 대리인 아니었다&quot;&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;북, 5년만에 사거리 2천㎞ 안팎 미사일 쏜듯…주일미군 사정권&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;靑, L-SAM·&#39;한국형 아이언돔&#39; 시험발사 성공 공식확인&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;與 &quot;불기소 처분 은행, 尹처가에 대출&quot;…野 &quot;엉뚱한 추정&quot;&amp;lt;/div&amp;gt;...$ curl -s https://www.naver.com | sed -rn &quot;s,&amp;lt;(a|A) [^&amp;lt;&amp;gt;]+&amp;gt;(.+)&amp;lt;/\\1&amp;gt;,\\2,gp&quot;&amp;lt;span class=&quot;opt_item&quot;&amp;gt;도움말&amp;lt;/span&amp;gt;&amp;lt;span class=&quot;opt_item&quot;&amp;gt;신고&amp;lt;/span&amp;gt;자동완성 끄기&amp;lt;i class=&quot;ico_mail&quot;&amp;gt;&amp;lt;/i&amp;gt;메일&amp;lt;li class=&quot;nav_item&quot;&amp;gt;카페&amp;lt;/li&amp;gt;&amp;lt;li class=&quot;nav_item&quot;&amp;gt;블로그&amp;lt;/li&amp;gt;&amp;lt;li class=&quot;nav_item&quot;&amp;gt;지식iN&amp;lt;/li&amp;gt;&amp;lt;li class=&quot;nav_item&quot;&amp;gt;&amp;lt;span class=&quot;blind&quot;&amp;gt;쇼핑&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;li class=&quot;nav_item&quot;&amp;gt;Pay&amp;lt;/li&amp;gt;...Boundary \\b : boundary가 맞는 표현식만 찾는다. 즉 단어 경계면을 기준으로 검색 \\B : boundary에 맞지 않는 표현식만 찾는다. 즉, 단어 경계면이 아닌 경우에만 검색$ var5=&#39;abc? &amp;lt;def&amp;gt; 123hijklm&#39;$ echo $var5 | egrep -o &quot;[a-j]+&quot;abcdefhij$ echo $var5 | egrep -o &quot;\\b[a-j]+\\b&quot;abcdef$ echo $var5 | egrep -o &quot;\\B[a-j]+\\B&quot;behij+ predefined character class이미 정의해놓은 클래스 클래스 설명 set [[:cntrl:]] 제어문자들을 의미 [\\x00-\\x1f\\x7f] [[:punct:]] 출력 가능한 특수문자들 [!-/:-@[-`{-~] [[:space:]] white space (공백으로 사용되는 6개의 문자 [ \\t\\r\\n\\v\\f] [[:print:]] 출력 가능한 문자들 [ -~] [[:graph:]] 공백을 제외한 출력 가능한 문자들 [!-~] \\x##는 hex code 표현으로 PCRE에서만 지원$ var5=&quot;sunyzero@email.com:010-8500-80**:Sun-young Kim:AB-0105R&quot;$ echo $var5 | egrep -o &quot;^[[:alpha:]@]+&quot;sunyzero@email$ echo $var5 | egrep -o &quot;^[[:alpha:]@]+\\.[a-z]+&quot;sunyzero@email.com$ echo $var5 | egrep -o &quot;[[:upper:][:digit:]-]{8}&quot;010-8500AB-0105RPOSIX REGEX and PCRE POSIX REGEX 간단한 패턴 매칭에 사용 복잡해지면 성능저하 PCRE 확장된 정규표현식 매우 빠른 속도, 확장된 표현식 C, C++, 기타 대부분의 언어가 지원 실무라면 PCRE를 사용 $ ipv4=&quot;123.456.12.1 12.467.56.5 323.666.893.12 111.11.11.1 \\&amp;gt;1123123.123123123.123123.123123123&quot;$ echo $ipv4 | egrep --color &quot;([0-9]|[0-9][0-9]|[0-1][0-9][0-9]|2[0-5][0-5])\\. \\&amp;gt;([0-9]|[0-9][0-9]|[0-1][0-9][0-9]|2[0-5][0-5])\\. \\([0-9]|[0-9][0-9]|[0-1][0-9][0-9] \\|2[0-5][0-5])\\.([0-9]|[0-9][0-9]|[0-1][0-9][0-9]|2[0-5][0-5])&quot;123.456.12.1 12.467.56.5 323.666.893.12 111.11.11.1 1123123.123123123.123123.123123123한글찾기$ egrep --color &#39;한.&#39; hangul-utf8.txt한글은 한국어에서 사용되는 문자이다. 여기 UTF8로 인코딩된 한글과 한자가 보이는가?$ egrep --color &#39;[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]+&#39; hangul-utf8.txt한글은 한국어에서 사용되는 문자이다. 여기 UTF8로 인코딩된 한글과 한자가 보이는가?REGEX 테스터기 사이트 https://regexr.com/ https://www.regextester.com/ https://regex101.com/Globunix혹은 linux 명령행에서 파일명 패턴에 쓰이는 문자열regex와 비슷하지만 regex는 아니다.glob 테스터 사이트 http://www.globtester.com/backtrackinggreedy matching 속성으로 인해 역탐색을 하는 행위, 이로 인해 성능이 많이 저하되기도 한다.4759번 검사했다는 것을 보여준다. 그래서 greedy한 표현식을 바꾸기 위해 .을 다르게 바꿔보았다.3개만 바꿔도 257로 줄어드는 것을 볼 수 있다.그래서 backtracking을 줄이는 연습을 많이 해야 한다. . 대신에 []* []+ 등을 많이 쓴다. 공백 문자도 계속 확장하게 만드는 요인이므로 되도록 사용 안하는 것이 좋다.summary backtracking을 피하려면 .* 혹은 .+ 과 같이 greedy한 표현식을 남발하지 않는다. 특히 .은 되도록 쓰지 않는게 좋다. 쓴다면 lazy quantifier를 사용하는 것이 좋다. 공백을 반복할 수 있는 표현식은 피한다. backtracking으로 손실되는 성능은 복잡도에 따라 다르지만, 일반적으로 non greedy한 표현식보다 100~1000%의 성능 하락 " }, { "title": "[데브코스] 2주차 - linux 기초 package and network ", "url": "/posts/packageandtcp/", "categories": "Classlog, devcourse", "tags": "linux, tcp, devcourse", "date": "2022-02-24 13:00:00 +0900", "snippet": "오답노트package 관리 redhatrpm database*yum debiandpkg(옛날거)apt-get*aptpackagepackage : 시스템을 구성하는 파일의 묶음관리의 편리함을 제공한다. package name - version &amp;amp; release - architecture.확장자 amd64는 x86 64bitapt를 주로 사용하고, dpkg는 중요한 기능 몇가지만 알아두면 된다.dpkg 중요 기능strace, gcc 패키지 리스트를 확인해보자. 옵션: -l (list)설치되지않아도 찾아볼 수 있다.$ dpkg -1 strace$ dpkg -1 gcc 옵션: -s (status)status에 install ok installed 이면 설치된 것, install ok unpacked는 설치된 것이 아니다.# dpkg -s stracestatus: install ok installed 옵션: -S (search)패키지 검색, 해당 파일이 어디에 존재하는지 확인 가능# dpkg -S &#39;*trace&#39;linux-headers-3.2.0-4-common: /usr/......에러가 나면 해결하는 방법으로 audit을 사용하기도 한다.aptapt는 debian에서 사용하는 툴로 가장 많이 사용하는 명령어다.dependency 탐색 및 설치 가능 e.g. A를 설치하기 위해 B를 설치해야 하고, B를 위해 C를 설치해야 한다면 그것들을 탐색해준다.예전에는 apt-get, apt-cache 등등이 있는데 요즘에는 이들을 통합한 apt를 사용한다.apt를 쓰기 위해서는 source list를 만들어야 한다.apt를 자동으로 다운하는데 어디에 패키지가 있는지 알아야 하기 때문에 그것을 지정$ vim /etc/apt/sources.list직접 추가할 경우 etc/apt/sources.list.d/에 *.list 파일명으로 추가해도 되지만, 가장 좋은 방법은 apt edit-sources를 사용하여 수정하는 것이다.sources.list안에는 deb와 deb-src가 있다. deb는 패키지를 받아오는 주소, deb-src는 소스코드를 받아오는 주소이다.deb [option1=value1 option2=value2] uri suite [component1] [component2] ... uri : deb 패키지를 제공하는 사이트의 URI 옵션은 대부분 필요x suite : 코드네임 디렉코리 이름을 뜻하는데, 16.04 = xenial, 18.04 = bionic component : suite의 구성 요소 및 라이선스 종류별 분류, 최소 1개 이상의 컴포넌트를 지정해야 함 main: 우분투에서 직접 패키징하는 것들 restricted: 제한된 무료 라이선스 universe: 대체로 절반 무료 라이선스, 대부분이 이에 해당됨 security updates kakao uri의 apt를 참고하고자 한다. 그 이유는 kakao uri가 대체로 더 빠르다.$ sudo apt edit-sources kakao.list실행 한 후 아래 코드를 추가한다.deb http://mirror.kakao.com/ubuntu/ bionic main restricted universedeb http://mirror.kakao.com/ubuntu/ bionic-updates main restricted universedeb http://mirror.kakao.com/ubuntu/ bionic-security main restricted universe 편집할 파일은 /etc/apt/sources/list.d/kakao.list 이다. # sudo select-editor vim 을 실행하여 기본 에디터를 vim으로 변경핤 수 있다. 먼저 http://mirror.kakao.com/ubuntu/ 에 접속한다. 그러면 dists/폴더가 있다. 여기를 들어가서 리스트를 쭉 보고 설치할 패키지 이름을 본다. bionic, bionic-updates, 등이 있다. # sudo apt edit-sources kakao.list 작성패키지 목록 출력 listapt list [option] [package pattern]모든 리스트 출력$ apt list설치된 것만$ apt list --installed업그레이드 가능한 패키지만$ apt list --upgradable모든 버전$ apt list --all-versionspackage pattern을 설정하여 그에 해당하는 것만 출력$ apt list bash*searchapt search [-n] &amp;lt;regex&amp;gt;name이 아닌 설명에 bash가 들어간 경우까지 검색$ apt search bashname 중간에 bash가 있어도 검색$ apt search -n bash시작 부분에 bash가 있는 경우만 검색$ apt search -n &#39;^bash&#39;showapt show &amp;lt;package name&amp;gt;[=version]정보가 나옴$ apt show bash모든 버전 보기 위함$ apt list --all-versions bash한 개의 버전만$ apt show bash=4.4.18-2ubuntu1apt remove, purge, autoremoveapt &amp;lt;remove|purge|autoremove&amp;gt; &amp;lt;package&amp;gt;[=version] remove: 패키지만 삭제 - (config파일은 남겨둠) 설정이 꼬여서 재설치를 할 때 purge: 패키지 삭제 - 완전 삭제, 다시는 안쓸 것 같다. autoremove: 의존성이 깨지거나 버전 관리로 인해 쓰이지 않는 패키지 자동 제거$ apt -y install htop$ apt show htop$ apt purge htop패키지 이름만 보고 싶은 경우에는 apt-cache를 사용pcp가 들어가는 패키지 다$ apt search -n &#39;^pcp*&#39;pcp가 들어가는 이름만$ apt-cache pkgnames pcp-y를 뒤에 적어도됨$ apt install pcp -y네트워크에 필요한 기초 용어 hostname: primary hostname, FQDN TCP/IP : IP address(IPv4, IPv6), subnet mask, gateway NIC: Network Interface Care == 랜카드 Wired Network (Wired connection) : 유선 네트워크 (유선 연결) Wireless Network (Wireless connection) : 무선 네트워크 (무선 연결) LAN : Local Area Network WAN : Wide Area Networkhostname컴퓨터의 이름 : access.redhat.com이라 하면, 호스트이름은 사람의 이름과 비슷하게 만들어져 있다. 컴퓨터의 성은 redhat.com이 된다. 도메인주소는 사람의 성에 해당하는 것으로 redhat.com이다.FQDN, hostnamehostname에는 두가지 중의적 의미가 있다. domain을 제외한 호스트 이름 domain을 포함한 FQDNFQDN: fully qualifed domain name 도메인 내에서 유일하게 구별 가능한 이름, 즉 겹치지 않게 구별해주는 이름 e.g. fedora.redhat.com = hostname = FQDN도메인 주소는 체계적인 구조를 가진다. e.g. devel.fclinux.or.kr kr: 한국의 주소 or: 단체 fclinux: 단체의 이름 devel: 단체 내에서 유일한 이름 hostname 중에서 special hostname라는 것이 있다. localhost 항상 자기 자신을 의미하는 주소와 맵핑된다. IPv4 = 127.0.0.1 IPv6 = ::1 IP주소IPv4 32bit 주소 체계, 8bit씩 끊어서IPv6 128bit 주소 체계IPv6에는 IPv4를 포함한 주소 표기법이 있다. 이를 IPv4-mapped IPv6이라 한다. e.g. 58.232.1.100 ⇒ ::ffff:58.232.1.100CIDR(Classless Inter-Domain Routing)CIDR : IP 클래스와 상관없이 서브넷을 지정하여 자르는 것을 의미 xxx.xxx.xxx.xxx/## ##에는 서브넷 매스크의 on 배트의 개수를 표기 111.111.111.11/24public IP/private IP public IP: 공인 주소(인터넷에서 유일한 주소) private IP: 사설 주소(인터넷에 직접 연결되지 않는 유일하지 않은 주소)SELinuxSecuriy Enhanced Linux : 커널 레벨에서의 중앙 집중식 보안 기능 대부분의 리눅스는 기본으로 설치되지만, ubuntu는 설치되지 않는다SELinux의 보안레벨 enforcing (강제) : SELinux를 사용, 보안설정에 걸리면 강제로 막음 permissive (허가) : SELinux를 사용, 보안설정에 걸리면 허용하되 로그 출력 disabled (비활성) : SELinux를 사용하지 않음서버 구성시에는 SELinux에 대한 이해가 필요하다. 실습을 할 때는 허가레벨이 적절하다.debian계열 Network Configuration네트워크 설정은 2가지 방식이 있다. legacy networkmanager여기서 네트워크 매니저만 사용한다. 사용하면 안되는 legacy부분을 간단히 볼 것이다. Debian/etc/network/interfaces 에서 설정하고,ifdown,ifup,ifconfig, eth0를 사용하는 것은 다 옛날 버전이다. RedHat/etc/sysconfig/netowkr-scripts/ifcfg-* 를 사용하는 것은 다 옛날 버전이다.networkManager장점 daemon으로 작동하면서 network cofiguration을 수행 자동으로 network connection을 관리 Dbus 기반으로 동적 상태를 감지할 수 있기 때문에 다른 애플리케이션이나 daemon들에게 네트워크 정보를 제공하거나, 관리를 위한 권한을 줄 수 있다 통일된 명령어를 사용하여 systemd 기반의 다른 Linux distribution들에게도 동일한 방식의 경험을 제공할 수 있다. Ethernet, wi-fi, moblie broadband 등 다양한 기능들에게 플랫폼을 제공하므로, 네트워크 연결 관리가 좀 더 쉬워졌다.NMCLI (network manager CLI tool) 네트워크에 관련된 대부분의 기능을 가지고 있다. 조회 및 설정 가능 (root계정이거나 sudo 를 사용해야 함)nmcli g[eneral]현재 상태 조회# nmcli gSTATE CONNECTIVITY WIFI-HW WIFI WWAN-HW WWAN 연결됨 전체 사용 사용 사용 사용state : connected / asleepconnectivity : full/nonenmcli n[etworking]네트워크 상태 조회네트워크를 끊거나 연결할 때는 nmcli n on nmcli n off간혹 off되어 있어서 설정이 안되는 경우가 있다. 따라서 상태부터 조회해봐야 한다.# nmcli nenabled# nmcli n connectivityfullnmcli d[evice]장치 확인# nmcli dDEVICE TYPE STATE CONNECTION ens33 ethernet 연결됨 유선 연결 1 lo loopback 관리되지 않음 -- 추가 정보 - en : ethernet - wl : wireless lan - ww : wireless wan 그 뒤에 붙는 것들이o&amp;lt;index&amp;gt; : on-board device index number s&amp;lt;slot&amp;gt; : hotplug slot index numberp&amp;lt;bus&amp;gt; : PCI locationnmcli r[adio]무선 관련 설정# nmcli rWIFI-HW WIFI WWAN-HW WWAN enabled enabled enabled enablednmcli c[onnection] s[how](default)디바이스와 이름의 연결 상태와 넘버,타입 등# nmcli cNAME UUID TYPE DEVICE 유선 연결 1 226b498c-6a18-3983-869d-665b9c680b39 ethernet ens33# nmcli c s특정 연결 이름에 대한 설정된 속성# nmcli c s ens33이 때, cmcli c s ens33 을 했을 때 소문자로 된 것과 대문자로 된 것들이 있다.소문자는 설정된 값(offline일때도 보임), 대문자는 할당된 값(online일때만 보임)주요 속성 ipv4.method auto manual auto = dhcp manual = static ip ip주소에 대해서는 다음과같이 적어야 한다. ipv4.addr = CIDR 표기법 : 192.168.110.50/24 라면 24이므로 앞에 192.168.110까지만 옵션 + : 기존의 것에 추가 - : 기존의 것에서 삭제 none : 교체 # nmcli dDEVICE TYPE STATE CONNECTION ens33 ethernet connected 유선 연결 1 lo loopback unmanaged --# nmcli con down &quot;유선 연결 1&quot;Connection &#39;유선 연결 1&#39; successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/1)# nmcli c s &quot;유선 연결 1&quot;connection.id: 유선 연결 1connection.uuid: 226b498c-6a18-3983-869d-665b9c680b39connection.stable-id: --connection.type: 802-3-ethernetconnection.interface-name: --connection.autoconnect: yesconnection.autoconnect-priority: -999connection.autoconnect-retries: -1 (default)connection.auth-retries: -1...# nmcli con up &quot;유선 연결 1&quot;Connection &#39;유선 연결 1&#39; successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/1)IP4.* 부분 관찰# nmcli c s &quot;유선 연결 1&quot;nmcli 속성 변경 - 이름 변경한글인 이름을 변경시켜본다.# nmcli con modify “변경전이름” 속성 변경후이름# nmcli dDEVICE TYPE STATE CONNECTIONens33 ethernet connected 유선 연결 1lo loopback unmanaged --# nmcli con modify &quot;유선 연결 1&quot; connection.id ens33# nmcli dDEVICE TYPE STATE CONNECTION ens33 ethernet connected ens33 lo loopback unmanaged --nmcli 속성 변경 - ip주소 변경ip 변경 전에 현재 시스템의 ip부터 메모해놓아야 한다.# nmcli c sNAME UUID TYPE DEVICE ens33 226b498c-6a18-3983-869d-665b9c680b39 ethernet ens33# nmcli c s ens33ipv4.method: autoipv4.dns: --...IP4.ADDRESS[1]: 192.168.40.128/24IP4.GATEWAY: 192.168.40.2IP4.ROUTE[1]: dst = 0.0.0.0/0, nh = 192.168.40.2, mt = 100... auto = DHCP# nmcli c mod ens33 ipv4.method manual ipv4.addresses 192.168.40.128/24 \\&amp;gt;ipv4.gateway 192.168.40.2 +ipv4.dns 8.8.8.8대문자는 그대로지만, 소문자 속성만 바뀌어 있다.# nmcli c s ens33ipv4.method: manualipv4.dns: 8.8.8.8IP4.ADDRESS[1]: 192.168.40.128/24IP4.GATEWAY: 192.168.40.2다시 원상복구이 때 &amp;amp;&amp;amp;은 앞에꺼가 True 이면 뒤에꺼도 실행# nmcli c down ens33 &amp;amp;&amp;amp; nmcli c up ens33Connection &#39;ens33&#39; successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)virtual IP 추가/삭제# nmcli c mod ens33 +ipv4.addr 192.168.40.181/24# nmcli c up ens33# nmcli c mod ens33 -ipv4.addr 192.168.40.181/24기존의 설정을 삭제했다가 새로 생성# nmcli n del ens33# nmcli c sdevice 정보는 나오지만 connection은 없음# nmcli d sifname이 디바이스 이름# nmcli c add con-name ens33 ifname ens33 type ethernet \\&amp;gt; ip4 192.168.50.128/24 만약 디바이스 자체가 올라오지 않는 경우status 확인# nmcli dev sdisconnected라고 되어 있으면 connect로 바꿔야 함# nmcli dev connect ens33connect가 실패한다면 status 확인# nmcli gasleep이라면 이 명령은 되지 않을 수 있다. 되지 않는다면# nmcli device connect ens33# nmcli networkingdisabled라면# nmcli networking on확인# nmcli gss (socket statistics) netstat은 구식 명령어상태 확인# ss -ntl옵션 -n : —numeric -a : —all -l : —listening -e : —extended -o : —options -m : —memory -p : —processesfilter로 state, address를 지정하여 추출할 수 있다.filter = [state TCP-state] [EXPRESSION] TCP-STATE established fin-wait-2 close-wait TCP state 실선: 클라이언트 점선: 서버 서버 측에서 요청을 들을 수 있도록 listen을 하면 listen 상태가 된다. 클라이언트에서도 connect를 해야하고, 하게 되면 SYN_SENT (싱크 보내기)를 한다. 받은 싱크는 무조건 다시 echo, ack를 해줘야 한다. 싱크를 보낸측을active opne이라 하고, 받은 측을 passive open 이라 부른다. 싱크를 보내는 것을 SYN_SENT → 싱크를 잘 받았다고 다시 보내는 것을 SYN_RCVD → 또 그것을 받았다는 것을 ack해준다. 이를 three way handshaking 3번 핸드쉐이킹이 끝나면 established 상태가 된다. 데이터를 주고 받음 종료할 때는 대체로 클라이언트가 종료하게 되므로 클라이언트가 active close, 서버가 passive close ,, 그러나 서버측이 먼저 끊을 수도 있다. 그러면 저 FIN부터 다 반대로 작동한다고 보면 된다. close를 하게 되면 바로 ack와 echo를 돌려보내줘야 하는데, 먼저 ack를 주고받는다. 이를 기다리는 것이 FIN_WAIT1, FIN_WAIT2가 echo echo를 잘 받았다는 ack를 기다리는 것이 LAST_ACK 중복되어 받는 것들을 처리해주는 것을 TIME_WAITthree-way handshakingTCP 접속을 만드는 과정 3번 왔다갔다 하기 때문 문제가 발생하기 드뭄Four-way handshakingTCP 접속을 해제하는 과정 4번 왕복하지만 아주 드물게 동시에 접속이 해제되면 3번만에 끝난다. 문제가 종종 생긴다. 특히 passive close에서🎈 close-wait 와 fin-wait-2 는 심각한 문제다. 그 중 close-wait는 시한 폭탄과도 같다.위 2가지 TCP상태가 발생하는지 확인하기 위해서는 다음과 같이 명령# ss -nt state close-wait state fin-wait-21초마다(-n 1) 확인(watch)하기 위는 방법# watch -d -n 1 ss -nt state close-wait state fin-wait-2ss -ntoption : numeric, tcp# ss -ntState Recv-Q Send-Q Local Address:Port Peer Address:PortESTAB 123123 0 192.168.0.71:40632 42.123.16.377.364:12370ESTAB 123123 0 [::ffff::192.168.0.71]:40632 42.123.16.377.364:12370 ESTAB: established Q: 소켓 버퍼를 의미, 저 숫자만큼 쌓여 있다는 것 ffff가 없는 것은 IPv4, 있는 것은 IPv6-mapped ipv4ss state &amp;lt;tcp state&amp;gt;ESTAB 상태를 보려면# ss state establishednumeric과 ipv6만 보여달라는 옵션# ss -n6 state establishedaddress filter주소나 포트 번호를 통해 필터를 걸 수도 있다. 지정을 위해 2가지 방법이 있는데, symbolic보다는 literal을 더 많이 쓴다. symbolic literal description &amp;gt; gt greater than &amp;lt; ls less than &amp;gt;= ge greater than or equal &amp;lt;= le less than or equal == eq equal != ne not equal   and and   or or dport가 22인거만 추출, 작은따옴표로 묶어주는 것이 좋다.# ss -n &#39;dport = :22&#39;ss -soption: statistics통계정보, 열고 있는 통계정보, 몇개를 열고 있는지 볼 수 있는 중요한 기능ss -utlp프로세스의 정보를 보여준다. 할 때는 root계정으로 해야 다 볼 수 있다.user에서 첫번째값은 실행파일의 이름, pid ,fd 값들임 # ss -utlpState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* users:((&quot;systemd-resolve&quot;,pid=561,fd=13)) LISTEN 0 5 127.0.0.1:631 0.0.0.0:* users:((&quot;cupsd&quot;,pid=712,fd=7)) LISTEN 0 5 [::1]:631 [::]:* users:((&quot;cupsd&quot;,pid=712,fd=6))# ss -n &#39;src 192.168.110.0/24&#39;dport가 ssh인거거나 sport가 ssh인것들을 보여달라# ss -n &#39;dport = :ssh or sport = :ssh&#39;dport가 ssh이면서 sport가 ssh인 것(여기서 괄호 사이에 공백은 일부러 넣은것으로 안하면 오루가 날 수도 있다)그리고 src번호가 이것인거만 추출# ss -n &#39;( dport = :ssh or sport = :ssh ) and src 192.168.110.134&#39;192.168.110.0/24 라는 필터를 걸면 24bit 즉, 앞에 8bit씩 끊어서 앞에 192.168.110인것들을 모두 보여준다는 의미다.ss와 같이 쓰이는 명령어lsof, fuser열린 파일을 검색하거나 액션을 행할 수 있는 기능을 가진다. 특정 소켓 주소를 점유하고 있는 프로세스를 찾아내거나 할 때 사용한다.ping 또는 server 관련 명령어ping상대 호스트의 응답을 확인함ping [-c count] [-i interval] [-s size] [-t ttl] targetcount를 지정하지 않으면 계속 보내질수도 있으며, interval을 지정하지 않으면 초단위로 보내진다. 다 하고 나면 통계정보(min,avg,max,mdev등)를 알려준다. 이 때 표준편차를 주의깊게 봐야 한다.# ping -c 3 192.168.0.1PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data.--- 192.168.0.1 ping statistics ---3 packets transmitted, 0 received, 100% packet loss, time 2043ms0.2sec 간격으로 2000번 테스트# ping -c 2000 -i 0.2 -s 1000 102.168.1.1# ping -c 2000 -i 0.2 -s 1000 192.168.1.1PING 192.168.1.1 (192.168.1.1) 1000(1028) bytes of data.--- 192.168.1.1 ping statistics ---1064 packets transmitted, 0 received, 100% packet loss, time 216897msarparp 테이블: IP와 MAC 주소를 매핑해주는 것# arpAddress HWtype HWaddress Flags Mask Iface_gateway ether 00:50:56:f4:31:a8 C ens33수동으로 매핑해줄 수도 있다. NIC 교체 후 통신이 실패한다면 → 고정 IP주소를 사용하는 기관이나 회사의 경우 보안이나 IP주소 관리를 위해 고정 ARP테이블을 사용한다. 따라서 ARP 테이블을 확인하여 IP와 MAC주소가 제대로 매칭되는지 확인(nmcli , arp), 교체하면 네트워크 관리자에게 NIC의 MAC과 IP를 알려주어야 함resolver : name serviceIP address나 hostname을 해석해주는 것으로, 이를 이용해서 특정 이름을 지정해서 볼 때 dig 명령어를 사용한다.# dig dkssud8150.github.io...;; ANSWER SECTION:dkssud8150.github.io. 3600 IN A 185.199.108.153dkssud8150.github.io. 3600 IN A 185.199.109.153dkssud8150.github.io. 3600 IN A 185.199.110.153dkssud8150.github.io. 3600 IN A 185.199.111.153;; Query time: 49 msec;; SERVER: 127.0.0.53#53(127.0.0.53)nameserver를 직접 지정가능 dig [@server] target# dig @8.8.8.8 dkssud8150.github.io...;; ANSWER SECTION:dkssud8150.github.io. 3600 IN A 185.199.111.153dkssud8150.github.io. 3600 IN A 185.199.108.153dkssud8150.github.io. 3600 IN A 185.199.109.153dkssud8150.github.io. 3600 IN A 185.199.110.153;; Query time: 41 msec;; SERVER: 8.8.8.8#53(8.8.8.8)# dig @1.1.1.1 dkssud8150.github.io;; ANSWER SECTION:dkssud8150.github.io. 3600 IN A 185.199.108.153dkssud8150.github.io. 3600 IN A 185.199.109.153dkssud8150.github.io. 3600 IN A 185.199.110.153dkssud8150.github.io. 3600 IN A 185.199.111.153;; Query time: 44 msec;; SERVER: 1.1.1.1#53(1.1.1.1)nameserver == DNS 유명한 것은 알아두자. Cloudflare DNS : 1.1.1.1 , ipv6= 2606:4700:4700::1111 CISCO openDNS : 208.67.222.222 , 208.67.200.200 google DNS : 8.8.8.8네임서버를 지정해주는 이유는 지정하는 것마다 속도가 다르다. 그렇기에 어떤 것이 제일빠른지 비교해보고 사용하면 된다. @server를 지정해주지 않을 경우 /etc/resolv.conf에 기록되어 있는 nameserver를 사용한다.ethtool간혹 시스템이 느려지는 경우 duplex 문제일 가능성이 있다. 저전력, powersave모드 사용시 발생할 수 있다.여기서 speed와 duplex를 봐야 하는데, 시스템에서 지원하는 속도와 duplex와 다를 경우 문제가 있다. 그래서 속도와 duplex를 지정해주면 원상복귀된다.# ethtool enp0s3lf6supported link modes : 1000baseT/full...speed : 100Mb /sDuplex : Half# ethtool -s enp0s3lf6 speed 1000 duplex full# ethtool enp0s3lf6...speed : 1000Mb/sduplex : fullssh serverssh(secure shell) 개념ssh는 통신 구간을 암호화하는 서비스로 기본적으로 리눅스 서버들은 ssh서비스가 탑재되어 있다. 없으면 설치를 추천한다. 리눅스의 ssh는 openssh를 사용한다. sshd : ssh daemon, 즉 ssh server를 의미한다.ssh : ssh client이며, ssh 명령어가 바로 ssh client CLI이다. ms윈도우에서 접속하려면 putty나 mobaxterm 툴을 사용한다.sshd 서버 준비 sshd 서버의 설치 여부 확인 sshd 서비스가 실행 중인지 확인 Listen 상태인지도 확인 : ss -nlt or ss -nltp (프로세스 이름까지 보려면) ssh port(22/tcp)가 방화벽에 허용되어 있는지 확인 ssh port는 22를 사용한다. sshd 서버 설치 여부 확인 debian# open list openssh*...openssh-server/bionic-updates 1:7.6p1-4ubuntu0.6 amd64# apt openssh-server[installed]되어 있어야 함 RH계열# rpm -qa openssh-serversshd 서비스가 실행중인지 확인 systemd 기반이라면 systemctl로 확인# systemctl status sshd● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: ena Active: inactive (dead)sshd 서비스가 정지된 경우 systemd 기반이라면 systemctl start sshd# systemctl start sshd# systemctl status sshd● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: ena Active: active (running) since Fri 2022-02-25 01:51:51 KST; 5s ago Process: 4494 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS) Main PID: 4495 (sshd) Tasks: 1 (limit: 2292) CGroup: /system.slice/ssh.service └─4495 /usr/sbin/sshd -D 2월 25 01:51:51 jaeho-vm systemd[1]: Starting OpenBSD Secure Shell server... 2월 25 01:51:51 jaeho-vm sshd[4495]: Server listening on 0.0.0.0 port 22. 2월 25 01:51:51 jaeho-vm sshd[4495]: Server listening on :: port 22. 2월 25 01:51:51 jaeho-vm systemd[1]: Started OpenBSD Secure Shell server. 부팅할 때 sshd 서비스가 실행되도록 하고 싶다면 systemctl enable sshd 여기서 enable에 —now 옵션을 사용하면 start기능까지 수행됨 listen 상태 확인sshd는 22번 포트이므로 22번 포트 확인# ss -nltState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* **LISTEN 0 128 0.0.0.0:22 0.0.0.0:*** LISTEN 0 5 127.0.0.1:631 0.0.0.0:* **LISTEN 0 128 [::]:22 [::]:*** LISTEN 0 5 [::1]:631 [::]:*방화벽에 허용되어 있는지 확인# iptables -nLChain INPUT (policy ACCEPT)target prot opt source destination Chain FORWARD (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destinationubuntu의 경우# ufw status상태: 비활성# ufw enable방화벽이 활성 상태이며 시스템이 시작할 때 사용됩니다# ufw status상태: 활성# ufw allow 22/tcp규칙이 추가되었습니다규칙이 추가되었습니다 (v6)# ufw allow 80/tcp규칙이 추가되었습니다규칙이 추가되었습니다 (v6)accept가 허용 즉, 잘 되어 있다는 것이다. 허용되어 있지 않다면 허용시켜라. 안그럼 다 해킹당할 수 있음ssh clientssh [-p port] [username@] &amp;lt;host address&amp;gt;username을 적지않을 경우 지금 사용중인 유저네임으로 자동으로 선택됨, 다른 유저명을 사용하고 싶은 경우 적어주면된다.ssh-keygenssh를 사용하다보면 보안 문제로 인해 키 기반 통신을 하는 경우가 많다. 그래서 키를 생성하는 방법은 다음과 같다.ssh-keygen -N “” -N: passphrase - 키를 풀때마다 키를 넣어야 하는 것을 “”(공백)으로, 즉 이를 사용하지 않겠다는 것을 의미한다.키 생성# ssh-keygen -N &quot;&quot;접속할 서버를 카피# ssh-copy-id sunyzero@192.168.52.110카피 한 후에는 암호를 기입하지 않고도 바로 접속 가능# ssh sunyzero@192.168.52.110ssh는 접속 뿐만 아니라 어떤 명령어를 접속한 서버에서 실행한 결과를 볼 수도 있다.w는 앞의 서버에서 실행된 결과값을 알려주는 것이다.# ssh -t sunyzero@192.168.52.110 wcurl [options] URL기반의 통신하는 기능# curl https://dkssud8150.github.io&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;html lang=&quot;en&quot; data-mode=&quot;light&quot; &amp;gt;&amp;lt;head&amp;gt;&amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&amp;gt;&amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, shrink-to-fit=no&quot;&amp;gt;&amp;lt;meta name=&quot;day-prompt&quot; content=&quot;days ago&quot;&amp;gt;...결과를 파일로 저장하고 싶은 경우, manual.html파일로 저장# curl -O https://www.mycompany.com/docs/manual.html뒤에 붙이지 않고 파일명으로 직접 지정#curl -o myblog.html https://dkssud8150.github.io % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 22068 100 22068 0 0 468k 0 --:--:-- --:--:-- --:--:-- 468kcurl -C - -O 파일 다운로드에도 사용할 수 있다.-C는 다운로드 중간에 끊기더라도 계속해서 다운로드 가능# curl -C - -O http://blah.org/blah.isoAPI 서버로 접속해서 값들을 받아오는 것도 가능하다.날씨 api# curl v2.wttr.in/Seoul환율 api# curl https://api.exchangeratesapi.io/lastest?base=USDwget wget과 curl은 대부분 기능이 비슷하나 curl이 더 많은 기능을 가진다. 그러나 wget은 파일 다운로드에 특화된 기능이 존재# wget https://.../a.pngnc (netcat)nc: network 기능이 가능한 cat을 뜻한다.server, client의 양쪽 기능이 가능 간단한 간이 서버, 클라이언트로 사용 가능 바이너리 통신 가능TCP 5000# nc -k -l 5000 뒤의 주소와 tcp에 접속해서 hello를 보내게 됨# echo Hello | nc 127.0.0.1 5000그냥 바로 접속# nc 127.0.0.1 5000... interactive modewireless networknmcli r[adio]radio wifi [on|off]무선 네트워크 기능이 활성화되어 있는지 확인하는 코드로, on,off를 통해 키거나 끄기도 가능하다.# nmcli radioWIFI-HW WIFI WWAN-HW WWAN enabled enabled enabled enabled# nmcli r wifienabled# nmcli r wifi on만약에라도 wifi가 disabled 되어 있다면 우선적으로 blocked되어 있는지 봐야한다.# rfkill list...2: phy0: wireless LAN Soft blocked: no Hard blocked: nosoft blocked는 rfkill unblock으로 해제 가능하지만, hard(ware) blocked인 경우는 bios나 장치 fireware 기능이 막힌 것이기에 이것들에 접근해서 찾아야 한다. wicd는 옛날 것이므로 사용할 수 없다. 삭제해야 한다.nmcli dev [list]여기서 와이파이가 보여야 한다., list는 보통은 생략하지만, 구버전은 꼭 명시해야 하는 경우도 있다.현재 vmware에 와이파이 커넥터가 없어서 안뜸# nmcli devDEVICE TYPE STATE CONNECTION ens33 ethernet connected ens33 lo loopback unmanaged --접속 가능한 모든 wifi에 대해 보기, channel, 속도, signal 등을 알 수 있음 ,signal이 높을수록 좋음, wpa1은 보안이 취약# nmcli dev wifi해당 와이파이에 접속# nmcli dev wifi connect Dev_wifi4 password qwer01234접속되었는지 확인# nmcli d접속 끊기# nmcli d disconnect wlan0wifi hotspot ( ap mode )공유기 역할을 하는 것으로 핫스팟처럼 하는 것으로, 이를 만드는 방법은 다음과 같다.nmcli cifname은 아무거나 ssid(와이파이 스캔할 때 보여질 부분)# nmcli c add type wifi ifname &#39;*&#39; con-name syhotspot autoconnect no ssid syhotspot_rpi알맞게 설정하기# nmcli c mod syhotspot 802-11-wireless.mode ap 802-11-wireless.band bg ipv4.method shared \\&amp;gt; ipv4.addresses 192.168.100.1/24키 생성# nmcli c mod syhotspot 802-11-wireless-security.key-mgmt wpa-psk 802-11-wireless-security.psk suny1234활성화 (psk가 너무 짧으면 에러가 발생)# nmcli c up syhotspot확인# nmcli c sNAME...syhotspothotspot 정보 출력# nmcli -p c s syhotspot정지# nmcli c down syhotspotmode값을 ap로 고정했다. 속도에 따라서 band가 달라지는데 bg는 2.4gb에서 사용하는 band 값이다.hostapd네트워크매니저보다 더 많은 기능을 사용할 수 있고, AP로 구동을 위한 기능을 제공, daemon으로 작동한다.5GHz를 사용할 때는 그 나라마다 전파 채널, 속도 등이 다르다(regulatory domain rule). 그래서 나라를 설정해줘야 한다. 그렇지 않으면 커널을 수정해야 하기 때문에 주의해야 한다." }, { "title": "[데브코스] 2주차 - linux 기초(vim)", "url": "/posts/vim/", "categories": "Classlog, devcourse", "tags": "linux, vim, devcourse", "date": "2022-02-23 21:00:00 +0900", "snippet": "오답노트vim editorvim(vim - vi improved) editor는 linux에서 가장 많이 쓰이는 텍스트 편집기다. vim.org에서 윈도우 vim을 사용하 수 있다.vim설치$ sudo apt -y install vim이 때, 의존성 문제가 발생할 수 있다. 그래서$ sudo apt edit-sources를 치고, 자신이 원하는 에디터를 고른 후, 아래를 빈 줄에 타이핑한다.deb http://kr.archive.ubuntu.com/ubuntu/ bionic-updates main restricted universe그 후, 다시$ sudo apt update &amp;amp;&amp;amp; sudo apt install vimvim 시작vim특정 파일명을 열면서 시작$ vim mytext.txt파일명이 &quot;-&quot;일 경우 -stdin-을 의미함.$ find . -name &quot;*.txt&quot; | vim -vim mode 일반모드: normal mode (or command mode) 입력모드: insert mode 명령행모드: command-line mode ( or colon mode) [추가] 비주얼모드: visual mode = 마우스를 대신하는 드래그 모드normal에서는 타이핑이 불가능하기에 insert mode로 가야 타이핑이 가능하다. 그래서 a를 외워서 변경해준다. 타이핑을 실패하거나, 잘라내기 등을 할때는 노말모드로 해야 하기에 esc를 누른다. 특정 파일명을 지정하거나, 복잡한 명령을 써야하는 경우 커맨드라인 모드로 가야한다. 이때는 :을 사용한다.노말 모드는 간단하게 단축키를 쓸 수 있게 해주는 모드이다.a,i,o에 대한 뜻이 다 있는데, a = append: 현재 커서 위치에서 한 칸 뒤로 이동한 후 입력 모드 i = insert: 현재 커서 위치에서 입력모드 o = open line: 현재 행 아래에 새로운 행을 하나 만든 후 입력 모드하지만, a를 가장많이 쓰므로, a만 외워도 괜찮다.노말 모드가 필요한 이유text-based에서는 GUI 메뉴가 없어서 단축키로 해야 한다.$ vim clientlist.txta (입력 모드로 전환)텍스트 입력&amp;lt;ESC&amp;gt; (일반 모드로 전환):w (= write, 저장):q (= quit, 나가기)노말 모드에서는 화살표키 대신 hjkl(좌하상우)을 사용한다. 명령어 설명 [#]h 좌로 #칸 이동, # 디폴트값은 1 [#]j 아래로 #칸 이동 [#]k 위로 #칸 [#]j 우로 #칸 ^ 행 맨 앞으로 이동 $ 행 맨 끝으로 이동 ctrl+B = 위로 한 화면 스크롤 ctrl+F = 아래로 한 화면 스크롤 ctrl+U 위로 1/2화면 스크롤 ctrl+D 아래로 1/2화면 스크롤 [#]gg #행으로 이동, #이 생략되면 1을 의미 [#]G #행으로 이동, #이 생략되면 마지막 행을 의미 :# #행으로 이동 ctrl+G 현재 문서 위치 정보를 하단 상태 바에 표시 force 명령!를 뒤에 추가한다. e.g. :wq!저장이나 창을 닫을 경우 팝업창이 뜨는데, vim에서는 이런 팝업창을 띄워주는 것이 없다. 그래서 저장을 하지 않으면 에러가 난다. 저장하지 않고, 나가려고 하면 q!를 해주어야 한다.vim 일반 모드 기능buffer (register 기능)buffer: delete, cutvim에서의 삭제는 임시 버퍼에 잘라내기가 된다. 명령어 설명 x 커서에 위치한 문자 삭제 dd 현재 행을 삭제 D 현재 컬럼 위치에서 현재 행의 끝부분까지 삭제 J 아래 행을 현재 행의 끝에 붙임 p 현재 행에 붙여넣는다. P 현재 행의 위쪽에 붙는다. yy or Y 현재 행을 레지스터에 복사 u undo, 바로 이전에 행한 명령 한 개를 취소 ctrl+R redo, 바로 이전에 취소했던 명령을 다시 실행 3dd라 하면 3개의 행을 삭제, 5p이면 5번을 붙이게 된다.명령행 모드 범위를 지정해서 명령 명령어 설명 :20d 20번 행을 삭제 :10,25d 10~25번 행 삭제 :10,$d 10~마지막 행 삭제 :%y 문서 전체를 복사, %는 1,$와 동일 :.,+20y 현재 행부터 아래로 20행 복사 :-10,+5d 현재 행부터 위로 10, 아래로 5행 총 16행 삭제 :40pu 40번 행에 레지스터 내용 붙여넣기 범위 연산 기호 의미 . 현재 행 $ 마지막 행 +# 현재 위치에서 #만큼 아래 행 -# 현재 위치에서 #만큼 위 행 % 문서 전체 드래그 기능 명령어 설명 v 일반 비주얼 모드로 현재 커서 위치에서 블록을 지정 V visual line mode로, 현재 커서가 위치한 행에서 행 단위로 블록을 지정 ctrl+V visual block mode로 열 단위로 블록을 지정 드래그 한 상태로 colon(:)을 누르면 범위 지정해서 명령 가능 비쥬얼 블록 열 편집 명령어 ctrl+V, ESC*2 I: insert A: append C: change ~: switch case 특정 열에 문자열을 삽입하거나 교체할 때 많이 사용 1줄씩 편집하는 것이 아닌 전체를 통으로 수정 ⇒ gg → ^V → G → I → 0000 → ESC,ESC 하게 되면 전체 줄에 대해 0000을 삽입비주얼 모드 gv: 이전 강조 텍스트 영역 불러오기 o: 강조 텍스트 블록의 시작으로 이동 한번 더 누르면 끝으로 이동vim의 바이너리 종류 이름 설명 vi vi 호환의 vim, 가장 적은 기능을 가지기에 tiny vim이라 부른다. vim 일반적인 vim, enhanced vim이라고도 함 vimx 가장 많은 기능을 가진 vim, vim+clipboard, X window clipboard 기능이 포함됨 기능 redHat 패키지 이름 debian 패키지 이름 일반 vi vim-minimal (명령어:vi) vim-tiny(명령어:vi) enhanced vim vim-enhanced (명령어: vim) vim-nox(명령어:vim) vim with X vim-X11 (명령어: vimx,gvim) vim-gnome or vim-athena (명령어: vim, gvim, evim) 설치$ sudo apt -y install vim-gnome제거$ sudo apt -y --auto-remove remove vim-gnome일반 유저의 경우 alias vi=vim으로 잡혀있다. root 유저의 경우 vi로 실행과 vim이 서로 다르다.클립보드 설정x window 환경에서 vim과 클립보드를 공유한다는 것은 x window와 유닉스의 클립보드가 공유되지 않기 때문에, x window와 vim이 클립보드를 공유할 수 있도록 하기 위해 vim-gnome을 설치하고, 설정을 더 해야 한다.~/.vimrc : vim config설치 후 아래 부분을 추가해줘야 한다. &quot; 는 vim에서 주석을 의미한다.&quot; clipboard settings : Copy/Paste/Cutif has(&#39;unnamedplus&#39;) set clipboard=unnamed, unnamedplusendif서로 공유가 되는지 확인을 위해서 2개의 터미널을 켜고, 1개는 vimx, 2번째는 gedit를 켜서 vimx에서는 복사는 y, 붙여넣기는 p gedit에서는 복사는 ctrl+c, 붙여넣기는 ctrl+v명령행 모드 옵션 설정주로 사용하는 옵션 옵션 설명 nu 화면에 행 번호 표시 rnu 현재 행을 기준으로 상하의 상대적 행 번호 표시 ai 자동 들여쓰기 cindent C언어 스타일의 들여쓰기 ts = value 화면에 표시될 탭 크기를 value로 지정(기본은 8인데, 4가 자주 쓰임) sw = value 자동 블록 이동 시 열의 너비(대부분 ts와 같은 수나 배수로 함) fencs = value 지원할 파일 인코딩 방식 리스트 *복수 개 지정 시 공백 없이 콤마로 구분 fenc = value 현재 파일 인코딩 방식을 지정 vim custom colorscheme라고 검색하면 github에 많이 있다. 사람들이 많이 쓰는 스킨테마는 molokai, jellybeans, material 이다. 원하는 컬러를 골라 적용하면 된다.vim 매뉴얼vim은 help를 지원한다. 입력모드 접두어 예 일반 없음 :help x 입력 i_ :help i_CTRL_N 명령행 : :help :w 비주얼 v_ :help v_u vim 실행인수 -   옵션 ‘   명령행 모드 특수키 c_   설명 글을 보면 색이 칠해져 있는 것들이 있다. 이 때, ^] 를 누르면 그것에 대한 설명으로 이동한다. 다시 돌아오는 키는 ^T 이다. help창과 edit창이 이분할되는데 이 두개를 왓다갔다하는 방법은 ^W^W 이다.vim의 에러파일을 중복해서 오픈한 경우 두 가지를 분별해야 한다. 다른 프로그램이 동일한 파일을 수정한 경우 에디트 세션이 충돌되어 죽은 경우두가지를 분별하는 방법은 process ID에 still running이라 나오면 1번 ⇒ 읽기 전용으로 보든지, 아니면 나가거나 process ID에 still running이 아니면 2번 ⇒ recover(R)을 해라. ⇒ 그래도 에러가 뜨면서 swp 파일 뭐라 뜨면 (d)elete라고 뜨면 d하고, 아니면 나가서 rm -rf .&amp;lt;파일 이름&amp;gt;.&amp;lt;확장자&amp;gt;.swp 문자열 관련 기능formatting정렬하는 것으로 center 정렬 할 경우 width가 몇인지 지정해줘야 함, 기본은 80으로 되어 있다. 50으로 지정:center 50find 명령어 설명 fc, f/, f; 문자 c를 전방 검색 Fc, F/, F; 문자 c를 후방 검색 ; 최근 검색을 재검색 , 최근 검색을 반대방향으로 재검색 f, 를 한 후 ;를 누르면 →방향으로 다음, 다음 다음 ,를 하면 ←방향으로 뒤로일반모드 검색 - 명령어 명령어 설명 /string string을 전방탐색함 ?string string을 후방탐색 * 현재 커서에 위치한 단어를 바로 전방탐색 # 현재 커서에 위치한 단어를 바로 후방탐색 n 다음 탐색 결과를 찾아냄(전방이면 전방, 후방이면 후방) N n과 반대방향으로 다음 검색 % 괄호의 짝을 찾아줌 대소문자를 구분하지 않고 하기 위해서는 \\c처럼 \\를 추가한다.e.g. /inter : 대소문자 구분해서 /\\cinter : 대소문자 구별하지 않고또는 대소문자 구분을 키거나 끄는 옵션을 설정하려면 켜기(default) → :set ignorecase 끄기 → :set noignorecase검색 후 밝게 표시된 단어를 해제하고 싶을 때는 :nohl 로 설정한다.[section]에서 section을 정규표현식이 아닌 이 단어 자체를 찾고자 한다면 magic을 끄도록 /\\M[section] 이라 해야 한다.교체sed의 기능이 추가된 것으로 sed와 문법이 같다.명령행으로:[range]s/&amp;lt;찾는 문자열&amp;gt;/교체할 문자열&amp;gt;/&amp;lt;옵션&amp;gt; s는 sebstitute라는 의미 구분자는 / 와 , 둘다 가능 구분자로 구분자가 아닌 진짜 슬러시라는 것을 표시하기 위해 \\를 덧붙인다. (e.g. s/\\/usr\\/linux/\\/usr\\/linuxer/g) 옵션은 g,i,c,e 가 잇는데 g(검색된 문자열 모두를 교체) 한다고 하는 것을 많이 씀 이 때, 특수문자도 찾을 수 있다. NewLine = 개행 문자, windows/DOS 의 파일을 unix로 가져오면 깨지거나 이상한 문자가 붙고, unix파일을 windows로 가져가면 다 붙어서 불려온다. 그러나 실전에서는 가져왔을 때는 :set ff=dos 라고 하면 dos포맷, 가져다 주려면 :Set ff=unix로 하면 unix 포맷으로 되고 바꾼 후 주면 된다.파일 관련 기능용어 buffer : 파일을 편집하기 위한 임시 공간 예를 들어 파일명 없이 vim을 실행하면 이름이 없는 buffer에 작업하게 된다. 그러다가 :w myfile.txt 처럼 저장하면 buffer는 myfile.txt에 저장되면서 이름을 가지게 된다. register: 텍스트 일부를 저장하고 잇는 임시 공간 , dd로 삭제하고 p로 붙여넣을 수 있게 해주는 공간vim에서 다른 파일을 편집하려고 할 때는 명령어 설명 :e [filename] filename을 편집모드로 오픈, filename이 생략되면 현재 파일을 다시 오픈 :e #[count] (==ctrl+6) count번째 파일을 오픈, count를 생략하면 바로 이전 파일 종료 기능 명령어 설명 :q! 현재 창 종료 :qa! 모든 창 종료, 복수 창 열려 있을 때 유용 :x 저장하면서 종료 (일반 모드에서는 ZZ)(저장할 게 있으면 저장 없으면 그냥 종료) vim에서 디렉토리 열기현재 디렉토리를 vim으로 열고자 하면, vim . 하면 출력값이 주욱 나오고, 커서를 해당 파일로 가져가 엔터치면 열린다. 또는 :e &amp;lt;path&amp;gt;로 실행해도 된다.디렉토리에서 &amp;lt;F1&amp;gt; 누르면 명령키가 나온다. enter을 누르면 현재 창에 열어준다 ⇒ ctrl + 6누르면 다시 돌아옴 i: 파일 표시 방법을 변경시켜준다. s: 정렬 방식을 바꾼다. o: 해당 파일을 수평 분할된 새창으로 열어줌 v: 해당 파일을 수직 분할된 새창으로 열어줌 p: 미리보기분할 기능 같은 파일을 분할하려면 수평 분할 &amp;lt;ctrl+w&amp;gt;+ s or :sp [file] 수직 분할 &amp;lt;ctrl+w&amp;gt; + v or :vs [file] 창을 이동하는 명령 | 명령어 | 설명 | | — | — | | &amp;lt;ctrl+w&amp;gt; + 방향키 | | | &amp;lt;ctrl+w&amp;gt; + w | 현재 창에서 오른쪽 방향으로 이동 | 창 크기 조절 | 명령어 | 설명 | | — | — | | &amp;lt;ctrl+w&amp;gt; = | 모든 창 크기 동일 | | &amp;lt;ctrl+w&amp;gt; [#]+ | #크기만큼 크기 키움 | | &amp;lt;ctrl+w&amp;gt; [#]- | #크기만큼 크기 줄임 |++ -d 옵션 (diff)vim -d file1 file2다른 부분을 찾아주고, 어디가 구체적으로 다른지 표시해준다.소스 코드 비교할 때, 설정 파일 비교할 때 많이 사용되며, 실시간으로 비교해준다. do를 타이핑하면 해당 부분을 바로 카피하고, dp를 타이핑하면 삭제한다.tabpage분할할 때마다 원래 창의 크기가 줄어들기에 탭페이지 기능을 많이 사용하는 기능이 있다.옵션 : -pvim -p file1 file2 file3vim안에서 추가하고 싶다면 명령행 모드로 명령어 설명 :[#]tabe[dit] file #번째 탭에 파일을 연다. #을 생량하면 현재 탭 뒤에 생성, 번호는 0번 부터 [#]gt ( colon은 너무 많이 써야됨) 다음 탭으로 이동, #에 숫자를 지정하면 탭 번호로 이동 [#]gT 이전 탭으로 이동 buffer:files :buffers 둘 중 아무거나 타이핑하면 현재 저장 전인 것들의 리스트를 볼 수 있다.추가 옵션으로는 다음과 같다. % : 현재 편집 중인 버퍼 # : 바로 이전에 열었던 버퍼, ctrl+6누르면 #이 표시된 파일 열림이 때, 경로명을 적으면 경로명을 인식해서 열기가 가능하다. gf : 파일이 켜져 있는 상태로 이동 , ctrl+6를 누르면 다시 돌아옴 c언어에서 헤더파일을 볼수도 있다. &amp;lt;ctrl+w&amp;gt; + f : 분할된 창에 열어주는 것 &amp;lt;ctrl+w&amp;gt; + gf : 파일명을 탭에 열어줌파일 encoding줄여서 fencs파일을 읽어서 확인할 encoding list를 설정변환이 필요한 경우 지원한다. 따라서 .vimrc에 설정되어 있어야만 읽어올 수 있다.:set fenc=value구분은 ,로 한다. ,중간에 공백은 있으면 안된다. 앞부터 순서대로 점검한다. 순서가 중요하다.ucs-bom: 무조건 적어야함, 바이코드마크가 있는지 체크해주는 것latin-1: asciikorea: 유닉스에서는 euc-kr, 윈도우에서는 cp949로 자동변환함japan도 마찬가지:help encoding-values 라 하면 많이 나온다.fencs는 설정 리스트이고, fenc는 설정하는 것~/.vimrc 에서는 set fencs =일반 파일에서 다른 파일로 설정하기 위해서는 :set fenc=텍스트 파일을 윈도우로 가져갈 때는 :set ff=dos :set fenc=korea :wq 다른 이름으로 저장하고 싶은 경우 :sav new_dosfile.txt 윈도우에서 가져온다면 :set ff=unix :set fenc=utf8 :wq편리한 기능단어 경계 이동 w: 단어 바로 앞에 e: 단어 바로 뒤에 ^: 공백이 아닌 실제 내용이 있는 시작 열 $: 마지막 열괄호, 블록 단위 이동 %: 괄호 짝으로 이동 (, ) / {, } / [, ] 문장, 문단, 블록 단위의 시작 위치, 끝 위치로 이동단축어 기능 - 특정 단어를 사용자 설정으로 쓸 수 있다.ab, ia 기능ia: insert mode에서만 작동하는 기능ca: commandline mode에서만 작동하는 기능ca: 한글상태 오타를 변환할 수 있다. ca ㅈ w ca ㅈㅂ wq ca ㅂ q ca ㅌ x간혹 .vimrc 파일이 euc-kr로 되어 잇으면 편집시 작동하지 않을 수도 있다.:set all 로 치면 fenc가 어떻게 설정되어 있는지 확인 가능 명령어 설명 :ab [lhs] 현재 설정된 모든 약어 목록 출력, lhs 지정하면 해당 약어 정보만 :ab {lhs} {rhs} lhs 약어를 rhs로 변환 :unab {lhs} 약어 lhs 해제 :abclear 설정된 모든 약어 해제 :ia {lhs} {rhs} ab와 기능은 같으나 입력 모드에서만 작동 :ca {lhs} {rhs} 명령행 모드에서만 작동 편리하게 모든 곳에서 사용하도록 vimrc에 저장시켜놓을 수도 있다.: 함수를 작성해서 넣을 수 있다. 마지막 은 과 같이 enter를 치라는 것## autocmd특정 상황에서 자동으로 실행할 명령*.c 파일을 열 때 자동으로 실행되어야 하는 명령으로```bashautocmd BufRead,BufNewFile *.txt colo evening```하면 버퍼를 읽거나, 새로운 파일을 만들 때 txt파일이면 테마를 evening으로 해라편리한 기능으로 스왑파일이 있으면 그 파일을 읽기 전용으로만 하게끔```bashau SwapExists * let v:swapchoice = &#39;o&#39;```au는 축약어파일 확장자 말고, 경로가 될 수 도 있다. */include/*## 편리한 기능2### 들여쓰기를 다시 하기어디서 불러온 경우 들여쓰기가 안되어 있는 경우가 있다. 이를 retab이라 하는데 두 가지 방법이 있다.- ={motion} - motion에는 이동 관련 키 gg,G, )), ]] 등을 사용할 수 있다. - gg → = → G - 를 하면 맨 앞부터 맨뒤까지 =, 즉 retab해준다.- visualmode에서 = - 비주얼모드에서 라인 선택 후 = 키를 눌러도 된다. ### 기존의 탭 문자를 공백 4칸으로 전환하고 싶으면:set et ts=4:ret반대로 공백 4칸을 탭으로 하려하면:set noet ts=4:ret!강제 변환이므로 !### 자동완성 기능기존의 단어가 존재할 경우 단어를 타이핑하다가 ctrl+N 누르면 앞서 사용한 단어를 보고 추론하여 자동완성해준다. 그러나 include 구문 안에 있는 것들도 인식해서 찾아준다.이때 ctrl+x를 누르고 ctrl+n을 하면 사전 기반 검색이라고 하는데 특수 문자, 경로명도 찾을 수 있다. 취소하려면 ESC### vim-bootstrap자동으로 vimrc를 만들어주는 곳이 잇다.[https://vim-bootstrap.com/](https://vim-bootstrap.com/)다하고 나서 제너레이트 누르면 파일을 하나 만들어주는데, 이를 .vimrc로 변환해서 자신의 홈디렉토리로 복사더 수정해서 넣어야 할것들fencs. ia, ca 등" }, { "title": "[데브코스] 2주차 - linux 기초(필수 명령어, process)", "url": "/posts/linux3/", "categories": "Classlog, devcourse", "tags": "linux, devcourse", "date": "2022-02-23 13:00:00 +0900", "snippet": "Linux 필수 개념i18ninternationalizationi~n사이의 알파뱃 개수절대 경로 , 상대 경로절대 경로(absolute path = abs-path) root directory를 시작으로 하는 경로 e.g. /usr/local/bin 상대 경로(relative path) 현재 디렉토리 . 를 시작으로 하는 경로 (.은 생략가능하다) e.g. ../../tmp == ./../../tmp e.g. work/day1 == ./work/day1unix file modefile mode bit는 3+9 bit 체계 숨겨진 3bit를 포함하여 12표기 방법 symbolic mode octal mode: bit를 8진수법으로 표기 → 이방법을 많이 사용디렉토리의 경우 기본 mode값은 umask값을 뺀 나머지가 된다.umask가 022라면 디렉토리가 777 - 022 = 755가 생성시 기본 mode가 된다. 파일은 666 - 022 = 644가 생성시 기본 mode가 된다.linux 필수 명령어Tab완성 기능이라는 것이 있다. &amp;lt;tab&amp;gt;키를 누르면 자동 완성이 나온다. 그 단어로 시작하는 것이 여러 개일 경우에는 tab-tab을 쳐야 나온다. e.g. ls /s 만 치고 tab하면 ls /usr/로 자동완성된다. 하지만 ls/usr/s 하고 tab을 누르면 안나온다. 더블탭을 누르게 되면 s로 시작하는 여러 개를 보여준다.LANG은 대체로 영여로 해놓는 것이 좋다.$ export LANG=en_US.utf8lsls [option] a : all l : long t : sort by mtime (newest first) r : reverse$ ls -aldrwx------ 3 jaehoyoon jaehoyoon 4096 2월 21 15:37 .gnupgdrwx------ 3 jaehoyoon jaehoyoon 4096 2월 21 15:37 .localdrwx------ 3 jaehoyoon jaehoyoon 4096 2월 21 15:41 .mozilla-rw-r--r-- 1 jaehoyoon jaehoyoon 807 2월 21 15:18 .profile-rw-r--r-- 1 jaehoyoon jaehoyoon 0 2월 21 15:46 .sudo_as_admin_successfuldrwx------ 6 jaehoyoon jaehoyoon 4096 2월 21 15:41 .thunderbirdls는 파일정보를 나타내는 명령어다.맨 앞에 d,-는 파일 타입을 나타내는데, -는 일반 파일, d는 디렉토리, l은 symbolic link에 해당하고, 그 다음 숫자는 링크의 갯수를 나타낸다. r의 권한은 dir의 목록을 읽을 수 있고, x의 권한은 있어야만 목록에 있는 링크에 access할 수 있다. x를 가지면 파일의 명만 알면 access할 수 있지만, r의 권한이 없으면 파일의 명을 외부에서 볼 수는 없다.ls -lt는 길게 정렬하여 보여주는데, 최신파일이 맨 위에 있기에 뒤집어서 보는 것이 편하다. reverse를 시켜주기 위해 -ltr를 옵션으로 둔다.directionecho &#39;hello world&#39; &amp;gt; testdir/hello.txt라고 작성하면 hello world라는 것을 hello.txt에 저장하겠다는 것이다.chmodmode값을 변경하기 위해서는 chmod$ chmod 664 testdir$ ls testdirchmod 664 대신 -x, +x, =rx 등으로 할 수도 있다.mkdir, rm디렉토리를 만드는 건 mkdir, 삭제할 때는 rm -rf로 사용한다. 여기서 여러 개의 디렉토리를 만들려면 mkdir -p, -p를 꼭 두어야 한다.$ mkdir -p mka/asmsrm: removecp, mvcp: copy &amp;lt;원래 디렉토리&amp;gt; &amp;lt;복사할 디렉토리&amp;gt;mv: move &amp;lt;원래 디렉토리&amp;gt; &amp;lt;이동할 디렉토리&amp;gt;mv에서 이동될 디렉토리에서 디렉토리인데, ~/까지만 적으면 원래의 디렉토리 이름 그대로, 이름까지 적어주면 새로운 이름으로 이동됨$ mv ~/bashrc_example ~/old_bashrc$ ls -l !$!$는 그 전의 명령어를 다시 불러오는 것이다.chown, chgrp소유자, 그룹을 변경하는 명령어다.# chown root helloworld# ls -l helloworld-rwxrwx-wx 1 root jaehoyoon ...이 때 root 유저으로 해야 가능하다.filefile명령어는 파일의 타입 확인하는 명령어다. 고유의 표식을 근거로 파일의 종류를 분류하는데 이 근거를 magic 데이터라 한다. 이 magic데이터의 위치는 대부분 /usr/share/file/magic에 있다.statstat은 status of file로, meta data 즉, 수식하는 정보, 파일 이름, 생성시간, 권한 등을 출력한다. !$와 비슷하게 &amp;lt;ALT + .&amp;gt; 을 눌러도 맨 마지막 인수 단어를 카피해온다.이 출력 중 중요한 것은 access: 마지막으로 접근한 시간 modify: data가 변경된 시간 (==mtime) change: meta data가 변경된 시간 (==ctime)$ stat ~/.bashrcFile: /home/jaehoyoon/.bashrc Size: 3771 Blocks: 8 IO Block: 4096 일반 파일Device: 801h/2049d Inode: 2228228 Links: 1Access: (0644/-rw-r--r--) Uid: ( 1000/jaehoyoon) Gid: ( 1000/jaehoyoon)Access: 2022-02-21 15:37:32.595367589 +0900Modify: 2022-02-21 15:18:16.532580138 +0900Change: 2022-02-21 15:18:16.532580138 +0900$ cp ~/.bashrc ~/old_bashrc$ stat &amp;lt;ALT+.&amp;gt; -&amp;gt; stat ~/old_bashrcFile: /home/jaehoyoon/old_bashrc Size: 3771 Blocks: 8 IO Block: 4096 일반 파일Device: 801h/2049d Inode: 2228798 Links: 1Access: (0644/-rw-r--r--) Uid: ( 1000/jaehoyoon) Gid: ( 1000/jaehoyoon)Access: 2022-02-22 14:14:22.880796911 +0900Modify: 2022-02-22 14:14:22.880796911 +0900Change: 2022-02-22 14:14:22.880796911 +0900$ mv ~/old_bashrc ~/old_bashrc2$ stat ~/old_bashrc2File: /home/jaehoyoon/old_bashrc2 Size: 3771 Blocks: 8 IO Block: 4096 일반 파일Device: 801h/2049d Inode: 2228798 Links: 1Access: (0644/-rw-r--r--) Uid: ( 1000/jaehoyoon) Gid: ( 1000/jaehoyoon)Access: 2022-02-22 14:14:22.880796911 +0900Modify: 2022-02-22 14:14:22.880796911 +0900Change: 2022-02-22 14:15:36.803985016 +0900이렇게 했을 때cp를 하게 되면 전체가 바뀌고, mv는 ctime만 변경되는 것을 볼 수 있다.touch파일이 존재할 경우 메타 정보를 업데이트하고, 없을 경우 빈 파일을 생성$ touch emptyfile$ stat emptyfile File: emptyfile Size: 0 Blocks: 0 IO Block: 4096 일반 빈 파일Device: 801h/2049d Inode: 2228800 Links: 1Access: (0664/-rw-rw-r--) Uid: ( 1000/jaehoyoon) Gid: ( 1000/jaehoyoon)Access: 2022-02-22 14:20:45.939490334 +0900Modify: 2022-02-22 14:20:45.939490334 +0900Change: 2022-02-22 14:20:45.939490334 +0900 Birth: -$ touch emptyfile$ stat emptyfile File: emptyfile Size: 0 Blocks: 0 IO Block: 4096 일반 빈 파일Device: 801h/2049d Inode: 2228800 Links: 1Access: (0664/-rw-rw-r--) Uid: ( 1000/jaehoyoon) Gid: ( 1000/jaehoyoon)Access: 2022-02-22 14:20:56.535844324 +0900Modify: 2022-02-22 14:20:56.535844324 +0900Change: 2022-02-22 14:20:56.535844324 +0900 Birth: -find파일을 조건에 맞게 검색하고, 검색 후 작업을 할 수도 있다. size에 정확한 n을 찾는 경우 n, n보다 큰 것은 +n, 작은 것은 -n으로 실행한다. name에는 *와 같은 와일드카드를 사용할 수 있다. maxdepth를 사용하려면 모든 조건 맨 앞에 와야 한다.find 검색 조건으로는 -name filename : filename의 이름과 같은 파일을 검색 -size n : 크기가 n인 파일을 검색 -mtime n : 변경된 시간이 n인 파일을 검색 (단위는 day) -mmin n : 변경된 시간이 n인 파일을 검색 (단위는 minute) -inum n : inode number가 n인 파일을 검색 -samefile file : file과 같은 inode를 가진 파일을 검색 (= 같은 하드링크를 검색) -maxdepth level : 탐색할 위치의 하위디렉토리 최대 깊이가 level인 파일을 검색 -mindepth level : 탐색할 위치의 하위디렉토리 최소 깊이가 level인 파일을 검색$ mkdir ~/tmpp; cd !$$ for i in {8..21}; do dd bs=100000 count = $i if=/dev/zero of =./${i}00k.dat; done1.$ find . -name &#39;[89]*k.dat2.$ find . -name ‘*k.dat’ -a -size 1M3.$ find . -name ‘*k.dat’ -a -size +1500k -size -1800k./1600k.dat./1700k.dat./1800k.dat ‘[89]*k.dat’: .은 기준 디렉토리, -name은 조건에서 [89]는 8또는 9가 들어가는 마지막이 k인 .dat들을 검색한다. -a는 AND(생략 가능), -o는 OR 이다. 사이즈는 단위가 M단위이므로 0~1M에 걸쳐지는 것들을 검색한다. 여기서 주의해야 할 것들은 와일드카드를 사용하면 꼭 ‘’나 “”를 사용해줘야 한다.작업 지시 (-exec 명령어) find … -exec 명령어 \\; = 매번 찾을때마다 실행 find … -exec 명령어 + = 다 구한 다음 실행파일이 a.tmp,b.tmp,c.tmp가 있다고 가정할 때 find … rm {} \\;를 하면 rm a.tmp rm b.tmp rm c.tmp +를 하면 rm a.tmp b.tmp c.tmp파일 크기가 많으면 +가 더 빠르고 효율적이다. 그러나 수십만 개라 하면 +가 에러가 난다.실습 현재 디렉토리 아래에서 최근 24시간 이내의 일반 파일을 찾아 mtime_b24.txt로 저장 현재 디렉토리에서 3단계 아래를 넘어가는 경우는 검색하지 않고, 조건에 만족하는 것들을 ~/backup 디렉토리에 복사$ find ./ -mtime -1 -type f &amp;gt; mtime_b24.txt$ find ./ -maxdepth 3 -mtime -1 -type f -exec cp {} ~/backup \\\\;$ stat ~/backupFile: /home/jaehoyoon/backup Size: 1500000 Blocks: 2936 IO Block: 4096 일반 파일Device: 801h/2049d Inode: 2228820 Links: 1Access: (0664/-rw-rw-r--) Uid: ( 1000/jaehoyoon) Gid: ( 1000/jaehoyoon)Access: 2022-02-22 14:49:32.418745318 +0900Modify: 2022-02-22 14:49:19.940008219 +0900Change: 2022-02-22 14:49:19.940008219 +0900 Birth: -stdio표준 입출력에 대한 명령어이다.file channel: file에 입출력하기 위한 통로 file channel에 입출력하기 위해 하드웨어를 직접 접근하지 않고, 표준화된 입출력 방식을 통하도록 하는 가상화 레이어의 일종파일채널은 파일에 입출력하기 위한 메타 정보를 가지는 객체이므로 프로세스 종료시 정보 증발한다. 파일 채널을 지칭할 때 파일 서술자(file descriptor = fd)라는 유일한 식별자가 존재한다. 0번부터 시작하고, 0은 stdin(표준 입력), 1은 stdout(표준출력), stderr(표준에러) -&amp;gt; 0은 입력방향, 1,2는 출력방향에 해당한다.find 명령의 출력 (stdout)이 wc 명령의 입력(stdin)과 연결하는 것으로 find ~의 출력이 wc -l의 입력으로 들어간다.$ find ~ | wc -l174-&amp;gt; 이를 | 안쓰고 하게 되면$ find ~ &amp;gt; tmp.txt;wc -l &amp;lt; tmp.txt; rm tmp.txtfind ~ 는 홈 디렉토리, 즉 홈 디렉토리를 다 찾아서 wc(word count) -l는 line 수를 카운트한다.pipePIPE: 프로세스 사이에 통신으로 사용, IPC(inter process communication)의 일종파이프의 종류 annoymous pipe(익명) - temporary 프로세스 종료하면 사라짐 named pipe - persistency 프로세스 종료해도 남아있음 annoymous pipe 임시로 생성되었다가 소멸되는 파이프 프로세스들의 직렬 연결 명령행에서 로 사용 e.g. A B C A,B,C를 직렬로 연결해서 A의 출력이 B의 입력으로 연결,, 줄여서 파이프라고도 많이 함 named pipe 유닉스에서는 FIFO pipe라고 부른다. path를 가지는 것을 명명되었다고 표현 mkfifo 명령을 사용해서 생성 redirection방향 다른 곳으로 연결한다는 것으로 출력을 입력으로 넣거나 할 때 사용한다. A &amp;gt; B: A의 stdout을 파일 B로 연결 e.g. ls &amp;gt; filelist1.txt e.g. strace ls 2&amp;gt; strace.txt -&amp;gt; 2&amp;gt;는 2번 파일서술자(stderr)를 파일로 연결 A &amp;lt; B: A의 stdin을 파일 B로 연결 e.g. sort &amp;lt; names.txt A &amp;gt;&amp;gt; B: 방향은 &amp;gt;과 같고, 추가하는 모드catstdout와 파일을 자유롭게 연결해주는 기본 필터이다.가장 자주 쓰이는 쓰임새는 파일의 내용을 stdout으로 출력 stdin의 입력을 redirection해서 파일로 출력 → 타이핑 치고 ctrl + D 치면 저장$ cat ~/.bashrc$ cat $? &amp;gt; hello.txtarchive아카이브는 보관용 묶음, compress는 압축이다.UNIX계열은 여러 파일을 묶는 작업과 압축이 분리이 존재한다. 아카이브 유틸: tar, cpio 압축 유틸: gzip, bzip2, zstd(속도), xz(압축율)tartar [ctxv] [f archive-file] files... c: create t: test x: extract v: verbose ( 거의 안씀 ) f archive-file: 입출력할 아카이브 파일명 —exclude file: 대상 중 file을 제외 (특정 파일을 제외할 때 사용)$ tar cf arc_c.tar *.cf옵션을 주어 *.c의 출력이 arc_c.tar에 저장한다는 것이다.compressxz, zstd를 많이 쓴다.xzxz [-cdflrv] &amp;lt;file ...&amp;gt; d: 압축해제 c: stdout 표준 출력으로 결과물 보냄 1,-9: fast, better 등의 압축 레벨실습e.g. 압축 → 해제$ tar c /etc/*.conf | xz -c &amp;gt; etc.tar.gz$ xz -cd etc.tar.gz | tar xzstdzstd [options] [-|input-file] [-o output-file]options - #: 압축 레벨 설정 1-19 [ default:3] tar, gzip 사용 예제 (정통파 방법)$ mkdir ~/tmp$ cd !$$ cp -rf /etc .$ tar c . | gzip -c &amp;gt; bak_etc.tar.gztar → . 디렉토리 즉, 현재 디렉토리에 전부를 묶어서 생성(c)한 결과를 gzip → 표준출력으로 결과물을 보냄보낸 결과를 bak_etc.tar.gz로 압축 이 정통파 방법이 중요한 이유는 요즘 방법을 사용할 때는 멀티쓰레드를 쓰지 못하여 정통파 방법을 통해 멀티쓰레드를 사용해야 하기 때문이다. 최근 방법GNU tar 방법이라 해서 tar cfa 에서 a를 붙이게 되면 알아서 뒤의 확장자를 보고 판단한다. 대신 멀티 쓰레드를 못쓴다.$ tar cfa bak_data.tar.xz ./data ./exp$ tar cfa bak_data.tar.zst ./data ./exp멀티 쓰레드 사용$ tar c ./data ./exp | zstd -T0 &amp;gt; bak_data.tar.zst위는 ./exp의 결과를 ./data의 폴더에 bak_data이름으로 압축하는 코드다.압축을 풀 때는$ tar xfa bak_data.tar.xz$ tar xfa bak_data.tar.zst멀티 쓰레드 사용$ zstd -dcT0 bak_data.tar.zst | tar x실습$ git clone [https://github.com/htop-dev/htop](https://github.com/htop-dev/htop)htop 디렉토리를$ htop.tar.gz , htop.tar.xz, htop.tar.zst로 3회씩 압축 ( tar cfa htop.tar.zst htop)$ 압축에 걸린 시간과 압축률 계산 ( 맨 앞에 time)linkfile 관련 명령어이다.ln 명령어를 사용하고, 하드 링크와 심볼릭 링크로 나뉜다 하드 링크 심볼릭 링크: 축약시 symlink라고도 함 ( -s 옵션)i-nodei-node: 파일의 메타정보 및 관리용 객체 파일은 고유의 i-node를 1개 가진다. i-node는 disk partition내에서 유일한 식별자 i-num이라고도 함 같은 파티션내에서의 식별자hard link 동일 파티션내에서만 생성 가능 일반 파일만 가능 → 일반 파일이 아닌 디렉토리 등은 하드링크를 생성할 수 없다. 실체를 가진 파일symlink 위치만 가리키므로 다른 파티션, 모든 종류의 file에 만들 수 있다. 가르키는 대상의 UNIX file mode를 따라가므로 symlink의 권한은 777이며 의미는 없다.하드 링크 생성 예제$ mkdir -p ~/work/testdir$ chmod 775 ~/work/testdir$ cd !$$ touch hello.txt$ ls -li2228823 -rw-rw-r-- 1 jaehoyoon jaehoyoon 12 2월 22 15:59 hello.txt1개 상위의 디렉토리에 하드링크 생성$ ln hello.txt ../hardlink.txt$ ls -li2228823 -rw-rw-r-- 2 jaehoyoon jaehoyoon 12 2월 22 15:59 hello.txt$ ls -li ..2228823 -rw-rw-r-- 2 jaehoyoon jaehoyoon 12 2월 22 15:59 hardlink.txt2228822 drwxrwxr-x 2 jaehoyoon jaehoyoon 4096 2월 22 15:59 testdir$ ln -s ../hardlink.txt symlink.txt$ ls -l-rw-rw-r-- 2 jaehoyoon jaehoyoon 12 2월 22 15:59 hello.txtlrwxrwxrwx 1 jaehoyoon jaehoyoon 15 2월 22 16:04 symlink.txt -&amp;gt; ../hardlink.txt$ ls -li2228824 lrwxrwxrwx 1 jaehoyoon jaehoyoon 15 2월 22 16:04 symlink.txt -&amp;gt; ../hardlink.txt여기서 중요한 것은 링크의 수가 증가했다는 것. 왜냐하면 같은 파일즉, 2228823이라는 i-node를 가르키는 링크가 2개가 되었기 때문이다. symlink를 생성할 경우 링크의 개수는 증가하지 않지만, 경로를 나타내어 줌, symlink이므로 l로 시작symlink는 만들 디렉토리로 가서 생성을 해야 한다. 그렇지 않다면 옵션을 추가해야 하지만, 그냥 단순하게 해당 디렉토리로 가서 생성하는 것이 좋다.which파일의 path를 검색$ which find/usr/bin/findreadlinksymlink가 여러 단계를 가리키는 파일이 있을 수 있다. 즉 A → B → C라고 하면 누가 마지막인지 애매해진다.그래서 symlink의 canonical path를 따라가는 기능이 있다. f 옵션은 마지막 링크를 제외한 모든 링크가 존재할 때 성공 e 옵션은 모든 링크가 존재할 때 성공💡 canonical란 컴퓨팅 환경에서 실체를 가지는 standard, officaial의 의미를 가진다.현재 cwd(current working directory)에 따라 이동할 위치가 달라지는데, 이 cwd를 정의해야만 이동할 위치도 정의된다. 이를 canonicalization이라 한다.따라서 위의 symlink또한 실체를 가리키는 것이 아니기 때문에 따라가기 전에는 대상을 한정할 수 없다. 그래서 순차적으로 따라가야 canonical path를 알 수 있게 된다.실습 sym3 → sym2 → sym1 → hardlink형태를 만들자$ dirhello.txt$ ln -s hello.txt sym1$ ln -s sym1 sym2$ ln -s sym{2,3}$ ls -l sym* hello.txt-rw-rw-r-- 2 jaehoyoon jaehoyoon 12 2월 22 15:59 hello.txtlrwxrwxrwx 1 jaehoyoon jaehoyoon 9 2월 22 16:23 sym1 -&amp;gt; hello.txtlrwxrwxrwx 1 jaehoyoon jaehoyoon 4 2월 22 16:23 sym2 -&amp;gt; sym1lrwxrwxrwx 1 jaehoyoon jaehoyoon 4 2월 22 16:23 sym3 -&amp;gt; sym2$ readlink -e sym2/home/jaehoyoon/work/testdir/hello.txt$ readlink -f sym2/home/jaehoyoon/work/testdir/hello.txt$ rm -rf hello.txt$ readlink -f sym3/home/jaehoyoon/work/testdir/hello.txt$ readlink -e sym3-e는 마지막까지 포함, -f는 전까지 readlink사용해보기$ which locate/usr/bin/locate$ ls -l /usr/bin/locatelrwxrwxrwx 1 root root 24 2월 21 15:12 /usr/bin/locate -&amp;gt; /etc/alternatives/locate$ ls -l /etc/alternatives/locatelrwxrwxrwx 1 root root 16 2월 21 15:11 /etc/alternatives/locate -&amp;gt; /usr/bin/mlocate$ ls -l /usr/bin/mlocate-rwxr-sr-x 1 root mlocate 43088 3월 2 2018 /usr/bin/mlocate$ readlink -e /usr/bin/locate/usr/bin/mlocatels -l 로 계속 symlink를 따라가기는 힘들다. 이를 readlink하나로 해결가능symlink를 사용하는 예 음악 스트리밍 음악 파일은 음반별로 저장된다. 추천 100 음악을 /exp/malon/rec100/에 1.mp3 ~ 100.mp3로 만들어야 하는데 매일 복사하는 것은 너무 비효율적이다. hot 100에 해당하는 symlink를 생성하게 되면 원래 파일의 경로를 symlink를 통해 rec100에 담는다. $ ln -s /exp/disc/chet_atkins_1/02.sails.mp3 /exp/malon/rec100/2.mp3 library 버전 관리 기존 라이브러리의 실제 파일은 libflamegraph.so.0.4.5가 있다. 라이브러리 업그레이드로 0.5.0이 설치된다면 최신 버전을 가르키기 위해 기존 라이브러리를 제거하는 것보다 symlink가 가르키는 canonical path만 변경하면 된다. $ ln -s /usr/lib/libflamegraph.so.0.5.0 /usr/lib/libflamegraph.so 실제로 ls -/usr/lib을 보면 관찰할 수 있다. $ ls -l /usr/lib/python3.6 lrwxrwxrwx 1 root root 31 2월 21 15:12 [sitecustomize.py](&amp;lt;http://sitecustomize.py/&amp;gt;) -&amp;gt; /etc/python3.6/sitecustomize.py process 관련 명령어ps현재 세션의 프로세스를 보여준다. PID: 프로세스 ID TTY: 터미널 ID TIME: CPU 시간(누적 시간) = CPU를 점유했던 시간의 누적값, 즉 CPU를 사용한 시간 현실 시간에 대한 건 ETIME이란 다른 항목이 있다. CMD: command = 프로세스 이름 (argv[0]를 의미옵션 -e: 전체를 다 보여줌 -a: 터미널과 연결되어 있는 프로세스 및 세션 리더 -f: full format → UID, PID, PPID, C, STIME, TTY, TIME UID: 해당 프로세스의 소유권자, 숫자인 경우 UID, 심볼인 경우 Isername PPID: 부모 프로세스 ID(parent PID) C: CPU 사용량 STIME: 프로세스 시작한 시간 (시 : 분) - 현실 시간 -l: long format → F,S,PRI,NI,SZ,C F: 프로세스 플래그 S: 상태 코드 (State code) PRI: 실시간 우선 순위 NI: 나이스 우선 순위 SZ: 사용되는 프로세스 코어 이미지의 메모리 크기 C 여기서 가장 많이 사용되는 방법은-ef, -el, -ej 즉, 전체(e)를 full format, 전체(e)를 long format…$ ps -ef | grep bashjaehoyo+ 1984 1974 0 13:30 pts/0 00:00:00 bashjaehoyo+ 2830 1984 0 17:14 pts/0 00:00:00 grep --color=auto bashbash 단어가 들어간 행만 뽑아준다.💡 그러나 앞의 3가지는 잘 쓰지 않고, 가장 많이 사용되는 것은 eo이다. 하지만 eo는 중급자 수준이다.process controlkillkill이라는 명령어가 있다. 이는 프로세스에 시그널을 보내는 기능$ kill -l 을 하면 시그널 리스트 확인 가능하다.대표적이어서 외워야 하는 것들 (sig는 시그널, 뒤의 단어가 의미) SIGHUP: hang up - 연결이 끊겼을 때(로그아웃 등) SIGINT: interrupt, &amp;lt;ctrl +c&amp;gt; - 프로그램을 죽이는 것, 일반적으로 프로그램을 죽일 때 작동: SIGINT를 받는 프로세스가 프로세스 그룹 리더라면 프로세스 그룹에 속한 모든 프로세스에게 시그널이 전파되고, 자식 프로세스도 전부 종료 SIGQUIT: QUIT, &amp;lt;ctrl + \\&amp;gt; - 프로그램을 죽이는 것, 일반적이지 않은 프로그램이 이상할 때, 메모리를 덮을 때 사용 작동: SIGINT와 같으나 core가 생성됨 → 프로그래머를 위한 기능 동일하게 프로세스 그룹 리더에게 전달되면 다 전파됨 SIGKILL: kill, 강제로 죽이는 기능, 강제이기에 파일이 깨질 수 있음, 거의 최후의 수단으로 사용 SIGSEGV: segment violation, 이상 작동해서 메모리 침범으로 죽었을 때 SIGTERM: terminate, 죽이기 요청, 상태가 이상하면 요청을 거부하기에 kill로 함 SIGTSTP: terporary stop, &amp;lt;ctrl+z&amp;gt;, 잠깐 정지 명령어 설명 kill 13011 PID 13011 프로세스에 SIGTERM(default) 시그널 보냄 kill -QUIT 13013 PID 13013 프로세스에 SIGQUIT 시그널 보냄 kill -9 13012 PID 13012 프로세스에 9번 시그널인 SIGKILL 보냄 실습터미널 2개로 1번에는 less ~/.bashrc, 2번에는 ps -e | grep less하여 PID확인 후 kill &amp;lt;PID number&amp;gt;job controlforeground / background process foreground process: 현재 session에서 제어 터미널을 가진 프로세스 background process: 현재 session에서 제어 터미널을 잃어버린(소유하지 않은) 프로세스 CTRL+Z foreground프로세스가 sigtstp 시그널을 전달받음 작동 → 잠시 정지시킴 = 결과적으로 background에 stop상태로 내려가게 됨 session세션은 멀티유저 시스템에서 통신 객체를 구별하기 위해 사용된다. 세션은 제어 터미널을 가질 수 있다. 세션에도 ID를 받는데, SID == PID인 프로세스를 Session Leader라고 부른다.특징 세션안에서도 여러 그룹을 만들 수 있다. GID == PID인 프로세스를 프로세스 그룹 리더라고 한다. 세션 리더로부터 파생된 자식 프로세스는 모두 같은 세션을 가진다. logout시 세션이 파괴되면서 세션에 속한 프로세스들은 모두 종료된다.controlling terminal제어터미널은 사용자의 제어를 받는 터미널 장치 e.g. 키보드 입력을 받아 작동 CUI에서 멀티 태스킹을 위한 제어 방법 제어터미널을 소유한 프로세스는 키보드 입력을 가진다. 이런 프로세스를 foreground process라고 부른다. 하나의 세션에서 foregorund process는 최대 1개까지 가질 수 있다. 제어 터미널 규격 ps 명령어 출력에서 TTY 필드 부분에 2가지 방식으로 출력이 된다. pts/# : UNIX98 tty# : control terminal 세션에서 제어 터미널을 가지지 않는 경우(서버 시스템, 즉 키보드 입력이 안되는 경우)에는 ps의 tty필드에 ?로 나타난다.process group프로세스 그룹 리더: ProcessGroup ID == PID프로세스 그룹에 시그널을 보낼 수도 있는데, PID가 음수면 그룹 ID라고 보면 된다. daemon 프로세스란? orphan process and session leader stdio를 모두 /dev/null로 리다이렉션을 걸고 제어 터미널을 가지지 않는 프로세스 ⇒ tty필드에 ?즉, background에 시스템 관련 작업을 하는 프로세스를 가르킴 실습 jobs : stoped, background process의 리스트 출력 fg %# #에는 jobs의 작업 번호 %는 생략 가능 지정한 프로세스를 foreground로 가져옴 bg %# 정지된 백그라운드 프로세스를 running 상태로 변경 실행 - job control$ vi^Z[1]+ 정지됨 vi$ vi hello.c^Z[2]+ 정지됨 vi hello.c$ cat &amp;gt; printdate.sh#!/bin/bashwhile : ;do date &amp;amp;&amp;amp; sleep 1done^D$ bash printdate.sh2022. 02. 22. (화) 18:05:26 KST2022. 02. 22. (화) 18:05:27 KST2022. 02. 22. (화) 18:05:28 KST2022. 02. 22. (화) 18:05:29 KST2022. 02. 22. (화) 18:05:30 KST2022. 02. 22. (화) 18:05:31 KST2022. 02. 22. (화) 18:05:32 KST2022. 02. 22. (화) 18:05:33 KST2022. 02. 22. (화) 18:05:34 KST2022. 02. 22. (화) 18:05:35 KST2022. 02. 22. (화) 18:05:36 KST2022. 02. 22. (화) 18:05:37 KST^Z[3]+ 정지됨 bash printdate.sh$ jobs[1] 정지됨 vi[2]- 정지됨 vi hello.c[3]+ 정지됨 bash printdate.sh이 때 +는 foreground, -는 background$ fg %22번 작업을 포그라운드로 불러옴^Zvi hello.c[2]+ 정지됨 vi hello.c$ jobs[1] 정지됨 vi[2]+ 정지됨 vi hello.c[3]- 정지됨 bash printdate.sh$ bg %33번 작업을 background에서 running 상태로 변경시키는 것이므로 ctrl+c를 눌러도 명령이 멈추지 않음이는 background에서 작동하면 제어터미널을 잃기 때문에, 현재 제어터미널은 셀이 가지고 있어서 ctrl+c는 셸로 전달된다. 즉 현재 foreground process는 셸이를 죽이는 방법은 fg%3을 통해 불러와서 ctrl+c를 누르든 시그널을 보내서 직접 죽이던 해야 함" }, { "title": "[데브코스] 2주차 - linux 기초(VMware 설치, 자주 쓰이는 명령어)", "url": "/posts/linux2/", "categories": "Classlog, devcourse", "tags": "linux, devcourse", "date": "2022-02-22 13:00:00 +0900", "snippet": "linux설치: VMware 가상 머신 기반 ubuntu 18.04 설치가상 머신(virtual machine)현재 사용되는 OS위에 다른 OS를 애플리케이션처럼 운용 현재 운영체제: 호스트 운영체제(Host OS) 가상 운영체제: 게스트 운영체제(Guest OS)가상머신의 장점 간편한 설치 및 구성, 백업가상머신의 단점 가상화로 인한 느린 속도 일부 호스트 OS의 하드웨어 장치(GPU)를 사용할 수 없다.가상 머신의 한계 및 주의가상 머신 사용 시 성능 및 하드웨어 제한이 존재한다. 일정 레벨 이상의 학습을 위해서 리얼 머신에 리눅스를 설치하는 것을 추천한다. 리얼 머신에서 리눅스를 직접 사용해야만 빠르게 학습 효과를 볼 수 있기 때문이다.가상 머신 설치 시 주의 리얼 머신에 설치하는 경우, 실수로 인해 기존 하드 디스크 내용이 날아갈 수 있다. 그래서 익숙하지 않은 경우 기존 디스크는 꼭 분리하고, 리눅스는 물리적으로 다른 디스크에 설치하고 부팅시 BIOS 메뉴로 선택하도록 하자.가상 머신의 종류 full virtualization(전가상화) 낮은 성능의 단점 / 높은 독립성 호스트 os위에서 하나의 애플리케이션으로 인식 해당 애플리케이션을 hypervisor이라 지칭 - CPU나 플랫폼에서 가상화를 돕는 가속화 기능 필요 (optional) intel VT-x, VT-d, AMD-V 종류 windows VMware, Virtualbox(vbox), WSL(ms에서 배포한 linux) OSX VMware, Virtualbox(vbox), parallels Linux VMware, Virtualbox(vbox) 라이선스 virtualbox는 opensource, 그 외는 상업적 소프트웨어 para virtualiation(반가상화)다운로드 VMware workstation player https://www.vmware.com/kr/products/workstation-player/workstation-player-evaluation.html ubuntu 18.04.5.iso https://mirror.kakao.com/ubuntu-releases/18.04.6/open-vm-toolsopen-vm-tools란 VMware의 vm-tools의 패키지 버전 기능 화면 조절: 화면 크기의 조절이 가능, 그래픽 가속 클립 보드: 윈도에서 복사한 텍스트를 리눅스에 shift-insert로 붙이기 가능, 반대로 리눅스에서 복사한 것을 윈도우에 ctrl-v로 붙여넣기 가능 파일 복사: 윈도 탐색기에서 파일을 끌어서 가져오기 가능(단, 드래그한 뒤 바로 놓지 말고 잠깐 기다렸다가 +기호가 보일 때 놓아야 함) 설치터미널을 켜서(ctrl+alt+t)$ sudo apt install open-vm-tools{,-desktop}재부팅$ sudo reboot데일리 서비스 비활성화그 다음 데일리 서비스를 비활성화해줘야 한다.sudo systemctl disable --now apt-daily.timersudo systemctl disable --now apt-daily-upgrade.timer암호 설정새 unix 암호를 입력해야 한다.$ sudo passwd암호 입력재입력root 계정 사용$ su -암호 입력확인을 위해# id나가기 위해서는 # exit 또는 ctrl+D가상 머신 종료할 때는 절전이 아닌 terminal에서 poweroff x window 우측 상단 톱니바퀴에 컴퓨터 끄기VT(Virtualization Technology) 활성화, VMware: preferencesVMware setting좀 더 편리하게 사용하기 위한 설정 virtual machine setting processors: virtualization engine(==virtualization technology) preferences 설정 VT 설정은 필수, 3번째꺼는 있으면 키면 된다. ubuntu GUI설정 터미널 실행 → Ctrl+alt+t이지만, 설정 → 접근성 → 키보드에 가면 편한 단축키로 바꿀 수 있다. 글꼴 변경 기본 설정 → 사용자 지정 글꼴 단축키 터미널 이동 → super(window키) + 마우스 좌클릭 터미널 크기 조정 → super + 마우스 휠 드래그 터미널 새 페이지 → ctrl + shift + t 터미널 다른 페이지 이동 → ctrl + 숫자 키 한영 전환 → shift + space 화면 잠금 설정 개인 정보 → 화면 잠금 해제 전원 → 절전 빈화면 안함 나눔 폰트 설치 $ sudo apt -y install fonts-nanum* fonts-naver* login shell and environment운영체제(OS)란? Modern operating system의 구성 kernel: 하드웨어를 제어하는 핵심 코드 application shell : 다른 application을 구동시킴 web browser, music player … 실행 영역에 따른 구분 system(kernel) user (kernel의 도움이 필요 없는 영역) 모든 프로세스는 user + system으로 작동된다. UI(User interface) 시스템의 조작 및 통신을 위해 외부의 접속을 받아들이는 부분 ⇒ CLI/CUI, TUI, GUI Shell은 기본적으로 CLI/CUI 방식 사용 CLI(command line interface) / CUI(character user interface) ⇒ 반대되는 개념은 GUI(graphic user interface) 윈도우의 cmd도 CUI방식의 일종의 shell Command line interface prompt에서 명령행 타이핑을 통해 실행 텍스트 베이스 방식 장점: 오버헤드가 적다(서버에서는 매우 중요) ⇒ 입출력 결과를 파일로 지정 가능 (stdio) 단점: 사용자 친화적이지 않다. TUI(Text-base user interface) 텍스트를 이용해서 구현하는 그래픽을 사용하는 UI 장점: 오버헤드 적음 단점: 화면 크기, 글꼴에 따라 화면이 깨짐 주로 ncurses, newt 라이브러리로 구현된다.GUI(graphical user interface) VGA 그래픽 방식 사용하는 UI 위젯 표현을 변경 장점: 섬세한 그래픽과 색상 단점: 높은 통신 대역폭, 메모리가 필요하므로 높은 하드웨어 성능이 필요 UNIX/Linux는 CLI/CUI 방식을 선호한다. 왜냐하면 수행 능력이 좋기 때문이다. → 낮은 대역폭에서도 잘 작동 / 저전력console/terminalconsole: 사전적 의미: 건반과 패달을 가진 연주대 → 컴퓨터에 물리적으로 직접 연결된 제어용 송수신 장치를 의미terminal: 사전적 의미: 종단, 끝 → 데이터를 송수신하는 목적지나 출발점이 되는 장치따라서 콘솔은 장치의 형태, 터미널은 장치의 위치에 따른 논리적 분류 용어이다. 즉 둘은 비교 가능한 성질이 아니다. e.g. console terminal / remote terminal물리적 위치에 따라 다음과 같이 2가지로 분류한다. console terminal 시스템에 물리적으로 부착되어 있는 콘솔에서 액세스한 터미널 terminal 부분을 생략하고 그냥 console이라 불린다. remote terminal 원격에서 액세스한 터미널: ssh, web shell ,x terminal 등등으로 접속 보통 remote를 생략하고, terminal로 부르는 경우가 많음 학술적으로 보면 웹 브라우저도 terminal software에 속함 대부분의 UNIX, Linux 서버는 text console을 사용하기도 한다.text console 바꾸는 방법 일반적으로 ALT + F1 ~ ALT + F6 에 맵핑되어 있고, tty라고 부른다. F2~F7인 경우도 있다. 그래픽 콘솔에서 텍스트 콘솔로 이동하는 경우에는 ctrl을 조합 그래서 ctrl + alt + f1 ~ 으로 이동 그래픽 콘솔(gui 사용)을 x window를 사용하는 콘솔이라 하여 x console이라고도 부른다. x window를 원격에서 접속하면 x (remote) terminal이라 한다. 그래픽 콘솔은 F1 or F7에 맵핑된다. F1에 그래픽 콘솔을 맵핑하면 tty는 F2~F7, F7에 그래픽 콘솔이면, F1~F6이 tty이 된다.login의 의미 접속을 요청하여 알맞은 인증과정을 거쳐 시스템 내부와 통신 가능한 상태가 되면 시스템에 명령을 내릴 수 있는 인터페이스가 실행된다.그래픽 콘솔은 DE(desktop environment)에 따라 다르다. ubuntu: GNOME Debian, RH계열: GNOME, KDE, Xfce텍스트 콘솔에서 로그인 시스템에서 ctrl + alt + F3 을 누른다. login 한다. jaehoyoon passwd - qwer1234 로그인 하고 난 후 prompt에 $ 는 일반, #는 root 유저를 의미 다시 그래픽 콘솔로 가고 싶은 경우 alt+f1 or alt+f7로그인하면 session이 만들어지는데 이를 해제하는 것을 로그아웃이라 한다. 세션은 서버 측에 존재하는 출근부와 같은 것이다. 로그아웃할 때는 exit, logout 을 타이핑하거나 &amp;lt;Ctrl+D&amp;gt;누른다.종료 소프트웨어적인 종료는 shutdown -h, sudo halt 하드웨어 전원을 끄는 경우 sudo poweroffunix 계정계정 종류 system account root 및 system service 전용 계정 로그인 불가 계정도 많다. normal account 나중에 추가하는 로그인이 가능한 일반 유저 이 둘을 구분짓는 이유는 계정의 권한을 분리하고, 접근할 수 있는 디렉토리와 파일, 프로세스를 구분하기 위함이다.user, group 관리 명령어 user: useradd(추가), usermod(수정),userdel(삭제) id, whoami group groupadd,groupmod, groupdel user 교체, 권한 명령어 user su ( substitute user ) passwd group newgrp ( log in to a new group ) gpasswd man pageman page: 특정 아이템이 대해 설명해주는 페이지unix 계열은 CLI로 작동하는 online manual이 있다. 이를 man page라 한다. 표준 unix 명령어는 xpg로부터 유래man [option] &amp;lt;item&amp;gt;man ueradd, man -f printf 문법 표기 무괄호, &amp;lt;&amp;gt; : 필수 항목 []: 선택가능한 항목 문법 표기 예제 shutdown [-rhc] [message] -rhc와 message는 생략 가능 time은 생략 불가능 언어 설정에 따라 다른 언어로 출력할 수 있다. 한국어 설정 상태에서 man페이지를 보면 한국어 메뉴얼이 존재하면 한국어, 한국어 매뉴얼이 존재하지 않으면 영어로 보인다.$ man sutext console은 한글을 표시할 수 없기 때문에 영어로 나오고 한글은 이상하게 출력될 것이다.LANG 환경 변수LANG : locale + character set locale: 사용자의 언어 및 지역 정보 language: ISO 639-1 contry: ISO 3166-1 character set: 입출력 문자표LANGUAGE를 사용하는 경우 그냥 비워두는 것이 좋다.~$ export LANGUAGE=export LANG = C라고 하면 영문으로 출력하는 것이다. man su에서 한글이 깨지게 나오면 영문 모드(export LANG=en_US.UTF-8)로 바꾸면 된다.로케일 생성 명령어$ locale-gen ja_JP.utf8system의 locale을 확인 및 설정$ localectl set-locale LANG=en_US.utf8로케일을 변경 후 재로그인하면 된다.추가) putty / mobaxterm외부에서 터미널을 통해 윈도우즈에 접속하는 경우 putty/mobaxterm 을 사용하는데, 여기서 창 → 변환 → 수신한 데이터를 UTF-8로 변경해야 함✨LANG에서 가장 중요한 부분$ export LANG=en_US.utf8; export LANGUAGE=이를 설정해놓는 것이 좋다. en_US는 영어, ko_KR.utf8은 한글이다." }, { "title": "[데브코스] 2주차 - linux 기초(역사)", "url": "/posts/linux1/", "categories": "Classlog, devcourse", "tags": "linux, devcourse", "date": "2022-02-21 13:00:00 +0900", "snippet": "리눅스의 기초(1)linux역사, 배포판index linux의 조상, unix의 역사, 배경 c언어는 unix를 만드는 언어 c언어 국제 표준: ISO/IEC 9899 UNIX의 양대산맥: SysV, BSD와 호환성 문제 POSIX 표준 등장: 최소한의 호환성 Linux의 등장과 성공 요인 GNU와 Free Software Foundation linux 배포판: 레드햇, 데비안역사를 다루는 이유 표준안의 존재 의미를 알려준다. 표준화된 지식과 규격은 호환성을 높이고, 효율적으로 협력가능한 작업을 가능하게 한다. 지금도 표준화 이전의 세계가 존재한다. 인터넷 검색에서 나오는 출처가 의심스러운 문서에 주의하자. 구글, 네이버, 각종 카페 ,위키 등에서 나오는 문서들 중 틀린 정보를 걸러낼 수 있어야 한다. 결론: 항상 표준 문서와 공식 문서를 먼저 보자.역사적 배경을 다루는 또 다른 이유 인과관계, 상관관계를 이해할 수 있게 된다. 어떤 기술이나 표준 규격이 나오게 된 인과관계를 알아두면, 역사는 반복되기 때문에 현재 사용되는 기술들이 어떻게 변해가는지 큰 숲을 볼 수 있게 된다. → 테크닉만 외우면 초급 이상이 되기 힘들다. 중급/고급 엔지니어가 되려면 역사적 배경과 철학을 이해하는 것이 중요하다. 생각해야 할 이슈: UNIX, Linux는 무엇 때문에 만들어졌는가 기술의 발전은 어떤 결핍을 해결하기 위해 만들어진다. 결핍을 해결하는 과정은 점진적 진행되다가 특이점에서 점프하게 된다. 이런 과정 속에 기존의 것을 향상시키거나 아예 버리고 새로운 것을 만들기도 한다. 기술의 발전 방향을 예측하고, 올바른 방향으로 가기 위해서는 기술의 역사적 배경, 중요한 인물들이 했던 행동이 중요하다. 또한, 기술만 볼 것이 아니라 그 기술들을 창조하거나 개선한 사람이 매우 중요하다. 컴퓨터 기술은 지적 재산과 관련이 깊기 때문에 대체적으로 폐쇄적인 성향을 띄게 된다. 하지만 폐쇄적인 환경은 호환성이나 성능의 결핍이 따라오기 마련이다. UNIX는 이를 해결하려 했다. 그 결과 UNIX는 점점 호환성, 성능이 높아졌고, Linux는 호환성 측면에서 더 높은 수준을 달성했다. key point 다양한 표준을 기억하고, 되도록 메모한다. 표준을 준수한다는 것은 호환성을 보장한다는 것이다. 어설프게 아는 것은 모르는 것보다 위험하다. 컴퓨터 시스템에서 개방적 호환성은 교환의 효율을 높인다. 개방적인 표준이 주는 호환성의 장점을 생각해보자. OS의 API나 개념들이 표준화되고 통일되면 개방적인 표준을 근거로 서로 다른 vendor에서 만들어지는 소프트웨어, 하드웨어의 다양한 조합이 제대로 작동된다는 보장이 가능하다. 개방적인 표준을 통해 여러 소프트웨어, 하드웨어는 같은 규격 내에서 경쟁을 하면서 발전한다. 공식 문서 UNIX: www.opengroup.orgLinux 문서 및 교육 자료- Red Hat: access.redhat.com- Linux Foundation: www.linuxfoundation.org책, 만화 등- 책: 성당과 시장 - Eric Raymond- 책: **UNIX의 탄생** - Brian KernighanMultics Project UNIX의 씨앗 C언어의 시작: 포팅의 편리함을 위한 도구 C언어 국제 표준: C99로부터 시작된 Modern C Multics: GE,Bell lab과 MIT AI lab의 MAC 프로젝트의 산물 목표: 운영체제의 복잡한 기능을 감추고 시분할, 페이지/세그먼트 메모리 관리, 프로세스 관리, 파일 시스템, 주변장치 관리, 양방향 인터페이스 등등 다양한 기능을 실험하는 프로젝트 ( 지금의 window, osx,linux 등에는 당연히 탑재되어 있음) ken thompson이 UNIX는 게임을 위해서 만들어졌다. 게임을 어떻게 좋아해서 만들게 되었는지에 대한 뒷얘기 www.bell-labs.com/usr/dmr/www/ken-games.html 기존의 UNIX는 Assembly로 작성되어 새로운 CPU가 나올 때마다 거의 새로 작성해야만 했다. 포팅에 너무 많은 시간이 걸린다는 말이다. 그래서 B언어를 개량해서 C언어를 개발했다.UNIX를 C언어로 작성하여 얻게된 장점 하드웨어가 달라져도 rebuild 혹은 약간의 수정만으로 포팅이 가능 그 결과 유닉스가 여러 기계에 이식됨C언어의 표본 책 - the C ( brian w.kernighan &amp;amp; dennis M.ritchie)C언어의 특징 고급언어면서도 어셈블리에 버금가는 성능 낮은 추상화 지원 sudio, file 객체 저수준의 하드웨어 조작 가능 쉬운 언어적 특성C언어 국제 표준: ISO/IEC 9899 ANSI-C (C89): 1989년도 표준 C99: 1999년도 표준 현재 산업계의 실질적 표준 호환성 문제branches &amp;amp; compatibility서로 다른 UNIX 벤더에서 다 다르게 코딩되어 실행되지 않았다. 그래서 JAVA를 탄생시키게 된다. 벤더들끼리의 스스로 표준화의 필요성이 대두되면서 전기전자공학회(IEEE)의 주도하에 규격인 파직스, POSIX 1988를 발표했다. 추후 POSIX 1003.1-1990은 ISO 승인되어 IEEE std 1003.1-1988로도 표기된다.Minor vendors: OSF의 등장 배경AT&amp;amp;T, Sun microsystems는 업계 표준인 UNIX인 SVR4를 만들어낸다. 대기업이었던 sun microsystem은 자사의 UNIX를 solaris로 개작했다. minor vendor들은 자사의 UNIX가 SVR4에 묻히게 될것을 우려하여 DEC가 회의를 주최한다. 이에 OSF(Open software foundation)을 설립했다. OSF에서 통합 유닉스로서 OSF/1을 발표했다.유럽의 컴퓨터 제조업체들이 open System의 표준화를 위해 출범한 단체가 X/open인데 여기서 표준화를 위해 가이드라인인 XPG(X/open Portability Guide)를 만들었다.하지만 POSIX로는 호환성이 부족했는데 ,이때 빌게이츠의 microsoft사의 서버용 os가 제작되면서 모든 unix그룹들이 통합된다. 통합된 유닉스 단일 표준안 SUS(single unix specification)이 만들어진다. SUSv1: XPG 4.2 혹은 UNIX95라 명명, issue 420 SUSv2: UNIX98 실질적인 첫 SUS 표준, issue 500 SUSv3: SUS 2002, issue 600 SUSv4: SUS 2007, issue 700open System이라 하면 API와 OS 구조가 공개되어 표준이 만들어진 시스템은 UNUX가 시초로서 open system = UNIX라 하기도 하지만, Linux까지 포함시키기도 한다.UNIX 이후의 세계GNU(gnu is not unix)와의 협력linux는 gnu의 컴파일러와 각종 유틸리티의 도움이 없었다면 발전이 힘들었다.Linux 배포판배포판을 분류하는 기준 일반적으로 패키지 시스템으로 계열을 나누고 계열 내에서 특정 벤더나 단체로 다시 나뉜다.DEB 계열 debian, ubuntu, mint…RPM 계열 RHEL, CentOS, Fedora, Suse,…debian은 GNU사의 공식 배포판Redhat 계열 - RPM 패키지 사용 RHEL(redhat enterprise linux) fedora centOS(community enterprise operating system) centOS를 사용하는 경우 EPEL을 사용하는 경우가 많다. Extra Packages for enterprise linux EPEL에서는 다양한 목적의 패키지를 제공 따로 추가 패키지를 제공하는 이유는 라이선스나 배포 문제 https://fedoraproject.org/wiki/EPEL ** Debian 계열 - deb 패키지 사용 : 수직 버전으로 차이가 발생하는 것이 아니다. debian linux - GNU의 Ian murdock에 의해 개발 데비안 기본 - 작고 가벼운 시스템 구축이 목적 ubuntu linux 예쁘고 사용이 편리한 데스크탑 리눅스 구축 기본 설치시 가장 미려한 데스크탑을 가지고 있다. kail linux: security, hacking 특화그러나 데비안은 패키지 지원이 부족하다.배포판별 top.3 특징 CentOS 포털 및 스타트업 기업이 가장 많이 사용 RHEL의 클론이라서 엔터프라이즈 환경에서 사용하기 편리하다. 엔터프라이즈 환경에서 가장 많이 쓰인다. Ubuntu 화면이 아름답고 초보자가 쓰기 편리하다. 엔터프라이즈 환경에서 사용하는데 약간 불편하고 보안이 취약하다. 소형 기기에 최적화된 스핀오프가 많아 IOT embedded에서 많이 쓰인다. Fedora 최신 기술을 확인하기 편리하여 선행기술 개발, 보안 시스템 개발에 쓰인다. enterprise linux대형 시스템에 특화된 리눅스 대용량 웹 서비스 cloud service 게임 서비스 증권embedded linux작은 기기에 특화된 리눅스 하드웨어는 ARM 기반의 CPU를 주로 사용 안드로이드, 셋탑박스, 디지털 티비… 주요 임베디드 리눅스용 보드 및 모듈 라즈베리파이, 오렌지파이, 라떼판다, gateworks… summary mutic로부터 unix가 탄생 = ken thompson AT&amp;amp;T와 BSD로 갈라졌다. 갈라진 이유와 결과가 중요 BSD에서 Sun microsystem이 탄생 표준화: POSIX, X/OPEN, OSF/1의 관계 SysV release 4(SVR4)가 탄생 = 업계의 실질적 표준 window NT로 인해 UNIX 업계 통합 SUS의 탄생" }, { "title": "[데브코스] 1주차 - 알고리즘 문제 풀기 (2)", "url": "/posts/algorithm2/", "categories": "Classlog, devcourse", "tags": "algorithm, devcourse", "date": "2022-02-17 13:00:00 +0900", "snippet": "스택 (stack)스택: 자료를 보관할 수 있는 선형 구조 x.isempty() : 스택이 비어있는지 판단한다. x.peek() : 스택에 가장 나중에 저장된 데이터 원소를 참조한다. x.size() x.push() x.pop()단, 넣을 때는 한 쪽 끝에서 밀어 넣어야 하는 push 연산, 꺼낼 때는 같은 쪽에서 뽑아 꺼내야 하는 pop 연산에 대한 제약이 있다.이를 후입선출(LIFO - Last-In-First-Out) 특징을 가지는 선형 자료구조라 한다.def __init__(self): self.data = []def size(self): return len(self.data)def isEmpty(self): return self.size() == 0def push(self, item): self.data.append(item)def pop(self): return self.data.pop()def peek(self): return self.data[-1]수식의 후위 표기법중위 표기법: 연산자가 피연산자들의 사이에 위치(A + B) * (C + D) 1    3    2후위 표기법: 연산자가 피연산자들의 뒤에 위치A B + C D + * 1     2 3각 연산자의 우선순위를 비교하는데, 스택에 있는 연산자가 읽어온 연산자보다 우선순위가 높거나 같으면 pop, 아니면 push한다.스택의 응용 - 후위 표기 수식 계산후위 표기법으로 된 식을 계산하는 방법은 다음과 같다. 중위 표기법 → 후위 표기법 → 후위 계산이를 변환할 때는 이제 문자열이 아닌 리스트에 추가하여 리스트를 리턴할 것이다. 피연산자를 받으면 스택에 추가하고, 연산자를 만날 경우 맨 위의 2개를 빼내서 연산자에 맞게 계산한 후 다시 스택에 집어넣는다. 그렇게 마지막까지 간 후 스택에 담긴 값은 1개일 것이고, 이 값이 최종값이 된다.def splitTokens(exprStr): tokens = [] val = 0 valProcessing = False for c in exprStr: if c == &#39; &#39;: continue if c in &#39;0123456789&#39;: val = val * 10 + int(c) valProcessing = True else: if valProcessing: tokens.append(val) val = 0 valProcessing = False tokens.append(c) if valProcessing: tokens.append(val) return tokens이는 중위 표현식을 받아 리스트로 변환하는 함수이다. 가장 중요한 것은 10진법을 표기해야 하기 때문에, 10진법을 구현하는 것을 주의하며 구현한다.이제 리스트로 넣은 중위 표현식을 후위 표현식으로 변환해야 한다.def infixToPostfix(tokenList): prec = { &#39;*&#39;: 3, &#39;/&#39;: 3, &#39;+&#39;: 2, &#39;-&#39;: 2, &#39;(&#39;: 1, } opStack = ArrayStack() postfixList = [] for s in tokenList: if type(s) is int: postfixList.append(s) else: if opStack.isEmpty(): opStack.push(s) else: if s == &#39;)&#39;: while prec[opStack.peek()] &amp;gt; prec[&#39;(&#39;]: postfixList.append(opStack.pop()) opStack.pop() else: if prec[opStack.peek()] &amp;gt;= prec[s] and s != &#39;(&#39;: postfixList.append(opStack.pop()) opStack.push(s) while not opStack.isEmpty(): postfixList.append(opStack.pop()) return postfixList이제 변환된 후위 표현식을 계산한다.def postfixEval(tokenList): valStack = ArrayStack() lists = [] for t in tokenList: if type(t) is int: valStack.push(t) else: a = valStack.pop() b = valStack.pop() if t == &#39;*&#39;: valStack.push(b*a) elif t == &#39;/&#39;: valStack.push(b/a) elif t == &#39;+&#39;: valStack.push(b+a) elif t == &#39;-&#39;: valStack.push(b-a) return valStack.pop()def solution(expr): tokens = splitTokens(expr) print(&quot;tokens: {}&quot;.format(tokens)) postfix = infixToPostfix(tokens) print(&quot;postfix: {}&quot;.format(postfix)) val = postfixEval(postfix) return val최종적으로 후위 표현식을 계산하는 함수로, 2개를 pop한 후 계산하여 다시 스택에 넣는 식이다. solution에서 모든 함수를 실행시키면 결과를 얻을 수 있다.큐 (queue)큐: 자료를 보관할 수 있는 선형 구조단 넣을 때는 한 쪽 끝에서 밀어 넣어야 하는 enqueue연산, 꺼낼 때는 반대 쪽에서 뽑아 꺼내야 하는 dequeue연산의 제약이 있다. 이를 선입선출(FIFO - First-In Fisrt-Out) 특징을 가진다고 한다.알고리즘 테스트 중 대기열 문제가 큐에 해당된다.큐 연산의 종류 size(): 현재 큐에 들어 있는 데이터 원소의 수를 구함 isEmpty(): 현재 큐가 비어 있는지를 판단 enqueue(): 데이터 원소 x를 큐에 추가 dequeue(): 큐의 맨 앞에 저장된 데이터 원소를 반환 (제거) peek(): 큐의 맨 앞에 저장된 데이터 원소를 반환 (제거 x)이때, dequeue()만 O(n)이고, 나머지는 O(1)의 복잡도를 가진다. 이는 맨 앞의 원소를 꺼내게 되면 나머지 원소들이 앞으로 당겨져야 하기 때문에, 순차적인 방식이 된다.def __init__(self): self.data = DoublyLinkedList()def size(self): return self.data.getLength()def isEmpty(self): return self.data.getLength() == 0def enqueue(self, item): node = Node(item) self.data.insertAt(self.data.getLength()+1,node)def dequeue(self): return self.data.popAt(1)def peek(self): return self.data.getAt(1).data환형 큐(Circular queue)큐의 활용: 자료를 생성하는 작업과 그 자료를 이용하는 작업이 비동기적(asynchronously)으로 일어나는 경우 자료를 생성하는 작업이 여러 곳에서 일어나는 경우 자료를 이용하는 작업이 여러 곳에서 일어나는 경우 자료를 생성/이용 둘 다 여러 곳에서 일어나는 경우 자료를 처리하여 새로운 자료를 생성하고, 나중에 그 자료를 또 처리해야 하는 작업의 경우환형 큐(circular queue) : 정해진 개수의 저장 공간을 빙 돌려가며 사용정해진 개수의 공간이므로 큐가 가득 차면 더이상 원소를 넣을 수 없다.큐보다 추가된 점 isfull()이라는 큐의 데이터가 꽉 차있는지 확인하는 함수를 더 추가한다. 마지막 원소 저장된 공간을 rear, 처음을 front라 한다. 전체 공간을 정의하는 self.maxcount = n을 정의한다. data를 None으로 초기화하기 위한 self.data를 정의한다.class CircularQueue:def __init__(self, n): self.maxCount = n self.data = [None] * n self.count = 0 self.front = -1 # 데이터가 존재하는 첫 위치보다 1개 전 self.rear = -1 # 데이터가 들어있는 마지막 위치def size(self): return self.countdef isEmpty(self): return self.count == 0def isFull(self): return self.count == self.maxCountdef enqueue(self, x): if self.isFull(): raise IndexError(&#39;Queue full&#39;) self.rear = (self.rear+1) % self.maxCount self.data[self.rear] = x self.count += 1def dequeue(self): if self.isEmpty(): raise IndexError(&#39;Queue empty&#39;) self.front = (self.front+1) % self.maxCount x = self.data[self.front] self.count -= 1 return xdef peek(self): if self.isEmpty(): raise IndexError(&#39;Queue empty&#39;) return self.data[(self.front+1)%self.maxCount]우선순위 큐(Priority Queues)우선순위 큐: 큐가 FIFO 방식을 따르지 않고, 원소들의 우선순위에 따라 큐에서 빠져나오는 방식우선순위 큐 활용: 운영체제의 CPU 스케줄러우선순위 큐 구현: 서로 다른 방식이 가능함 Enqueue할 때 우선순위 순서를 유지하도록 Dequeue할 때 우선순위 높은 것을 선택 이때는 1번이 조금 더 유리하다. 2번은 O(n)의 복잡도를 가지지만, 1번은 이미 줄지어 집어넣었기 때문에 맨 뒤나 맨 앞만 보면 되기 때문이다.큐를 만드는 두 가지 재료 중 골라서 사용 가능하다. 선형 배열 연결 리스트시간 적으로 볼 때는 연결 리스트가 유리하다. 하지만 메모리 적으로는 연결 리스트가 많이 들기 때문에 적합한 방법을 사용하는 것이 좋다.17강: 트리(trees)트리: 정점(node)와 간선(edge)을 이용하여 데이터의 배치 형태를 추상화한 자료 구조이 때, 뿌리를 root node, 이파리를 leaf node라 한다. 이는 각각 노드에 대해 부모(parent), 자식(child)로도 표현할 수 있다.같은 부모 아래 자식들을 sibling이라 한다. 그리고, 부모의 부모를 조상(ancestor), 자식의 자식을 후손(deancestor)이라 한다. child는 많을 수 있지만, 부모는 오직 1개이다. 노드의 수준(level)뿌리를 level0이라 하고, 내려갈수록 level이 1씩 증가한다. 이 때 뿌리를 0과 1 중 하나로 둘 수 이는데 0이 더 편하다. 트리의 높이(height)높이 = 최대 수준(level) + 1깊이(depth)라고도 한다. 부분트리(= 서브트리(subtree))각각의 부분에 대해 서브트리라고도 부를 수 있다. 노드의 차수(degree)차수 = 자식(서브트리)의 수degree는 각 노드의 자식의 수와도 같다. 맨 마지막의 node 즉 degree=0인 노드를 leaf nodes라고 부른다. 이진 트리(binary tree)모든 노드의 차수가 2이하인 트리 포화 이진트리 (full binary tree)모든 레벨에서 노드들이 채워져 있는 이진 트리 = 모든 차수가 2이다. 높이가 k이고, 노드의 개수는 2^k - 1 이다. 완전 이진트리 (complete binary tree)높이가 k인 완전 이진트리의 경우 레벨 k-2 까지는 모든 노드가 2개의 자식을 가진 포화 이진 트리 레벨 k-1 까지는 2개씩이 아니더라도 왼쪽부터 노드가 순차적으로 채워져 있는 트리이진 트리(binary tree)이진 트리의 연산 종류 size(): 현재 트리에 포함되어 있는 노드의 수를 구함 depth(): 현재 트리의 깊이 (또는 높이) 를 구함 traverse()이진 트리 노드의 기본 구조는 다음과 같다.# 이진 트리 노드의 기본 구조class Node: def __init__ (self,item): self.data = item self.left = None self.right = Nonesize# 이진 트리 초기화class BinaryTree: def __init__ (self,r): self.root = r# 재귀적 방법 가능# 오른쪽 tree size + 왼쪽 tree size + 1(자신)class Node: def size(self): l = self.left.size() if self.left else 0 r = self.right.size() if self.right else 0 return l + r + 1class BinaryTree: def size(self): if self.root: return self.root.size() else: return 0depthdef depth(self): if self.root: return self.root.depth() else: return 0Traversal(순회) 종류 깊이 우선 순회 중위 순회(in-order traversal): 왼 → 자신 → 오 전위 순회(pre-order traversal): 자신 → 왼 → 오 후위 순회(post-order traversal): 왼 → 오 → 자신 넓이 우선 순회이진 트리 넓이 우선 순회(breadth first traversal)수준(level)이 낮은 노드를 우선으로 방문한다. 같은 수준의 노드들 사이에는 부모 노드의 방문 순서에 따라 방문하고, 왼쪽 자식 노드를 먼저 방문한다.설계(큐 사용) 빈 리스트 traversal, 빈 큐 q 선언 빈 트리가 아니면, root node를 q에 추가 (enqueue) q가 비어있지 않는 동안 3-1. q에서 node를 추출 3-2. node 방문 3-3. node의 왼, 오 자식 q에 추가 q가 빈 큐가 되면 종료이진 탐색 트리(binary search trees)모든 노드에 대해 왼쪽 서브트리에 있는 데이터는 모두 현재의 노드 값보다 작음 오른쪽 서브트리에 있는 데이터는 모두 현재의 노드 값보다 큼의 성질을 만족하는 이진 트리가 이진 탐색 트리이다.이진 탐색과 비슷하지만, 트리의 장점은 데이터 원소의 추가, 삭제가 용이하다. 그러나, 단점은 공간 소요가 크다.추상적 자료구조 각 노드는 (key, value)의 쌍으로 되어 있다.연산의 종류 insert(key, data): 트리에 주어진 데이터 원소를 추가 remove(key): 특정 원소를 트리로부터 삭제 lookup(key): 특정 원소를 검색 inorder(): 키의 순서대로 데이터 원소를 나열 min(),max(): 최소 키, 최대 키 원소를 탐색# 기본 노드 구성class Node: def __init__(self,key,data): self.key = key self.data = data self.left = None self.right = Noneclass BinSearchTree: def __init__(self): self.root = Noneinorder# inorder traversalclass Node: def inorder(self): traversal = [] if self.left: traversal += self.left.inorder() traversal.append(self.data) if self.right: traversal += self.right.inorder() return traversalclass BinaryTree: def inorder(self): if self.root: return self.root.inorder() else: return []preorder# preorder traversalclass Node: def preorder(self): traversal = [] traversal.append(self.data) if self.left: traversal += self.left.preorder() if self.right: traversal += self.right.preorder() return traversalclass BinaryTree: def preorder(self): if self.root: return self.root.preorder() else: return []postorder# postorder traversalclass Node: def postorder(self): traversal = [] if self.left: traversal += self.left.postorder() if self.right: traversal += self.right.postorder() traversal.append(self.data) return traversalclass BinaryTree: def postorder(self): if self.root: return self.root.postorder() else: return []min# minimum valueclass Node: def min(Self): if self.left: return self.left.min() else: return self # 계속 내려가다가 더 내려갈 곳이 없으면 자신이 최솟값이므로lookup 입력은 찾으려는 대상의 키 리턴은 찾은 노드와 부모 노드(없으면 둘다 None)class BinSearchTree: def lookup(self,key): if self.root: return self.root.lookup(key) else: return None,Noneclass Node: def lookup(self,key,parent=None): if key &amp;lt; self.key: if self.left: return self.left.lookup(key,self) # self.left 의부모는 self이므로 else: return None,None elif key &amp;gt; self.key: ... else: return self, parentremove원소 삭제의 메커니즘 키를 이용해서 노드를 찾는다. 1-1. 해당 키가 없으면, 삭제할 것도 없음 1-2. 찾은 노드의 부모 노드도 알고 있어야 함 찾은 노드를 제거하고도 이진 탐색 트리의 성질을 만족하도록 트리의 구조를 정리 입력: 키 출력: 삭제한 경우 True, 해당 키가 없는 경우 Falseclass BinSearchTree: def remove(self,key): node,parent = self.lookup(key) if node: ... return True else: return False트리 구조를 계속 유지해야 하기 때문에 삭제하는 노드가 말단 노드인 경우 하나의 자식을 가진 경우 둘의 자식을 가진 경우 를 다 고려해야 한다.힙(heap)힙: 이진 트리의 한 종류힙의 특징 루트 노드가 언제나 최댓값 또는 최솟값을 가짐 최대 힙, 최소 힙 완전 이진 트리여야 함이진 탐색 트리와의 비교 둘 다 원소들은 안전히 크기 순으로 정렬되어 있음 특정 키 값을 가지고 빠르게 원소를 찾을 수 있는가? 이진 탐색트리 - O 힙 - X 부가 제약 조건 힙의 경우 완전 이진 트리여야 함 최대 힙의 연산의 종류 __init__(): 비어있는 최대 힙을 생성 insert(item): 새로운 원소를 삽입 remove(): 최대 원소(root node)를 반환 및 제거노드 번호m을 기준으로 왼쪽 자식의 번호: 2*m 오른쪽 자식의 번호: 2*m + 1 부모 노드의 번호: m // 2완전 이진 트리이므로 노드의 추가/삭제는 마지막 노드에서만 이루어진다.__init__class Maxheap: def __init__(Self): self.data = [None]0번 인덱스는 버리기 위해 none삽입(insert) 트리의 마지막 자리에 새로운 원소를 임시로 저장 부모 노드와 키 값을 비교하여 위로 이동시킴복잡도: 원소의 개수가 n인 최대 힙에 새로운 원소 삽입 후 부모 노드와 계속 비교 → log2(n) 두 변수 값을 바꾸는 방법 a,b = b,a def bft(self): traversal = [] q = ArrayQueue() if self.root: q.enqueue(self.root) while not q.isEmpty(): k = q.dequeue() traversal.append(k.data) if k.left: q.enqueue(k.left) if k.right: q.enqueue(k.right) return traversal제거(remove)최대 힙에서 원소를 삭제 루트 노드의 제거 → 이것이 원소들 중 최댓값 트리 마지막 자리 노드를 임시로 루트 노드의 자리에 배치 → 완전 이진 트리를 만들기 위함 자식 노드들과의 값 비교 → 아래로 이동 → 자식이 둘이라면, 자식 중 더 큰 값 선택복잡도: 원소의 개수가 n인 최대 힙에서 최대 원소 삭제 → 자식 노드들과의 대소 비교 최대 회수: 2 x log2(n) → 최악 복잡도가 O(logn)의 삭제 연산최대/최소 힙의 응용 우선 순위 큐 enqueue할 때 느슨한 정렬을 이루고 있도록 함 dequeue할 때 최댓값을 순서대로 추출 힙 정렬 정렬되지 않은 원소들을 아무 순서로나 최대 힙에 삽입 O(logn) 삽입이 끝나면 힙이 비게 될 때까지 하나씩 삭제 O(logn) 원소들이 삭제된 순서가 원소들의 정렬 순서 정렬 알고리즘의 복잡도: O(nlogn) # 힙 정렬의 코드 구현def heapsort(unsorted): H = MaxHeap() for item in unsorted: H.insert(item) sorted = [] d = H.remove() while d: sorted.append(d) d = H.remove() return sorted 알고리즘을 풀면서 느낀 점c++ 아직 너무 약하다long long, string에 대해서만 알아도 lv2까지는 풀 것 같다. LLONG_MIN, LLONG_MAX, 계산을 long long으로 한정하기 위한 1*LL string(메모리 용량 정의), vector(메모리 용량 정의)next_permutation do { } while (next_permutation(dungeons.begin(), dungeons.end()));행렬 테두리 회전, 피로도, 모음 사전" }, { "title": "[데브코스] 1주차 - 알고리즘 문제 풀기 (1)", "url": "/posts/algorithm/", "categories": "Classlog, devcourse", "tags": "algorithm, devcourse", "date": "2022-02-15 13:00:00 +0900", "snippet": " 코드에 대한 주석들은 추후에 달도록 하겠습니다. 현재는 블로그 정리만 하려고 올리는 것입니다.자료구조와 알고리즘자료구조 문자열 (str) “this is a string” 리스트 (list) [4,9,2,7] 사전 (dict) {‘a’:2, ‘bc’,:6} 순서쌍 (tuple), 집합 (set), …해결할 문제에 대한 적합한 구조를 사용하는 것이 중요하다.알고리즘 사전적 정의: 어떤 문제를 해결하기 위한 절차, 방법, 명령어들의 집합 프로그래밍적 의미: 주어진 문제의 해결을 위한 자료구조와 연산 방법에 대한 선택해결하고자 하는 문제에 따라 최적의 해법이 서로 다르기 때문에 최적의 방법을 찾기 위해 자료구조를 이해해야 한다.선형 배열배열이란 원소들을 순서대로 늘어놓은 것이다. 이는 인덱스가 부여되는데, 0부터 시작한다. 파이썬에서는 리스트를 사용한다. 다른 언어들은 같은 타입의 원소만으로 구성되야 하지만, 파이썬은 다른 타입의 원소들로도 구성이 가능하다.리스트는 대괄호[], 튜플은 소괄호(), 딕셔너리는 중괄호{} 이다.리스트 연산 원소 덧붙이기: x.append() 끝에서 꺼내기: x.pop()이들은 원소의 길이와 상관없이 맨 앞 또는 맨뒤에 것들에 대한 작업을 진행한다. 이처럼 빠르게 할 수 있는 작업을 리스트의 길이와 무관한 것을 O(1)이라 한다. 원소 삽입하기: x.insert(3,65) -&amp;gt; 3 위치에 원소 65를 삽입 원소 삭제하기: del(x[2]) -&amp;gt; 2 위치에 원소를 삭제이들은 원소의 길이가 길수록 시간이 오래걸린다. 이처럼 리스트의 길이가 길면 오래 걸리는 작업을 리스트의 길이에 비례(선형 시간) 이를 O(n)이라 한다.pop과 del의 차이는 추출한 원소를 저장하느냐 하지 않느냐의 차이다. a = x.pop(2) 처럼 pop을 통해 x의 2인덱스의 원소를 a에 저장할 수 있다. 원소 탐색하기: x.index(65)배열 - 정렬과 탐색정렬 sorted(x,reverse=True) : 정렬된 새로운 리스트를 얻어내어 다른 곳에 저장 가능 x.sort(reverse=True) : 해당 리스트를 정렬default값이 오름차순인데, 내림차순하고 싶은 경우 reverse = True, 문자열로 이루어진 리스트의 경우 알파벳 순서를 따르고, 길이 순서로 정렬하고 싶은 경우 key를 사용sorted(x, key=lambda x: len(x))탐색 선형 탐색 : 탐색이 오래걸리는 O(n)에 해당 이진 탐색 : 리스트를 절반씩 줄여가며 탐색 이진탐색은 리스트가 크기순으로 되어 있다는 것을 가정하기에 탐색하려는 리스트를 정렬하고나서 사용가능하다. 이진탐색은 O(log n)에 비례하는 복잡도를 가진다. 리스트를 반으로 자르고, 계속 반으로 자르는 재귀함수를 사용해야 할 것이다.재귀 알고리즘(recursive algorithm) 기초재귀함수(recursive algorithm)란 하나의 함수에서 자신을 다시 호출하여 작업을 수행하는 것 e.g. 이진 트리(binary trees)왼쪽 서브트리의 원소들은 모두 작거나 같은 것, 오른쪽 서브트리의 원소들은 모두 큰 것으로 이루어져 있다.재귀 함수는 종결 조건을 설정하는 것이 매우 중요하다. recursive(재귀) version Vs iterative(반복) version복잡도 측면에서 보면 두 방법이 O(n)으로 같다. 하지만 효율성에 대해서는 재귀가 더 좋다.재귀 알고리즘 - 응용조합의수조합의 수 : n개의 서로 다른 원소에서 m개를 택하는 경우의 수이는 특정한 하나의 원소의 입장에서 볼 때, 이 원소를 포함하는 경우와 그렇지 않은 경우를 따로 계산해서 더하는 것이다.이것은 재귀로서def combi(n,m): return combi(n-1,m) + combi(n-1,m-1)로 표현이 가능하다. 하지만 여기서 종결 조건이 없기 때문에 이를 설정하여 다시 표현하면def combi(n,m): if n == m: return 1 elif m == 0: return 1 else: return combi(n-1,m) + combi(n-1,m-1)하노이탑시간이 날 때, 재귀함수로 짜보고자 한다.알고리즘의 복잡도 시간 복잡도 (Time Complexity)문제의 크기와 이를 해결하는 데 걸리는 시간 사이의 관계 공간 복잡도 (Space Complexity)문제의 크기와 이를 해결하는 데 필요한 메모리 공간 사이의 관계이번 강의에서는 시간 복잡도에 대해서만 강의한다.시간 복잡도에도 종류가 나뉜다. 평균 시간 복잡도 (Average Time Complexity)임의의 입력 패턴을 가정했을 때 소요되는 시간의 평균 최악 시간 복잡도 (Worst-case Time Complexity)가장 긴 시간을 소요하게 만드는 입력에 따라 소요되는 시간Big-O Notation점근 표기법(asymptotic notation)의 하나로 어떤 함수의 증가 양상을 다른 함수와의 비교로 표현하는 것이다. 알고리즘의 복잡도를 표현할 때 흔히 쓰인다.O(logn), O(n), O(n^2), O(2^n) 등으로 표기한다. 이는 입력의 크기가 n일 때, O(logn): 입력의 크기의 로그에 비례하는 시간 소요 O(n): 입력의 크기에 비례하는 시간 소요 O(n) - 선형 시간 알고리즘n개의 무작위로 나열된 수에서 최댓값을 찾기 위해 선형 탐색 알고리즘을 적용한다. 여기서 최댓값을 찾기 위해서는 끝까지 다 살펴보기 전까지는 알 수 없게 된다. 따라서 O(n)에 해당한다. O(logn) - 로그 시간 알고리즘n개의 크기 순으로 정렬된 수에서 특정 값을 찾기 위해 이진 탐색 알고리즘을 적용한다. O(n^2) - 이차 시간 알고리즘삽입 정렬이 있다. 알고리즘이 복잡하기에 간단하게만 정리한다. 알고리즘의 실행 시간이 n의 제곱에 비례하기 때문에 10개를 푸는 시간이 s 라면, 100개를 푸는 시간은 100 * s 가 된다. O(nlogn) - 병렬 정렬정렬할 데이터를 반씩 나누어 각각을 정렬시킨다. 이는 O(logn), 정렬된 데이터를 두 묶음씩 합친다. 이는 O(n). 따라서 O(nlogn)이 된다.연결 리스트 (Linked Lists) (1)추상적 자료구조 (Abstract Data Structures) Data: 정수, 문자열, 레코드 … A set of perations: 삽입, 삭제, 순회, 정렬, 탐색1개의 노드에는 값인 data와 다음을 가르키는 link(next) 값이 포함되어 있다. 노드는 숫자뿐만 아니라 문자, 레코드 등 다양하게 들어올 수 있다. 또한, 연결 리스트에는 기본적으로 처음과 끝을 가르키는 head와 tail, 총 길이를 가르키는 # of nodes가 있어야 한다.초기 node로서 코드는class Node: def __init__(self, item): self.data = item self.next = None그 후 연결 리스트를 구성하기 위해class LinkedList: def __init__(self): self.nodeCount = 0 self.head = None self.tail = None이 연결 리스트를 이용한 연산들에는 특정 원소 위치 참조 리스트 순회 길이 얻어내기 원소 삽입 원소 삭제 두 리스트 합치기가 있다.대부분의 연결 리스트에서 인덱스는 0이 아닌 1부터 시작한다. 그 이유는 0번을 head로 두기 때문이다.특정 원소 참조하는 코드(특정 위치의 노드)는def getAt(self, pos): if pos &amp;lt;= 0 or pos &amp;gt; self.nodeCount: return None i = 1 current = self.head while i &amp;lt; pos: current = current.next i += 1 return current일반 배열과 비교를 해보면저장 공간: 배열: 연속된 위치 연결리스트: 임의의 위치특정 원소 지칭: 배열: index를 통한 매우 간편함 연결리스트: 선형탐색과 유사하게 처음부터 다 탐색해야 함따라서 배열은 O(1), 연결리스트는 O(n)의 복잡도를 가지지만, 다른 장점이 있기 때문에 사용한다.연결 리스트 (Linked Lists) (2)이번에는 위의 1~6중에 4~6에 대해 알아보자.연결 리스트 연산 - 원소의 삽입def insertAt(self, pos, newNode): prev = self.getAt(pos - 1) newNode.next = prev.next prev.next = newNode self.nodeCount += 1이 때, pos는 1 &amp;lt;= pos &amp;lt;= nodeCount + 1 의 범위 안에 있어야 하므로 pos에 대한 조건을 정의해야 한다.두 가지 주의 사항이 존재하는데 삽입하려는 위치가 리스트 맨 앞일 때 prev 없음 Head 조정 필요 삽입하려는 위치가 리스트 맨 끝일 때 Tail 조정 필요 따라서 코드는def insertAt(self, pos, newnode): if pos &amp;lt; 1 or pos &amp;gt; self.nodeCount + 1: return False if pos == 1: newnode.next = self.head self.head = newnode elif pos == self.nodeCount + 1: self.tail = newnode else: if pos == self.nodeCount + 1: prev = self.tail # 맨 뒤 일 경우 처음부터 읽을 필요없이 tail만 사용 else: prev = self.getAt(pos - 1) newnode.next= prev.next prev.next= newnode if pos == self.nodeCOunt + 1: self.tali = newNode self.nodeCount += 1 return True이 때, 맨 앞에 삽입하는 경우는 O(1), 맨 끝에 삽입하는 경우도 O(1), 중간에 삽입하는 경우가 O(n)이 된다.연결 리스트 연산 - 원소의 삭제def popAt(self, pos):인데, 이때도 주의 사항은 삭제하려는 node가 맨 앞의 것일 때 prev 없음 head 조정 필요 리스트 맨 끝의 node를 삭제할 때 tail 조정 필요 맨 끝의 node를 삭제하는 경우 그 전의 pos -1에 대한 값을 가져올 방법이 없으므로 다 순회해야 한다. 따라서 맨 앞에 삽입하는 경우는 O(1), 중간 또는 맨 끝에 삽입하는 경우는 O(n)에 해당한다.연결 리스트 연산 - 두 리스트의 연결def concat(self,L): self.tail.next = L.head if L.tail: self.tail = L.tail self.nodeCount += L.nodeCount이때는 L이 None이 아닌지 판단해야 하기 때문에, if를 삽입했다.연결리스트 (Linked Lists) (3)매번 할 때마다 getAt를 통해 리스트를 순회하는 것은 너무 불필요하다. 따라서 insertAt,popAt가 아닌 insertAfter(prev, newnode),popAfter를(prev) 통해 이전의 값을 넣으면 그 다음에 노드를 지칭하도록 만들고자 한다. 그러기 위해 0 index에 None인 dummy node를 만들어준다.class LinkedList: def __init__(self): self.nodeCount = 0 self.head = Node(None) self.tail = None self.head.next = self.tail이를 통해 dummy node를 생성하고, 그 노드가 원래의 1 index를 가르키도록 만든다.리스트 순회def traverse(self): answer = [] curr = self.head if curr.data == None: return answer answer.append(curr.data) while curr.next != None: curr = curr.next answer.append(curr.data) return answer&#39;&#39;&#39; ---- dummy 추가 ---- &#39;&#39;&#39;def traverse(self): answer = [] curr = self.head while curr.next: curr = curr.next answer.append(curr.data) return answerk번째 원소 얻기def getAt(self, pos): if pos &amp;lt;= 0 or pos &amp;gt; self.nodeCount: return None i = 1 current = self.head while i &amp;lt; pos: current = current.next i += 1 return current&#39;&#39;&#39; ---- dummy 추가 ---- &#39;&#39;&#39;def getAt(self, pos): if pos &amp;lt; 0 or pos &amp;gt; self.nodeCount: return None i = 0 current = self.head while i &amp;lt; pos: current = current.next i += 1 return current원소 삽입def insertAfter(self, prev, newnode): newnode.next= prev.next if prev.next is None: self.tail = newnode prev.next= newnode self.nodeCount += 1 return TrueinsertAfter를 활용하여 insertAt을 구현하고자 한다. 이때 고려할 사항들은 pos 범위 조건 확인 pos == 1인 경우 head 뒤에 새 node삽입 pos == nodeCount + 1 인 경우 prev가 tail 그렇지 않은 경우 prev = getAt()def insertAt(self, pos, newnode): if pos &amp;lt; 1 or pos &amp;gt; self.nodeCount + 1: return False if pos != 1 and pos == self.nodeCount + 1: prev = self.tail # pos가 1이라면 nodeCount가 0, 즉 빈 리스트이기 때문에 prev가 존재하지 않을 것이다. 따라서 예외 처리 else: prev = self.getAt(pos - 1) return self.insertAfter(prev,newnode)prev가 상황마다 달라지기 때문에 그에 따른 prev를 각각 정의해준 후 insertAfter에 넣은 것이다.원소 삭제고려 사항들에는 prev가 마지막 노드일 때(prev.next == None) 삭제할 노드가 없다. return None 리스트의 맨 끝의 노드를 삭제할 때(curr.next == None) tail 조정 필요 두 리스트 연결 def popAfter(self, prev): curr = prev.next if curr.next is None: self.tail = prev prev.next = curr.next self.nodeCount -= 1 return curr.datadef popAt(self, pos): if pos &amp;lt; 1 or pos &amp;gt; self.nodeCount: raise IndexError if pos == self.nodeCount + 1: return None if pos == 1: prev = self.head else: prev = self.getAt(pos - 1) return self.popAfter(prev)리스트 연결여기서는 dummy를 뺀 첫번째 값들부터 연결시켜야 한다.self.tail.next = L2.head.nextself.tail = L2.tail로 연결시켜준다.def concat(self,L): self.tail.next = L.head.next if L.tail: self.tail = L.tail self.nodeCount += L.nodeCount양방향 연결 리스트한 쪽 방향으로만 하면 뒤로 돌아오는 것이 불가능하기에 양방향으로 연결하여 이진 탐색 구조로 탐색이 가능하도록 만든다.class Node: def __init__(self,item): self.data = item self.prev = None self.next = None여기서는 처음과 끝에 dummy node를 만들어야 한다.class DoublyLinkedList:def __init__(self): self.nodeCount = 0 self.head = Node(None) self.tail = Node(None) self.head.prev = None self.head.next = self.tail self.tail.prev = self.head self.tail.next = None리스트 역순환def reverse(self): ans = [] reverse_answer = [] curr = self.tail while curr.prev.prev: curr = curr.prev ans.append(curr.data) reverse_answer.insert(0,curr.data) # 리스트를 역순으로 배치하는 것도 생각해보았습니다. return ans, reverse_answer원소 참조양방향 연결 리스트를 활용하여 특정 원소 참조 함수를 정의하면 다음과 같다.def getAt(self, pos): if pos &amp;lt; 0 or pos &amp;gt; self.nodeCount: return None if pos &amp;gt; self.nodeCount // 2: i = 0 curr = self.tail while i &amp;lt; self.nodeCount - pos + 1: curr = curr.prev i += 1 else: i = 0 curr = self.head while i &amp;lt; pos: curr = curr.next i += 1 return curr원소 삽입def insertAfter(self, prev, newNode): next = prev.next newNode.prev = prev newNode.next = next prev.next = newNode next.prev = newNode self.nodeCount += 1 return Truedef insertBefore(self, next, newNode): curr = next.prev if curr.prev is None: self.head.next = newNode newNode.prev = self.head elif next.next is None: self.tail.prev = newNode newNode.next = self.tail curr.next = newNode next.prev = newNode newNode.prev = curr newNode.next = next self.nodeCount += 1 return Truedef insertAt(self, pos, newNode): if pos &amp;lt; 1 or pos &amp;gt; self.nodeCount + 1: return False prev = self.getAt(pos - 1) return self.insertAfter(prev, newNode)원소 삭제def popAfter(self, prev): curr = prev.next if curr.next is None: self.tail = prev prev.next = curr.next curr.next.prev = curr.prev self.nodeCount -= 1 return curr.datadef popBefore(self, next): curr = next.prev if curr.prev is None: self.head = next next.prev = curr.prev curr.prev.next = curr.next self.nodeCount -= 1 return curr.datadef popAt(self, pos): if pos &amp;lt; 1 or pos &amp;gt; self.nodeCount: raise IndexError if pos == 1: prev = self.head else: prev = self.getAt(pos - 1) return self.popAfter(prev)두 리스트 연결def concat(self, L): self.tail.prev.next = L.head.next L.head.next.prev = self.tail.prev if L.tail: self.tail = L.tail self.nodeCount += L.nodeCount return True" }, { "title": "[데브코스] 카테고리 설명 및 개요", "url": "/posts/start/", "categories": "Classlog, devcourse", "tags": "devcourse", "date": "2022-02-14 13:00:00 +0900", "snippet": "데브코스라는 교육을 받으면서 배웠던 내용을 기재하고자 한다. 최대한 많은 내용을 담고 싶지만, 저작권으로 인해 적지 못하는 부분도 많을 것 같다. 원래는 노션에 기록하고 있었지만, 매번 블로그에 기재하던 것처럼 블로그에 업로드해야 계속 기록에 남기도 하고 다음 번에도 계속 볼 것 같다." }, { "title": "[깃허브 프로필 꾸미기] waka box 만들기", "url": "/posts/waka/", "categories": "Review, Github", "tags": "Gihub, waka box", "date": "2022-02-13 01:44:00 +0900", "snippet": "이번에는 깃허브 프로필을 꾸미기 위해 waka box를 만들어보고자 한다. 이는 내가 어떤 언어를 가장 많이 사용하는지 볼 수 있다.레포지토리 포크아래 페이지를 포크한다.https://github.com/xrkffgg/waka-boxGIST_ID 생성gist는 ui를 보여줄 공간이므로 내용은 아무렇게나 해도 상관없다. 여기서 중요한 것은 create secret gist가 아닌 create public gist로 해야 한다.GH_TOKEN 생성gist만 선택해주고 generate token을 클릭한다.WAKA 계정 생성 및 apihttps://wakatime.com/signup여기의 wakatime 홈페이지를 회원가입한다. 회원가입은 깃허브 계정으로 하면 된다.그 후 profile을 수정해야 한다.여기서 빨간색 박스 두개를 체크하고 save한다.wakatime api 저장wakatime api를 저장할 때, vscode를 사용하여 api를 지정해줄 것이다.그 전에, https://wakatime.com/settings/api-key 이 사이트를 통해 자신의 api key를 복사한다.vscode를 실행한 후 wakatime 플러그인을 설치한다. 그 후 플러그인의 설명대로 위에 뜨는 곳에 api를 붙여넣기하고 enter를 누른다.secret 환경변수 생성GIST_ID,GH_TOKEN,WAKATIME_API_KEY 총 3가지를 생성해야 한다.이름은 GIST_ID로 지정하고, 2번에서 생성한 gist에 대한 url에서 https://gist.github.com/username/뒤에 있는 70344be470a5945f7005f2c7d1c88f76를 복사하여 위의 환경변수에 저장한다.GH_TOKEN와 WAKATIME_API_KEY도 동일하게 생성한다.actions 설정위의 i understand my workflows, go ahead and enable them을 클릭한다. 또, 그 후 노란색 느낌표를 클릭하여 enable workflow를 누른다.schedule.yml 설정name: Update Giston: push: branches: master schedule: - cron: &quot;12 */12 * * *&quot;jobs: update-gist: runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: Install run: npm ci - name: Update gist run: node ./index.js env: GH_TOKEN: $ GIST_ID: 70344be470a5945f7005f2c7d1c88f76 WAKATIME_API_KEY: $ TIMEZONE: Asia/Seoulworkflow/schedule.yml 파일을 위의 코드처럼 수정한다.Pin 설정나의 깃허브 홈 화면에서 핀 설정을 통해 확인해볼 수 있다. dk라는 gist 공간을 핀 설정하고나면 wakatime에 대해 수정된 것을 볼 수 있다.Reference 참고 블로그" }, { "title": "[detection] 얼굴 이미지를 통해 동물상 테스트", "url": "/posts/facedetection/", "categories": "Projects, detection", "tags": "detection, face-detection", "date": "2022-02-12 19:02:00 +0900", "snippet": "깃허브 주소Abstract업로드된 이미지에 대해 어떤 동물을 닮았는지 분류하는 테스트를 만들었다.before upload imageprediction result about upload imagepython을 통해 딥러닝 모델을 구축하고, flask로 서버와 html을 연결했다. directoryface └── data ├── train ├── cat ├── 0.jpg ├── 1.jpg . . ├── dinosasur ├── 0.jpg ├── 1.jpg . . . . ├── test ├── Angelina ├── 0.jpg ├── 1.jpg . . ├── cha ├── 0.jpg ├── 1.jpg . . . . └── google.py ├── static ├── cat.jpg ├── dinosaur.jpg └── img.jpg . . ├── templates ├── index.html ├── test.html └── upload.html ├── weight └── model_best_epoch.pt ├── chromedriver.exe ├── app.py ├── train.py ├── ngrok.yml . . └── README.md INDEX 이미지 크롤링 (data/, google.py, chromedriver.exe) 훈련/테스트 (weight/, train.py) 서버와 연결 (html, templates/, app.py) 구동 테스트 (static/) 이미지 크롤링훈련, 테스트 셋을 다운받기 위해 이미지 크롤링을 진행했다.이미지 크롤링 코드와 방법은 조코딩 유튜버님의 영상을 참고했다. chromedriver.exe일단 우선 chromedriver를 다운해서 해당 폴더에 chromedriver.exd를 넣어야 한다. 위의 사이트로 들어가서 다운로드 한 후 exe파일을 옮겨준다. google.py이미지 크롤링을 하기 위해서는 일단 파일명을 google.py로 지정을 해야한다고 한다. 파일을 생성한 후 영상을 참고하여 코드를 구성했다.# import libraryfrom selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.common.by import Byimport timeimport urllib.requestimport os크롤링을 위한 라이브러리를 임포트했다.# name list for downloadingtrain_namespace = [&quot;dog&quot;,&quot;cat&quot;,&quot;dinosaur&quot;,&quot;rabbit&quot;,&quot;fox&quot;]test_namespace = [&quot;song min ho&quot;,&quot;Angelina Jolie&quot;, &quot;Keanu Reeves&quot;, &quot;Mark Ruffalo&quot;, &quot;Elon Musk&quot;, &quot;Robert Pattinson&quot;,&quot;IU &quot;,&quot;cha eun woo&quot;,&quot;lee dong wook&quot;,&quot;han hyo joo&quot;]# your base urlbaseurl = &quot;C:/Users/dkssu/Github/Animalface-detector/&quot;데이터셋 라벨로 사용할 이름과 나의 workspace를 지정해주었다.for name in train_namespace: # make fold k = os.path.join(baseurl,&quot;data/train&quot;,name.split(&quot; &quot;)[0]) os.makedirs(k, exist_ok=True) # solving chrome error options = webdriver.ChromeOptions() options.add_argument(&#39;headless&#39;) options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-logging&#39;]) # chrome excute driver = webdriver.Chrome(options=options) driver.get(&quot;https://www.google.co.kr/imghp?hl=ko&amp;amp;authuser=0&amp;amp;ogbl&quot;) elem = driver.find_element(By.NAME, &quot;q&quot;) elem.send_keys(name) elem.send_keys(Keys.RETURN)이미지를 저장할 폴더를 생성하고, 자동화 크롤링을 위한 webdriver를 실행하는 코드다. headless는 코드를 실행할동안 chrome 화면이 뜨지 않도록 하는 옵션이다.그리고 driver를 실행하면 다음과 같은 화면이 뜬다.여기서 내가 원하는 이름의 이미지를 검색해서 다운받아야 하기 떄문에, 검색창의 요소를 찾아야 한다. 그 이름이 q였다. 그래서 find_element를 한 후에 name, 즉 내가 검색할 이름을 key로 보낸다. # Get scroll height last_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;) while True: # Scroll down to bottom driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) # Wait to load page time.sleep(1) # Calculate new scroll height and compare with last scroll height new_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;) if new_height == last_height: try: driver.find_element(By.CSS_SELECTOR, &quot;.mye4qd&quot;).click() except: break last_height = new_height이 코드는 검색을 하면 이미지가 20장 정도 한정되어 나올 것이다. 그러나 내가 딥러닝 모델을 돌리기 위해서는 더 많은 데이터가 필요하기 때문에 스크롤을 해주는 코드다. 스크롤을 하는 이유는 스크롤을 하지 않으면 출력되는 이미지가 한정적인데, 스크롤을 하게 되면 더 많은 이미지가 검색되고 출력되기 때문이다. 그 후, 100장 정도 출력되면 다음과 같은 버튼이 보인다.결과 더보기 버튼을 눌러야 더 많은 이미지가 나오기 때문에 이를 클릭해주는 코드다. 모든 이미지가 다 나오면 결과 더보기가 안 뜨므로 try &amp;amp; except 코드를 통해 결과 더보기 버튼이 있으면 누르고 없으면 루프를 빠져나오도록 했다. images = driver.find_elements(By.CSS_SELECTOR, &quot;.rg_i.Q4LuWd&quot;) # number of images cnt = 0 for img in images: if not os.path.isfile(k + &quot;/&quot; + str(cnt) + &quot;.png&quot;): try: start = time.time() img.click() cnt+=1 time.sleep(2) src = driver.find_element(By.XPATH, &quot;/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div/a/img&quot;).get_attribute(&quot;src&quot;) if start &amp;gt; 1*10^-5: pass urllib.request.urlretrieve(src, k + &quot;/&quot; + str(cnt) + &quot;.png&quot;) if cnt == 500: print(name,&quot; Finish!&quot;) break except: print(&quot;Do Not this&quot;) pass else: cnt+=1 print(&quot;Exists already&quot;) passdriver.quit()이미지들의 주소는 images 변수에 저장된다. 저장된 주소를 통해 데이터 폴더에 저장한다. 시간이 너무 오래 걸리거나 사진을 찾을 수 없으면 지나가도록 했으며, 500개를 저장하면 루프를 빠져나온다. 또는 동일한 이름의 이미지가 이미 존재하면 바로 다음 이미지 주소로 넘어간다.중요한 것은 마지막에 driver를 종료해주어야 한다. 아니면 창이 백그라운드에 계속 남아 있다.&#39;&#39;&#39;print(&quot;\\nTest image download\\n&quot;)for name in test_namespace: # make fold k = os.path.join(baseurl,&quot;data/test&quot;,name.split(&quot; &quot;)[0]) os.makedirs(k, exist_ok=True) # solving chrome error options = webdriver.ChromeOptions() options.add_argument(&#39;headless&#39;) options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-logging&#39;]) # chrome excute driver = webdriver.Chrome(options=options) driver.get(&quot;https://www.google.co.kr/imghp?hl=ko&amp;amp;authuser=0&amp;amp;ogbl&quot;) elem = driver.find_element(By.NAME, &quot;q&quot;) elem.send_keys(name) elem.send_keys(Keys.RETURN) # Get scroll height last_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;) while True: # Scroll down to bottom driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) # Wait to load page time.sleep(1) # Calculate new scroll height and compare with last scroll height new_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;) if new_height == last_height: try: driver.find_element(By.CSS_SELECTOR, &quot;.mye4qd&quot;).click() except: break last_height = new_height images = driver.find_elements(By.CSS_SELECTOR, &quot;.rg_i.Q4LuWd&quot;) # number of images cnt = 0 for img in images: if not os.path.isfile(k + &quot;/&quot; + str(cnt) + &quot;.jpg&quot;): try: start = time.time() img.click() time.sleep(2) src = driver.find_element(By.XPATH, &quot;/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div/a/img&quot;).get_attribute(&quot;src&quot;) if start &amp;gt; 1*10^-5: pass urllib.request.urlretrieve(src, k + &quot;/&quot; + str(cnt) + &quot;.jpg&quot;) cnt+=1 if cnt == 20: print(name,&quot; Finish!&quot;) break except: print(&quot;Do Not this&quot;) pass else: print(&quot;Exists already&quot;) passdriver.quit()&#39;&#39;&#39;이는 테스트 데이터셋을 생성하기 위한 코드다. 위와 동일한 구성이다. 전체 코드from selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.common.by import Byimport timeimport urllib.requestimport os# name list for downloadingtrain_namespace = [&quot;dog&quot;,&quot;cat&quot;,&quot;dinosaur&quot;,&quot;rabbit&quot;,&quot;fox&quot;]test_namespace = [&quot;song min ho&quot;,&quot;Angelina Jolie&quot;, &quot;Keanu Reeves&quot;, &quot;Mark Ruffalo&quot;, &quot;Elon Musk&quot;, &quot;Robert Pattinson&quot;,&quot;IU &quot;,&quot;cha eun woo&quot;,&quot;lee dong wook&quot;,&quot;han hyo joo&quot;]# your base urlbaseurl = &quot;C:/Users/dkssu/Github/face&quot;for name in train_namespace: # make fold k = os.path.join(baseurl,&quot;data/train&quot;,name.split(&quot; &quot;)[0]) os.makedirs(k, exist_ok=True) # solving chrome error options = webdriver.ChromeOptions() options.add_argument(&#39;headless&#39;) options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-logging&#39;]) # chrome excute driver = webdriver.Chrome(options=options) driver.get(&quot;https://www.google.co.kr/imghp?hl=ko&amp;amp;authuser=0&amp;amp;ogbl&quot;) elem = driver.find_element(By.NAME, &quot;q&quot;) elem.send_keys(name) elem.send_keys(Keys.RETURN) # Get scroll height last_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;) while True: # Scroll down to bottom driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) # Wait to load page time.sleep(1) # Calculate new scroll height and compare with last scroll height new_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;) if new_height == last_height: try: driver.find_element(By.CSS_SELECTOR, &quot;.mye4qd&quot;).click() except: break last_height = new_height images = driver.find_elements(By.CSS_SELECTOR, &quot;.rg_i.Q4LuWd&quot;) # number of images cnt = 0 for img in images: if not os.path.isfile(k + &quot;/&quot; + str(cnt) + &quot;.jpg&quot;): try: start = time.time() img.click() cnt+=1 time.sleep(2) src = driver.find_element(By.XPATH, &quot;/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div/a/img&quot;).get_attribute(&quot;src&quot;) if start &amp;gt; 1*10^-5: pass urllib.request.urlretrieve(src, k + &quot;/&quot; + str(cnt) + &quot;.jpg&quot;) if cnt == 500: print(name,&quot; Finish!&quot;) break except: print(&quot;Do Not this&quot;) pass else: cnt+=1 print(&quot;Exists already&quot;) passdriver.quit()훈련/테스트 train.py모델 훈련을 위한 파일이다. 전체 코드import torchimport torch.nn as nnimport torch.optim as optimimport torchvisionfrom torchvision import datasets, models, transformsimport sklearnfrom sklearn.model_selection import train_test_splitimport numpy as npimport matplotlib.pyplot as pltimport timeimport osfrom glob import globimport matplotlibimport matplotlib.pyplot as pltimport matplotlib.font_manager as fm# print(fm.findSystemFonts(fontpaths=None, fontext=&#39;ttf&#39;))# 한글 폰트 설정하기fontpath = &#39;C:/Windows/Fonts/NanumGothicLight.ttf&#39;font = fm.FontProperties(fname=fontpath, size=10).get_name()plt.rc(&#39;font&#39;, family=font)# use GPUdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)print(device)#CUDA_LAUNCH_BLOCKING=1# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의transforms_train = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation) transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)])transforms_test = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])data_dir = &#39;./data&#39;train_datasets = datasets.ImageFolder(os.path.join(data_dir,&#39;train&#39;), transforms_train)train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=4, shuffle=True, num_workers=0)#valid_dataloader = torch.utils.data.DataLoader(valid_datasets, batch_size=4, shuffle=False, num_workers=0)print(&#39;학습 데이터셋 크기:&#39;, len(train_datasets))#print(&#39;테스트 데이터셋 크기:&#39;, len(valid_datasets))class_names = train_datasets.classesprint(&#39;학습 클래스:&#39;, class_names)def imshow(input, title): # torch.Tensor를 numpy 객체로 변환 input = input.numpy().transpose((1, 2, 0)) # 이미지 정규화 해제하기 mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) input = std * input + mean input = np.clip(input, 0, 1) # 이미지 출력 plt.imshow(input) plt.title(title) plt.show()# 학습 데이터를 배치 단위로 불러오기iterator = iter(train_dataloader)# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화inputs, classes = next(iterator)out = torchvision.utils.make_grid(inputs)# imshow(out, title=[class_names[x] for x in classes])# implement modelmodel = models.resnet34(pretrained=True)num_features = model.fc.in_features# transfer learningmodel.fc = nn.Sequential( nn.Linear(num_features, 256), # 마지막 완전히 연결된 계층에 대한 입력은 선형 계층, 256개의 출력값을 가짐 nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, num_features), # Since 10 possible outputs = 10 classes nn.LogSoftmax(dim=1) # For using NLLLoss())criterion = nn.NLLLoss()optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)model = model.to(device)num_epochs = 30best_epoch = Nonebest_loss = 5&#39;&#39;&#39; Train &#39;&#39;&#39;# 전체 반복(epoch) 수 만큼 반복하며for epoch in range(num_epochs): model.train() start_time = time.time() running_loss = 0. running_corrects = 0 # 배치 단위로 학습 데이터 불러오기 for inputs, labels in train_dataloader: inputs = inputs.to(device) labels = labels.to(device) # 모델에 입력(forward)하고 결과 계산 optimizer.zero_grad() outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # 역전파를 통해 기울기(gradient) 계산 및 학습 진행 loss.backward() optimizer.step() running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(train_datasets) epoch_acc = running_corrects / len(train_datasets) * 100. # 학습 과정 중에 결과 출력 print(&#39;#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s&#39;.format(epoch, epoch_loss, epoch_acc, time.time() - start_time)) if epoch_loss &amp;lt; best_loss: best_loss = epoch_loss best_epoch = epoch print(&quot;best_loss: {:.4f} \\t best_epoch: {}&quot;.format(best_loss, best_epoch))os.makedirs(&#39;./weight&#39;,exist_ok=True)torch.save(model, &#39;./weight/model_best_epoch.pt&#39;)&#39;&#39;&#39; Valid with torch.no_grad(): model.eval() start_time = time.time() running_loss = 0. running_corrects = 0 for inputs, labels in valid_dataloader: inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) # 한 배치의 첫 번째 이미지에 대하여 결과 시각화 print(f&#39;[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})&#39;) imshow(inputs.cpu().data[0], title=&#39;예측 결과: &#39; + class_names[preds[0]]) epoch_loss = running_loss / len(valid_datasets) epoch_acc = running_corrects / len(valid_datasets) * 100. print(&#39;[valid Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s&#39;.format(epoch_loss, epoch_acc, time.time() - start_time)) &#39;&#39;&#39;&#39;&#39;&#39; Test &#39;&#39;&#39;valid_images = []valid_dir = data_dir + &#39;/test&#39;val_folders = glob(valid_dir + &#39;/*&#39;)for val_folder in val_folders: image_paths = glob(val_folder + &#39;/*&#39;) for image_path in image_paths: valid_images.append(image_path)import randomnum = random.randint(0,len(valid_images)-1)valid_image = valid_images[num]from PIL import Imageimage = Image.open(valid_image)image = transforms_test(image).unsqueeze(0).to(device)with torch.no_grad(): outputs = model(image) _, preds = torch.max(outputs, 1) imshow(image.cpu().data[0], title=&#39; 학습 결과 : &#39; + class_names[preds[0]])코드를 상세히 설명하자면 먼저 필요한 라이브러리들을 임포트한다.import torchimport torch.nn as nnimport torch.optim as optimimport torchvisionfrom torchvision import datasets, models, transformsimport sklearnfrom sklearn.model_selection import train_test_splitimport numpy as npimport matplotlib.pyplot as pltimport timeimport osfrom glob import globimport matplotlibimport matplotlib.pyplot as pltimport matplotlib.font_manager as fmfontpath = &#39;C:/Windows/Fonts/NanumGothicLight.ttf&#39;font = fm.FontProperties(fname=fontpath, size=10).get_name()plt.rc(&#39;font&#39;, family=font)그 후 출력을 한글로 해야 하는데, matplotlib에는 한글 패치가 적용되어 있지 않다. 설정하지 않은 채로 한글을 출력하면 ▯형태로 출력된다. 그래서 직접 설정해주었다.device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)print(device)gpu를 사용하기 위해 설정해주었다.# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의transforms_train = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation) transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)])transforms_test = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])데이터 증강을 위해 transform을 해야 한다. 이를 선언해주었다. 이미지 크기를 맞춰서 넣기 위해 resize를 하고, augmentation을 위해 flip(뒤집기)를 해준다. pytorch를 사용할 예정이므로 ToTensor를 넣어줘야 한다. 마지막으로 이미지 정규화를 위해 normalize를 추가했다. test에서는 데이터 증강을 하면 안되기 때문에 flip부분을 삭제했다.data_dir = &#39;./data&#39;train_datasets = datasets.ImageFolder(os.path.join(data_dir,&#39;train&#39;), transforms_train)train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=4, shuffle=True, num_workers=0)#valid_dataloader = torch.utils.data.DataLoader(valid_datasets, batch_size=4, shuffle=False, num_workers=0)print(&#39;학습 데이터셋 크기:&#39;, len(train_datasets))#print(&#39;테스트 데이터셋 크기:&#39;, len(valid_datasets))class_names = train_datasets.classesprint(&#39;학습 클래스:&#39;, class_names)robust하지는 않지만, 간단하게 학습하기 위해 customdataset이 아닌 iamgefolder과 dataloader 메서드를 사용하여 데이터셋을 생성했다. 컴퓨터 성능을 고려해서 batch_size를 작세 설정했다.추후 손실 함수에 사용될 class name도 변수에 저장해놓는다.def imshow(input, title): # torch.Tensor를 numpy 객체로 변환 input = input.numpy().transpose((1, 2, 0)) # 이미지 정규화 해제하기 mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) input = std * input + mean input = np.clip(input, 0, 1) # 이미지 출력 plt.imshow(input) plt.title(title) plt.show()validation에서 사용할 imshow함수다. 단순하게 검증한 이미지를 예측값과 함께 출력한다.# implement modelmodel = models.resnet34(pretrained=True)num_features = model.fc.in_features# transfer learningmodel.fc = nn.Sequential( nn.Linear(num_features, 256), # 마지막 완전히 연결된 계층에 대한 입력은 선형 계층, 256개의 출력값을 가짐 nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, num_features), # Since 10 possible outputs = 10 classes nn.LogSoftmax(dim=1) # For using NLLLoss())criterion = nn.NLLLoss()optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)model = model.to(device)num_epochs = 30best_epoch = Nonebest_loss = 5커스터마이징을 하지 않았기 때문에 pretrained model을 사용했다. 여기서 마지막 fc layer는 반드시 수정해줘야 한다. 원래의 resnet34의 출력 개수와 내가 출력하고자 하는 출력의 크기가 다르기 때문이다. NLLLoss를 사용하기 위해 logsoftmax 층을 추가했다.손실함수는 NLLLoss, 최적화 함수는 SGD를 사용했다. learing rate와 weight decay를 설정했다.gpu를 사용할 수 있다면 gpu를 사용하여 연산하기 위해 to(device)를 했다.반복할 횟수를 지정해주고, 최고의 성능을 저장할 것이기 때문에 그것을 저장할 변수도 생성한다.&#39;&#39;&#39; Train &#39;&#39;&#39;# 전체 반복(epoch) 수 만큼 반복하며for epoch in range(num_epochs): model.train() start_time = time.time() running_loss = 0. running_corrects = 0 # 배치 단위로 학습 데이터 불러오기 for inputs, labels in train_dataloader: inputs = inputs.to(device) labels = labels.to(device) # 모델에 입력(forward)하고 결과 계산 optimizer.zero_grad() outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # 역전파를 통해 기울기(gradient) 계산 및 학습 진행 loss.backward() optimizer.step() running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(train_datasets) epoch_acc = running_corrects / len(train_datasets) * 100. # 학습 과정 중에 결과 출력 print(&#39;#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s&#39;.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))모델을 훈련시키는 과정은 다음과 같다. 분류 모델이므로 가장 score가 높은 값을 추출하면 되므로 max를 사용했다. 만약 1위,2위,3위를 출력하기 위해서는 topk를 사용하면 된다. if epoch_loss &amp;lt; best_loss: best_loss = epoch_loss best_epoch = epoch print(&quot;best_loss: {:.4f} \\t best_epoch: {}&quot;.format(best_loss, best_epoch))os.makedirs(&#39;./weight&#39;,exist_ok=True)torch.save(model, &#39;./weight/model_best_epoch.pt&#39;)가장 성능이 좋았던 epoch을 저장하여 pt파일로 저장한다. 간단하게 모델 테스트 해보기valid_images = []valid_dir = data_dir + &#39;/test&#39;val_folders = glob(valid_dir + &#39;/*&#39;)for val_folder in val_folders: image_paths = glob(val_folder + &#39;/*&#39;) for image_path in image_paths: valid_images.append(image_path)import randomnum = random.randint(0,len(valid_images)-1)valid_image = valid_images[num]from PIL import Imageimage = Image.open(valid_image)image = transforms_test(image).unsqueeze(0).to(device)with torch.no_grad(): outputs = model(image) _, preds = torch.max(outputs, 1) imshow(image.cpu().data[0], title=&#39; 학습 결과 : &#39; + class_names[preds[0]])서버와 연결 app.py파이썬과 flask를 연동해서 사용했다. 전체 코드&#39;&#39;&#39; 분류 모델 API 학습된 모델을 다른 사람들이 사용할 수 있도록 api를 만들어 배포 &#39;&#39;&#39;from PIL import Imageimport torchfrom torchvision import transformsimport numpy as npimport matplotlib.pyplot as pltimport torchvisionfrom torchvision import datasets, models, transformsimport osimport matplotlibimport matplotlib.font_manager as fmfrom glob import glob# print(fm.findSystemFonts(fontpaths=None, fontext=&#39;ttf&#39;))# 한글 폰트 설정하기fontpath = &#39;C:/Windows/Fonts/NanumGothicLight.ttf&#39;font = fm.FontProperties(fname=fontpath, size=10).get_name()plt.rc(&#39;font&#39;, family=font)device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)transforms_train = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation) transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)])transforms_test = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])data_dir = &#39;./data&#39;train_datasets = datasets.ImageFolder(os.path.join(data_dir,&#39;train&#39;), transforms_train)class_names = train_datasets.classesmodel = torch.load(&quot;./weight/model_best_epoch.pt&quot;)&#39;&#39;&#39;웹 API 개방을 위해 ngrok 서비스 이용 API 기능 제공을 위해 Flask 프레임워크 사용 &#39;&#39;&#39;# 필요한 라이브러리 설치하기import io# 이미지를 읽어 결과를 반환하는 함수def get_prediction(image_bytes): image = Image.open(io.BytesIO(image_bytes)) image = transforms_test(image).unsqueeze(0).to(device) with torch.no_grad(): model.eval() outputs = model(image) outputs = torch.exp(outputs) topk, topclass = outputs.topk(3, dim=1) # argmax와 비슷하게 top-k에 대한 결과 값을 받는다. classes = [class_names[i] for i in topclass.cpu().numpy()[0]] scores = [round(i*100,2) for i in topk.cpu().numpy()[0]] print(classes, scores) return classes,scoresfrom flask_ngrok import run_with_ngrokfrom flask import Flask, jsonify, request, render_templatefrom flask import jsonifyapp = Flask(__name__)run_with_ngrok(app)@app.route(&#39;/&#39;)def hello(): return render_template(&#39;index.html&#39;)@app.route(&#39;/upload_image&#39;, methods=[&#39;POST&#39;,&#39;GET&#39;])def upload_image_file(): if request.method == &#39;POST&#39;: file = request.files[&#39;uploaded_image&#39;] if not file: return &quot;No Files&quot; image_bytes = file.read() up_image = Image.open(io.BytesIO(image_bytes)) up_image.save(&quot;./static/img.jpg&quot;,&quot;jpeg&quot;) # 분류 결과 확인 및 클라이언트에게 결과 반환 classes,scores = get_prediction(image_bytes=image_bytes) class_name = classes[0] &#39;&#39;&#39; 예측된 클래스에 대한 무작위 사진 가져오기 class_images = [] class_dir = &#39;./data/train/&#39; + class_name train_paths = sorted(glob(class_dir + &#39;/*&#39;),key= lambda x: x.split(&quot;\\\\&quot;)[-1].split(&#39;.&#39;)[0]) for train_path in train_paths: class_images.append(train_path) import random num = random.randint(0,10) img_path = class_images[num] pr_image = Image.open(img_path) pr_image.save(&quot;./static/&quot; + class_name + &quot;.jpg&quot;,&quot;jpeg&quot;) &#39;&#39;&#39; return render_template(&#39;upload.html&#39;, classes = classes, scores = scores, label = class_name, upload_img = &#39;img.jpg&#39;, predict_img = class_name + &#39;.jpg&#39;) else: return jsonify({&quot;Methods == &quot;:request.method}) @app.route(&#39;/test&#39;, methods=[&#39;POST&#39;,&#39;GET&#39;])def testing(): if request.method == &#39;POST&#39;: result = request.form return render_template(&#39;test.html&#39;,label = result) else: return render_template(&#39;test.html&#39;,label = request.method)if __name__ == &quot;__main__&quot;: app.debug = True app.run()## 사용 방식# curl -X POST -F file=@{이미지 파일명} {Ngrok 서버 주소}## 사용 예시# curl -X POST -F file=@dongseok.jpg http://c4cdb8de3a35.ngrok.io/# 참고# https://velog.io/@qsdcfd/%EC%9B%B9-%ED%8E%98%EC%9D%B4%EC%A7%80-%EB%A7%8C%EB%93%A4%EA%B8%B0from PIL import Imageimport torchfrom torchvision import transformsimport numpy as npimport matplotlib.pyplot as pltimport torchvisionfrom torchvision import datasets, models, transformsimport osimport matplotlibimport matplotlib.font_manager as fmfrom glob import globimport iofontpath = &#39;C:/Windows/Fonts/NanumGothicLight.ttf&#39;font = fm.FontProperties(fname=fontpath, size=10).get_name()plt.rc(&#39;font&#39;, family=font)device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)transforms_train = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation) transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)])transforms_test = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])data_dir = &#39;./data&#39;train_datasets = datasets.ImageFolder(os.path.join(data_dir,&#39;train&#39;), transforms_train)class_names = train_datasets.classestest 결과를 웹에서 출력하기 위해서 test에 필요한 변수들을 지정해준다.model = torch.load(&quot;./weight/model_best_epoch.pt&quot;)저장했던 모델을 불러온다.# 이미지를 읽어 결과를 반환하는 함수def get_prediction(image_bytes): image = Image.open(io.BytesIO(image_bytes)) image = transforms_test(image).unsqueeze(0).to(device) with torch.no_grad(): model.eval() outputs = model(image) outputs = torch.exp(outputs) topk, topclass = outputs.topk(3, dim=1) # argmax와 비슷하게 top-k에 대한 결과 값을 받는다. classes = [class_names[i] for i in topclass.cpu().numpy()[0]] scores = [round(i*100,2) for i in topk.cpu().numpy()[0]] print(classes, scores) return classes,scores이미지를 인자로 받아 이미지를 열고, 모델에 집어넣어 예측한다. 이 때, 1,2,3 순위를 보기 위해 topk를 사용했다. 예측한 클래스들과 점수들을 리턴한다.from flask_ngrok import run_with_ngrokfrom flask import Flask, jsonify, request, render_templatefrom flask import jsonifyapp = Flask(__name__)run_with_ngrok(app)flask를 위한 초기 세팅을 한다. 이 때, ngrok.yml 파일이 필요하다.@app.route(&#39;/&#39;)def hello(): return render_template(&#39;index.html&#39;)웹사이트를 접속했을 때 가장 먼저 나오는 화면에 대한 코드다. index.html은 다음과 같다. templates/index.html&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;html lang=&quot;en&quot;&amp;gt;&amp;lt;head&amp;gt; &amp;lt;meta charset = &quot;UTF-8&quot;&amp;gt; &amp;lt;title&amp;gt; main page &amp;lt;/title&amp;gt;&amp;lt;body&amp;gt; &amp;lt;!-- &amp;lt;h1&amp;gt;Site URL&amp;lt;/h1&amp;gt; &amp;lt;form action = &quot;/test&quot; method = &quot;POST&quot;&amp;gt; &amp;lt;a href=&quot;../test&quot;&amp;gt;GET test&amp;lt;/a&amp;gt; &amp;lt;button&amp;gt;POST test&amp;lt;/button&amp;gt;&amp;lt;br&amp;gt; &amp;lt;/form&amp;gt; &amp;lt;a href=&quot;../upload_image&quot;&amp;gt;upload_image&amp;lt;/a&amp;gt; &amp;lt;br&amp;gt; --&amp;gt; &amp;lt;form action = &quot;/upload_image&quot; method = &quot;POST&quot; enctype = &quot;multipart/form-data&quot;&amp;gt; &amp;lt;h2&amp;gt;이미지 업로드 하기&amp;lt;/h2&amp;gt; &amp;lt;div&amp;gt; &amp;lt;label for=&quot;file&quot;&amp;gt;Choose file to upload&amp;lt;/label&amp;gt; &amp;lt;input type=&quot;file&quot; name=&quot;uploaded_image&quot; id=&quot;uploaded_image&quot; accept=&quot;image/*&quot; required=&quot;True&quot; value=&quot;업로드&quot;&amp;gt; &amp;lt;br&amp;gt; &amp;lt;img src=&quot;no&quot; id=&quot;img_section&quot; style=&quot;width: 400px; height: 400px;&quot;&amp;gt; &amp;lt;script&amp;gt; const reader = new FileReader(); reader.onload = (readerEvent) =&amp;gt; { document.querySelector(&quot;#img_section&quot;).setAttribute(&quot;src&quot;, readerEvent.target.result); //파일을 읽는 이벤트가 발생하면 img_section의 src 속성을 readerEvent의 결과물로 대체함 }; document.querySelector(&quot;#uploaded_image&quot;).addEventListener(&quot;change&quot;, (changeEvent) =&amp;gt; { //upload_file 에 이벤트리스너를 장착 const imgFile = changeEvent.target.files[0]; reader.readAsDataURL(imgFile); //업로드한 이미지의 URL을 reader에 등록 }) &amp;lt;/script&amp;gt; &amp;lt;button&amp;gt;이미지 업로드&amp;lt;/button&amp;gt; &amp;lt;/div&amp;gt;&amp;lt;/form&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;/html&amp;gt;@app.route(&#39;/upload_image&#39;, methods=[&#39;POST&#39;,&#39;GET&#39;])def upload_image_file(): if request.method == &#39;POST&#39;: file = request.files[&#39;uploaded_image&#39;] if not file: return &quot;No Files&quot; image_bytes = file.read() up_image = Image.open(io.BytesIO(image_bytes)) up_image.save(&quot;./static/img.jpg&quot;,&quot;jpeg&quot;) # 분류 결과 확인 및 클라이언트에게 결과 반환 classes,scores = get_prediction(image_bytes=image_bytes) class_name = classes[0] &#39;&#39;&#39; 예측된 클래스에 대한 무작위 사진 가져오기 class_images = [] class_dir = &#39;./data/train/&#39; + class_name train_paths = sorted(glob(class_dir + &#39;/*&#39;),key= lambda x: x.split(&quot;\\\\&quot;)[-1].split(&#39;.&#39;)[0]) for train_path in train_paths: class_images.append(train_path) import random num = random.randint(0,10) img_path = class_images[num] pr_image = Image.open(img_path) pr_image.save(&quot;./static/&quot; + class_name + &quot;.jpg&quot;,&quot;jpeg&quot;) &#39;&#39;&#39; return render_template(&#39;upload.html&#39;, classes = classes, scores = scores, label = class_name, upload_img = &#39;img.jpg&#39;, predict_img = class_name + &#39;.jpg&#39;) else: return jsonify({&quot;Methods == &quot;:request.method})이미지가 입력되면 그 이미지를 받아 읽어서 모델에 집어넣는다. 그 후 출력된 클래스 상위 3개를 웹페이지로 전송시킨다. 이 때, label을 설정해준 이유는 예측이 되었는지를 확인하기 위한 장치로 설정했다. templates/upload.html&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt; upload page &amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;label for=&quot;label&quot;&amp;gt;예측 결과 : &amp;lt;/label&amp;gt; &amp;lt;br&amp;gt; &amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;# flask 실행if __name__ == &quot;__main__&quot;: app.debug = True app.run()Summary모델 자체를 커스터마이징 하거나, 웹페이지를 더 예쁘게 꾸몄다면 사람들에게도 테스트를 해보고자 했지만, 웹페이지를 꾸밀 시간도 없었고, 1달이라는 짧은 시간안에 제작을 목표로 하여 pretrained model을 사용했기에 혼자만의 프로젝트로만 남겨주고자 한다.다음번에는 객체 검출을 통해 바운딩 박스 표기도 하고 더 정확도 높고, robust한 모델을 사용하여 만들어보고자 한다.데이터도 너무 한계가 있었다. 구글링하여 이미지를 다운받았기 때문에 얼굴 사진이 아닌 그 사람이 썼던, 글귀나 많은 사람들과 찍은 사진 등등 이상한 사진이 많았다. 특히 공룡의 경우 사진이 너무 없어서 2~300개의 데이터만을 사용했다." }, { "title": "[깃허브 프로필 꾸미기] productive box 만들기", "url": "/posts/product/", "categories": "Review, Github", "tags": "Gihub, productive box", "date": "2022-02-12 13:00:00 +0900", "snippet": "깃허브 프로필을 꾸미기 위해 productive box를 만들어보고자 한다. 이는 내가 언제 커밋을 많이 하는지 알아볼 수 있다.1. GIST 생성https://gist.github.com/ gist를 생성한다. 저 내용들은 저대로 해야 하는 것인지는 잘 모르겠으나, 참고한 블로그의 내용대로 기입했다. create screts이 아닌 pubilc으로 해야 나중에 업로드 되는 결과를 볼 수 있다.2. GH_TOKEN 생성 expiration(기한)은 토큰이 파기되는 기한을 지정하는 것으로 기한없음(no expiration)을 체크한다. 토큰을 생성하고 나서 새로고치거나 다시 들어가면 토큰 주소를 볼 수 없다.3. 레포지토리 포크https://github.com/dkssud8150/productive-boxhttps://github.com/jogilsang/productive-boxhttps://github.com/maxam2017/productive-box 포크한 후, actions에 들어가서 go ahead and enable them 버튼을 클릭한다.그 다음 노란색 느낌표가 있는 곳을 들어가 enable workflow를 누른다.4. settings -&amp;gt; secrets -&amp;gt; actions 클릭 후, newrepository secret을 통해 환경변수 생성총 2개를 생성해야 한다. 아까 생성했던 gist에 주소를 복사하여 넣는다. 여기서 중요한 것은 전체 주소에서 맨 뒤에 부분 즉, username/뒤인 66cead553c5a43ac88609fbcd26fca4e을 복사해야 한다.그리고는 환경변수 내용으로 기입하여 생성한다. 생성했던 토큰을 복사한다.동일하게 기입하여 생성한다.5. fork한 곳에 workflows/schedule.yml 수정이유는 모르겠으나 uses에 나의 이름대신 maxam2017로 해야 작동이 되었다. 위와 같이 secrets 함수로 잘 되어 있는지 확인한다.6. 홈 화면에 pin설정을 통해 gist를 불러온다.여기서 pin을 설정하면 고정이 되어 홈화면에서아래와 같이 진행된 것을 볼 수 있다.생성한 gist가 보이지 않는다면 1번에서 public이 아닌 secret으로 생성한 것은 아닌지 확인해볼 필요가 있다.Reference https://blog.naver.com/jogilsang/222350143664" }, { "title": "[깃허브 프로필 꾸미기] github 프로필 만들기", "url": "/posts/gitpage/", "categories": "Review, Github", "tags": "Github, profile", "date": "2022-02-11 15:00:00 +0900", "snippet": "https://github.com/dkssud8150/dkssud8150레포지토리 생성레포지토리는 자신의 계정 이름과 동일한 이름으로 설정해야 한다. readme를 추가하여 생성하고 나면 홈 화면에 이 레포지토리가 나오는 것을 볼 수 있다.README 꾸미기Capsule render![header](https://capsule-render.vercel.app/api?type=waving&amp;amp;color=gradient&amp;amp;height=120&amp;amp;animation=fadeIn&amp;amp;section=footer&amp;amp;text=🚗🚘🚛&amp;amp;fontAlign=70)Capsule render라고 불리는 이것은 대체로 상단 메인으로 사용한다.아래 사이트를 참고하여 다양한 capsule render를 사용할 수 있다.https://github.com/kyechan99/capsule-render나는 wave를 사용하였다.GitHub-reamde-Stats&amp;lt;a href=&quot;s&quot;&amp;gt; &amp;lt;img src=&quot;https://github-readme-stats.vercel.app/api/top-langs/?username=dkssud8150&amp;amp;exclude_repo=dkssud8150.github.io&amp;amp;layout=compact&amp;amp;theme=tokyonight&quot; /&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;a href=&quot;s&quot;&amp;gt; &amp;lt;img src=&quot;https://github-readme-stats.vercel.app/api?username=dkssud8150&amp;amp;theme=tokyonight&amp;amp;show_icons=true&quot; width=&quot;42%&quot; /&amp;gt;&amp;lt;/a&amp;gt;이 또한, 많은 사람들이 사용하는 것으로 아래 사이트를 참고하여 작성하면 된다.https://github.com/anuraghazra/github-readme-statsgithub-stats-transparentgithub stats와 거의 유사하나 배경이 투명하여 다크모드를 사용하는데 적합하다. 하지만 github stats는 config를 수정하여 쉽게 수정할 수 있지만, 이는 fork를 통한 조금 복잡한 방법으로 사용해야 하며, 수정하는데도 html과 css를 조금 알고 접근해야 색상이나 크기 수정이 가능하다.&amp;lt;img src=&quot;https://raw.githubusercontent.com/dkssud8150/github-stats-transparent/output/generated/languages.svg&quot; width=&quot;49.2%&quot; /&amp;gt;https://github.com/rahul-jha98/github-stats-transparent위의 레포지토리의 readme를 천천히 참고하여 제작하면 된다.readme-typing-svg텍스트를 타이핑하는 형식의 SVG 이미지 생성기이다.https://github.com/DenverCoder1/readme-typing-svg[![Typing SVG](https://readme-typing-svg.herokuapp.com/?color=f0f6fc&amp;amp;lines=Hello+World🐯🤖&amp;amp;font=Redressed&amp;amp;size=40)](https://git.io/typing-svg)여기서 ?color=는 자신이 원하는 글자 색, lines=는 타이핑 내용, font=는 자신이 원하는 폰트인데 이는 구글 폰트에 포함되어 있는 어떤 것이든 상관없다. 폰트 종류들은 이 사이트를 참고하길 바란다. 그리고 size=는 자신이 원하는 글자의 크기이다.github-readme-activity-graph홈 화면의 잔디가 히트맵이라면, 이는 우리가 커밋한 숫자와 날짜를 이용한 그래프로 표현된다.https://github.com/Ashutosh00710/github-readme-activity-graph[![Ashutosh&#39;s github activity graph](https://activity-graph.herokuapp.com/graph?username=dkssud8150&amp;amp;theme=nord)](https://github.com/ashutosh00710/github-readme-activity-graph)github-profile-summary-cardsgithub readme stats와 비슷하나 좀 더 다양한 지표를 볼 수 있는 카드이다.https://github.com/vn7n24fzkq/github-profile-summary-cards![](https://github-profile-summary-cards.vercel.app/api/cards/profile-details?username=dkssud8150&amp;amp;theme=nord_dark)github-readme-streak-stats깃허브 총 contribution 수와, 연속으로 커밋한 날짜를 계산해주는 카드이다.https://github.com/DenverCoder1/github-readme-streak-stats[![GitHub Streak](https://github-readme-streak-stats.herokuapp.com/?user=dkssud8150&amp;amp;theme=tokyonight)](https://git.io/streak-stats)여기서 ?user= 부분만 자신의 계정으로 바꾸면 되고, 테마는 사이트를 참고하여 변경하면 된다.Trophy[![trophy](https://github-profile-trophy.vercel.app/?username=dkssud8150&amp;amp;theme=flat&amp;amp;column=7)](https://github.com/dkssud8150/)Hits[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fdkssud8150%2F&amp;amp;count_bg=%232AB4E5D6&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=views&amp;amp;edge_flat=false)](https://hits.seeyoufarm.com)Productive-boxproductive-box를 만드는 방법은 다음 블로그에 리뷰했다. 참고하기 바란다.waka-boxwaka-box를 만드는 방법은 다음 블로그에 리뷰했다. 참고하기 바란다.waka-box를 만들었다면, waka box에 대해 출력해주는 카드가 있다.이 사이트 하단에 보면 wakatime week stats 라는 목록이 있다. 이를 참고하여 username만 수정하여 입력하면 된다.[![willianrod&#39;s wakatime stats](https://github-readme-stats.vercel.app/api/wakatime?username=dkssud8150)](https://github.com/anuraghazra/github-readme-stats)Tech Icons많은 사람들이 img.shields.io를 사용하여 아래와 같은 아이콘을 많이 사용한다.사용하는 방법은 shields 사이트를 참고하고, 아이콘에 대한 사이트는 이 곳을 참고하면 된다.하지만, 나는 이런 색상과 모양이 마음에 들지 않았기 때문에, 다른 방법을 찾아보았다. 아래는 나의 프로필에 삽입된 아이콘들이다.먼저 https://cdn.icons.com/에 들어가서 검색창에 자신이 원하는 언어나 프레임워크를 검색한다. 그리고는 자신이 표시하고 싶은 이미지를 클릭한 다음 이미지를 우클릭하여 주소를 복사한다.그 다음 아래 코드에 src부분을 변경하고, 그에 맞게 텍스트도 변경한다. alt는 이미지가 출력되지 않을 경우 표기되는 부가 설명같은 것이고, height는 높이에 해당한다.&amp;lt;code&amp;gt;&amp;lt;img alt = &quot;3.1 Python&quot; height=&quot;20&quot; src=&quot;https://cdn.icon-icons.com/icons2/2699/PNG/512/pytorch_logo_icon_170820.png&quot;&amp;gt; pytorch&amp;lt;/code&amp;gt;OPGC rankingOPGG라는 롤 사이트를 참고하여 제작했다는 OPGC(Over Programmed Good Coding) 이라는 툴이다. 제작자 개발 블로그는 https://jay-ji.tistory.com/84 이다.깃허브 프로필에 적용하는 방법은 OPGC 사이트을 들어가서, 자신의 github 아이디를 입력하고 검색을 클릭한다.검색을 하고 나면, README.md 에 프로필 추가하기 버튼을 클릭하면 프로필에 붙여넣을 수 있는 코드가 복사된다.&amp;lt;a href=&quot;https://opgc.me/#/users/dkssud8150&quot; target=&quot;_blank&quot;&amp;gt;&amp;lt;img src=&quot;https://api.opgc.me/githubs/users/dkssud8150/tag/?theme=basic&quot; /&amp;gt;&amp;lt;/a&amp;gt;backjoon profile코딩 테스트 사이트 백준의 티어를 프로필에 보여줄 수 있다. 제작하는 방법은 이 곳 이나 이 곳을 참고하길 바란다.간단하게 아래 코드에 자신의 백준 아이디로 바꿔 넣어주기만 하면 된다.[![Solved.ac Profile](http://mazassumnida.wtf/api/v2/generate_badge?boj=백준아이디)](https://solved.ac/백준아이디/)github-profile-3d-contrib깃허브 커밋에 대한 잔디를 그래프로 사용하는 것은 위에서 살펴보았다. 이제는 3D로 잔디를 볼 수 있다. 이 깃허브가 공식 레포인지는 모르겠으나, 해당 깃허브 readme에서 테마 종류를 살펴볼 수 있다. 3d 잔디를 보기 위해서는 github actions을 작업해줘야 한다. 그에 대한 참고 사이트는 이 곳이다. 먼저 생성된 자신의 프로필 레포지토리에서 actions를 들어가 set up a workflow yourself를 클릭한다.그리고, main.yaml을 profile-3d.yml로 수정 후, 아래 코드를 그대로 가져온다.name: GitHub-Profile-3D-Contribon: schedule: # 03:00 JST == 18:00 UTC - cron: &quot;0 18 * * *&quot; workflow_dispatch:jobs: build: runs-on: ubuntu-latest name: generate-github-profile-3d-contrib steps: - uses: actions/checkout@v2 - uses: yoshi389111/github-profile-3d-contrib@0.6.0 env: GITHUB_TOKEN: $ USERNAME: $ - name: Commit &amp;amp; Push run: | git config user.name github-actions git config user.email github-actions@github.com git add -A . git commit -m &quot;generated&quot; git push여기에 run부분에 git config 에 대한 github-actions 을 본인의 계정으로 수정한다. schedule에 있는 시간은 workflow가 reset되는 시간으로 하루에 1번씩 reset된다. 이 때, token을 프로필 레포지토리에도 추가해주어야 한다. token을 제작하고 추가하는 방법은 해당 사이트를 참고하길 바란다. 또는 위의 참고 사이트를 살펴봐도 나온다.token을 프로필 레포지토리에 secrets token으로 추가해준 후, actions을 다시 들어가 제작한 Github-Profile-3D-Contrib을 들어가 run workflow를 클릭한다. 다 돌아가면 초록색 체크가 생겨나고, 아래 코드를 프로필에 추가해주면 3D 잔디를 볼 수 있다.![](./profile-3d-contrib/profile-night-rainbow.svg)Reference https://malangdidoo.tistory.com/34 https://hing9u.tistory.com/78" }, { "title": "[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud", "url": "/posts/pointrcnn/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, PointRCNN", "date": "2022-02-02 01:17:00 +0900", "snippet": "PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract본 논문에서는 포인트 클라우드로부터 3D 객체 검출에 대한 PointRCNN을 제안한다. 전체 프레임워크는 2단계로 구성되어 있는데, 1단계는 상향식 3D 제안 발생을 진행하고, 2단계에서는 표준 좌표안에서 제안을 정제하여 최종적인 객체 결과를 얻는다. RGB 이미지로부터 제안을 발생하거나 이전 방식대로 조감도나 복셀들로 포인트 클라우드를 투영하는 것 대신, 이 1단계 서브 네트워크는 전체 포인트 클라우드를 전경/배경으로 분할을 통해 상향식으로 포인트 클라우드에서 작은 수의 하이 퀄리티의 3D 제안을 직접 생성한다. 2단계 서브 네트워크는 각 제안의 풀링된 포인트를 표준 좌표로 변환하여 더 나은 로컬 공간적 특징을 학습하고, 이는 1단계에서 학습된 각 포인트의 전역 의미론적 특징과 결합되어 정확한 박스 세분화 및 신뢰도 예측을 제공한다. KITTI 데이터셋의 3D 객체 벤치마크에서의 실험은 제안된 아키텍쳐가 입력으로서 포인트 클라우드만을 사용하여 최첨단 방법들을 엄청난 차이를 보이면서 능가한다는 것을 보여준다. 코드는 이 주소를 참고하면 된다.1. Introduction딥러닝은 객체 검출과 인스턴스 세그멘테이션을 포함한 2D 컴퓨터 비전에서 엄청난 진보를 이루고 있다. 2D 장면을 이해하는 것 이상으로 3D 객체 탐지는 중요하고 로봇이나 자율주행과 같은 현실에 적용하기에 필요 불가결하다. 최근 개발된 2D 탐지 알고리즘은 이미지에서 큰 범위의 뷰 포인트 및 배경 클러터를 처리할 수 있지만, 포인트 클라우드가 있는 3D 개체를 감지하는 것은 여전히 3D 개체의 6DoF(Degrees-of-Freedom)의 불규칙한 데이터 형식과 대규모 검색 공간에서 큰 어려움에 직면해 있다.자율주행에서 보통 3D 센서로 장면들의 3D 구조를 포착하여 3D 포인트 클라우드를 발생시키는 LIDAR 센서를 사용한다. 포인트 클라우드 기반의 3D 객체 탐지의 어려움은 주로 포인트 클라우드의 불규칙성에 있다. 최첨단 3D 객체 탐지 방법들은 위의 그림a처럼 조감도 뷰나 전방뷰 또는 규칙적인 3D 복셀로 포인트 클라우드를 투영하는 좋은 2D 탐지 프레임워크를 활용하는데, 이는 최적이 아니며 정보 손실을 겪을 수 있다. 특징 학습을 위해 포인트 클라우드를 복셀이나 다른 규칙적인 데이터 구조로 변환하는 대신, 포인트 클라우드 분류와 분할에 대한 포인트 클라우드 데이터로부터 직접적으로 3D 표현을 학습하기 위해 PointNet을 제안한다.위의 그림b와 같이, 이 아키텍처는 pointNet을 적용하여 2D RGB 탐지 결과로부터 자른 frustum 포인트 클라우드를 기반으로 한 3D 바운딩 박스를 측정했다. 그러나 이 방법의 성능은 2D 탐지에 크게 의존하고, 강인한 바운딩 박스 제안을 생성하기 위한 3D 정보의 장점을 활용할 수 없다.2D 이미지로부터 객체 탐지와는 달리 자율주행에서 3D 객체는 주석이 달린 3D 바운딩 박스에 의해 자연스럽고 잘 분리된다. 즉, 3D 객체 탐지에 대한 훈련 데이터는 직접적으로 3D 객체 분할에 대한 의미론적 마스크를 제공한다. 이는 2D 탐지와 3D 탐지 훈련 데이터 사이의 중요한 차이가 된다. 2D 객체 탐지에서 바운딩 박스는 오직 시멘틱 분할에 대한 약한 지도(supervision)를 제공할 수 있다.이 관찰을 기반으로 정확하고 강인한 3D 탐지 성능을 달성하고 3D 포인트 클라우드에서 직접적으로 동작하는 PointRCNN이라는 새로운 2단계 3D 객체 탐지 프레임워크를 제시한다. 이에 대한 설명은 위의 그림c를 통해 볼 수 있다. 이 제시된 프레임워크는 2단계로 구성되어 있는데, 1단계는 상향식으로 3D 바운딩 박스 제안 생서을 목표로 한다. GT 분할 마스크를 생성하기 위한 3D 바운딩 박스를 활용하여 1단계는 배경 점을 분할하고, 분할된 점들로부터 소수의 바운딩 박스 제안을 생성한다. 이러한 전략은 전체 3D 공간 안에서 많은 3D 앵커 박스 사용을 피하고, 계산을 줄인다.두번쨰 단계에서는 표준 3D 박스 교정을 수행한다. 3D 제안이 발생된 후에, 포인트 클라우드 영역 풀링 작업을 적용하여 1단계로부터 학습된 점 표현을 풀링한다. 전역 박스 좌표를 직접적으로 측정하는 3D 방법들과 달리 풀링된 3D 점들은 표준 좌표로 변형되고, 상대 좌표 교정을 학습하기 위해 1단계의 분할 마스크뿐만 아니라 풀링 포인트 특징과 결합된다. 이 전략은 1단계의 분할과 제안으로부터 제공된 모든 정보를 활용한다. 더 효과적인 좌표 정제를 위해, 제안 생성 및 개선을 위한 전체 bin 기반의 3D 상자의 회귀 손실도 제안하며, 절제 실험 결과 다른 3D 상자의 회귀 손실보다 수렴 속도가 빠르고 회수율이 높은 것으로 나타났다.이 논문을 통해 3가지를 기여할 수 있다. (1)첫번째는 포인트 클라우드를 전경 객체 및 배경으로 분할하여 소수의 고품질 3D 제안을 생성하는 새로운 상향식 포인트 클라우드 기반의 3D 바운딩 박스 제안 알고리즘을 제안한다. 분할로부터 학습된 점 표현은 제안 발생에 좋을 뿐만 아니라 추후의 박스 정제에도 도움을 준다. (2)제안된 표준 3D 바운딩 좌표 정제는 1단계로부터 생성된 높은 리콜 박스 제안의 이점을 활용하고, 강력한 bin 기반의 손실로 표준 좌표의 박스 좌표 정제를 예측하는 방법을 학습한다. (3)제안된 3D 탐지 프레임워크인 pointRCNN은 엄청난 차이로 최첨단 기술들을 능가하고, 입력으로 포인트 클라우드만을 사용함으로써 KITTI 3D 탐지 테스트에서 공개된 모든 모델 중 1등을 차지했다.2. Related Work 3D object detection from 2D images이미지로부터 3D 바운딩 박스를 추정하는 방법들이 있다. 3D와 2D 바운딩 박스 사이의 공간적 제약 조건을 활용하여 3D 객체 자세를 복구하기도 하며, 이미 정의된 3D 박스들을 평가하기 위한 에너지 함수로서 객체의 3D 구조적 정보를 형성하기도 한다. 이러한 작업들은 깊이 정보가 부족하여 대략적인 3D 감지 결과만 생성할 수 있으며, 외관 변화에 상당한 영향을 받을 수 있다. 3D object detection from point clouds최첨단 3D 객체 탐지 방법들은 희박한 3D 포인트 클라우드로부터 뚜렷한 특징을 학습하기 위한 다양한 방법들을 제안했다. 포인트 클라우드를 조감도로 투영하고, 2D CNN을 활용하여 3D 박스 발생을 위한 포인트 클라우드 특징을 학습하기도 한다. 점들을 복셀로 그룹화하고, 3D CNN을 활용하여 3D 박스들을 발생시키기 위한 복셀의 특징들을 학습하기도 한다. 그러나 조감도 투영과 복셀화는 데이터 정량화하는 동안에 정보 손실을 겪고, 3D CNN은 메모리와 계산이 너무 많다. 그래서 정확한 2D 탐지기를 활용하여 이미지로부터 2D 제안을 생성하고, 각 잘린 이미지 영역안에서 3D 점의 크기를 줄이기도 했다. 그런 다음 PointNet은 3D 박스 추정에 대한 포인트 클라우드의 특징들을 학습하는데 사용되기도 했다. 그러나 2D 이미지 기반의 제안 생성은 3D 공간에서만 잘 관찰될 수 있는 일부 까다로운 사례에 대한 것은 잘 적용되지 않는다. 그러한 실패는 3D 박스 추정 단계로는 복구할 수 없었다.대조적으로 우리의 상향식 3D 제안 생성 방법은 포인트 클라우드에서 직접적으로 강력한 3D 제안을 생성하는데, 이는 효율적이다. Learning point cloud representations복셀이나 멀티뷰 포맷으로서 포인트 클라우드를 표현하는 대신, 포인트 클라우드 분류 및 분할의 속도와 정확도를 크게 높이는 포인트 클라우드에서 점들의 특징을 직접 학습할 수 있는 PointNet 아키텍처를 제시한다. 후속 작업에서는 포인트 클라우드의 로컬 구조를 고려하여 추출된 특징 품질을 더욱 개선한다.우리의 작업은 점 기반의 특징 추출기를 3D 포인트 클라우드 기반의 객체 탐지로 확장하여 원시 포인트 클라우드에서 3D 박스 제안과 감지 결과를 직접 생성하는 새로운 2단계 3D 탐지 프레임워크를 제시한다.3. PointRCNN for Point Cloud 3D Detection이번 섹션에서는 불규칙적인 포인트 클라우드로부터 3D 객체를 검출하기 위한 2단계 검출 프레임워크인 PointRCNN을 제안한다. 전체 구조는 아래 그림과 같다. 1단계는 상향식 제안 발생 단계와 표준 바운딩 박스 정제 단계로 구성되어 있다.3.1 Bottom-up 3D proposal generation via point cloud segmentation현존하는 2D 객체 탐지 방법들은 1단계와 2단계 방법들로 분류할 수 있는데, 1단계 방법들은 일반적으로 빠르지만 정제없이 객체 바운딩 박스를 직접 추정하며, 2단계 방법들은 제안을 먼저 발생시키고 나서 두번째 단계에서 제안과 신뢰도를 정제한다. 그러나, 2D에서 3D로의 2단계의 직접적인 확장은 포인트 클라우드의 불규칙한 포맷과 매우 큰 3D 탐색 공간때문에 필요하다. AVOD는 제안을 생성하기 위해 각 앵커마다 80~100K개의 앵커 박스를 3D 공간에 배치하고 멀티뷰에 각 앵커에 풀링을 배치한다. F-PointNet은 2D 이미지로부터 2D 제안들을 생성하고, 2D 영역으로부터 잘라낸 3D 점들을 기반으로 3D 상자를 추정한다. 이는 3D 공간에서만 명확하게 관찰할 수 있는 까다로운 객체를 놓칠 수 있다. 그래서 전체 포인트 클라우드 분할을 기반으로한 1단계 서브 네트워크로서 정확하고 강인한 3D 제안 발생 알고리즘을 제시한다. 그리고 각기 다른 중첩없이 자연스럽게 분리된 3D 장면들안에서 객체를 관찰한다. 모든 3D 객체 분할 마스크는 그들의 3D 바운딩 박스 주석을 통해 직접적으로 얻어진다. 즉, 3D 상자 내부의 3D 점이 전경 점들로 간주한다.그러므로 상향식에서 3D 제안을 생성할 것을 제안한다. 특히, 포인트별 특징을 학습하여 원시의 포인트 클라우드를 분할하는 동시에 분할된 전경 포인트들로부터 3D 제안을 생성한다. 상향식 전량을 기반으로 3D 공간에서 미리 정의된 큰 용량의 3D 박스들을 사용하지 않고, 3D 제안 발생을 위한 탐색 공간을 많이 제한한다. 실험을 통해 제안된 3D 박스 제안 방법이 3D 앵커 기반의 제안 발생 방법들보다 엄청 높은 리콜을 달성한다는 것을 볼 수 있다. Learning point cloud representations뚜렷한 포인트별 특징을 학습하여 원시의 포인트 클라우드를 설명하기 위해, 백본 네트워크로서 멀티 스케일 그룹화와 함께 PointNet++을 활용한다. 몇몇의 다른 포인트 클라우드 네트워크 구조에 대한 모델이나 희박한 컨볼루션을 하는 VoxelNet이 있는데, 이 또한 백본 네트워크로서 적용할 수 있다. Foreground point Segmentation전경 점들은 객체의 위치와 방향을 예측하는데 많은 정보를 제공한다. 전경 점들을 분할하는 법을 학습하고 나면, 포인트 클라우드 네트워크는 정확한 포인트별 예측을 만들기 위해 맥락적 정보를 포착해야만 하는데, 이는 3D 박스를 발생하는데 효과적이다. 상향식 3D 제안 발생 방법을 설계하여 전경 점들로부터 직접적으로 3D 박스 제안을 생성한다. 즉, 전경 분할과 3D 박스 제안 발생은 동시에 진행된다.포인트별 특징을 고려할 떄, 백본 포인트 클라우드 네트워크를 살짝 수정하여 사용하는데, 전경 마스크를 추정하기 위한 1개의 분할 헤드와 3D 제안을 발생시키기 위한 1개의 박스 회귀 헤드를 삽입한다. 점 분할동안 GT 분할 마스크는 3D GT 박스에 의해 자연스럽게 제공된다. 전경 점들의 갯수는 큰 크케일의 바깥 배경보다 훨씬 더 작다. 그래서 우리는 focal loss를 사용하여 불균형 문제를 다룬다. 식은 다음과 같다.포인트 클라우드 분할을 훈련하는 동안, 기본값으로 αt = 0.25, γ = 2 로 정한다. Bin-based 3D bounding box generation위에서 언급한 것과 같이, 박스 회귀 헤드는 전경 점 분할과 동시에 상향식 3D 제안을 생성하기 위해 추가된다. 훈련동안 오직 전경 점들로부터 3D 바운딩 박스 위치를 회귀하기 위한 박스 회귀 헤드만을 필요로 한다. 이 떄, 배경 점들에 대한 회귀는 하지 않지만, 포인트 클라우드 네트워크의 수용 필드때문에 이러한 점들은 박스 생성을 위한 추가적인 정보도 제공한다.3D 바운딩 박스는 LIDAR 좌표계에서 (x,y,z,h,w,l,θ)로서 표기된다. 이 떄, (x,y,z)는 객체의 중심 위치, (h,w,l)은 객체 크기, θ는 조감도에서의 객체 방향이다. 생성된 3D 박스 제안을 제한하기 위해 bin-based 회귀 손실을 제안하여 객체의 3D 바운딩 박스를 추정한다.객체의 중심 위치를 추정하기 위해 위 그림과 같이, 각 전경 점들의 주변 영역을 X축과 Z축을 따라 일련의 연속되지 않는 bin(점)으로 분할한다. 특히, 현재 전경 점의 각 X,Y축에 대해 탐색 범위인 S를 설정하고, 각 1D 탐색 범위는 X-Z평면에서 또 다른 객체 중심(X,Z)를 나타내기 위해 균일한 길이 δ의 bin으로 나뉜다. smooth L1 Loss로의 직접적인 회귀대신에 X,Z축에 대한 cross-entropy loss를 통한 bin 기반의 분류를 사용하게 되면 더 정확하고 강인한 중심 위치를 불러올 수 있다. X,Z축에 대한 위치화 loss는 두 개의 항으로 구성되는데, 1개는 각 X,Z축에 따른 bin 분류에 대한 항이고, 다른 하나는 분류된 bin 내의 남은 회귀 분석에 대한 항이다. 수직의 Y축에 따른 중심 좌표 y에 대해서는 대부분의 객체의 y값이 매우 작은 범위 안에 있기 때문에 smooth L1 loss를 통해 회귀한다. L1 loss를 사용하는 것은 정확한 y값을 얻기에 충분하다.그렇게 되면 타겟의 위치는 다음과 같이 형성된다. (x^(p),y^(p),z^(p)): 전경 관심 지점의 좌표 (x^p,y^p,z^p): 해당 객체의 중심 좌표 binx^(p), binz^(p): X,Z축을 따라 배치되는 GT bin resx^(p), resz^(p): 할당된 bin안에 추가적인 위치 정제를 위한 잔류 GT C: 정규화를 위한 bin 길이타겟의 방향 θ와 크기 (h,w,l) 추정을 위해 2π를 n개의 bin으로 나누고, x와 z의 예측과 같은 방법으로 bin 분류 타겟인 binθ^(p)와 잔류 회귀 타겟인 resθ^(p)를 계산한다. 객체 크기 (h,w,l)은 전체 훈련 셋에서 각 클래스의 평균 객체 크기에 대한 잔류(resh^(p),resw^(p),resl^(p))를 계산하여 직접적으로 회귀된다.추론 단계에서 bin 기반으로 예측된 파라미터 x,z,θ에 대해 먼저 예측된 신뢰도가 가장 높은 bin 중심을 선택하고, 예측된 잔류를 추가하여 정제된 파라미터를 얻는다. y,h,w,l과 같이 직접적으로 회귀된 파라미터들은 그들의 초기값에 예측된 잔류를 더한다.전체 3D 바운딩 박스 회귀 손실인 Lreg는 다음과 같이 공식화된다. Npos: 전경 점들의 갯수 binu^(p)_hat, resu^(p)_hat: 전경 점p의 예측된 bin의 수행과 잔류 binu^(p), resu^(p): 이전에 계산한 GT 타겟 Fcls: cross entropy 분류 loss Freg: smooth L1 loss중복된 제안을 제거하기 위해, 조감도로부터 IoU에 기반한 NMS를 사용하여 고품질 제안의 작은 개수를 발생시킨다. 0.85의 조감도 IoU threshold를 사용하고, NMS이후에 훈련동안 2단계 서브 네트워크 훈련하기 위해 탑 300 제안을 유지한다. 추론동안에는 0.8 IoU threshold를 통한 oriented NMS를 사용하고, 2단계 서브 네트워크의 정제을 위해 오직 상위 100개의 제안을 유지한다.3.2 Point Cloud region pooling3D 바운딩 박스 제안을 얻은 후에, 이전에 발생된 박스 제안들을 기반으로 한 박스의 위치와 방향을 정제하는 것을 목표로 한다. 각 제안의 더 구체적인 지역적 특징을 학습하기 위해, 각 3D 제안의 위치에 따라 1단계로부터 3D 점들과 해당 점 특징을 풀링한다.각 3D 박스 제안bi = (xi,yi,zi,hi,wi,li,θi에 대해 이것을 약간 확대하여 새로운 3D 박스bi^e = (xi,yi,zi,hi + η,wi + η, li + η, θi)를 생성해서 이 것의 문맥으로부터 추가 정보를 인코딩한다. 여기서 η는 상자 크기를 확대하기 위한 상수이다.각 점p = (x^(p),y^(p),z^(p))에 대해 점 p가 확대된 바운딩 박스인 bi^e 안에 있는지에 대한 여부를 결정하기 위해 내부/외부 테스트를 수행한다. 내부에 있다면 그 점과 해당 점의 특징들은 박스bi를 다듬기 위해 유지될 것이다. 내부 점 p와 연관되어 있는 특징에는 3D 점 좌표(x^(p),y^(p),z^(p)) ∈ R^3, 레이저 반사 강도r(p) ∈ R, 1단계에서의 예측된 분할 마스크m^(p) ∈ {0,1},그리고 1단계의 C차원의 학습된 점 특징 표현f^(p) ∈ R^C가 있다.또한, 확대된 박스,bi^e 내에서 예측된 전경/배경 점들을 구별하기 위해 분할 마스크,m^(p)를 포함한다. 학습된 점 특징,f^(p)는 세분화 및 제안 생성을 위한 학습을 통해 중요한 정보를 인코딩하므로 여기에도 포함된다. 그런 후, 다음 단계에서 내부 점이 아닌 제안을 제거한다.3.3 Canonical 3D bounding box refinement위의 그림b를 보면, 풀링된 점들과 각 제안에 대한 해당 특징들은 2단계 서브 네트워크에 공급되어 3D 박스 위치와 전경 객체의 신뢰도를 정제한다. Canonical transformation1단계의 높은 회수율 제안을 활용하고 제안의 박스의 파라미터들의 잔류만을 추청하기 위해, 각 제안에 속하는 풀링된 점들을 해당 3D 제안의 표준 좌표계로 변환한다.위의 그림과 같이 1개의 3D 제안에 대한 표준 좌표계는 몇 가지 특징을 가지는데, (1) 원점은 박스 제안의 중심에 위치된다. (2) 지역적 X’축 및 Z’축은 지면에 대략적으로 평행하며, X’축은 제안의 헤드 방향을 향하고 Z’축은 X’에 수직이다. (3) Y’축은 LIDAR 좌표계의 축과 동일하게 유지된다.모든 박스 제안의 풀링된 점들의 좌표p는 적절한 회전과 번역을 통해 p~로서 표준 좌표계로 변환되어야 한다. 제안된 표준 좌표계를 사용하면 박스 개선 단계가 각 제안에 대한 더 나은 지역적인 공간 특징을 학습할 수 있다. Feature learning for box proposal refinement3.2 섹션에서 언급한 것과 같이, 정제 서브 네트워크에서는 더 많은 박스와 신뢰도 정제를 위해 변환된 지역적인 공간 특징p~와 1단계의 전역적인 의미론적인 특징 f^(p)를 결합한다.표준으로의 변환은 강인한 지역적인 공간 특징 학습을 가능하게 해줌에도 불구하고, 불가피하게 각 물체의 깊이 정보를 잃게 된다. 예를 들어, LIDAR 센서의 고정된 각도 스캔 해상도 때문에, 스캔 멀리 있는 객체는 일반적으로 가까이 있는 객체보다 매우 작은 점들을 가진다. 손실된 깊이 정보를 보완하기 위해 센서까지의 거리d^(p) = ((x^(p))^2 + (y^(p))^2 + (z^(p))^2)^1/2를 점p의 특징에 포함시킨다.각 제안에 대해 해당 점들의 지역적인 공간 특징 p~과 추가적인 특징(r^(p),m^(p),d^(p))은 연결된 후에 몇몇의 FC layers에 공급되어 그들의 지역적 특징을 전역적 특징f^(p)의 동일한 차원으로 인코딩한다. 즉, 각 local 특징들을 global 특징의 차원으로 바꾸기 위해 지역적 공간 특징과 다른 추가적인 특징들을 연결한 후 FC layer에 공급된다. 그런 다음 지역적 특징과 전역적 특징은 연결되고, 네트워크로 공급되어 신뢰도 분류와 박스 정제를 위한 뚜렷한 특징 벡터를 얻는다. Losses for box proposal refinement제안 정제을 위해 bin 기반의 회귀 loss를 선택한다. 만약 3D IoU가 0.55보다 크다면 GT 박스가 3D 박스 제안에 할당되어 박스 정제를 학습한다. 3D 제안과 그들에 따른 GT 박스들은 표준 좌표계로 변환되는데, 이는 3D 제안bi = (xi,yi,zi,hi,wi,li,θi)와 3D GT 박스bi^gt = (xi^gt,yi^gt,zi^gt,hi^gt,wi^gt,li^gt,θi^gt)가 다음과 같이 bi~와 bi^gt~로 변환된다는 것이다.i번째 박스 제안의 중심좌표에 대한 훈련 타겟인 (bin∆x^i,bin∆z^i,res∆x^i,res∆z^i,res∆y^i)은 3D 제안의 위치를 정제하기 위한 더 작은 검색 범위 S를 사용하는 것을 제외하고는 위의 eq.(2)과 같은 방식으로 설정된다. 풀링된 희박한 점들은 보통 제안 크기(hi,wi,li)에 대한 충분한 정보를 주지 못하기 때문에, 크기 잔류(res∆h^i, res∆w^i, res∆l^i)를 훈련 셋에서 각 클래스의 평균 객체 사이즈에 관해 직접적으로 회귀한다.방향을 정제하기 위해, 제안과 GT 박스 사이의 3D IOU가 최소 0.55이상이라는 사실에 기초하여 GT 방향과의 각도 차이θi^gt - θi가 [-π/4,π/4] 범위 내에 있다고 가정한다. 그러므로, bin 크기 w와 함께 π/2를 개별 bin으로 나누고, 다음과 같이 bin 기반의 방향 타겟을 예측한다.그래서, 2단계 서브 네트워크에 대한 전체적인 loss는 다음과 같이 형성된다. β: 1단계에서의 3D 제안의 셋 βpos: 회귀에 대해 긍정적 제안을 저장 prob^i: bi~의 추정된 신뢰도 label^i: 레이블 Fcls: 예측된 신뢰도를 평가하기 위한 cross entropy loss Lbin^(i)~, Lres^(i)~: bi~와 bi^gt에 의해 계산되는 새로운 타겟, Eq.(3)에서의 Lbin^(p), Lres^(p)와 유사한 역할이를 통해 도출된 결과를 0.01 조감도 IOU threshold의 oriented NMS를 적용하여 중첩된 바운딩 박스를 제거하고, 탐지된 객체에 대한 3D 바운딩 박스를 생성한다.4. ExperimentsKITTI 데이터셋의 3D 객체 탐지 벤치마크에서 PointRCNN을 평가했다. Sec 4.1에서 PointRCNN의 상세한 구현을 생성한다. Sec 4.2에서 최첨단 3D 감지 방법들과 비교한다. 마지막으로, Sec 4.3에서 PointRCNN을 분석하기 위해 광범위한 절제 연구를 수행한다.4.1 Implementation Details Network Architecture훈련 셋안에서 각 3D 포인트 클라우드 장면에 대해, 입력으로서 각 장면마다 16384개의 점을 서브 샘플링한다. 16384개보다 적은 장면들에 대해서는 무작위로 점을 반복시켜 16384개 점을 얻는다. 1단계 서브 네트워크에 대해, 멀티 스케일로 그룹화하는 각 4개의 레이어를 사용하여 각각 4096, 1024, 256, 64의 크기의 그룹으로 점을 하위 샘플링한다. 그러고 나서 4개의 특징 전파 레이어들은 각 점들의 분할과 제안 발생을 위한 특징 벡터를 얻는데 사용된다. 박스 제안 정제 서브 네트워크동안, 우리는 정제 서브 네트워크의 입력으로서 각 제안의 풀링된 지역에서 512개의 점을 무작위로 샘플링한다. 단일 스케일로 그룹화(128,32,1의 그룹 사이즈)하는 각 3개의 레이어는 객체 신뢰도 분류와 제안 위치 정제를 위한 단일 특징 벡터를 생성하는데 사용된다. The training schemeKITTI 데이터셋에 대부분의 샘플이 있기 떄문에 자동차에 대한 훈련 세부 정보를 보고하고, 보행자와 자전거 이용자에 대한 하이퍼 파라미터는 릴리즈된 코드에서 찾을 수 있다.1단계 서브 네트워크에서 3D GT 박스안에 있는 모든 점들은 전경 점들로 구성되고, 다른 점들은 배경으로 간주된다. 3D GT 박스가 작은 편차를 가질 수 있기 떄문에 강력한 분할을 위해 3D GT 박스를 물체의 각 면을 0.2m씩 증가시켜 물체 경계 근처의 배경 점을 무시한다. bin 기반의 제안 발생에 대한 하이퍼파라미터를 탐색 범위 S = 3m, bin 크기 δ = 0.5m, 방향 bin 개수 n = 12로 설정한다.2단계 서브 네트워크를 훈련시키기 위해, 3D 제안을 무작위로 작은 변형을 통해 데이터를 증가시켜 제안의 다양성을 높였다. 또한, 박스 분류 헤드를 훈련시키기 위해 제안이 GT 박스와의 3D IOU가 0.6보다 크다면 positive, 0.45보다 작으면 negative로 간주한다. 그리고 박스 회귀 헤드의 훈련을 위한 제안의 촤소 threshold로서 0.55 3D IOU를 사용한다. bin 기반의 제안 정제에서는 탐색 범위는 S = 1.5m, 위치화의 bin 크기는 δ = 0.5m, 방향의 bin 크기는 ω = 10◦로 설정한다. 포인트 클라우드 풀링의 맥락적 길이는 η = 1.0m이다.PointRCNN의 두 단계의 서브 네트워크는 개별적으로 훈련된다. 1단계 서브 네트워크는 배치사이즈 = 16, lr = 0.002에서 200epochs동안 훈련되지만, 2단계 서브 네트워크는 배치사이즈 = 256, lr = 0.002에서 50epochs동안 훈련된다. 훈련동안 데이터 증강으로서 [0.95,1.05]의 범위에 대한 random flip, [-10◦,10◦] 범위에서 수직 Y축을 중심으로 rotation을 사용했다. 다양한 환경에서 객체를 형성하기 위해 무작위로 겹치지 않는 박스들을 선택하여 다른 장면의 여러 새로운 GT 박스들과 그들 안에 있는 점들을 현재 훈련 장면에 배치한다. 이 증강을 다음 세션에서 GT-AUG로 정의한다.4.2 3D Object Detection on KITTIKITTI의 3D 객체 탐지 벤치마크는 7481개의 훈련 샘플과 7518개의 테스트 샘플로 구성되어 있다. 이 훈련 샘플을 3712개의 훈련 샘플과 3769개의 검증 샘플로 분할한다.KITTI 데이터셋에서 검증 셋과 테스트 셋에서 최첨단 3D 객체 탐지 방법들과 PointRCNN을 비교한다. Evaluation of 3D object detection3D 탐지 벤치마크의 KITTI 테스트 서버에서 방법을 평가했다. 그 결과는 아래 테이블에서 볼 수 있다.차와 자전거 이용자의 3D 탐지에서 이 논문의 방법은 이전의 최첨단 방법들을 모든 3가지 난이도에서 엄청난 차이로 능가하고, 제출 당시에 출시된 모든 방법들 중에서 KITTI 테스트 보드에서 1등을 했다. 대부분의 이전 방법들은 입력으로 포인트 클라우드와 RGB 이미지를 둘다 사용함에도 불구하고, 이 논문에서의 방법이 입력으로 오직 포인트 클라우드만을 사용하면서 더 효과적인 성취를 했다. 보행자 탐지에서 LIDAR만을 사용하는 방법들과 비교했을 때, 이 논문에서의 방법이 더 좋거나 비슷한 결과를 달성했다. 하지만, 다중 센서를 사용하는 방법보다는 약간 안좋은 성능을 보였다. 이는 입력으로 오직 희박한 포인트 클라우드만을 사용했기 때문이라 생각한다. 하지만 보행자는 매우 작은 크기를 가지고, 이미지는 포인트 클라우드보다 3D 탐지에 사용할 더 상세한 정보를 포착할 수 있기도 하다. 가장 중요한 차량 카테고리의 경우 검증 셋에서의 3D 탐지 결과의 성능을 보고했고, 이는 아래 테이블에서 볼 수 있다.이를 통해 검증셋에서 이전의 최첨단 방법들을 큰 차이로 능가했다. 특히 어려움 난이도에서 이전의 최고 AP보다 8.28% AP더 높게 받았고, 이는 PointRCNN이 효과적이라는 것을 보여준다. Evaluation of 3D proposal generation상향식 제안 발생 네트워크의 성능은 제안의 갯수와 3D IOU threshold를 통한 3D 바운딩 박스의 리콜을 계산을 통해 평가된다.위의 테이블에서 볼 수 있듯이, GT-AUG없이도 이전의 방법들보다 엄청 더 높은 리콜을 달성했다. 50개의 제안만을 사용했을 때, 차량 분류에 대해 중간 난이도에서 0.5 IOU threshold에서 96.01% 리콜을 얻었는데 반해 AVOD는 91% 리콜로 약 5.01%이 더 낮지만, 중요한 것은 이 방법은 입력으로 포인트 클라우드만을 사용하는 PointRCNN과 달리 입력으로 2D 이미지와 포인트 클라우드를 둘 다 사용한다.300개의 제안을 사용했을 때, 0.5 IoU threshold에서 98.21% 리콜을 달성했다. 여기서 이미 최대 리콜을 달성했기 때문에 제안 개수를 더 증가시키는 것은 의미가 없다.대조적으로 위의 테이블을 보면, 추론에서는 0.7 IOU threshold를 사용하여 3D 바운딩 박스의 리콜을 작성했다. 300개의 제안을 사용할 때, 0.7 IOU threshold에서 82.29% 리콜을 달성했다. 이는 다소 낮은 수치일 수 있으나 아직 강인하고 정확한 상향식 제안 발생 네트워크라고 볼 수 있다.4.3 Ablation Study이번 세션에서는 pointRCNN의 또 다른 구성들의 효과를 분석하기 위해 확장된 절제 실험을 한다. 모든 실험은 GT-AUG 없이 차량 클래스에 대해 훈련 셋에서 훈련시키고, 검증 셋에서 평가했다. Different inputs for the refinement sub-network세션 3.3에서 언급했듯이, 정제 서브 네트워크의 입력은 표죽적으로 변형된 좌표와 각 풀링된 점들의 풀링된 특징으로 구성된다.1개씩 제거함으로서 정제 서브 네트워크에서 각 특징들의 효과를 분석한다. 모든 실험은 공정한 비교를 위해 같은 1단계 서브 네트워크를 사용한다. 이 결과는 다음과 같다.제안된 표준적 변형없이, 정제 서브네트워크의 성능은 상당히 떨어지는데, 이는 표준 좌표계로의 변환은 많은 회전과 위치 변동성을 많이 제거하고, 2단계에 대한 특징 학습의 효율성을 향상시킨다. 분할과 제안 발생으로부터 학습되는 1단계 특징인 f^(p)를 제거하는 것이 중간 난이도에서는 2.71% mAP정도를 감소시키는데, 이는 1단계에서 의미론적 분할에 대한 학습의 이점을 보여준다.또한, 위의 테이블에서, 3D 점인 p에 대한 카메라 깊이 정보인 d^(p)와 분할 마스크인 m^(p)가 표준 변환 중에 제거된 거리 정보를 나타내고, 분할 마스크가 풀링된 영역의 전경 지점을 나타내기 떄문에 최종 성능에 약간 기여안다는 것을 볼 수 있다. Context-aware point cloud pooling세션 3.2에서 각 제안의 신뢰도 추정과 위치 회귀를 위한 더 맥락적 점들을 풀링하기 위해 bi를 마진 η를 통해 bi^e로 확장하는 것을 소개했다.위의 테이블에서 서로 다른 풀링된 맥락적 넓이인 η의 영향을 볼 수 있다. η = 1.0m일 때, 제안된 프레임워크가 최고 성능을 보였다. 맥락적 정보가 없이 풀링되면 특히 어려움 난이도에서, 정확도가 상당히 떨어진다. 어려움 난이도는 객체가 폐색되거나 센서로부터 멀리 떨어져있어 제안 정제와 분류를 위한 더 많은 맥락적 정보가 필요하기 떄문에, 일반적으로 제안안에 점이 별로 없다. 위의 테이블을 보면, 매우 큰 η는 현재 제안의 풀링된 지역이 다른 객체들의 전경 점들을 포함하기 때문에 성능을 떨어뜨린다. Losses of 3D bounding box regression세션 3.1에서 3D 박스 제안을 생성하기 위한 bin 기반의 위치화 loss를 제시했다. 이번 파트에서는 각기 다른 타입의 3D 박스 회귀 loss를 사용했을 때의 성능을 평가하고자 한다. loss의 종류는 RB loss(residual-based loss), RCB loss(residual-cos-based loss), CN loss(corner loss), PBB loss(partial-bin-based loss), BB loss(full bin-based loss)이 있다. 여기서 RCB loss는 각도 회귀의 모호성을 제거하기 위해 (cos(∆θ),sin(∆θ))에 의한 residual based loss의 ∆θ를 인코딩한다.1단계에서 100개의 제안에 대한 마지막 리콜(0.5,0.7 IOU)는 평가 방식으로서 사용되고, 이 결과는 아래 그림에서 볼 수 있다.그래프는 전체 bin 기반의 3D 바운딩 박스 회귀 loss의 영향을 보여준다. 특히, BB loss를 사용한 1단계 서브 네트워크는 더 높은 리콜을 달성하고, 다른 loss보다 더 빠르게 수렴하는데, 사전 지식으로 타겟 즉 위치화를 제한함으로서 효과를 얻는다. 부분 bin 기반의 loss는 비슷한 리콜을 달성하지만, 수렴 속도가 다소 느리다. 전체와 부분 bin 기반의 loss는 0.7 IOU threshold에서 다른 함수들보다 더 높은 리콜을 가진다. RCB loss는 각도 회귀 타겟을 개선함으로서 RB loss보다 더 높은 리콜을 얻는다.5. Conclusion원시 포인트 클라우드로부터 3D 객체를 탐지하기 위해 새로운 3D 객체 탐지기인 PointRCNN을 제시했다. 제시된 1단계 서브 네트워크는 상향식으로 포인트 클라우드로부터 3D 제안을 직접 생성하는데, 이 제안 발생방법이 이전의 방법들보다 상당히 더 높은 리콜을 가진다. 2단계 서브 네트워크에서는 지역적인 공간 특징과 의미론적 특징을 결합하여 그 제안을 표준좌표로 정제한다. 게다가 새로 제안된 bin 기반의 loss는 3D 바운딩 박스 회귀에 대한 효과를 볼 수 있었다. 실험을 통해 KITTI 데이터셋의 3D 탐지 벤치마크에서 PointRCNN이 이전의 최첨단 방법들을 큰 차이로 능가했다.Reference PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud" }, { "title": "[논문 리뷰] Monocular 3D Object Detection for Autonomous Driving", "url": "/posts/monocular/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, monocular", "date": "2022-01-25 23:53:00 +0900", "snippet": "Monocular 3D Object Detection for Autonomous Driving 논문에 대한 리뷰입니다. 이 논문은 2016 CVPR에 투고된 논문이며 약 669회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract본 논문은 자율주행에서 단안 이미지로부터 3D 객체 검출을 수행하는 것이 목표이다. 먼저 높은 퀄리티의 객체 검출을 얻기 위한 기본적인 CNN 파이프라인을 통해 돌아가는 클래스 객체 제안 후보 셋을 발생시키는 것을 목표로 한다. 이 논문의 중점은 제안 발생이다. 특히, 물체는 지면 위에 있어야만 한다는 사실을 이용하여 객체 후보를 3D로 배치하는 에너지 최소화 접근법을 제시한다. 그러고 나서 의미 분할, 문맥 정보, 이전의 크기 및 위치, 일반적인 객체 모양을 인코딩하는 몇가지 직관적인 잠재력을 통해 이미지 평면으로 투영된 각 후보 박스에 대해 점수를 매긴다. 우리의 실험 결과는 우리의 객체 제안 발생 접근법이 모든 단안 접근법을 엄청나게 능가하고, 단안 경쟁 모델 중 까다로운 KITTI benchmark 에서 최고의 검출 성능을 달성한다는 것을 볼 수 있다.1. Introduction최근에, 자율주행은 요즘 많이 주목받고 있다. 초기에는 벨로다인, 환경을 손으로 그린 맵과 같이 비싼 LIDAR 시스템에 의존한다. 그러나, 최근에는 LIDAR를 대부분 현대의 차들에 손쉽게 사용가능한 값싼 카메라로 바꾸려고 하고 있다.또한 이 논문의 중점은 자율주행에서 단안 이미지로부터 높은 성능의 2D 와 3D 객체 검출이다. 대부분의 최근 객체 검출 파이프라인들은 전형적으로 상대적으로 빠른 계산과 높은 리콜을 가진 객체 제안의 다양한 셋을 발생시킨다. 이렇게 함으로서, CNN시리즈와 같이 계산이 강도 높은 분류기는 대량의 필요 없는 후보 세트에 대한 계산을 피할 수 있다.다른 타입의 객체 제안 방법들 중, 일반적인 접근 방식은 이미지를 매우 작은 픽셀로 세분화하고, 몇 가지 유사성 측정을 사용하여 이들을 그룹화한다. 단순한 객체 특징 또는 외곽선 정보를 사용하여 전체 창들을 효율적으로 탐색하는 접근법도 제안되었다. 대부분의 최근 작업들은 이진 세분화 모델, 파라메트릭 에너지, CNN 특징 기반의 윈도우 분류기 중 하나를 사용하여 좋은 객체 후보를 제안하는 방법을 만드는 것을 목표로 한다.이 제안 발생 접근 방식들은 다소 루즈한, 즉 검출이 GT와 0.5 IOU가 생기는 경우를 긍정으로 보는 경우로 하는 위치화 개념을 필요로 하는 PASCAL VOC 챌린지에 매우 효과적인 것으로 나타났다. 그러나 자율 주행에서는 자동차 스스로 잠재적 장애물까지의 거리를 정확하게 추정하기 위한 더 많은 엄격한 중복을 요구한다. 결과적으로, R-CNN과 같은 인기있는 KITTI와 같은 자율주행 벤치마크에서의 다른 경쟁자들보다 엄청나게 떨어진다. 그러나, 대부분의 차들은 단일 카메라가 장착되어 있고, 그래서 단안 객체 검출은 중요한 기술이다.이러한 접근 방식에 영감을 받아, 이 논문은 의미론뿐만 아니라 상황별 모델을 활용하여 매우 높은 회수율로 클래스별 3D 객체 제안을 생성하는 방법을 제안한다. 이 제안들은 지면에 3D 경계 상자를 철저히 배치하고 단순하고 효율적으로 계산 가능한 이미지 기능을 통해 점수를 매김으로써 생성된다. 특히, semantic 과 instance 세그멘테이션, 컨텍트 뿐만 아니라 형상 특징들과, 박스들을 점수 매기기 위한 이전 위치 정보을 사용한다. 저자는 각각의 개별 객체 클래스에 적응되어 있는 S-SVM을 사용한 피쳐들에 대한 각 클래스마다의 가중치들을 학습한다. 그러고 나서 가장 높은 객체 후보들은 CNN을 통해 점수 매겨지고, 검출의 최종 셋의 결과로 추출된다. 이 논문의 실험은 우리의 접근 방식이 KITTI에서 모든 공개된 단안 객체 검출을 능가하고 스테레오 이미지를 이용하는 선두모델과 거의 동등하게 수행하고 있음을 보여준다.2. Related Work본 논문의 작업은 객체 제안 발생 뿐만 아니라 단안 3D 객체 검출에 대한 방법들과 관련이 있다. 자율주행에서 문학적 리뷰를 주로 집중할 것이다. 심층 신경망에서의 중요한 프로세스는 슬라이딩 윈도우 창을 계산적으로 만드는 심층망때문에 객체 제안 발생에 대한 방법에 사용된다. 제안 발생에 대한 대부분의 현존하는 작업은 RGB, RGB-D 또는 영상을 사용한다. RGB에서 대부분의 방법들은 색깔과 텍스쳐를 사용하는 것과 같이 여러 유사성 함수를 통해 슈퍼픽셀들과 큰 지역과 결합한다. 이러한 접근 방식은 PASCAL VOC에서 거의 완벽한 호출을 달성하면서 이미지당 약 2K개의 제안으로 전체 윈도우를 전환한다. BING 제안들은 객체성에 대한 프록시로서 객체 측정에 기반된 박스들을 점수매긴다. Edgeboxes는 각 창의 경계와 내부 윤곽 정보를 기반으로 전체 위도우 셋에 점수를 매긴다. 우리와 가장 관련있는 접근법들은 객체를 제안하는 방법을 배우는 것을 목표로 한다.우리의 접근 방식은 후보들을 점수매기기 위해 통합 이미지를 활용하기도 하지만, 이전의 정보들을 사용하여 3D 경계 상자를 배치하고 의미론적 특징으로 점수매긴다. 저자는 grid CNN의 출력 계층에서 픽셀 단위의 클래스 점수와 상황별 및 모양 특징을 사용한다. 또한, 저자는 3D bounding box를 평가하지만, semantic 과 instance 세그멘테이션과 3D priors를 사용하여 지면에 제안들을 배치한다. 이 논문의 RGB 전위는 부분적으로 2D 객체 검출을 위해 효율적으로 계산된 세그멘테이션 전위를 활용하는 데 영감을 받았다.3. Monocular 3D Ojbect Detection본 논문에서 정확한 3D 객체 검출을 수행하기 위해 세그멘테이션(segmentation), 컨텍트(context), 이전 위치(location priors)들을 추출하는 객체 검출에 대한 접근법을 제시한다. 특히 먼저 지면 가까이에 있는 물체를 제안하기 위해 지면을 사용한다. 단안 이미지를 입력으로 하기 때문에, 지상 평면은 이미지 평면과 직각을 이루고, 카메라에서 멀리 떨어져 있다고 가정한다. 그 값은 보정을 통해 알 수 있다고 가정한다. 이 지면은 각각의 이미지에서 완벽한 현실을 반영하지 않을 수 있으므로, 물체를 지면에 놓지 않고 가까이 있도록 한다. 그러고 나서 클래스 세그멘테이션, 인스턴스 레벨 세그멘테이션, 형상이나 문맥적 특징과 이전 위치들을 활용하여 이미지 평면에서 3D 객체 후보들을 점수매긴다. 그러고 나서 결과로 나온 3D 후보들은 점수를 기준으로 정렬되고, NMS를 통해 추출된 가장 높은 확률의 것들은 CNN을 통해 또다시 점수매겨진다. 이 과정을 통해 3D 검출에 대해 빠르고 정확하게 된다.3.1 Generating 3D Object Proposals3D bounding box를 다음과 같이 정의한다.y = (x,y,z,0,c,t), (x,y,z): 3D box의 중심 좌표, 0: 방위각, c: object class, t: 훈련 데이터로부터 학습된 대표 3D 템플릿 셋t를 사용하여 bounding box의 사이즈를 정의한다. 각 클래스마다 3개의 템플릿과 두 개의 각도(0∈{0,90}) 를 사용한다. 그 후 semantic and instance segmentation과 location prior, context를 결합하여 정의된 스코어 함수는 다음과 같다.이 함수를 좀 더 디테일하게 들여다 보자. Semantic segmentation맨 앞부분부터 보자면, 이 부분은 픽셀 단위의 의미 분할(semantic segmentation)을 입력으로 받는다. 그 후 두 가지의 특징을 통합하여 의미 분할을 수행한다. 첫번째 특징은 관련 클래스로 분류된 픽셀들의 확률을 활용해서 객체의 bounding box를 구성한다. 사용하는 식은 다음과 같다.이 때, Ω(y)는 이미지 평면으로 3D 박스인 y를 투영하여 생성된 2D 박스의 픽셀 집합이고, Sc는 클래스c의 세그멘테이션 마스크이다.두번째 특징은 픽셀들이 해당 클래스 대비 다른 클래스로 속할 확률을 계산한다.이는 크게 두 가지로 구성되어 있는데, 1개는 로드를 의미하고, 다른 1개는 다른 모든 클래스들을 합친 것을 의미한다. 이 식을 통해 bounding box안에 다른 객체를 뜻하는 픽셀의 비율을 최소화한다.이 특징들은 클래스 수만큼의 통합 이미지만을 사용하여 매우 효율적으로 계산할 수 있다.SegNet의 경우, fully convolutional 인코더-디코더를 통해 semantic 라벨링을 수행한다. 저자는 PASCAL VOC + COCO에서 차량 세그멘테이션에 대해 pretrained model을 사용한다. 그리고, 다른 클래스의 불일치를 줄이기 위해, 보행자와 자전거 이용자에 대한 pretrained SegNet model을 사용한다. KITTI에는 semantic 주석(annotation)이 거의 없으므로 모델을 fine-tune을 하지 않았다. 게다가 KITTI의 로드 benchmark에서 주석을 추출했고, 로드에 대해 네트워크를 fine-tune했다. Shape여기서는 객체의 형상을 포착한다. 특히, 먼저 원래의 이미지가 아닌 세그멘테이션의 출력에서 윤곽을 계산한다. 그런 다음 2D 후보 상자에 대해 두 개의 그리드를 만든다. 하나는 단일 셀만 포함하고, 다른 하나는 K x K크기의 셀을 나타낸다. 각 셀에 대해 그 안에서 윤곽 픽셀의 수를 센다. 무엇보다, 이는 우리에게 모든 셀을 거쳐 (1+KxK)개의 특징 벡터를 준다. 이는 객체 주변에 타이트하게 bounding box를 두어 그리드 내에 윤곽의 공간적 분포가 특정 클래스의 예측한 형상과 일치하도록 유도한다. 이러한 특징은 윤곽 픽셀을 세어 매우 효율적으로 계산된다. Instance Segmentation박스 내부와 외부의 세그멘트 양을 기록하는 인스턴스 레벨 세그멘테이션 특징을 활용한다. 그러나 간단하게 IOU 중첩을 기반으로 각 bounding box에 대해 최고의 세그먼트를 고른다. 이는 계산 속도를 올린다. 이 방식은 다른 인스턴스를 형성할 때 가려지는 개체를 검출하는 것을 돕는다. 이러한 기능은 세그멘테이션을 구성하는 인스턴스 수만큼의 통합 이미지를 사용하여 매우 효율적으로 계산할 수 있다. CNN을 사용하여 인스턴스 레벨의 픽셀 라벨링과 깊이에 대한 정렬을 모두 만드는 접근 방식을 이용하여 인스턴스 세그멘테이션을 계산한다. 이 방법은 자동차에 대해서만 적용 가능하다. Context이 단락에서는 자동차는 도로 위에 있고 그래서 차 밑에 길을 볼 수 있는 것과 같이 맥락적 라벨을 인코딩한다. 3D bounding box의 2D 투영에 맥락적 영역으로서 직사각형을 그린다. 직사각형의 크기는 박스의 높이의 1/3, 같은 너비로 설정한다. 그런 다음 맥락적 영역에서 semantic segmentation 특징들을 계산한다. Location이 단락에서는 이미지 평면과 조감도에서 객체의 이전 위치들을 인코딩한다. 본 논문에서는 3D prior에 대해 4m의 고정된 표준 편차를 갖는 커널 밀도 추정(Kernel Density Estimation)(=KDE)을 사용하여 prior을 학습하고, 이미지에 대해 2개의 픽셀을 학습한다. 3D GT bounding box를 사용하여 3D prior를 학습시킨다.3.2 3D Proposal Learning and Inference모든 특징들을 통합 이미지로 변환할 수 있기 때문에 효율적으로 추론이 가능하다. 특히, 단일 코어에서 추론에 걸리는 시간은 1.8s로, real-time이라 가정할 수 있다. SVM을 통해 모델의 가중치를 학습하고, 평행 절단면 구현을 사용한다. 또한, task loss로서 3D IOU를 사용한다.3.3 CNN Scoring of Top Proposals이번 섹션에서는 CNN을 통해 NMS를 거친 TOP후보들이 어떻게 더 많은 점수를 받는지 설명한다. 저자는 Fast R-CNN을 기반으로 네트워크를 구성했다. 이것은 전체 이미지로부터 컨볼루션 특징들을 계산하고, 마지막 컨볼루션 레이어인 conv5를 지나고 나면 그것은 2개의 브런치로 나뉘어진다. 1개는 제안 영역으로부터 특징들을 인코딩하고, 다른 하나는 제안 영역을 1.5배로 확대하여 얻은 컨텍스트 영역을 인코딩한다. 두 브런치는 ROI pooling layer와 2개의 FC layer로 구성되어 있다. 제안이나 컨텍스트 영역을 conv5 특징맵에 투영하여 ROI를 얻는다. 그러면 두 브런치를 출력 특징들에 연결하여 최종적인 특징 벡터를 얻을 수 있다.이 논문에서는 multi-task loss를 사용하여 카테고리, bounding box offset, 객체 방향을 예측한다. 배경에 대한 박스의 loss는 범주 레이블 손실에만 사용된다. 각 loss는 동일한 가중치를 부여하고, smooth L1인 방향 loss와 2D bounding box의 4개 좌표에 대한 smooth L1인 boundinb box offset loss를 합친 cross entropy로 카테고리 loss를 정의한다.3.4 Implementation Details Sampling Strategy저자는 복셀 크기가 각 길이가 0.2m가 되도록 3D 공간을 구분한다. 제안 발생 모델에서 추론시에 조사 공간을 줄이기 위해, 3D 후보 박스를 지면에 배치한다. 입력으로 오직 단안 이미지를 사용하기 때문에 정확한 로드 평면을 추청할 수 없다. 대신에, KITTI 데이터에서 카메라 위치를 알 수 있기 때문에, 카메라의 Y축과 직교를 이루는 지면을 가진 모든 이미지에 대해 고정된 지면을 사용하고, 평면에서 카메라의 거리인 hcam = 1.65m라고 가정한다. 길이 경사면인 경우와 같이 지면 에러에 대해 강인해지기 위해 기본 평면을 여러 각도로 기울인 추가적인 평면들에 대한 후보 상자들도 샘플링한다. 그리고 이 평면들의 법선을 고정하고 hcam = (1.65 + δ)m라고 고정한다. 여기서 δ는 기본 지면과 객체의 거리의 가우시안 분포로 추정한 표준편차의 MLE 추정인 σ에 대해, 차의 경우 δ ∈ {0,±σ}를 만족하고, 보행자와 자전거 이용자의 경우 δ ∈ {0,±σ±2σ}를 만족하도록 설정한다. 작은 물체들이 오류에 더 민감하기에 보행자와 자전거 이용자에 더 많은 평면을 사용한다. 이 때, 모든 픽셀이 길로 라벨링되어 3D 평면에서 매우 낮은 prior 확률을 가진 박스들은 제거하여 샘플링되는 박스의 수를 더 줄인다. 이 결과, 지면, 템플릿, 이미지 당 약 14k개의 후보 상자가 된다. 이 샘플링 전략은 샘플링 박스 개수를 28%로 줄여 추론 속도를 엄청 올린다. Networks SetupImageNet으로 훈련된 VGG16모델을 사용하여 네트워크를 초기화했고, VGG16의 FC layer의 가중치를 통해 2개의 브런치를 초기화했다. KITTI 이미지들안의 매우 작은 객체들을 다루기 위해, 입력 이미지의 크기를 3.5배 키우는 방법을 사용했다. 그리고 훈련과 테스트시에 이미지에 대한 단일 스케일을 사용했다. 그리고 이미지에 대한 배치 사이즈는 N = 1, 제안에 대한 베치 사이즈는 R = 128로 설정하고, 30K 반복까지는 lr = 0.001의 SGD를, 그 후의 10K 반복은 lr = 0.0001을 사용했다.4. Experimental Evaluation까다로운 KITTI 데이터셋에서 평가헸다. KITTI 데이터셋은 자동차, 보행자, 자전거 이용자 총 3개의 클래스에 대한 7481개의 훈련 이미지와 7518개의 테스트 이미지로 구성되어 있다. 각 클래스에 대한 검출은 객체의 겹침이나 절단의 레벨에 따른 3개의 난이도로 평가되는데, 각 easy, moderate, hard이다. Metrics가장 높은 리콜을 사용하여 제안들을 평가했다. 이 논문에서 가장 높은 리콜을 오라클 리콜(oracle recall)이라 부르는데, 이 오라클 리콜은 GT(ground truth)와의 IOU가 특정 threshold이상으로 중복되는 제안들의 퍼센트를 계산한다. 차에 대해서는 0.7 threshold, 보행자와 자전거 이용자에 대해서는 0.5 threshold를 적용한다. 또한 AR(average recall)도 정리했다. 3D 객체 검출 모델의 전체 파이프라인을 KITTI의 2가지 태스크에 대해 평가했다. 이는 객체 검출과 객체 검출 및 방향 추정이다. 표준 KITTI 세팅에 따라 객체 검출 태스크에서는 AP(average precision) 지표를 사용하고, 객체 검출 및 방향 추정 태스크에서는 AOS(Average Orientation Similarity) 지표를 사용했다. Baselinevalidation set에서 이 논문에서의 제안 발생 방법과 몇몇의 가장 높은 성능의 접근법(3DOP, MCG-D, MCG, SS(Selective Search), BING, EB(Edges Boxes)과 비교했다. 3DOP과 MCG-D는 깊이 정보를 활용하는 반면, 나머지 방법과 이 논문에서의 방법은 오직 단일 RGB 이미지를 사용한다. 3DOP를 제외한 모든 방법들은 모든 전방 객체를 탐지하도록 훈련되는 클래스-독립적이지만, 이 논문의 방법은 클래스별 가중치와 semantic segmentation을 사용한다. Proposal Recall이 실험에서는 validation 셋에서 발생되는 제안에 대한 오라클 리콜을 평가했다. 위의 그림은 제안의 수에 대한 모델들의 리콜을 보여준다. 이 논문의 접근이 차와 보행차에 대한 500개의 제안보다 작게 사용할 때 모든 베이스라인들보다 엄청 더 높은 리콜을 달성한다. 그리고, easy 난이도에서 오직 100개의 차량에 대한 제안과 300개의 보행자에 대한 제안을 통해 90%의 리콜을 달성한다. 이는 다른 2D 방법들보다 훨씬 더 적은 제안을 필요로 한다는 것이다. 2k개 제안을 사용할 때, 3D 접근법 중 가장 좋은 성능을 보이는 것은 3DOP로 20% 정도 다른 베이스라인들보다 높았다. 3DOP와 MCG는 깊이 정보를 사용함으로써 단안 이미지를 사용하는 우리의 접근 방식과 조금 달라 비교하기가 어렵다.그런 다음, 탑 500개의 제안에 대한 IOU별 리콜을 측정했다. 모든 IOU threshold에서 스테레오를 사용하는 3DOP를 제외한 모든 베이스라인을 능가했다. 그러나 보행자와 자전거 이용자에 대한 높은 IOU threshold를 사용할 때, 3DOP보다 조금 낮게 나왔다. Ablation Study특징별 객체 제안 리콜에 대한 효과를 연구했다. 인스턴스나 시멘틱 특징들은 300개 보다 더 작은 제안을 사용했을 때 리콜을 향상시켰다. 인스턴스 특징없이도 1000개의 제안을 사용하면 90% 리콜을 달성할 수 있었다. 인스턴스와 형상 정보를 둘 다 제거했을 때, 90% 리콜을 달성하려면 2배 더 많은, 즉 2000개의 제안이 필요했다. Object Detection and Orientation Estimation객체 검출에 대한 제안을 점수매기기 위해서 섹션 3.3에서 설명된 네트워크를 사용했다. KITTI 테스트 셋에서 검출을 평가한 내용은 다음과 같다.테이블을 보면 공개된 단안 방법들을 능가했다는 것을 볼 수 있다. AP의 관점에서 2단계 중 최고 방법인 Fast R-CNN을 차, 보행자, 자전거 이용자 각각 어려움 난이도에서 7.84%, 2.26%, 2.97% 정도의 엄청난 차이를 보이며 능가했다. 방향 측정에 대해서는 어려움 난이도에서 3DVP보다 12.73% AOS 더 높게 달성했다. Comparison with Baselines저자의 CNN을 평가하기 위해 강력한 다른 제안 방법인 3DOP, EB(EdgeBoxes), SS(Selective Search)을 사용했다.위의 테이블은 KITTI 검증셋에서 검출과 방향 측정에 대한 결과를 보여준다. 이를 통해 EB, SS를 20% AP와 20% AOS 더 능가했다는 것을 볼 수 있다. 3DOP는 단안 이미지를 사용하는 우리와 달리 스테레오 이미지를 사용하기에 비교하기 어렵다. 그럼에도 불구하고, 비슷한 성능을 보여주었다.중간 난이도에서의 차에 대한 제안의 수의 함수로서 AP를 평가했다. 오직 각 이미지당 10개의 제안을 사용할 때, 35.7%를 달성하는 3DOP를 능가하여 53.7%의 AP를 달성했다. 100개보다 더 많이 사용하게 되면, AP는 3DOP와 거의 비슷했다. 5000개를 사용해야 EB는 최고 성능인 78.7% AP를 달성하는데 반해, 이 논문의 방법은 200개의 제안만을 사용해도 80.6% AP를 달성할 수 있었다. Qualitative Results3D 검출에 대한 결과는 위의 그림과 같다.5. Conclusions높은 퀄리티의 객체 검출을 얻기 위해 기본적인 CNN 파이프라인을 통해 실행되는 후보 클래스별 객체 제안을 생성하는 단안 3D 객체 검출에 대한 접근 방식을 제안해왔다. 이 목표를 위해, 물체가 지면 위에 있어야 한다는 사실을 이용하여 객체 후보를 3D로 배치한 후, semantic segmentation, 맥락 정보, 이전의 크기 및 위치 및 일반적인 객체 모양을 인코딩하는 여러 직관적인 잠재력을 통해 각 후보 상자에 점수를 매기는 에너지 최소화 방법을 제안했다. 이 논문의 객체 제안 발생 접근법은 모든 단안 접근법을 상당히 능가했고, KITTI benchmark에서 최고의 성능을 보여주었다.Reference Monocular 3D Object Detection for Autonomous Driving" }, { "title": "[논문 코드 구현] PointCNN: Convolution on X-transformed points", "url": "/posts/pointcnn2/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, PointCNN", "date": "2022-01-23 13:00:00 +0900", "snippet": "pointCNN 깃허브에 올라와 있는 코드를 직접 실행하고 리뷰한 내용입니다. 모든 코드는 colab에서 진행하였습니다.논문에 대한 리뷰가 궁금하신 분들은 이 링크를 참고해주세요.PointCNN 깃허브 주소: https://github.com/yangyanli/PointCNNClassificationModelNet40먼저 분류 모델 중 ModelNet40에 대해서 코드를 실행해볼 것이다.전체 코드cd data_conversionspython3 ./download_datasets.py -d modelnetcd ../pointcnn_cls./train_val_modelnet.sh -g 0 -x modelnet_x3_l4 가장 먼저 pointcnn 깃허브를 클론한다.!git clone https://github.com/yangyanli/PointCNN.git 그 후 블로그에 설명을 따라 data_conversions 경로로 들어간다.!cd PointCNN/data_conversions/ 지정되어 있는 argment를 따라 호출해준다.!python3 /content/PointCNN/data_conversions/download_datasets.py -d modelnet 주소는 깃허브에는 상대적 주소를 넣어놨는데, 실행해보니 안되서 절대주소로 넣었다. download_datasets.py를 실행시키는데, -d는 데이터셋 arg를 의미하고 modelnet에 대한 데이터셋을 다운받는다. 여기서 SSL 인증서에 대한 오류가 발생할 수도 있는데, 나의 경우 urllib3에 대한 오류와 request에 대한 오류 두 가지가 있었다.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=’shapenet.cs.stanford.edu’, port=443): Max retries exceeded with url: /media/modelnet40_ply_hdf5_2048.zip (Caused by SSLError(SSLCertVerificationError(1, ‘[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1091)’)))먼저 위와 같이 출력되는 urllib3에 대한 오류는 urllib3 라이브러리를 업데이트하면 해결된다.!pip install urllib3==1.24.1requests.exceptions.sslerror httpsconnectionpool(host=’’ port=443) max retries exceeded with url그리고 위와 같이 출력되는 request에 대한 오류는 신뢰할 수 없는 SSL 인증서로 인해 발생되는거라고 한다. 이때는 코드에 verify=False 라고만 추가해주면 된다.request.get(url)#=====&amp;gt;request.get(url,verify=False) train/val을 위한 세팅을 한다.먼저 경로를 변경시켜준다.!cd ..!cd PointCNN/pointcnn_cls!sh /content/PointCNN/pointcnn_cls/train_val_modelnet.sh -g 0 -x modelnet_x3_l4# Train/Val with setting modelnet_x3_l4 on GPU 0!이때도, 경로를 수정하여 실행했다. evaluation 진행일단 가지고 있는 이미지 파일인 caltech_10을 사용하여 돌려보니&amp;gt;&amp;gt;&amp;gt; python3 /content/PointCNN/evaluation/eval_s3dis.py -d /content/drive/MyDrive/data/caltech_10//content/drive/MyDrive/data/caltech_10/valid/bear/label.npy does not exist, skipping/content/drive/MyDrive/data/caltech_10/valid/giraffe/label.npy does not exist, skipping/content/drive/MyDrive/data/caltech_10/valid/ostrich/label.npy does not exist, skipping/content/drive/MyDrive/data/caltech_10/valid/skunk/label.npy does not exist, skipping/content/drive/MyDrive/data/caltech_10/valid/porcupine/label.npy does not exist, skipping/content/drive/MyDrive/data/caltech_10/valid/.ipynb_checkpoints/label.npy does not exist, skippingFound 0 predictionsEvaluating predictions:Classes: 0 0 0 0 0 0 0 0 0 0 0 0 0Positive: 0 0 0 0 0 0 0 0 0 0 0 0 0True positive: 0 0 0 0 0 0 0 0 0 0 0 0 0Traceback (most recent call last): File &quot;/content/PointCNN/evaluation/eval_s3dis.py&quot;, line 72, in &amp;lt;module&amp;gt; print(&quot;Overall accuracy: {0}&quot;.format(sum(true_positive_classes)/float(sum(positive_classes))))ZeroDivisionError: float division by zero이미지 파일과 npy 파일을 가지고 있어야 eval을 진행할 수 있는 듯 했다." }, { "title": "[논문 리뷰] PointCNN: Convolution on X-transformed points", "url": "/posts/pointcnn/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, PointCNN", "date": "2022-01-21 13:00:00 +0900", "snippet": "pointCNN: Convolution on X-transformed points논문에 대한 리뷰입니다. 이 논문은 2018에 투고된 논문이며 약 1100회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract저자는 point cloud로부터 피쳐 학습에 대한 간단하고 일반적인 프레임워크를 소개하고자 한다. CNN의 성공의 열쇠는 grid에서 조밀하게 표현된 데이터의 공간적-지역적(local) 상관관계를 활용할 수 있는 convolution 연산이다. 그러나 point cloud는 불균일하고 무질서하기 때문에, 점과 관련된 피쳐에 대해 커널을 직접 연산을 하면 형상 정보가 사라지고 점의 정렬에 대한 변형이 발생한다. 이 문제를 해결하기 위해 저자는 두 가지 원인: (1) 점과 관련된 입력 피쳐의 가중치와 (2) 잠재적이고 표준적인 순서에 대한 점들의 순열, 을 동시에 촉진하기 위해 입력 점들로부터 x-transform을 배울 것을 제안한다. 일반적인 convolution 연산자의 요소별 곱과 합 연산은 x-transformation기능 이후에 적용된다. 이 방법은 point cloud로부터 피쳐 학습에 대한 전형적인 CNN의 표본이므로, 저자는 이를 PointCNN이라 부른다.1. Introduction공간적-지역적(spatially-local) 상관관계는 데이터 표현과 독립적인 다양한 유형의 데이터의 아주 흔한 특성이다. 이미지와 같이 일반 도메인에 표시되는 데이터의 경우, convolution 연산은 다양한 작업에서 CNN에서의 주된 성공 요인으로서, 저 상관관계에 활용하면 효과적인 결과를 보여준다. 그러나 무질서하고 불균일한 point cloud 형태의 데이터에 대해서는 convolution 연산이 데이터에서 공간적-지역적 상관관계를 활용하는 것이 적합하지 않다.위의 그림은 point cloud에 convolution을 적용할 때의 문제를 설명하고 있다. (i)~(iv)에 대해 C차원의 입력 피쳐을 가진 무질서한 데이터를 F = {fa, fb, fc, fd} 라고 하고, 4xC 형태를 가진 1개의 커널을 K = [kα, kβ, kγ, kδ]^T 를 가진다고 가정해보자.(i)의 경우, 규칙적인 그리드 구조에 의해 규칙적인 정렬을 보여주고 있으며, 이는 2 x 2 patch 지역안의 fi = Conv(K,[fa,fb,fc,fd]^T)의 형태를 가진 K로부터 convolution되어 4xC 형상의 [fa,fb,fc,fd]^T가 될 수 있다. 이 때, Conv(,)는 요소별 곱을 뜻한다. 그리고, (ii)~(iv)의 경우 점들은 주변 점들에 의해 샘플링되므로 점의 순서가 임시적일 수 있다. 위의 그림에 적힌 순서를 따른다고 생각했을 때, 입력 피쳐 셋 F는 (ii)와 (iii)에서는 [fa,fb,fc,fd]^T이 될 것이고, (iv)에서는 [fc,fa,fb,fd]^T가 될 것이다. 만약 이 상태로 convolution 연산을 직접적으로 적용했을 때, 세 케이스의 출력 피쳐들은 위 그림의 (1a)처럼 계산된다. 모든 케이스에 대해 fii = fiii로 하고, 대부분의 케이스에 대해 fiii != fiv로 고정시킨다. 그렇게 되면, 결과적으로 직접적인 연산이 fii = fiii와 같은 형상 정보는 버리고, fiii != fiv와 같은 순서에 대한 변형은 남겨두는 결과를 초래한다,본 논문에서는 X = MLP(p1,p2,…,pk)와 같은 다층 퍼셉트론인 입력 점들(p1,p2,…,pk)인 K의 좌표에 대한 K x K의 X-transformation을 제안한다. 목표는 가중치 부여와 입력 피쳐 변형을 동시에 수행한 다음, 그 뒤에 변형된 피쳐들에게 전형적인 convolution을 적용하는 것이다. 이 과정을 X-conv라 하고, 이것이 PointCNN에 기본적인 블록이 된다. 이를 통해 fig1의 (ii)~(iv)에 대한 X가 4x4 행렬이고, K = 4인 X-conv를 적용하면 위 그림의 (1b) 방정식처럼 된다. 이 때, xii와 xiii는 각기 다른 점들로부터 학습된 것이기 때문에, 입력 피쳐의 가중치가 달라져 fii != fiii를 만들 수 있다. xiii와 xiv에 대해 ⫪가 (c,a,b,d)를 (a,b,c,d)로 순열하기 위한 순열 행렬일 때, Xiii = xiv x ⫪를 만족한다면, fiii = fiv를 만들 수 있다.fig1의 분석을 통해, 이상적인 X-transformation에서 X-conv는 순서를 바꾸지 않고, 점의 형상 정보를 얻을 수 있는 것을 확인했다. 실제로는, 순열 등식 측면에서 x-transformation 학습은 이상과 차이가 있다. 그럼에도 불구하고, X-conv를 통해 만들어진 PointCNN은 point cloud로 일반적인 convolution을 직접 적용하는 것보다 훨씬 좋고, PointNet++와 같이 point cloud 입력 데이터에 대해 설계된 최첨단 신경망보다도 더 좋다.2. Related Work Feature Learning from Regular DomainsCNN은 이미지(2D 규칙적인 그리드안의 픽셀들)안의 공간적-지역적 상관관계를 잘 활용해왔다. 3D voxels와 같이 고차원의 규칙적인 도메인에 대한 응용CNN도 잘 나와있다. 그러나 입력과 convolution 커널들은 모두 고차원이고, 그래서 매우 많은 계산 비용과 메모리를 필요로 한다. Octree, Kd-tree, Hash 기반의 접근 방식은 빈 공간에 대해서는 convolution 연산을 하지 않으며 계산을 절약했다. point cloud을 그리드로 분할하고, 3D 커널과 conv연산을 위해 그리드 평균 점과 벡터로 각 그리드를 나타낸다. 이러한 접근 방식에서 커널 그 자체는 밀도있고, 높은 차원이다. 그러나 이 접근 방식은 계측적 특징을 학습하는 데 반복적으로 사용할 수 없다. 이러한 방법들과 비교하여 PointCNN은 입력 표현과 conv커널이 밀도가 희박하다. Feature Learning from Irregular Domains3D 센싱의 급격한 발전에 따라 3D point cloud로부터 특징 학습의 발전이 더뎌졌다. PointNet과 Deep Sets는 입력에 대한 대칭 함수를 사용하면서 입력 순서 불변을 성공시켰다. 커널 상관성과 그래프 풀링은 PointNet과 같은 방법들을 개선시키기 위해 제안되기도 했다. RNN은 정렬된 point cloud 조각들로부터 풀링함으로써 특징들을 프로세싱하는데 사용되었다. 이 많은 대칭 풀링 기반의 방법들이 순서 불변성을 달성하는데 큰 도움을 주었지만 정보를 버리는 대가를 치뤄야만 했다. CNN 커널은 point cloud에 대한 CNN을 일반화하기 위해 이웃 점 위치에 대한 파라미터를 활용한 함수로 표현된다. 그 커널은 독립적으로 파라미터화된 각 점들과 연관이 있지만, 이 논문에서의 방법 안에 x-transformation는 각 이웃으로부터 학습되고, 그래서 지역 구조에 더 적응을 잘 할 수 있다. point cloud이외에 비규칙적인 도메인안의 희박한 데이터는 그래프나 그물망으로 표현되고, 몇몇 작업들은 이러한 표현들을 통해 피쳐 학습을 제안하기도 한다. Invariance Vs Equivariance불변성을 달성하는 데 있어 풀링의 정보 손실 문제를 해결하기 위해 등분할을 목표로 하는 선구적인 방법들이 제안되기도 했다. 저자의 x-transformation은 이상적으로 등가성 실현이 가능하게 했고, 실제로도 효과를 볼 수 있다. 저자는 PointCNN과 SPN(Spatial Transformer Networks)와 유사함을 발견했다. 두 가지 모두 정규화를 진행할 때 명시적인 손실이나 제약없이 입력을 더 많이 처리하기 위해 잠재된 표준 형태로 “변환”하는 메커니즘을 제공한다는 점에서 유사하다. 실제로 이 방법은 네트워크가 학습 메커니즘을 더 잘 활용할 수 있는 방법이라는 것을 밝혀냈다. pointCNN에서 x-transformation은 가중치 부여와 순열을 제공하며, 이는 일반적인 행렬로서 모델화된다. 그래서 순열 행렬이 바람직한 출력인 모델과는 다르게 이중 확률 행렬에 의해 근사될 것이다.3. PointCNNconvolution의 계층적 적용은 CNN을 통한 계층적 표현을 학습하는데 필수적이다. PointCNN은 같은 디자인으로 설계되어 있고, 이를 point cloud로 일반화한다. 먼저, PointCNN에서의 계층적 convolution을 소개한 후, X-conv 연산자에 대해 자세하게 설명하고, 다양한 태스크에 적합한 PointCNN 아키텍쳐를 제시할 것이다.3.1 Hierarchical ConvolutionPointCNN의 계층적 convolution을 설명하기 전에, 위 그림과 같이 일반 그리드에 대한 설명을 먼저 보고자 한다. 그리드 기반의 CNN의 입력은 R1xR1xC1의 형태를 가진 F1 특징맵이다. 이 때, R1은 해상도, C1은 피쳐의 채널 깊이이다. 로컬 패치의 형태인 KxKxC1의 커널을 가진 F1과는 달리, KxKxC1xC2의 형태를 가진 convolution의 커널 K은 R2xR2xC2의 형태를 가진 F2의 특징 맵을 만든다. fig2에서 파라미터, R1 = 4, K = 2, R2 = 3이라 가정해보자. F1과 F2를 비교했을 때, 보통 R2 보다 R1의 해상도가 높고, 깊이는 C1보다 C2가 더 깊으며 F2가 F1보다 더 높은 레벨의 정보를 가지고 있다. 위의 그림처럼 4x4 -&amp;gt; 3x3 -&amp;gt; 2x2 의 방식으로 재귀적으로 해상도가 감소하고, 깊이는 더욱 깊어지면서 특징맵을 만들게 된다.PointCNN의 입력은 다음과 같다.이 때, 점은 {p1,i : p1, i ∈ R^dim}, 특징은 {f1,i : f1,i ∈ R^C1} 에 해당되고, 그리드 기반의 CNN의 계층적 구조에 따라 X-conv를 F1에 적용하여 높은 레벨의 표현 F2를 얻고자 한다. 이 때, F2는 다음과 같다.이 때, {p2,i}는 {p1,i}의 대표 점이고, F2는 F1보다 더 작은 해상도와, 깊은 특징 채널을 가진다. F1에서 F2로 변환되는 X-conv 프로세스를 재귀적으로 적용시키면 fig2 하단에서처럼, 입력 점들의 특징은 더 작은 수의 점들로(9 -&amp;gt; 5 -&amp;gt; 2) 투영되나, 특징들은 더 풍부해질 것이다.저자는 분류 태스크에서 {p1,i}의 무작위 다운 샘플링을 적용하고, 분할 작업은 균일한 점 분포를 더 요구하기 때문에 분할 작업에서 가장 먼 점과 샘플링한다. 추가적으로 본 모델에서도 적용한 Deep Points와 같이 기하학적 처리에서 좋은 성능을 보여준 더 좋은 점 선택을 의심한다. 그래서 더 나은 대표 점 생성 방법에 대한 탐구를 추후에 진행할 예정이다.3.2 X-conv OperatorX-conv는 F1에서 F2로 변환하는 핵심적인 연산이다. 여기서는, 입력과 출력, 연산의 절차를 먼저 설명하고, 연산의 근거를 설명하겠다.그리드 기반 CNN의 convolution과 유사하게 공간적-지역적 상관관계를 활용하기 위해 X-conv는 local 지역에서 작동한다. 출력 특징이 대표점{p2,i}와 연관되어야 하므로 X-conv는 {p1,i}의 인접한 점과 관련 특징을 입력으로 한다. 간편함을 위해 {p2,i}를 p로 두고, p에 대한 특징을 f, {p1,i}의 K neighbor을 N으로 두어, p에 대한 x-conv의 입력은 다음과 같이 정의된다.이 때, S는 정렬되지 않은 상태다. S는 훈련 가능한 convolution 커널인 K에 대해 K x Dim의 행렬, P = (p1,p2,…,pk)^T 와 K x C1 행렬, F = (f1,f2,…,fk)^T로 표현될 수 있다. 이 입력을 통해 입력 특징들을 대표점 p로 투영한 특징 Fp를 계산할 수 있다.위의 과정, 즉 algorithm 1을 요약하면 다음과 같다.PointNet에서처럼, MLPδ()는 각 점이 독립적으로 적용된 다층 퍼셉트론이다. x-conv에 포함된 모든 연산들, 예를 들어 Conv(.,.)(= 행렬 곱 (.) x (.)) 와 MLP(.)(= MLPδ())은 미분이 가능하다. 따라서 x-conv도 미분이 가능하게 되고, 역전파를 통한 훈련을 위해 심층 네트워크에 연결할 수도 있게 된다.위의 알고리즘 안에 4-6번째 줄은 섹션 1에서 설명했던 1b방정식에 해당되는 x-transformation의 핵심 코드이다. 그리고 이제, 알고리즘1 안에 1-3번째 줄에 대한 근거를 자세히 설명할 것이다. x-conv는 local점에서 작동하도록 설계되어 있고, 출력은 p와 p의 인접 점들의 절대 좌표와는 독립적이어야 하지만, 상대 좌표에는 의존적이어야 한다. 그러기 위해 fig3-b와 알고리즘1의 1번 줄은 대표 점에 지역 좌표계를 위치시킨다는 뜻이다. 공통된 특징들에 대한 인접 점의 지역 좌표계이고, 이것이 출력 피쳐들을 정의한다.하지만, 지역 좌표계는 연관된 특징들과는 다른 차원과 표현이다. 이 문제를 해결하기 위해 알고리즘1의 2번줄에서 좌표를 더 높은 차원과 더 추상적인 표현에 대한 좌표로 만든다. 그리고 나서, 3번줄에서 더 나은 프로세싱을 위해 연관된 특징들과 이 좌표와 결합한다. 이에 대한 그림은 fig3-c에서 볼 수 있다.MLP(.)를 통해 지역 좌표계를 형상들과 동일하게 만들 수 있다.3.3 PointCNN Architecturesfig2를 통해 그리드 기반의 CNN에서의 Conv layer과 pointCNN에서의 X-conv의 차이점을 볼 수 있다. 그것은 2가지로 (1) 로컬 지역을 추출하는 방법 (KxK patches Vs 대표 점들 주변의 K neighboring points), (2) 그 로컬 지역으로부터 정보를 배우는 방법 (Conv Vs X-conv) 이다. 그러나 x-conv layer를 통해 심층 네트워크를 합치는 과정은 그리드 기반의 CNN과 유사하다.위 그림은 2개의 X-conv layer를 가진 간단한 pointcnn의 구조를 나타낸 것이다. 여기서 N은 출력 대표점 개수, C는 특징 차원, K는 각 대표점마다의 인접 점 개수, D는 x-conv의 팽창률을 의미한다. 그리고, a와 b는 분류를 위한 아키텍처, c는 분할을 위한 아키텍처 구조이다.K는 이웃 점 수를, N는 이전 layer에서의 점 개수에 대해 K/N비로서 각 대표 점의 수용 필드(receptive field)를 정의할 수 있다. 이를 전체적으로 적용하면 최종 점은 1.0의 수용 필드를 가지게 되는데, 이를 통해 전체 모양을 볼 수 있을 것이고, 형상에 대한 의미론적 학습에 도움이 된다. 그 후 마지막 x-conv뒤에 FC layer를 붙이고, 네트워크 훈련을 위한 loss를 구한다.fig4-a를 보면, 차원과 점들의 갯수가 점차 줄어듬에 따라 상위 x-conv layer은 훈련 샘플이 급격히 줄어들게 되어 과적합 우려가 발생한다. 이를 해결하기 위해 fig4-b처럼 x-conv layer안에 더 많은 점이 남아 있을 수 있도록 빽빽한 연결이 있는 pointCNN을 제시한다. 하지만 네트워크의 깊이와 수용 필드의 성장률도 유지되어야 하기 때문에, 상위로 올라갈수록 대표 점들이 전체 형상에서의 점점 더 큰 비율을 차지하도록 한다. 그리드 기반의 CNN에서의 확대된 convolution을 사용하여 이를 제작했다. 입력으로 항상 K neighboring 점들을 받는 대신, 팽창률 D에 대한 K x D를 균일하게 샘플링한다. 이 경우 이웃 점의 갯수나 커널 사이즈를 키우지 않고도, 수용 필드가 K/N이 아닌 (K x D)/N로 증가시킬 수 있다.fig4-b에서 두번째 x-conv layer을 보면 확장률 D = 2를 사용하여 1개가 아닌 4개의 대표점을 남길 수 있고, 이것들 모두를 예측하는데 사용할 수 있다. 이를 통해 상위의 x-conv layer를 더 철저하게 훈련시킬 수 있으며 더 많은 연결이 네트워크를 더 개선시킬 수 있다. 테스트 시에 다중의 대표 점 출력은 예측을 안정화시키기 위해 softmax바로 직전에 평균화한다.분할 태스크에서는 더 높은 해상도의 점 단위 출력이 필요하므로 fig4-c에서처럼, 글로벌 정보, 즉 Conv에 사용된 점들을 고해상도 예측으로 추가로 전달하는 역할을 하는 DeConv가 있는 Conv-DeConv 구조를 사용했다. Conv와 Deconv는 같은 x-conv 연산을 사용한다.과적합을 방지하기 위해 FC layer 이전에 dropout을 적용했다. 또한, “subvolume supervision”도 사용하여 과적합을 방지했다. 마지막 x-conv layer에서 수용 필드가 1보다 작도록 세팅되며, 대표 점에 의해 부분적인 정보만 볼 수 있도록 한다. 더 높은 성능을 위해 훈련시에는 부분 정보만 사용하는 방법을 사용하기도 한다. Data Augmentationx-conv의 파라미터를 훈련시키기 위해 특정 대표점에 대해 동일한 인접 점 집합을 동일한 순서로 계속 사용하는 것은 좋지 않다. 따라서 무작위 샘플링과 입력 점을 셔플하여 인접 점의 데이터와 순서가 매 반복마다 달라지도록 한다. 입력으로 N개의 점들을 받는 모델을 훈련시키고자 할 때, 가우시안 분포에 따른 N=(N,(N/8)^2)개의 점들이 훈련에 사용된다.4. Experiments분류 작업에서는 총 6개의 데이터셋(ModelNet40, ScanNets,TC-Berlin, Quick Draw, MNIST, CIFAR 10)을 적용하여 pointcnn을 평가했고, 분할 작업에서는 총 3개의 데이터셋(ShapeNet Parts, S3DIS, ScanNet)을 적용하여 pointcnn을 평가했다.4.1 Classification and Segmentation Results위의 그림은 ModelNet40과 ScanNet의 3D point cloud 분류 결과와 point cloud에 대한 몇몇의 신경망에 대한 비교를 요약한 것이다. ModelNet40의 3D 구조의 많은 부분이 수직 방향과 수평 방향에 미리 맞춰져 있다. 만약 무작위 수평 회전이 훈련이나 테스트에 적용되어 있지 않았다면 상대적으로 일관된 수평 방향만 활용되고, 이에 기반한 방법들은 수평 회전하는 모델과 직접적으로 비교할 수 없다. 그래서 ModelNet40과 ScanNet 둘다에 적용했는데, 두 데이터셋에서 모두 높은 성능을 보였다.이제 분할 태스크에 대해 비교를 해볼 것이다.분할은 ShapeNet Parts, S3DIS, ScanNet 데이터셋에서 비교했고 그 결과는 위의 테이블에서 볼 수 있다. PointCNN은 분할에 특화되어 있는 최첨단 기술인 SSCN, SP-Graph, SGPN등을 포함하여 비교된 모든 방법들보다 뛰어났다.스케치는 2D 공간에서의 1D 곡선이므로 2D 이미지들보다 point cloud으로 더 효과적으로 표현될 수 있다. pointCNN을 TU-Berlin과 Quick Draw sketches에서 평가했고, 그 결과를 image CNN 기반의 방법들을 포함해 PointNet++와 성능을 비교하여 위의 테이블에 나타냈다. pointCNN은 두 데이터셋에서 PointNET++을 능가했다. TU-Berline 데이터셋에서, pointCNN은 일반적인 image CNN인 AlexNet보다 약간 더 좋은 성능을 보였지만, Sketch-a-Net과는 조금 떨어지는 모습을 보였다. 이는 Sketch-a-Net에서의 구조적 요소들이 현재의 pointCNN보다 뛰어남을 보여준다.x-conv가 이상적으로 conv의 일반화 버전이라서 만약 기본 데이터가 같고 단지 다르게 표현된 것이라면, CNN과 성능이 비슷해야 한다. 이를 확인하기 위해 MNIST와 CIFAR10에서 point cloud의 표현에서의 pointCNN의 성능을 평가했다.MNIST 데이터에서 PointCNN은 다른 방법들과 비슷한 성능을 달성하여 숫자의 모양 정보에 대한 효과적인 학습을 보여줬다. 형상 정보가 대부분 없는 CIFAR10에서는 pointCNN은 RGB 특징에서의 공간적-지역적 상관관계에서 대부분 학습해야 했고, pointCNN과 유명한 image CNN과는 차이가 있긴 하나 꽤나 잘 수행했다. 이를 통해 일반 이미지에 대해서는 원래의 CNN들이 더 나은 선택이라는 결론이 도출된다.4.2 Ablation Experiments and Visualizations Ablatino test of the core X-conv operatorx-transformation의 효과를 확인하기 위해 알고리즘1에서 Fp &amp;lt;- Conv(K,F*)인 4-6번 줄을 제거한 baseline을 제시하고자 한다. 이를 pointCNN과 비교하면 훈련 가능한 파라미터가 더 적고, 알고리즘1의 4번줄인 MLP(.)을 제거했기 때문에 더 얕다. 공정한 비교를 위해 wider/deeper인 W/D 그리고, X-conv 존재 여부로 나누어 비교하고자 한다. 저 둘은 대량 같은 양의 파라미터를 가진다. pointCNN w/o X (deeper)의 모델의 깊이는 pointCNN에서 MLP(.)의 제거에 대해서 영향을 받는다. 결과는 다음 테이블에서 볼 수 있다.PointCNN은 다른 변형들보다 훨씬 뛰어났고, PointCNN과 PointCNN w/o X 의 차이는 모델의 파라미터 수나 모델의 깊이 때문이 아니다. Visualization of X-Conv features각 대표 점은 이웃한 점들을 특정한 순서로 나타내며, 그에 해당하는 F*과 C = Cδ + C1에 해당하는 R^(KxC)안의 Fx를 가진다. 같은 대표 점에서 네트워크로 근처 점들을 다른 순서로 넣게되면, F*과 Fx를 갖게 된다. 유사하게 F*을 PointCNN w/o X에서는 F0로 정의한다. 분명한 것은, 입력 점들의 순서의 차이는 다른 F*을 나타내기 때문에, F*은 R^(KxC)공간 안에서 분산될 수 있다. 다른 한편, 학습된 X가 F*를 완벽하게 표현할 수 있다면 Fx는 공간에서 표준 점에 있어야 한다.이를 확인하기 위해 ModelNet40 데이터셋에서의 15개의 무작위로 선택된 대표 점의 F0와 F*, Fx에 대한 T-SNE 시각화할 것이고, 이 결과는 아래 그림에서 볼 수 있다.fig5-a에서처럼 F0는 서로 다른 대표 점의 특징이 서로 차별적이지 않음을 나타내는 상당한 “불규칙”을 보인다. fig5-b에서처럼 F*은 F0보다 더 나은 살짝 모여있지만, fig5-c에서처럼 Fx는 X에 의해 집중되어 있음을 볼 수 있고, 따라서 각 대표 점들은 매우 큰 차별성을 보인다. 집중에 대한 정량적 기준을 고려할 때, 먼저 다른 대표점들의 중심 피쳐를 계산하고, 그 후 중심에서 가까운 정도를 기반으로 모든 특징 점을 그들이 속한 대표 점으로 분류한다. 분류 결과는 F0, F0, Fx 각각 76.83%, 89.29%, 94.72%의 분류 정확도를 가진다. 이 결과 한 점에 집중되지는 않지만, 피쳐 학습에서 pointCNN의 성능이 좋다는 것을 볼 수 있다. Optimizer, model size, memory usage and timing저자는 tensorflow, lr = 0.01의 ADAM optimizer을 사용했다. 위의 테이블에서 볼 수 있듯이 배치 사이즈 16, 1024개의 입력 점을 nVidia Tesla P100 GPU에서 다른 방법들과 분류 태스크에 대해 작업 시간을 비교했다. PointCNN은 이 셋팅에서 훈련/추론에 대해 각 배치마다 0.031/0.012sec의 시간을 달성했다. 게다가 4.4M개의 파라미터를 가진 2048개의 입력 점을 분할하는 모델에서 배치 사이즈 12, 훈련/추론에 대해 각 배치마다 0.61/0.25sec를 달성했다.5. Conclusion저자는 point cloud로 표현된 데이터에서 공간적-지역적 상관관계를 활용하도록 CNN을 일반화한 PointCNN을 제안했다. PointCNN의 핵심은 전형적인 convolution으로 작업되기 전에 입력 점과 특징들을 행렬 곱하고 가중치를 부여하는 X-conv 연산이다.오픈 소스는 이 페이지에 올려두었다.코드에 대한 리뷰는 다음 페이지에서 진행할 것이다.Reference PointCNN: Convolution On X -Transformed Points" }, { "title": "[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection", "url": "/posts/voxelnet/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, VoxelNet", "date": "2022-01-18 13:00:00 +0900", "snippet": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 논문에 대한 리뷰한 글입니다. 이 논문은 2018 CVPR에 투고된 논문이며 약 1615회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract3D point cloud안에서 정확한 객체 검출은 네비게이션이나 집 지킴이, AR/VR에 적용하기에는 아직 큰 문제들이 있다. 매우 희박한 LIDAR point cloud에 RPN(region proposal network)을 적용시키기 위해, 기존에는 조감도(bird’s eye view) 투영과 같이 수작업으로 제작된 피쳐 representation에 초점을 맞췄다. 이 논문에서는 3D point cloud를 위한 수동적인 특징 처리 대신, 심층 네트워크에서 훈련이 가능한 end-to-end이고, 단일 단계에서처럼 특징 추출과 bounding box 예측을 통합한 포괄적인 3D detection network인 VoxelNet을 제안한다. 특히 VoxelNet은 point cloud를 동등한 공간의 3D voxel로 나누고, 새로 도입된 VFE(voxel feature encoding) Layer를 통해 각 voxel내의 point 그룹을 통일된 특징 표현으로 변환한다. 이러한 방식으로, point cloud는 서술적 체적 표현으로서 인코딩되며, 이 표현은 RPN에 연결되어 detection을 생성한다. KITTI car detection benchmark에서의 실험에서 VoxelNet은 최첨단 LIDAR 기반의 3D detection 방법을 큰 차이로 능가했다. 게다가, 이 네트워크는 다양한 형상을 가진 물체의 효과적인 차별적 표현을 학습하여, LIDAR만을 기반으로 하여 보행자와 자전거 이용자를 3D detection하는 결과를 도출했다.1. Introductionpoint cloud 기반의 3D 객체 검출은 미래의 세상을 구성하는데 중요한 요소다. 이미지 기반의 detection과 달리 LIDAR는 객체를 정확하게 위치화하고, 그들의 모양을 정확하게 구분하는데 사용되는 신뢰할 수 있는 depth 정보를 제공한다. 그러나, 이미지와 달리 LIDAR point cloud는 매우 희박하고, 3D 공간의 비균일한 샘플링과 센서의 유효 범위, 충돌, 상대적인 모습 때문에 점 밀도가 변화하기 너무 쉽다. 이런 까다로움을 다루기 위해 많은 접근 방식듣이 3D 객체 감지를 위해 조정된 point cloud를 위해 수동으로 특징 표현을 만들었다. 몇몇의 방법들은 point cloud를 투영하고, 이미지 기반의 특징 추출 기술을 적용한다. 다른 방법으로는 point cloud를 3D voxel grid로 전환시키고 각 voxel을 수작업으로 조작하여 기능을 인코딩한다. 하지만, 이러한 수작업 설계 방법은 이런 접근 방식들이 3D 특징 정보와 검출 작업에 필요한 불변성을 효과적으로 이용하는 것을 막는 정보 병목 현상을 초래한다. 수작업 기능 제작에서 기계 기능 학습으로 변화되고 있기 때문에 이미지에 대한 인지와 검출 작업이 주요 돌파구가 될 것이다.최근에 point cloud에서 직접 포인트별 특징을 학습하는 end-to-end 심층 신경망인, PointNet을 제안했다. 이 접근 방식은 3D object detection, 3D object part segmentation 및 포인트별 semantic segmentation 작업에 대해서 인상적인 결과를 보여주었다. PointNet의 향상된 버전은 다른 크기들에게서 지역 구조를 배울 수 있다. 만족스러운 결과를 성취하기 위해 이 pointNet의 접근 방식들은 모든 입력 점들에 대해 특징 변환 네트워크를 훈련시켰다. 하지만 전형적인 point cloud방법들은 LIDAR를 사용하여 약 100K 점을 얻게 되는데, 이에 대해 아키텍처를 훈련시키는 것은 높은 계산과 메모리를 요구한다. 3D 특징 학습 네트워크를 훨씬 더 많은 점들과 3D detection작업으로 확장시키는 것은 본 논문에서 다루는 주요 과제이다.RPN(Region Proposal Network)는 효과적인 객체 검출에 매우 최적화된 알고리즘이다. 그러나 이 접근 방식은 빽빽하고, 전형적인 LIDAR point cloud에 대한 구조가 아닌 이미지나 비디오와 같은 텐서구조 안에서 정돈된 데이터를 요구한다. 본 논문에서는 3D detection 작업에 대한 점들의 특징 학습과 RPN 사이의 격차를 좁히고자 했다.그래서 본 논문에서는 point cloud에서 차별적 특징 표현을 동시에 학습하고 정확한 3D bounding boxes를 아래 그림과 같이 end-to-end방식으로 예측하는 일반적인 3D detection 프레임워크인 VoxelNet을 제안한다.저자는 점 단위 특징을 지역적으로 집계된 특징과 결합하여 voxel 내에서 점간 상호작용을 가능하게 하는 새로운 VFE(voxel feature encoding) layer을 설계한다. 여러 VFE layer를 쌓으면 로컬 3D 형상 정보를 특성화하기 위한 복잡한 특징을 학습할 수 있다. 특히, VoxelNet은 point cloud를 균등한 3D voxel 공간으로 나누고, 쌓인 VFE layer을 통해 각 voxel을 인코딩하고, 그렇게 되면 3D convolution은 로컬 voxel 특징을 추가로 통합하여 point cloud를 고차원 공간적 표현으로 변환한다. 결국, RPN은 공간적 표현을 소비하고 검출 결과를 출력하게 된다. 이 효과적인 알고리즘은 밀도가 희박한 점 구조와 voxel grid에서 효과적인 병렬 프로세스 둘 다에게 이점을 가져다준다.저자는 KITTI benchmark에서 제공되는 full 3D detection tasks와 조감도 검출에서 VoxelNet을 평가했다. 실험은 VoxelNet이 최첨단 LIDAR 기반의 3D detection을 큰 차이로 능가한다는 것을 보여준다. 또, VoxelNet이 LIDAR point cloud로부터 보행자와 자전거를 검출하는 결과를 잘 성취한다는 것을 보여준다.1.1 Related Work이전에 나왔던 특징 표현의 방법들은 특징들을 수작업했고, 이는 많고 자세한 3D 형상 정보를 사용했을 때 좋은 결과를 야기했다. 하지만, 더 복잡한 형상과 장면들에 대해서는 추론하지 못했고, 데이터로부터 필요한 불변성을 학습할 수 없었기에 자율 탐색과 같은 통제되지 않는 시나리오에서는 다소 제한적인 성공을 거두었다.더 자세한 텍스쳐 정보를 제공받았다고 한다면, 많은 알고리즘이 2D 이미지에서 3D bounding boxes를 추론했을 것이다. 하지만, 이미지 기반의 3D 검출 접근 방식의 정확도는 깊이 추정(depth estimation)에 따라 결정된다.몇몇의 LIDAR 기반의 3D 객체 검출 기술들은 voxel grid 표현을 활용한다. 아래의 리스트는 voxel grid를 활용한 사례들이다. Vote3Deep: voxel 내에 포함된 모든 점에서 6개의 정보들을 통해 비어있는지를 판단한 후, 안 비어있는 voxel을 인코딩함 로컬 정보들을 융합하여 각 voxel을 표현함 voxel grid에서 부정확한 부호들을 계산함 3D voxel grid에 대한 이진 인코딩을 사용함 MV3D: 조감도에서 다채널 형상 맵과 정면뷰에서 원통형 좌표를 계산하여 LIDAR point cloud에 대한 다중 뷰 표현을 도입함1.2 Contributions 본 논문에서는 point cloud 기반의 3D 검출을 위한 새로운 훈련 가능한 end-to-end 심층 아키텍처인 VoxelNet을 제안했다. 이 VoxelNet은 희박한 3D 점들에서도 잘 동작하고, 수작업을 통한 특징 작업에 의해 발생되는 정보 병목(bottleneck) 현상을 방지한다. 희박한 점 구조와 voxel grid에서 효과적인 병렬 프로세싱에 대해 이점이 있는 VoxelNet을 구현하는 효과적인 방법을 소개했다. KITTI benchmark에서 실험을 진행했다. 2. VoxelNet이제부터 VoxelNet의 훈련에 사용된 loss function 구조를 설명하고, 네트워크를 구현하는 효과적인 알고리즘을 설명하고자 한다.2.1 VoxelNet ArchitectureVoxelNet은 위의 그림처럼 Feature learning network,Convolutional middle layers,Region Proposal Network의 3가지 블록을 가지고 있다.2.1.1 Feature Learning Network Voxel Partitionfig2에서 볼 수 있듯이, point cloud의 경우, 3D 공간을 균등한 voxel 공간으로 나눈다. point cloud가 X,Y,Z축을 따라 W,H,D 범위의 3D 공간을 차지한다고 가정해보자. 그리고 각 voxel의 크기를 vD,vH,vW로 정의한다면 3D voxel grid의 크기 D’ = D/vD , H’ = H/vH , W’ = W/vW 가 될 것이다. 그렇다면 D, H, W가 vD, vH, vW의 배수라고 생각할 수 있다. Groupingvoxel에 따라 점들을 그룹화한다. 거리, 충돌, 객체의 상대적 모양, 비균일한 샘플링 등과 같은 요소들 때문에, LIDAR point cloud는 매우 희박하고, 공간 전체에 걸쳐 매우 가변적인 점 밀도를 가진다. 따라서, 그룹화후에는, voxel은 가변적인 점들을 포함할 것이다. fig2를 보면, Voxel-1은 Voxel-2, Voxel-4보다 매우 더 많은 점들을 포함하고 있고, Voxel-3는 점을 포함하고 있지 않다. Random Sampling전형적으로 고차원의 LIDAR point cloud는 최대 약 100k의 점들로 구성되어 있다. 모든 점을 프로세싱하는 것은 계산에 대한 메모리와 효율을 증대시킬 뿐만 아니라 공간 전체에 걸쳐 매우 가변적인 점 밀도를 증가시킬 수 있다. 이를 해결하기 위해, T개의 점보다 더 많이 포함되어 있는 voxels에 대해서 고정된 점들의 수, T를 무작위로 샘플링한다. 이 샘플링 전략은 두가지의 목적을 가지는데, (1) 계산상 절약, (2) voxel 사이의 점의 불균형을 감소시켜 샘플링 편향을 줄이고 훈련에 더 많은 변화를 더하고자 한다. Stacked Voxel Feature Encoding핵심은 VFE layer들의 나열이다. 단순성을 위해 fig2에서는 하나의 voxel에 대한 계층적 특징 인코딩 프로세스를 볼 수 있으며, 일반성을 잃지 않은 채로 VFE layer-1을 사용할 것이다. VFE layer-1는 위의 fig3에서 볼 수 있다.voxel V에 대해 t는 t &amp;lt;= T 개의 LIDAR 점, pi는 i번째 점에 대한 X,Y,Z 좌표를 포함하며, ri는 수신된 반사율을 의미한다. 먼저 voxel V = (vx, vy, vz)안의 모든 점의 중심으로서, 지역 평균을 계산한다. 그러면 각 점 pi의 상대적 중심 좌표인 w,r,t로 판단해볼 수 있으며, 입력 특징셋 Vin을 얻을 수 있다. Vin에 대해서는 다음과 같다.그 다음으로 각 pˆi은 FCN(fully connected network)를 통해 형상 공간을 변환되며, 여기서 voxel에 포함된 표면의 모양을 인코딩하기 위해 점 특징인,fi ∈ R^m 으로부터 정보를 수집할 수 있다. FCN은 선형 layer, BN(batch normalization) layer, ReLU layer로 구성되어 있다. 점 단위 특징 표현들을 얻은 후, V에 대한 각 voxel별 집계된 특징인,˜f ∈ R^m 을 얻기 위해 V와 연관된 모든 fi에서 요소별 Map Pooling을 적용한다. 마지막으로, 각 fi를 ˜f로 만들어서 점 단위 연결 특징으로서 fi,out를 형성한다. fi,out에 대해서는 아래와 같다.그렇게 되면, 출력 특징셋인 Vout을 얻을 수 있다.모든 비어 있지 않은 voxel들은 같은 방법으로 인코딩되고, FCN의 파라미터셋을 같이 공유한다.cin차원의 입력 피쳐를 Cout 차원의 출력 피쳐로 변환하는 i번째 VFE layer를 나타내는 VFE-i(Cin,Cout)을 사용한다. 선형 layer에서는 행렬 크기 Cin x Cout/2를 학습하고, 점 단위 결합은 Cout 차원의 출력을 리턴한다.출력 피쳐들은 점 단위 피쳐들과 지역적으로 집계된 특징들을 모두 결합하여 가지고 있기 때문에, VFE layer를 쌓는 것은 voxel 내의 점 상호작용이 인코딩되고, 최종 피쳐는 서술적인 형상 정보를 학습할 수 있다. voxel 단위의 피쳐는 fig2에서 보이는 voxel 단위의 피쳐의 차원을 C라고 할 때 FCN을 통해 VFE-n의 출력을 R^C로 변환하고, 요소별 Maxpooling을 적용함으로써 얻어진다. Sparse Tensor Representation비어 있지 않은 voxel들만을 프로세싱함으로써, 우리는 각각 비어 있지 않은 특정 voxel의 공간 좌표에 고유하게 연관된 voxel 피쳐 리스트를 얻는다. 얻어진 voxel 단위의 피쳐들은 fig2에서 볼 수 있듯이 C x D’ x H’ x W’의 크기를 가진 희박한 4D tensor로서 나타내진다. point cloud가 약 최대 100k 점들을 가지고 있지만, 90%가 비어 있다. sparse tensor로서 비어 있지 않은 voxel을 나타내는 것은 메모리 사용과 역전파 시에 계산 비용을 줄일 수 있으며, 이것은 효과적인 실행에 중요한 단계에 해당한다.2.1.2 Convolutional Middle Layers입력과 출력의 채널의 수를 나타내는 Cin,Cout과 각각 커널 사이즈, stride 사이즈, 패딩 사이즈에 해당하는 M차원의 벡터 k,s,p인 ConvMD(Cin,Cout,k,s,p)를 사용하여 M차원의 convolution 작업을 표현한다. M차원의 크기가 동일한 경우, k = (k,k,k)의 스칼라를 사용한다.각 convolutional middle layer은 3D convolution, BN layer, ReLU layer를 순차적으로 적용한다. 이 layer들은 점진적으로 확장하는 수용 범위(receptive field)내에서 voxel 단위의 피쳐들을 집계하여 형상 설명에 더 많은 내용을 추가한다. 필터들의 자세한 설명은 section 3에서 더 설명할 것이다.2.1.3 Region Proposal Network최근에, 지역 제안 네트워크는 최고 성능의 객체 탐지 프레임워크의 중요한 구성 요소가 되었다. 이 논문에서는 RPN 아키텍쳐에서 몇가지 중요한 수정을 하고, 이를 feature learning network와 convolutional middle layer와 결합하여 end-to-end로 훈련가능한 파이프라인을 형성할 것이다.이 논문에서의 RPN의 입력은 convolutional middle layer에서 제공된 피쳐 맵이다. 아키텍쳐는 아래 그림에서 자세히 볼 수 있다.이 네트워크는 fully convolutional layer의 3가지 블록이 있다. 각 블록의 첫번째 layer는 stride가 2인 convolution을 통해 특징맵을 1/2 크기로 다운샘플링한 다음, stride 1인 convolution을 차례로 진행한다. 각 convolution layer이후에, BN과 ReLU 작업을 수행한다. 다 진행하고 나면 모든 블록이 고정된 사이즈의 출력으로 업샘플링되고, 고해상도 피쳐 맵을 구성하기 위해 이들을 결합한다. 이 피쳐 맵은 (1) 확률 score 맵, (2) regression 맵에 각각 매핑된다.2.2 Loss Functionpositive anchor를 Npos와 negative anchor를 Nneg에 대해 각각그리고 3D GT(ground Truth) box를 (xc^g,yc^g,zc^g,l^g,w^g,h^g,θ^g)라 할 때, 각 파라미터에 대해 다음과 같이 설명한다. θ^g: Z축을 중심으로 회전 각도 (이때 z축을 중심으로 회전하는 것을 yaw rotation이라 하고, x,y축을 중심으로 회전하는 것을 각각 roll, pitch rotation이라 함) (l^g,w^g,h^g): box의 길이,너비,높이 (xc^g,yc^g,zc^g): 중심 좌표(xc^g,yc^g,zc^g,l^g,w^g,h^g,θ^g)로 정의되는 poisitive anchor과 GT box를 매칭시키기 위해, 중심 좌표(∆x, ∆y, ∆z)와, 3차원(∆l,( ∆w, ∆h), 회전(∆θ) 에 해당하는 7개의 regression targets를 포함하는 잔류 벡터인 u* ∈ R^7를 정의한다. 이 때, 각 파라미터는 다음과 같이 계산한다.이 때, d^a = ((l^a)^2 + (w^a)^2)^(1/2) 는 앵커 박스 밑면의 대각선을 의미한다. 저자는 oriented 3D box를 직접 추정하고, ∆x, ∆y를 대각선 d^a로 균일하게 정규화하는 것을 목표로 하고 있다. 그리고 손실 함수는 다음과 같이 정의한다.이때, pi^pos와 pj^neg는 각각 positive anchor ai^pos와 negative anchor aj^pos에 해당하며, ui ∈ R^7와 ui* ∈ R^7은 각각 regression 출력과 positive anchor ai^pos에 대한 GT에 해당한다.위의 손실함수를 세분화하여 따져보면,앞의 2개 텀을 보게 되면, Lcls는 binary cross-entropy loss를 나타내고, α, β는 상대적 중요도의 균형을 유지하는 양의 상수, 즉 가중치를 나타낼 때, 각각 {ai^pos}i=1…Npos 와 {aj^neg}j=1…Nneg 에 대한 정규화된 classification loss이다.마지막 텀에서 Lreg는 smoothL1 함수를 사용한 regression loss이다.2.3 Efficient ImplementationGPU들은 빽빽한 텐서 구조를 프로세싱하는데 최적화되어 있다. point cloud를 직접적으로 작업하는데에 있어서 문제점은 공간에서 점들이 희박하게 분포되어 있고, 각 voxel은 점들에 대한 가변적인 수를 가지고 있기 때문이다. 그래서 저자는 점들와 voxel들을 병렬로 처리할 수 있는 stacked VFE layer를 통해 point cloud를 고밀도 텐서 구조로 변환하는 방법을 고안했다. 이에 대한 그림은 아래에 있다.저자는 K x T x 7차원의 텐서 구조를 초기화하고, 여기에 비어 있지 않은 voxel들의 최대값을 K에, 각 voxel의 점의 최댓값을 T에, 각 점에 대해 입력 인코딩 차원을 의미하는 7을 voxel 입력 피쳐 버퍼를 저장한다. 그 점들은 프로세싱 전에는 무작위로 된다. point cloud의 각 점에 대해 해당voxel이 이미 존재하는지를 점검해봐야 한다. 이 점검 작업은 O(1)에서, voxel 좌표가 hash key로 사용되는 hash 테이블을 사용하여, 효율적으로 수행된다. 만약 voxel이 이미 초기화된 경우, 점의 갯수가 T보다 작으면 voxel에 점을 삽입하고, 그렇지 않으면 그 점은 무시한다. 그러나 voxel이 초기화되지 않았다면, voxel을 초기화하고, 이 voxel좌표를 voxel 좌표 버퍼에 저장하고, 이 voxel에 점을 삽입한다. voxel 입력 피쳐와 좌표 버퍼는 점 목록을 통과하여 구성될 수 있고, 그래서 그것의 복잡도는 O(n)이 된다. 메모리/계산 효율을 더 높이기 위해 제한된 voxel의 수 K만을 저장하고, 점이 별로 없는 voxel에 대한 점들은 무시할 수도 있다.voxel 입력 버퍼가 구성된 후에 쌓여있는 VFE는 GPU에서 병렬로 계산할 수 있는 point 레벨과 voxel 레벨 밀도 연산만 포함한다. VFE에서 결합 작업이 끝난 후에 점이 없는 곳에 해당하는 피쳐를 0으로 설정하여 계산된 voxel 피쳐에 영향을 미치지 않도록 한다. 마지막으로, 저장된 좌표 버퍼를 이용하여 계산된 희박한 voxel 단위의 구조들을 밀집 voxel grid로 재구성한다. 이렇게 되면, 다음에 오는 convolutional middle layer와 RPN 작업은 GPU에서 효율적으로 동작할 수 있는 밀집된 voxel grid에서 작동하게 된다.3. Training Details이번에는 VoxelNet의 구현 세부 정보와 트레이닝 절차를 설명한다.3.1 Network Details실험은 KITTI 데이터 셋중 LIDAR 셋을 기반으로 한다. Car Detection이 태스크에서의 셋팅은 다음과 같다. X,Y,Z축에 대해 [0,70.4] x [-40,40] x [-3,1] meters 범위 안에 있는 point cloud를 고려 이미지 경계 범위 밖에 투영된 점들은 제거 voxel 사이즈 vD = 0.4, vH = 0.2, vW = 0.2를 선택했으며, 이에 따른 D’ = 10, H’ = 400, W’ = 352로 정함 비어 있지 않은 각 voxel에 대해 무작위로 샘플링된 점들의 최대 수로서, T = 35로 세팅 VFE-1(7,32)와 VFE-2(32,128) layer를 사용 마지막 FCN은 VFE-2 출력을 R^128에 매핑 sparse tensor = (128 x 10 x 400 x 352) voxel 단위로 피쳐를 집계하기 위해 순차적으로 3개의 convolution middle layer을 적용하여 (64 x 2 x 400 x 352)의 4D 텐서 사이즈를 산출 =&amp;gt; Conv3D(128, 64, 3,(2,1,1),(1,1,1)), Conv3D(64,64,3,(1,1,1),(0,1,1)), Conv3D(64,64,3,(2,1,1),(1,1,1))reshape 후에, RPN의 입력은 각 channel, height, width의 차원에 대해 (128 x 400 x 352)의 사이즈의 3D 텐서 피쳐 맵이다. 아래의 그림을 통해 아키텍처를 더 자세하게 볼 수 있다.또한, l^a = 3.9, w^a = 1.6, h^a = 1.56 meters이고, 중심이 zc^a = -1.0 meter에 있으며, 0과 90도의 회전을 가지는 1개의 앵커 박스만을 사용한다. 앵커는 조감도에서 GT와 IOU가 가장 높거나 GT와의 IOU가 0.6 초과인 것에 대해서는 양의 값을 가진다. 또한, GT와 IOU가 0.45 미만인 것들은 음의 값을 가진다. 그 사이의 값들, 즉 0.45 &amp;lt;= IOU &amp;lt;= 0.6 에 대해서는 고려하지 않는다. 그리고, 손실 함수에서의 α = 1.5, β = 1로 둔다. Pedestrian and Cyclist Detection이 태스크의 셋팅은 다음과 같다. 입력 범위는 X,Y,Z축에 대해 [0,48] x [-20,20] x [-3,1] meters의 범위로 둔다. voxel 크기는 car detection과 동일하게 D = 10, H = 200, W = 240 형상 정보를 더 잘 포착하기 위해, 더 많은 LIDAR 점을 얻을 수 있는 T = 45로 한다. feature learning network와 convolutional middle layer는 car detection에서 사용되는 네트워크와 동일하게 둔다.RPN에서는 fig4에서와 같이 첫 2D convolution에서의 stride 사이즈를 2에서 1로 바꿈으로써 1개의 블록으로 수정한다. 이를 통해 보행자 및 자전거 이용자를 감지하는데 필요한 앵커 박스에서 보다 미세한 해상도를 얻을 수 있다.보행자에 대해서는 l^a = 0.8, w^a = 0.6, h^a = 1.73 meters, 중심은 zc^a = -0.6 meters에 위치하며, 0과 90도의 회전을 가지는 anchor box를 사용하고, 자전거 이용자에 대해서는 l^a = 1.76, w^a = 0.6, h^a = 1.73 meters, zc^a = -0.6 meters의 파라미터를 가지고, 0과 90도의 회전을 가지는 anchor box를 사용한다.GT와의 IOU가 0.5 초과이면 양, 0.35 미만이면 음의 값을 가지며, 0.35 &amp;lt;= IOU &amp;lt;= 0.5의 값들은 고려하지 않는다.훈련시에는 lr = 0.01, 150 epochs에 대한 SGD(stochastic gradient descent)을 사용하고, 그 다음 lr = 0.001, 10 epochs에 대한 SGD를 적용했다. 또한 batch size는 16으로 지정했다.3.2 Data Augmentation4000개 미만의 training point cloud에서는 네트워크를 처음부터 훈련하면 과적합이 발생할 수 있으므로 데이터 증강을 통해 과적합을 방지하고자 한다. 본 논문에서는 총 3가지의 데이터 증강 방법을 소개한다. 증강된 훈련 데이터들은 디스크에 저장할 필요 없이 즉시 생성된다.N개의 점들로 구성된 전체 point cloud M에 대해 다음과 같이 정의할 수 있다.그리고, 3D bounding box, bi에 대해 (xc,yc,zc)의 중심 좌표를 가지고, (l,w,h)의 각각 길이,너비,높이, Z축을 중심으로 회전하는 각도 θ에 대한 (xc,yc,zc,l,w,h,θ)로 정의하고, 전체 셋 M안에서 특정 LIDAR 점을 나타내는 p = [x,y,z,r]과 bi안에 모든 LIDAR 점들을 포함하는 셋으로서 Ωi을 다음과 같이 정의할 수 있다.데이터 증강의 첫번째 방법은 독립적으로 각 GT 3D bounding box와 그 박스안의 LIDAR점들에게 작은 변화를 주는 것이다. 특히, Z축을 중심으로 균등하게 분포되어 있는 무작위 가변적인 ∆θ ∈ [-π/10, +π/10]에 의해, (xc,yc,zc)에 대해 bi와 그와 관련된 Ωi를 회전시킨다. bi의 X,Y,Z 성분과 Ωi의 각 점에 (∆x, ∆y, ∆z)을 추가한다. 여기서 (∆x, ∆y, ∆z)는 평균 0, 표준편차 1.0을 갖는 가우스 분포로부터 독립적으로 나타내진다. 물리적으로 불가능한 결과를 피하기 위해, 추가한 후에 모든 두 상자 사이에 대한 충돌 테스트를 진행하고, 충돌이 감지되면 추가하기 전의 원래의 값으로 되돌린다. 이 작은 변화가 각 GT box와 그와 연관된 LIDAR 점들을 독립적으로 적용시키기 때문에, 네트워크는 원래 훈련 데이터보다 훨씬 더 많은 가변성을 배울 수 있다.두번째 방법은, 모든 point cloud M과 모든 GT box들에 전역 스케일링을 적용한다. 특히, X,Y,Z 좌표와 각 bi의 3개의 차원들, M에 있는 모든 점들의 X,Y,Z 좌표에 대한 균일한 분포[0.95,1.05]로부터 도출된 무작위 변수를 곱한다. 전역 스케일링 증강은 다양한 사이즈와 거리에 대한 객체 검출 네트워크의 강인함을 개선할 수 있다.마지막으로, 모든 GT box bi와 모든 point cloud M를 전역 회전시킨다. 회전은 Z축과 (0,0,0)에 대해 적용된다. 이 회전 범위는 균일한 분포[−π/4, +π/4]로부터 샘플링된다. 모든 point cloud를 회전함으로써, 좌/우회전하는 차량을 시뮬레이션할 수 있다.4. Experiments차, 보행자, 자전거 이용자에 대해서만 분류하고, 7481개의 훈련 이미지/point cloud와 7518개의 테스트 이미지/point cloud로 구성된 KITTI 3D Object detection benchmark에서 VoxelNet를 평가했다. 각 클래스마다 검출 결과는 객체 사이즈, 폐색 사이즈, 잘림 수준등에 따른 3가지 난도인 easy, moderate, hard에 기반하여 평가했다. 테스트셋의 GT는 이용할 수 없고, 테스트 서버는 제한되어 있기 때문에 훈련 데이터를 훈련과 검증 데이터셋으로 분할하여 3712개의 훈련 데이터 샘플, 3769개의 검증 데이터 샘플을 얻었다.차량 카테고리에서 이미지 기반의 접근 방식인 Mono3D, 3DOP과 LIDAR 기반의 접근 방식인 VeloFCN, 3D-FCN, 그리고 멀티모달 접근 방식인 MV를 포함한 높은 수행 알고리즘과 비교했다. Mono3D와 3DOP, MV는 초기화를 위한 pretrain model을 사용했지만, VoxelNet은 KITTI에서 제공하는 LIDAR 데이터만을 사용하여 훈련시켰다.end-to-end 학습의 중요성을 분석하기 위해 VoxelNet 아키텍쳐을 기본으로 하나, 위에서 제안한 feature learning network 대신에 수작업 제작된 피쳐들을 사용하는 강력한 베이스라인을 사용했다. 그래서 이 모델을 the hand-crafted baseline 이라 하여 HC-baseline라고 명명했다. HC-baseline은 MV3D에서처럼 0.1m의 해상도로 계산된 조감도 피쳐를 사용한다. 그러나 MV3D와 다른 점은 높이 채널의 수를 4에서 16으로 증가시켜 더 자세한 형상 정보를 얻는다. 그러나 이는 성능 개선으로는 이어지지 않았다. 또한, VoxelNet의 convolutional middle layer를 간단한 2D convolutional layer로 바꿨다. 2D convolutional layer는 Conv2D(16,32,3,1,1), Conv2D(32,64,3,2,1), Conv2D(64,128,3,1,1)로 구성되어 있다. 마지막으로 RPN은 voxelNet과 HC-baseline이 동일하다. VoxelNet과 HC-baseline의 총 파라미터의 수는 매우 간단하다. HC-baseline도 위의 VoxelNet 훈련 프로세스와 동일하게 진행했다.4.1 Evaluation on KITTI validation set Metrics차 IOU threshold는 0.7, 보행자와 자전거 이용자 IOU threshold는 0.5를 적용하는 공식적인 KITTI 평가 프로토콜을 따랐다. 그 IOU threshold는 조감도나 full 3D 평가에서도 동일하다. 많은 방법들과 AP(average precision)을 비교했다. Evaluation in Bird’s eye view평가 결과는 아래 테이블과 같다.VoxelNet은 모든 비교 접근 방식들보다 모든 난도에 대해 능가했다. HC-baseline도 최첨단 기술인 MV3D와 비교해도 만족스러운 성능을 보인걸로 보아 본 논문의 RPN이 효과적이라는 것을 알 수 있다. RPN을 제외하고는 다 다르게 세팅했기 때문이다. 조감도안에서 보행자와 자전거 이용자 검출 태스크에서 VoxelNet과 HC-baseline을 비교했다. VoxelNet은 까다로운 범주들에 대해서 HC-baseline보다 상당히 높은 AP를 산출했고, 이는 end-to-end 학습이 point cloud 기반의 탐지에 필수적이라는 것을 볼 수 있다. Evaluation in 3D2D 평면안에서 객체의 정확한 localization만을 요구하는 조감도 탐지를 비교할 때, 3D 탐지는 3D 평면안에서 형상의 미세한 localization을 요구하는 더 까다로운 작업이다.차 클래스에 대해서 VoxelNet은 모든 난도에 대해서 다른 모든 방법들보다 AP가 월등하게 높았다. 특히 LIDAR 데이터만 사용할 때, VoxelNet은 최신 기술인 LIDAR + RGB의 MV(BV+FV+RGB)를 기반으로 하는 기술들을 easy,moderate,hard 각각 10.68%, 2.78%, 6.29%의 차이를 보이며 월등하게 능가했다. HC-baseline은 MV 방법과 비슷한 성능을 보였다.조감도 평가와 같이 3D 보행자와 자전거 이용자 탐지를 HC-baseline과 VoxelNet을 비교했다. 3D 포즈와 형상의 가변성 때문에 이 두가지 카테고리의 성공적인 탐지는 더 나은 3D 형상 표현을 요구한다. table2에서 볼 수 있듯이 3D 탐지에서 최대 약 12%, 조감도 측면에서 최대 약 8%나 더 높음으로써 수작업 제작보다 VoxelNet에서 3D 형상 표현을 잡아내는 것이 더 효과적이라는 것을 볼 수 있기 때문에, VoxelNet의 개선된 성능은 3D 탐지 작업에서 더 강조된다.4.2 Evaluation on KITTI tset set공식 서버에 검출 결과를 제출하여 KITTI 테스트 셋에서의 VoxelNet을 평가했다. 이 결과는 아래 테이블에 요약되어 있다.VoxelNet은 모든 task(조감도, 3D 탐지)와 모든 난도(easy,moderate,hard)에서 이전의 공개된 MV3D과 같은 최첨단 기술들보다 월등하게 높았다. VoxelNet은 LIDAR만 적용했지만, 나머지 LIDAR과 RGB를 융합한 모델보다 더 월등한 성능을 보일 수 있었다.LIDAR를 사용한 3D box 탐지를 더 잘 보이게 하기 위해, fig6에서처럼 RGB 이미지로 투영해서 나타냈다.TitanX GPU와 1.7Ghz CPU에서 VoxelNet의 추론 시간은 33ms인데, 입력 피쳐 계산은 5ms, convolutional middle layers는 1ms, region proposal network는 11ms 걸렸다.5. Conclusion현존하는 대부분의 LIDAR 기반의 탐지는 조감도 투영과 같이 수작업 피쳐 제작에 의존하지만, 본 논문에서는 수작업 제작으로 인한 병목 현상을 제거하고 point cloud 기반의 3D 탐지를 위한 새로운 end-to-end로 훈련이 가능한 심층 아키텍처인 VoxelNet을 제안했다. 이 논문에서의 접근 방식은 희박한 3D 점들을 직접적으로 작업할 수 있고, 3D 형상 정보를 효과적으로 잡을 수 있다. 또한, voxel grid에서의 병렬 프로세싱과 point cloud의 희소성으로부터 이익을 줄 수 있는 VoxelNet의 효과적인 구현을 제시했다. KITTI 차 탐지 benchmark에서의 실험은 VoxelNet이 최첨단의 LIDAR 기반의 3D 탐지 방법들을 큰 차이로 능가한다는 것을 보여준다. 보행자와 자전거 이용자에 대한 3D 탐지와 같은 더 까다로운 작업에서 VoxelNet은 더 나은 3D 표현을 제공하는 좋은 결과를 가져왔다.Reference VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection" }, { "title": "[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving", "url": "/posts/multiview/", "categories": "Review, Autonomous Driving", "tags": "Autonomous Driving, MV3D", "date": "2022-01-17 13:00:00 +0900", "snippet": "Multi-View 3D Object Detection Network For Autonomous Driving 논문에 대한 리뷰한 글입니다. 이 논문은 2017 CVPR에 투고된 논문이며 약 1594회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.Abstract이 논문은 자율주행 시나리오에서 3D Object Detection의 높은 정확도를 목표로 하고 있다. 저자는 LIDAR point cloud와 RGB 이미지를 입력으로 하여 3D object 경계 상자를 예측하는 센서 퓨전 프레임워크인 MV3D(Multi-View 3D networks)를 제안했다. 또, 소량의 다중 뷰(multi-view)만으로 3D point cloud를 인코딩했다. 네트워크는 3D object 제안 생성을 위한 네트워크와, multi-view 특징 퓨전을 위한 네트워크로 구성되어 있다. 제안 네트워크(proposal network)는 3D point cloud를 조감도(bird`s eye view)적 표현을 통해 3D 후보 box(3D candidate box)를 생성한다. 저자는 여러 관점(view)에서 지역별 특징을 결합하고, 서로 다른 경로의 중간 레이어들 간의 상호 작용을 가능하게 하기 위해 깊은 결합 체계를 설계했다. KITTI benchmark에서의 실험은 저자의 접근 방식이 3D localization과 3D Detection 태스크에서 25~30% AP정도 능가했다고 한다. 게다가, 2D detection에서는 LIDAR 기반의 최신 기술보다 14.9% AP가 더 높았다.1. Introduction3D object detection는 자율주행차에 시각적 인지 시스템에서 중요한 역할을 한다. 최근 자율주행 차는 보통 LIDAR과 카메라와 같은 다중 센서를 탑재하고 있다. 레이저 스캐너는 정확한 깊이 정보를 제공하는 반면 카메라는 훨씬 더 디테일한 의미 정보를 보존한다. LIDAR point cloud와 RGB 이미지의 융합은 자율주행 차에서 더 높은 수행과 정확도를 성취하게 될 것이다.이 논문의 중점은 LIDAR와 image data 모두 활용한 3D Object Detection이다. 그래서 저자는 도로 상에서 3D localization과 물체 인지의 높은 정확도를 목표로 한다. 최근 LIDAR 기반의 방법론들은 3D voxel grid에 3D window를 배치하여 point cloud에 점수를 매기거나, dense box prediction 체계에서 전면 뷰 point 맵에 convolution network를 적용한다. image-based 방법론들은 대체로 먼저 3D box proposal들을 생성한 후 Fast R-CNN 파이프라인을 사용한 region-based 인지를 수행한다. LIDAR point cloud 기반의 방법론들은 보통 더 정확한 3D 위치를 성취하는 반면 이미지 기반의 방법들은 2D box 평가에 관한 더 높은 정확도를 가진다. 초기 또는 후기 융합 방식을 채택함으로써 2D dection을 위해 LIDAR과 이미지를 융합한다. 그러나 3D object detection 태스크에서는 여러 양식의 강점을 갖도록 더 잘 설계된 모델이 필요하다.이 논문에서, 다방면의 데이터를 입력으로 받고 3D 공간에서 3D 물체를 예측하는 MV3D(Multi-View 3D object detection network)을 제안한다. 다방면의 정보를 활용하는 중심 아이디어는 지역기반의 특징 융합을 수행하는 것이다. 먼저 3D point cloud의 작고 효과적인 표현을 얻기 위한 체계를 인코딩하는 multi-view를 제안한다.이 그림에서 볼 수 있듯이 multi-view 3D dectection 네트워크는 3D 제안 네트워크, Region-based 융합 네트워크 2개로 구성되어 있다. 3D 제안 네트워크는 더 정확한 3D 후보 상자들을 발생시키기 위한 새 관점에서의 표현을 활용한다. 3D object 제안의 이점은 3D 공간에서 모든 뷰들을 투영할 수 있다는 것이다. multi-view 융합 네트워크는 다중 뷰들로부터 특징맵에 대한 3D 제안들을 투영함으로써 지역별 특징을 추출한다. 다른 뷰들로부터 중간 레이어들 간의 상호 작용을 가능하게 하기 위해 깊은 융합 방식을 설계했다. drop-path training과 보조 손실이 결합된 방식이 초기/후기 융합 체계에 비해 우수한 성능을 보여준다. 다중 뷰 특징 표현을 고려할 때, 네트워크는 3D 공간에서 물체의 정확한 3D 위치, 크기 및 방향을 예측하는 oriented 3D box regression을 수행한다.저자는 까다로운 KITTI object detection benchmark에서 3D 제안 생성, 3D localization, 3D detection, 3D detection 수행에 대한 접근 방식을 평가했다. 실험은 우리의 3D 제안이 최근 3D 제안 방법론인 3DOP과 Mono3D를 엄청 능가했다는 것을 보여준다. 특히, 300개의 제안에서는 IoU threshold가 0.25일때는 99.1%, 0.5일 때는 91% 3d recall을 얻는다. 이 논문의 LIDAR 기반의 방식은 3D localization task에서는 25% 더 높은 정확도를, 3D object detection task에서의 3D AP(average precision)에서는 30% 더 높은 정확도를 성취했다. 이것은 까다로운 KITTI test 데이터셋에서의 2D detection에 대한 14.9% AP를 가지는 다른 LIDAR 기반의 방법론를을 능가했다. 이미지와 융합한다면, LIDAR 기반의 결과에 대한 추가적인 개선이 가능하다.2. Related Workpoint cloud나 이미지에서 3D object detection, 다양한 융합 방법 및 3D object proposals에 대한 기존 연구를 간략히 검토하고자 한다.3D object detection in point cloud대부분의 존재하는 방법들은 voxel grid 표현에서의 3D point cloud를 인코딩한다. sliding Shapes와 Vote3D는 SVM 분류기를 사용하여 지역 특징을 인코딩한 3D grid들을 계산한다. 최근 제안된 방법들은 3D convolution을 통한 특징 표현을 개선한다. 하지만 비싼 계산을 요구한다. 3D voxel 표현 외에도, VeloFCN은 FV(Front view)에 대한 point cloud를 투영하여 2D point map을 얻는다. 여기서는 2D point map에 FCN(fully convlutional network)를 적용하고, convolutional feature map으로부터 3D box들을 예측한다. 3D 객체 분류를 위한 point cloud의 크기와 다중 뷰 표현을 조사한다. 본 연구에서는 멀티 뷰 feature map으로 3D point cloud를 인코딩하여 다양한 융합을 위한 지역 기반 표현을 가능하게 한다.3D Object Detection in Images3DVP는 3D voxel 패턴을 소개하고, 2D detection과 3D pose estimation을 할 수 있는 ACF detector를 생산한다. 3DOP는 스테레오 이미지들로부터 Depth를 재구성하고, 에너지 최소화 접근 방식을 사용하여 3D box proposal을 생성하고, 이는 객체 인식을 위해 R-CNN 파이프라인에 공급된다. Mono3D는 3DOP와 같은 파이프라인을 구성하고 있고, 이것은 monocular images(단안상)으로부터 3D 제안들을 발생시킨다. 저자의 실험을 통해 3D localization을 개선하기 위한 LIDAR point cloud를 통합시키는 방법을 볼 수 있을 것이다.Multimodal Fusion자율주행에서 오직 소수의 모델들만이 다양한 양식의 데이터를 사용한다. 저자는 FractalNet과 Deeply-Fused Net을 기반으로 한 깊은 융합 방식을 설계한다. FractalNet에서, 기본 방식은 기하급수적으로 증가하는 경로를 통해 네트워크를 구성하기 위해 반복한다. 간단하게, 얕은 서브 네트워크들과 깊은 서브 네트워크들을 융합함으로써 deeply-fused network를 구성하는 것이다. 이 논문의 네트워크는 각 열마다 동일한 베이스 네트워크를 사용하지만 정규화를 위한 보조 경로와 손실을 추가한다는 점에서 두 네트워크와 다르다.3D Object proposals2D object proposals와 유사하게, 3D object proposal 방법들은 3D 공간에서 객체 대부분을 복사하기 위해 작은 3D 후보 상자들을 생성한다. 이것을 위해 3DOP는 스테레오 point cloud의 일부 depth 특징들을 설계하여 큰 3D 후보 상자들을 점수매긴다. Mono3D는 ground 평면을 먼저 활용하고, 일부 분할 특징을 사용하여 단일 이미지에서 3D proposal을 생성한다. Deep Sliding Shapes는 더 강력한 딥러닝 특징들을 사용한다. 그러나, 이는 계산적으로 비싼 3D convolution을 사용하고 3D voxel grid안에서 작동된다. 그래서 bird`s eye view 표현을 생산하고, 더 정확한 3D proposal 발생시키기 위한 2D convolution을 생산함으로써 더 효과적인 접근 방식을 제안한다.3. MV3D NetworkMV3D 네트워크는 이미지와 3D point cloud를 입력으로 하여 다중 뷰를 표현한다. 이는 먼저 bird`s eye view map와 region 기반의 표현을 통해 깊게 융합한 다중 뷰 특징들로부터 3D object proposal을 생성한다. 이 융합된 특징들은 classification과 3D box regression에 사용된다.3.1 3D point cloud Representation현존하는 모델들은 보통 3D LIDAR point cloud를 3D grid나 FV(front view) map로 인코딩한다. 대부분의 point cloud의 기초적인 정보들은 3D grid 표현들을 보존하는데, 이는 그 다음의 특징 추출을 위해 더 복잡한 계산을 요구하게 된다. 그래서 저자는 아래 그림에서 볼 수 있듯이 bird`s eye view와 FV에 대한 3D point cloud를 투영함으로써 더 간단한 표현을 제안한다. Bird’s eye view representationbird’s eye view, 즉 조감도는 height(높이), intensity(강도), density(밀도)로 인코딩되어 있다. 저자는 투영된 point cloud를 0.1m의 해상도의 2D grid로 나눈다.각 셀의 height 특징들은 셀 안의 point들의 최대 높이로 계산되어 있다. 좀 더 자세한 높이 정보를 인코딩하기 위해, point cloud는 M개의 slices로 균등하게 나누어진다. height map은 각각의 slice로 계산되어 있기 때문에 M height maps를 얻을 수 있다. intensity 특징들은 각 셀의 최대 높이를 가지는 점의 반사도이다. density는 각 셀의 점들의 갯수를 나타낸다.특징들을 정규화시키기 위해 min(1.0, log(N+1)/log(64)) 로 계산되는데 이때 N은 셀 안의 점들의 갯수를 나타낸다. 이 때, 중요한 것은 height feature은 M개의 slice에 의해, 즉 각 셀을 계산하는 반면, intensity와 density feature은 전체 point cloud에 대해 계산된다. 그래서 전체 조감도 map은 (M+2) - channel로 계산된다. Front View Representation전방 뷰는 조감도 표현에 대한 상호 보완적인 정보를 제공한다. LIDAR point cloud가 매우 밀도가 희박함에 따라, 이미지 평면에 이를 투영하는 것은 밀도가 희박한 2D point map를 얻는다. 따라서 밀도있는 전방 뷰 map을 발생시키기 위해 평면 대신 실린더 면(원통 면)에 투영한다. 3D point, p = (x,y,z)라고 할 때, 전방 뷰에 대한 point 표현은 pfv = (r,c)이고, r과 c에 대한 계산은 다음과 같다.∆ϴ와 ∆ø는 각각 레이저의 수평, 수직에 대한 표현이다. fig2애서 볼 수 있듯이 저자는 height, distance, intensity의 3채널 특징을 통해 FV map을 인코딩한다.3.2 3D Proposal Network최첨단 2D object detector의 핵심 구성요소인 RPN(Region Proposals Network)에서 영감을 받아, 먼저 3D object proposal을 발생시키는 네트워크를 설계했다. 일단 조감도 맵을 입력으로 사용한다. 3D object detection에서 조감도 맵은 전방 뷰나 이미지 평면에 비해 몇 가지 장점이 있다. 먼저, 객체는 사이즈 변화가 작은 조감도 맵으로 투영했을 때 물리적 사이즈가 보존된다. 두번째로, 조감도 맵에서의 물체는 서로 다른 공간을 차지하기 때문에 occlusion 문제를 피할 수 있다. 세번째로 로드 뷰에서 물체는 대부분 지면에 놓여있거나 수직적 위치의 변화가 작기 때문에, 조감도 맵은 더 정확한 3D bounding box를 얻을 수 있다. 따라서 조감도 맵을 사용하면 3D 위치 예측이 더욱 실현 가능해진다.네트워크는 3D prior boxes로부터 3D box proposals를 발생시킨다. 각 3D box는 (x,y,z,l,w,h)로 구성되어 있고, 각각 LIDAR 좌표계에서의 3D box의 중심 좌표(x,y,z)와 사이즈(meters)(l,w,h)를 나타낸다. 각 사전 box(prior box)에 해당하는 조감도 anchor(xbv, ybv, lbv, wbv)는 (x,y,l,w)를 통해 얻을 수 있다. 훈련 셋에서 GT(Ground Truth) object size를 군집 분류함으로써 N개의 3D prior boxes를 설계한다. 자동차 감지의 경우, (l,w)의 prior box는 {(3.9, 1.6), (1.0, 0.6)}의 값을 가지며, 높이 h는 1.56m를 가진다. 조감도 앵커를 90도 회전함으로써, N = 4의 prior boxes를 얻는다. (x,y)는 조감도 특징 맵에서의 위치이고, z는 카메라 높이와 물체 높이에 의해 계산된다. 저자는 제안 생성(proposal generation)에서 방향 회귀(orientation regression)를 하지 않고, 다음 예측 단계로 넘어간다. 3D box의 방향은 대부분의 로드 뷰에서의 물체의 실제 방향에 가까운 0~90도로 제한되어 있다. 이 간단함은 제안 회귀의 훈련을 쉽게 만들 것이다.0.1m 해상도에 따라, 조감도의 물체 박스들은 오직 5~40 픽셀만 차지한다. 초소형 물체를 감지하는 것은 아직 어려운 문제지만, 하나의 해결책이 있다면 입력을 높은 해상도로 하는 것이다. 그러나 이는 더더욱 큰 계산을 요구한다. 그래서 A unified multi-scale deep convolutional neural network for fast object detection 논문의 업샘플링 방법을 채택하여, proposal network안의 마지막 convolution layer이후에, 2배 이선형(bilinear) 업샘플링을 사용한다. front-end convolution은 3번의 pooling작업을 하여 1/8배 다운샘플링을 진행한다. 이를 2배 업샘플링과 결합하여 proposal network에 제공된 특징 맵은 조감도 입력과 관련하여 1/4배 다운샘플링된다.RPN과 유사하게 t = (∆x,∆y,∆z,∆l,∆w,∆h)에 대해 3D box regression한다. (∆x,∆y,∆z)는 anchor 크기에 의해 정규화된 중심 좌표이고, (∆l,∆w,∆h)는 다음과 같이 계산된 값이다.물체/배경을 동시에 분류하고, 3D box regression을 수행하기 위해 multi-task loss를 사용한다. 특히, objectness loss와 smooth l1에 대한 cross-entropy를 사용하여 3D box regression loss를 구한다. box regresson loss를 구할 때는 배경 anchor는 무시한다.훈련하는 동안에, anchors과 GT 조감도 boxes들 사이의 IoU를 계산한다. IoU가 0.7이상이면 anchor가 positive이고, 0.5미만이면 negative가 된다. 그 사이의 값은 무시한다. LIDAR point cloud data가 많이 빈 anchor가 많기 때문에, 훈련과 테스트에서 계산을 줄이기 위해 모든 빈 anchor들을 제거한다.마지막 convolution 특징 맵의 각 위치에서의 비지 않은 anchor에서 네트워크가 3D box를 발생시킨다. 불필요한 반복을 줄이기 위해, NMS(Non-Maximum Suppression)을 조감도 박스들에 적용시킨다. 물체가 지면에서 각각 다른 공간을 차지해야 하기 때문에 3D NMS는 적용하지 않았다.저자는 NMS를 위해 0.7 threshold의 IoU를 사용한다. 훈련에는 2000개의 top boxes를, 테스트에서는 300개의 top boxes를 유지시킨다.3.3 Region-based Fusion Network저자는 다중 뷰로부터 특징들을 효과적으로 결합하고, 객체 제안들을 분류하고, oriented 3D box regression을 하기 위해 region-based fusion network를 사용한다. Multi-View ROI Pooling다른 뷰/방식들에 대한 특징들은 다른 해상도들을 가지고 있기 때문에, 같은 길이에 대한 특징 벡터를 얻기 위해 각각의 뷰에 ROI pooling을 적용한다. 생성된 3D proposals를 고려할 때, 3D 공간에서 모든 뷰에 대해 투영할 수 있게 된다. 이 논문에서는 BV(bird’s eye view), FV(front view), RGB(image plane) 총 3개의 뷰에 대해 투영하였다.3D proposal, P3D를 고려할 때, T3D -&amp;gt; v는 각각 LIDAR 좌표계에서 조감도, 정면 뷰, 이미지 평면으로의 변환 기능을 나타내고, 위의 식에 따라 각 뷰에 대해 ROI를 얻을 수 있다. 각 뷰의 front-end network로부터 입력 특징 맵 x를 고려하여 ROI pooling에 의해 고정된 길이 특징,fv를 얻는다. Deep Fusion다른 특징들과 정보를 결합하기 위해, 사전 작업은 보통 초기 결합이나 후기 결합을 사용한다. 하지만 저자는 다중 뷰 특징을 계층적으로 융합하는 deep fusion 방식을 적용한다. 초기/후기 결합 네트워크와 deep fusion 네트워크 아키텍쳐의 비교는 위의 그림에서 볼 수 있다.L개의 layers를 가진 네트워크의 경우, 초기 결합은 입력 단계에서 다중 뷰들의 특징, fv들을 결합한다. 이 떄, {Hl,l = 1,…L} 은 특징 변환 함수이고, ⊕는 concatenation, summation과 같은 결합 연산이다.대조적으로, 후기 결합은 별도의 서브 네트워크를 사용하여 특징 변환을 독립적으로 수행하고 예측 단계에서 출력을 결합한다.그러나, 이 논문에서는 다른 뷰들에서 중간 레이어의 특징들 사이에 더 많은 상호작용을 가능하게 하기 위해 다음과 같은 deep fusion 프로세스를 설계했다. drop-path 훈련과 결합할 때 더 유연하기 때문에 이 논문에서는 deep fusion을 위한 결합 작업에 요소별 평균을 사용한다. Oriented 3D box Regression다중 뷰 네트워크의 특징 결합을 고려할 때, 3D 제안들로부터 oriented 3D boxes를 회귀한다. 특히, regression(회귀) 타겟들은 3D boxes에 대한 8개의 모서리들이다. (t = (∆x0,…,∆x7,∆y0,…,∆y7,∆z0,…,∆z7)). 제안 박스의 대각선의 길이에 의해 정규호된 모서리 좌표들로 인코딩되어 있다. 총 24D 벡터 표현은 oriented 3D box를 나타내는 데 중복되지만, 저자는 이 인코딩 접근 방식이 축 정렬 3D box로 회귀하는 중심 및 크기 인코딩 접근 방식보다 더 잘 작동한다는 것을 발견했다고 한다. 이 논문의 모델에서, 객체 방향은 예측된 3D box 모서리로부터 계산될 수 있다. 우리는 객체 카테고리 및 oriented 3D boxes를 예측하는 multi-task loss를 사용한다.제안 네트워크에서 category loss는 cross-entropy를 사용하고, 3D box loss는 smooth l1을 사용했다. 훈련할 때 positive/negative ROI들은 조감도 boxes의 IOU 중첩을 기준으로 결정된다. 조감도 IOU 중첩이 0.5 이상일 때는 3D 제안이 positive가 되고, 나머지는 negative가 된다. 추론에서는 3D bounding box regression이후에 3D boxes에 NMS를 적용한다. 조감도에 대한 3D box들을 투영함으로써 IOU 중첩을 계산한다. 조감도에서 같은 공간을 차지할 수 없는 객체들을 가리키는 불필요한 박스를 제거하기 위해 0.05 IOU threshold를 사용한다. Network Regularization저자는 지역 기반의 결합 네트워크를 정규화하기 위해 drop-path training과 auxiliary losses, 두 가지 접근 방법을 사용했다. 각 반복마다 무작위로 50% 확률로 global drop-path와 local drop-path를 무작위로 선택한다. 만약 global drop path가 선택되면 동일한 확률로 3개의 뷰안에서 1개의 단일 뷰를 선택한다. 반대로 local drop-path가 선택되면, 각 결합 노드의 입력 경로는 50% 확률로 무작위로 drop된다. 적어도 1개의 입력 경로는 유지되게 한다. 그리고, 각 뷰의 표현 능력을 더욱 강화하기 위해 네트워크에 보조 경로와 손실을 추가한다.위의 그림에서 볼 수 있듯이, 보조 경로는 메인 네트워크와 같은 수의 layer를 가지고 있다. 보조 경로의 각 layer는 메인 네트워크의 연관된 층의 가중치를 공유한다. 그리고 동일하게 각 보조경로를 역전파하기 위해 classification loss와 3D box regression loss를 더한 multi-task loss를 사용한다. 보조 loss을 포함한 모든 loss는 동일하게 가중치를 부여한다. 보조 경로들은 추론에서는 삭제된다.3.4 Implementation Network Architecture다중 뷰 네트워크에서 각 뷰는 같은 아키텍처를 가진다. backbone 네트워크는 16-layer VGGNet으로 만들었고, 몇가지를 모델에 맞게 수정했다. 수정한 내용은 다음과 같다. 채널을 원래 네트워크의 반으로 줄였다. 초소형 객체를 다루기 위해 특징 근사를 사용하여 고해상도 특징 맵을 얻었다. 특히, 우리는 마지막 convolution 특징 맵과 3D porposal 네트워크 사이에 2배 이선형 업샘플링 layer를 삽입했다. 또한, BV/FV/RGB의 ROI pooling layer 앞에 4x/4x/2x 업샘플링 layer도 삽입했다. VGG 네트워크 안에 있던 4번째 pooling 작업을 제거하여, convolution 부분에서 8배 다운샘플링을 진행한다. 다중 뷰 결합 네트워크에서 원래는 FC 6 layer이나 FC 7 layer 였던 FC에 1개 FC layer를 추가했다.imageNet으로 pretrained된 VGG16 네트워크의 가중치를 샘플링하여 파라미터를 초기화했다. 우리의 네트워크는 3개의 브런치를 가짐에도 불구하고 파라미터의 수는 VGG16 네트워크의 약 75%이다. 또, 1개의 이미지에 대한 네트워크의 추론 시간은 Titan X GPU에서 약 0.36s정도였다. Input RepresentationFV(front view)안에서의 객체 정보(annotations)가 제공되는 KITTI의 경우, (0, 70.4) X (-40, 40) meters의 범위 안의 point cloud를 사용했고, 이미지 평면에 투영했을 때 이미지 경계를 넘어가는 point들은 제거했다. 조감도에서 각 해상도는 0.1m로 나누었기 때문에, 조감도 입력은 704x800의 입력사이즈를 가진다. KITTI는 64-beam Velodyne 레이저 스캐너를 사용하기 때문에 우리는 64x512 map의 FV point를 얻는다. RGB 이미지는 가장 짧은 크기가 500 사이즈가 되도록 업스케일링한다. Training네트워크는 end-to-end로 훈련되고, 각 mini-batch마다 1개의 이미지와 128개의 ROI 샘플링하여 ROI의 25%가 positive로 유지되도록 한다. 그리고, iteration = 100K, learning rate = 0.001의 SGD를 사용하여 훈련한다. 그리고 나서 다른 훈련에서는 iteration = 20K, learning rate = 0.0001로 줄인다.4. ExperimentsKITTI object detection benchmark에서 MV3D 네트워크를 평가했다. 그 데이터셋은 트레이닝을 위한 7481개의 이미지와 테스트에 사용될 7581개 이미지로 구성되어 있다. 테스트에서는 2D detection만 평가하기에 training data를 1:1 비율로 training set과 validation set으로 나눈다. 그리고, validation set에서 3D box 평가를 구성했다. KITTI가 충분한 자동차 인스턴스를 제공하기 때문에, 우리는 자동차에만 초점을 맞춰 실험을 진행했다. KITTI 셋팅에 따라 easy, moderate, hard에 대한 어려움 정도를 세가지로 나눠 평가를 진행할 것이다. Metricsmetrics에 따라 3D box recall을 사용한 3D object proposals를 평가한다. 두 직사각형의 IOU 중첩을 계산하는 2D box recall와 달리, 여기서는 두 직육면체의 IOU 중첩을 계산한다. 직육면체는 축과 평행하는 것이 아니라 3D boxes의 방향을 나타낸다. 평가에서는 3D IOU threshold를 0.25와 0.5로 설정한다. 최종적인 3D detection 결과를 위해, 3D localization과 3D bounding box detection의 정확성을 측정하기 위해 두 방법을 사용한다. 3D localization에서는 방향성을 가진 조감도 box를 얻기 위해 지면에 3D boxes를 투영한다. 그런 다음 조감도 boxes에 대한 APloc(Average Precision)을 계산한다. 3D bounding box detection을 위해 저자는 full 3D bounding boxes를 평가하기 위한 방법인 AP3D(Average Precision)을 사용한다. 조감도 boxes와 3D boxes 둘 다 방향성이 있기 때문에, 객체 방향은 이 두 가지 지표에서 고려된다. 이미지 평면에 대한 3D boxes를 투영함으로써 2D detection의 수행을 평가한다. AP2D(Average Precision)은 측정 기준으로도 사용된다. KITTI 규칙에 따라 2D boxes는 IOU threshold를 0.7로 설정한다. Baselines이 작업이 3D Object Detection을 목표로 하고 있기 때문에, LIDAR 기반의 방법들인 VeloFCN, Vote3Deep, Vote3D뿐만 아니라 이미지 기반의 방법들인 3DOP와 Mono3D들과도 비교했다. 공정한 비교를 위해 조감도와 전면도(BV+FV)를 입력으로 사용하는 순수 LIDAR 기반의 모델과 LIDAR과 RGB 데이터(BV+FV+RGB)를 결합한 멀티모달에만 초점을 맞췄다. 3D box 평가에서는 validation set이 제공되는 VeloFCN, 3DOP, Mono3D와 비교했다. 공개적으로 사용할 수 있는 결과가 없는 Vote3Deep과 Voto3D에서는 테스트 셋에서 2D detection만을 비교했다. 3D Proposal Recall위 그림은 3D box recall을 나타낸 것이다. 300개의 제안을 사용하여 IOU 임계값의 함수로서 recall을 구성한다. 또한, 위 그림에서 볼 수 있듯이, 이 논문의 접근 방식은 모든 threshold에서 3DOP와 Mono3D를 엄청나게 능가했다. 그리고 IOU threshold가 0.25와 0.5아래인 제안들의 갯수에 대한 함수로 3D recall을 볼 수 있다. 이 논문의 방식은 0.25 IOU threshold에서 99.1% recall을, 0.5 IOU threshold에서 91% recall을 얻어냈다. 대조적으로 3DOP에서의 0.5 IOU threshold에 대한 최대 recall은 73.9%를 얻는다고 한다. recall이 크다는 것은 이미지 기반 방법에 비해 LIDAR 기반 접근 방식이 더 좋다는 것을 의미한다. 3D Localization3D localization 평가를 위해 0.5와 0.7 IOU threshold을 사용한다.KITTI validation set에서 APloc를 table 1에서 할 수 있다. 모든 LIDAR 기반 접근 방식은 스테레오 기반 방식인 3DOP 및 monocular 방식인 Mono3D보다 우수했다. LIDAR 기반 접근 방식중에서 BV+FV 방법은 0.5 IOU threshold에서 최대 25% APloc인 VeloFCN을 능가했다. IOU를 0.7로 기준을 잡으면, 이 모델의 성능은 더 높아지고, moderate와 hard 환경에서 최대 45% 더 높은 APloc를 달성할 수 있었다. RGB images와 결합하면, 성능은 더 개선된다. 아래 그림에서 몇 가지 사례의 localization result를 시각화했다. 3D Object Detection3D 중첩에 대해 설명하자면 0.5 3D IOU threshold와 LIDAR 기반의 방법들에 대해 0.7 IOU threshold에 초점을 맞췄다. 이러한 threshold들은 이미지 기반 방법에는 다소 엄격하기 때문에, 0.25 IOU threshold도 평가에 사용했다.Table2에서 볼 수 있듯이, 우리의 BV+FV 방법은 moderate setting에서 86.75%를 달성하는 0.5 IOU threshold를 적용할 때 VeloFCN보다 AP3D를 30% 더 높게 얻을 수 있다. 0.7 IOU threshold를 사용할 때, multimodal 접근 방법은 easy data에서 71.29%의 AP3D를 달성했다. moderate 셋팅에서는 0.25 IOU threshold를 사용한 3DOP에 의해 달성될 수 있는 최대 AP3D는 68.82%인 반면, 이 논문의 접근 방식은 0.5 IOU threshold를 사용할 때 89.05% AP3D를 달성했다. Ablation Studies먼저 초기/후기 결합 방식과 이 논문의 deep fusion 방식을 비교해보자. 결합 연산은 초기/후기 융합 방식에서 연결과 함께 인스턴스화된다. table 3에서 볼 수 있듯이 초기/후기 융합 접근은 매우 단순하다. 근사 loss의 사용없이, deep fusion 방법은 초기/후기 결합 방식보다 0.5% 더 향상된 결과를 얻을 수 있다. 근사 loss를 추가하면 1% 더 향상된다.table4에서 볼 수 있듯이, 입력으로 단일 뷰만 사용한다면 조감도, 즉 bird’s eye view 특징이 가장 높았고, front view가 가장 낮았다. 두개의 뷰를 섞는다면 단일 뷰보다 모든 뷰들이 더 높게 측정되었다. 이것은 서로 다른 뷰들의 특징들이 성호 보완적이라는 가정이 정당화된다. 세가지 뷰를 모두 결합하면 최상의 결과를 얻을 수 있다. 2D Object Detection우리는 마지막으로 KITTI 테스트 셋에서 2D detection 수행을 평가했다. 결과는 다음과 같다.LIDAR 기반의 방법들중에서 BV+FV 방식이 hard 셋팅에서 최근 제안된 Vote3Deep에 비해 AP2D가 14.93% 더 높게 결과가 나왔다. 전체적으로 이미지 기반의 방법들은 보통 2D detection에서 LIDAR 기반의 방법들보다 더 좋다. 이는 이미지 기반의 방법들이 3D boxes를 최적화하는 LIDAR 기반의 방법들과 달리 2D boxes를 직접적으로 최적화하기 때문이다. 하지만 이 논문의 방법이 3D boxes를 최적화함에도 불구하고 최첨단 2D detection 방법들보다 더 경쟁력있는 결과를 얻었다. Qualitative Resultsfig6에서 볼 수 있듯이, 접근 방식은 스테레오 기반의 방법인 3DOP과 LIDAR 기반의 방법인 VeloFCN에 비해 더 정확한 3D 위치, 사이즈, 객체의 방향성을 얻는다.5. Conclusion그래서 최종적으로 로드 뷰에서 3D object detection에 대한 멀티뷰 센서퓨전 모델을 제안한다. 우리의 모델은 LIDAR point cloud와 images 둘 다 강점을 보인다. 3D 제안을 생성하고 특징 추출을 위해 여러 뷰에 투영하여 서로 다른 양식을 정렬한다. region-based fusion network는 방향성을 가진 3D box regression을 하고, 다중 뷰 정보를 깊게 융합한다. 우리의 접근 방식은 KITTI benchmark에서 3D localization과 3D detection의 task에 대한 현존하는 LIDAR 기반과 image 기반 방법들을 엄청나게 능가한다. 3D detection으로부터 얻어진 2D box 결과들은 최첨단 2D detection 방법들과 비교해도 경쟁력이 있다.Reference https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf" }, { "title": "Coding Test[Python] - 카카오 및 코딩테스트 기출문제", "url": "/posts/codingtestkakao/", "categories": "Classlog, coding test review", "tags": "coding test", "date": "2022-01-14 13:00:00 +0900", "snippet": " 스택/큐 힙 해시 정렬 완전탐색 연습문제 카카오멀쩡한 사각형def solution(w,h): answer = 0 for i in range(1,h+1): c = w while True: if (h-i) * (w/h) &amp;gt;= c: break c -= 1 answer += c if c == 0 : return answer*2나는 이와 같이 코드를 짰는데, 코드를 돌려보면 시간 초과로 53점을 받는다. 그렇다면 시간을 줄일 수 있는 방법을 찾아야 할텐데, 그 방법은 아마 패턴을 찾는 것일 것이다.이 문제에서 [8,12] 사각형의 대각선이 지나가는 패턴을 보면, [2,3]의 크기로 대각선에 의해 제거되야 하는 사각형의 패턴이 보인다. 이 때, 최소공약수를 구해보면 다음과 같다.&amp;gt;&amp;gt;&amp;gt;print(gcd(8,12))4즉, 패턴이 4번 반복된다는 것을 알 수 있다.이해가 되지 않는다면 이에 대해 구글링하면 많이 나올 것이다.아무튼 이렇게 gcd함수를 사용하여 패턴을 구했다.w,h = [8,12]&amp;gt;&amp;gt;&amp;gt;print(w//gcd(w,h), h//gcd(w,h))2 3따라서 패턴은 [2,3]의 크기로 4번 반복된다. 그렇다면 [2,3] 크기 안에서 제거해야 할 사각형, 즉 대각선이 지나가는 사각형의 갯수를 구해야 한다. 이는 참고 블로그의 저자는 세로와 가로를 나누어 갯수를 센다. 대각선은 세로줄 기준으로 최소 1칸씩은 차지한다. 따라서 h//gcd(w,h)가 되고, 가로줄 기준으로는 1줄에 2개를 차지할 수 있다. 그 갯수는 w//gcd(w,h) - 1가 된다. 최종 코드는 다음과 같다.from math import gcddef solution(w,h): return w * h - (w//gcd(w,h) + h//gcd(w,h) -1) * gcd(w,h)짝지어 제거하기나의 전체 코드def solution(s): arr=[] s = list(s) for i in range(len(s)): if len(arr) == 0: arr.append(s[i]) else: if arr[-1] == s[i]: arr.pop() elif arr[-1] != s[i]: arr.append(s[i]) if len(arr) == 0: return 1 else: return 0처음에는 재귀함수를 사용해서 돌렸는데, 시간이 너무 오래걸려서 다른 방법을 생각했다. 굳이 for문을 계속 돌릴 필요없이 같으면 pop, 다르면 append 하면 0이 되거나 더이상 없을 때까지 돌릴 필요없이 1번에 다 진행시킬 수 있다.baabaa의 경우, b를 arr에 먼저 넣고, 그 다음 a를 가지고 와서 arr 맨 뒤 값인 b와 비교하여 같으면 b를 제거, 다르면 a를 arr에 삽입하는 식이다.행렬 테두리 회전하기나의 전체 코드def solution(rows, columns, queries): answer = [] c=1 num = [[c+i+j for j in range(columns)] for i in range(0,rows*columns,columns)] for i in queries: arr = [] x1,y1,x2,y2 = i x1-=1;y1-=1;x2-=1;y2-=1 a = num[x1][y1:y2+1] for c in range(1,y2-y1+1): num[x1][y1+c] = a[0] arr.append(a.pop(0)) for i in range(1,x2-x1+1): a.append(num[x1+i][y2]) for r in range(1,x2-x1+1): num[x1+r][y2] = a[0] arr.append(a.pop(0)) for i in range(y2,y1,-1): a.append(num[x2][i-1]) for c in range(y2-1,y1-1,-1): num[x2][c] = a[0] arr.append(a.pop(0)) for i in range(x2,x1,-1): a.append(num[i-1][y1]) for c in range(x2-1,x1-1,-1): num[c][y1] = a[0] arr.append(a.pop(0)) arr.append(a[0]) answer.append(min(list(map(int,arr)))) return answer이것이 좋은 코드는 아니지만, 다른 사람의 풀이를 봐도 비슷한듯하다. 회전을 하는 것이기에 이동하기 전 숫자를 미리 추출하여 a 리스트에 넣어놓고, 1칸씩 밀어서 리스트 값을 삽입한다. 나의 경우 사각형 기준 위, 오른쪽, 아래, 왼쪽 순으로 회전시켰다.메뉴 리뉴얼나의 전체 코드import itertools def solution(orders, course): answer = [] com = [] maxlen_order = len((sorted(orders, key=len,reverse=True))[0]) arr = [list(map(str,i)) for i in orders if len(i) &amp;gt; 1] for c in course: dic = {} for i in arr: com = [&quot;&quot;.join(sorted(co)) for co in list(itertools.combinations(i,c))] for a in com: if a not in dic: dic[a] = 1 else: dic[a] += 1 maxv = 1 for k,v in dic.items(): if maxv &amp;lt; v: maxv = v maxk = k if len(maxk) != maxlen_order and len(dic) != 0: answer.append([k for k,v in dic.items() if v==maxv and maxv &amp;gt; 1]) a = [] for i in answer: for k in i: a.append(k) return sorted(a)이는 itertools.combinations를 사용한 방법이다. orders를 str형 list로 변환한 후 이를 combinations 즉, 조합 배열을 만든다. 조합 배열이라 함은 list 중에서 c개 만큼을 뽑는데, 순서를 고려하지 않고 추출한다. 예를 들어 [1,2,3]의 리스트가 있고 이를 combinations를 사용해보자.&amp;gt;&amp;gt;&amp;gt; lists = [1,2,3]&amp;gt;&amp;gt;&amp;gt; k = list(itertools.combinations(lists,2))&amp;gt;&amp;gt;&amp;gt; print(k)[(1,2),(2,3),(1,3)]이렇게 만들어진 리스트를 알파벳들을 조합하여 하나의 문자로 만들어 리스트 com에 넣는다. 그것을 반복문을 사용하여 원소 i가 딕셔너리 dic에 존재하면 갯수를 의미하는 [1]에 +1을 하고, 없으면 삽입한다. 이를 통해 모든 경우의 수를 따진 dic 딕셔너리를 얻는다. 거기서 가장 많이 주문된 조합을 구하여 그것에 대한 조합을 answer에 넣는다. 이렇게 넣으면 [[],[]]의 형태가 되므로 이를 또 풀어줘야 한다.괄호 변환나의 전체 코드import syssys.setrecursionlimit(1000001)def correct(p): c = 0;cc=0 u=[] # 1 if p == &#39;&#39;: return p # 2 for i in p: if i == &quot;(&quot;: cc += 1 elif cc&amp;gt;0 and i ==&quot;)&quot;: cc-=1 if cc == 0: return p for i in p: if i == &quot;(&quot;: c+=1 elif i == &quot;)&quot;: c-=1 u.append(i) if c == 0: break u = &quot;&quot;.join(u) v = p[len(u):] # 3 cu = 0 for i in u: if i == &quot;(&quot;: cu += 1 elif cu&amp;gt;0 and i ==&quot;)&quot;: cu-=1 if cu == 0: return u + correct(v) # 4 q = u[1:len(u)-1] u=[] for i in q: if i == &quot;(&quot; : i = &quot;)&quot; elif i == &quot;)&quot; : i = &quot;(&quot; u.append(i) return &#39;(&#39; + correct(v) + &#39;)&#39; + &#39;&#39;.join(u)def solution(p): answer = &#39;&#39; return correct(p)이 문제는 문제 설명 자체에서 재귀적이라는 표현이 있는 것으로 보아 재귀함수를 사용해야 했다. 문제에 맞춰 코드만 잘 작성하면 알맞게 동작한다. 유의할 것은 cc나 cu에 대한 반복문을 쓸 때, 즉 올바른 괄호 문자열을 판별할 때 그냥 elif i == &quot;)&quot;라고 작성하게 되면 ))(( 에 대해서도 올바르다고 판별하게 나온다. 따라서 cu&amp;gt;0 이라는 표현, 즉 (가 먼저 나온 후 )가 나올 때만 +1을 하여 올바름을 판별한다.[1차]뉴스 클러스터링나의 전체 코드import mathdef solution(str1, str2): intersec=[] str1 = str1.lower();str2 = str2.lower() str1=list(str1);str2=list(str2) arr1 = [str1[i-1] + str1[i] for i in range(1,len(str1)) if (str1[i-1] + str1[i]).isalpha()] arr2 = [str2[i-1] + str2[i] for i in range(1,len(str2)) if (str2[i-1] + str2[i]).isalpha()] print(arr1,&quot; &quot;,arr2,&quot;\\n&quot;) if arr1 == [] and arr2 == []: return 65536 arr1_copy = arr1.copy() arr2_copy = arr2.copy() # 교집합 inter = [] for i in arr1: if i in arr2_copy: inter.append(i) arr1_copy.remove(i); arr2_copy.remove(i) answer = math.floor((len(inter) / len(inter + arr1_copy + arr2_copy)) * 65536) return answer일단 str.isalpha()는 문자열 str에 문자만 있는지를 판단하여 모두가 문자열이면 True, 1개라도 문자가 아닌 것이 있으면 False를 리턴하게 된다. 처음에는 set 함수만을 사용하려고 노력했다. 하지만 사용하지 않고도 간단하게 풀 수 있는 방법이 많기에 set은 순서와 중복을 제거할 때만 사용하도록 하자. 반대로 문자열이 숫자인지 아닌지를 판별해주는 isdigit함수도 있다.copy()는 복합 객체를 복사하는 것이다. 사용하지 않고 그냥 arr_copy = arr1으로 하게 되면 arr_copy의 원소를 변경하게 되면 arr1도 같이 변한다.isalnum이는 문자열 내 특수문자 사용 여부를 판별한다. c.isalnum()를 하게 되면 변수형 c가 모든 문자가 문자 또는 숫자인 경우 True, 그렇지 않은 경우 False를 반환한다.&amp;gt;&amp;gt;&amp;gt;string = &quot;Hello World!!&quot;&amp;gt;&amp;gt;&amp;gt;new_str = &#39;&#39;.join(char for char in string if char.isalnum())&amp;gt;&amp;gt;&amp;gt;print(new_str)HelloWorlda = list(map(list.add, p, x))" }, { "title": "Coding Test[Python] - 재귀함수", "url": "/posts/codingtestrecursion/", "categories": "Classlog, coding test review", "tags": "coding test, recursion", "date": "2022-01-13 13:00:00 +0900", "snippet": "재귀함수피보나치 수열def fibo(n): if n &amp;lt;= 1: return 1 return fibo(n-2) + fibo(n-1)def fibo_list(n): return fibo(n-1)이 코드가 기본적인 피보나치 수열의 코드이다. 하지만 이는 메모리적으로나 시간적으로나 효율이 좋지 않다. 그런 이유를 찾아보면, 계산이 했던 것을 계속해서 다시 계산을 하기 때문에 오래 걸리게 되는 것이다. 그렇다면 우리는 해결책으로 어딘가에 저장해서 그 어딘가에 없다면 추가하고, 있다면 그 값을 불러오기만 하면 되지 않을까?이에 대해 잘 정리된 블로그가 있어 참고로 넣는다.*참고 블로그import syssys.setrecursionlimit(10000000)dic = {0:0, 1:1}def fibonacci(n): if n in dic: return dic[n] dic[n] = fibonacci(n-2) + fibonacci(n-1) return dic[n] % 1234567def solution(n): return fibonacci(n)파이썬은 원래 재귀를 1000번까지로 제한을 한다. setrecursionlimit을 사용해 그 제한을 풀어줄 수 있다. 이보다 중요한 것은 dic이다. 개별적으로 보게 되면dic = {0:0, 1:1}을 선언하여 공간을 만들어놓고, 피보나치 수는 보통 2부터 진행하고 0일때는 0, 1일때는 1을 출력한다. 따라서 0과 1에 대해서는 입력시켜놓고, fibonachi 재귀함수를 선언하는 것은 동일하나if n &amp;lt;= 1: return 1return fibo(n-2) + fibo(n-1)if n in dic: return dic[n] # 1dic[n] = fibonacci(n-2) + fibonacci(n-1) # 2두 부분을 함께 보게 되면, 원래는 if n &amp;lt;= 1: return 1 을 통해 end condition을 적용하고, return 을 통해 재귀를 계속해준다. 이 대신 딕셔너리에 없으면 2번줄처럼 선언해서 집어넣는다. 하지만 딕셔너리에 값이 존재할 경우에는 그 값을 불러오기만 하면 된다. 100번째 피보나치 수를 찾으면 354,224,848,179,261,915,075이 나온다. 이 값을 찾기 위해 첫번째 방식을 사용하면 10초가 넘어가서 실행되지 않는다.아래의 방법으로 진행할 경우 둘다 실행은 되는 테스트 케이스인 6번을 기준으로 첫번째는 0.12~0.15ms / 10.2MB , 두번째 방식은 0.01ms / 10.2MB 이 나온다. 무려 12~15배나 빠르다는 것이다.t1 = Timer(&quot;fibonacci&quot;, &quot;from __main__ import fibonacci&quot;)t2 = Timer(&quot;fibo&quot;, &quot;from __main__ import fibo&quot;)print(&quot;fibonacci(30): &quot;, t1.timeit(number=1000), &quot;seconds&quot;)print(&quot;fibo(30): &quot;, t2.timeit(number=1000), &quot;seconds&quot;)이와 같은 방법으로 30번째 피보나치 수를 찾는 것을 1000번 실행을 시켰을 때의 시간을 얻을 수 있다고 한다." }, { "title": "Coding Test[Python] - 이진트리 구현하기", "url": "/posts/codingtestbina/", "categories": "Classlog, coding test review", "tags": "coding test, binary tree", "date": "2022-01-13 13:00:00 +0900", "snippet": "이진트리란?이진트리는 각 노드의 자식 수가 2 이하인 트리이다. 이진트리는 4가지 순회방식이 존재하는데, 방식은 각각 다르지만, 항상 트리의 루트부터 시작한다는 점이 동일하다. 또한, 모든 순회방식이 트리의 모든 노드를 반드시 1번씩 방문한 후 종료된다. 순회방식은 다음과 같다. 전위순회(Preorder) 중위순회(Inorder) 후위순회(Postoder) 레벨순회(Level-order)전위순회전위순회의 순회 방식 NLR이고 이는 다음과 같다. 현재 노드 n를 먼저 방문 (N = 현재 노드) 현재 노드의 왼쪽 서브트리를 순회 (L = 왼쪽) 현재 노드의 오른쪽 서브트리를 순회 (R = 오른쪽)def preoder(self, n): if n != None: print(n.item,&#39;&#39;,end=&#39;&#39;) # 노드 방문 if n.left: self.preorder(n.left) # 왼쪽 서브트리 순회 if n.right: self.preorder(n.right) # 오른쪽 서브트리 순회중위순회중위순회의 방식은 LNR로 현재 노드의 왼쪽 서브트리를 순회 (L = 왼쪽) 현재 노드 n를 먼저 방문 (N = 현재 노드) 현재 노드의 오른쪽 서브트리를 순회 (R = 오른쪽)def inorder(self, n): if n != None: if n.left: self.inorder(n.left) print(n.item, &#39;&#39;, end=&#39;&#39;) if n.right: self.inorder(n.right)후위순회후위순회의 방식은 LRN로 현재 노드의 왼쪽 서브트리를 순회 (L = 왼쪽) 현재 노드의 오른쪽 서브트리를 순회 (R = 오른쪽) 현재 노드 n를 먼저 방문 (N = 현재 노드)def postorder(self, n): if n != None: if n.left: self.postorder(n.left) if n.right: self.postorder(n.right) print(n.item, &#39;&#39;, end=&#39;&#39;)레벨순회레벨순회의 방식은 루트가 있는 곳부터 각 레벨마다 좌에서 우로 노드를 방문한다.def levelorder(self, root): q = [] q.append(root) while q: t = q.pop(0) print(n.item,&#39;&#39;,end=&#39;&#39;) # q에서 첫 항목을 삭제하고 삭제한 노드 방문 if n.left != None: q.append(t.left) # 왼쪽 자식 큐에 삽입 if n.right != None: q.append(t.right) # 오른쪽 자식 큐에 삽입이진트리는 높이 h에 대해 최대 2^(h-1)만큼 노드를 가질 수 있다. 높이 h는 루트 노드를 기준으로 두 자식노드의 높이 중 큰 높이를 구하면 된다.def height(self, root): if root == None: return 0 return max(self.height(root.left),self.height(root.right)) + 1이처럼 방문을 한다는 것은 print()함수로 표현한다. 그래서 전위순회를 기준으로 예를 들어보면, root노드를 먼저 방문한 후, 왼쪽부터 순회를 돈다. 그러면 1번 내려가고, 거기에 있는 노드를 읽은 후 다시 왼쪽, 또 내려가서 왼쪽을 읽다가 자식이 없으면 1단계 위로 올라가서 오른쪽 노드를 읽고, 자식이 없으면 다시 위로 올라간다.최종 코드는 다음과 같다.class Node: def __init__(self,item): self.item = item self.left = None self.right = None class BinaryTree(): def __init__(self): self.root = None #전위순회 def preoder(self, n): if n != None: print(n.item,&#39;&#39;,end=&#39;&#39;) if n.left: self.preorder(n.left) if n.right: self.preorder(n.right) #중위순회 def inorder(self, n): if n != None: if n.left: self.inorder(n.left) print(n.item, &#39;&#39;, end=&#39;&#39;) if n.right: self.inorder(n.right) #후위순회 def postorder(self, n): if n != None: if n.left: self.postorder(n.left) if n.right: self.postorder(n.right) print(n.item, &#39;&#39;, end=&#39;&#39;) #레벨순회 def levelorder(self, root): q = [] q.append(root) while q: t = q.pop(0) print(n.item,&#39;&#39;,end=&#39;&#39;) if n.left != None: q.append(t.left) if n.right != None: q.append(t.right) #트리 높이 def height(self, root): if root == None: return 0 return max(self.height(root.left),self.height(root.right)) + 1 tree = BinaryTree()n1=Node(10);n2=Node(20);n3=Node(30);n4=Node(40);n5=Node(50);n6=Node(60);n7=Node(70);n8=Node(80);#트리tree.root = n1n1.left=n2;n1.right=n3n2.left=n4;n2.right=n5n3.left=n6;n3.right=n7n4.left=n8print(&#39;트리 높이: {}\\n전위순회: {}\\n중위순회: {}\\n후위순회: {}\\n레벨순회: {}&#39;.format( tree.height(tree.root), # 트리 높이: 4 tree.preorder(tree.root), # 전위 순회: 10 20 40 80 50 30 60 70 tree.inorder(tree.root), # 중위 순회: 20 40 80 50 10 30 60 70 tree.postorder(tree.order), # 후위 순회: 20 40 80 50 30 60 70 10 tree.levelorder(tree.order))) # 레벨 순회: 10 20 30 40 50 60 70 80*참고 블로그1*참고 블로그2" }, { "title": "ROS 환경설정", "url": "/posts/ros2/", "categories": "Review, ROS", "tags": "ROS", "date": "2022-01-11 13:00:00 +0900", "snippet": "ROS 작업 공간 만들기$mkdir -p ~/catkin_ws/src$cd ~/catkin_ws$catkin_makecatkin_make는 catkin 작업공간으로, 작업하기 위한 편리한 도구다. 작업영역에서 처음 실행하면 src폴더에 링크가 생성된다. 그리고, devel/build 디렉토리가 함께 생성된다. devel 폴더 내에 여러 가지 설정이 있고, 이러한 파일 중 하나를 소싱하면 이 작업 영역이 나의 환경 위에 덮어진다.$source devel/setup.bash$echo $ROS_PACKAGE_PATH/home/ice/catkin_ws/src:/opt/ros/noetic/share그 후, 파일을 열어 아래 코드를 추가해줘야 한다.sudo gedit ~/.bashrcalias cm=&#39;cd ~/xycar_ws &amp;amp;&amp;amp; catkin_make&#39;source /opt/ros/melodic/setup.bashsource ~/xycar_ws/devel/setup.bashexport ROS_MASTER_URI=http://localhost:11311export ROS_HOSTname=localhost추가한 후 저장한다. 그 후 source코드를 통해 시스템에 반영한다.source ~/.bashrcROS환경변수가 어떻게 설정되었는지 확인해본다.printenv | grep ROSROS_VERSION=1ROS_PYTHON_VERSION=3ROS_PACKAGE_PATH=/home/ice/xycar_ws/src:/home/ice/catkin_ws/src:/opt/ros/noetic/shareROSLISP_PACKAGE_DIRECTORIES=/home/ice/xycar_ws/devel/share/common-lisp:/home/ice/catkin_ws/devel/share/common-lispROS_ETC_DIR=/opt/ros/noetic/etc/rosROS_MASTER_URI=http://localhost:11311ROS_HOSTname=localhostROS_ROOT=/opt/ros/noetic/share/rosROS_DISTRO=noetic이렇게 환경설정이 완료되었다.turtlesim 예제이 예제를 수행하기 위해서는 4개의 터미널이 필요하다. 노드1(publisher = turtle_teleop_key)에서 노드2(subscriber = turtlesim_node)로 토픽(turtle1/cmd_vel)을 전송한다. roscore rosrun turtlesim turtlesim_node rosrun turtlesim turtle_teleop_key rosnode실행, rostopic실행 rosrun rqt_graph rqt_graph 일단 터미널1에서 roscore을 실행한다.터미널2에서 rosnode list를 실행시켜보면 /rosout 만 출력되는 것을 볼 수 있다.그 다음 터미널2에 rosrun turtlesim turtlesim_node를 실행시켜 node를 생성하게 되면 거북이가 나오는 창이 하나 뜬다.&amp;lt;img src=”/assets/img/ros/rosrunnode.png” width = 50%&amp;gt;&amp;lt;img src=”/assets/img/ros/turtle.png” width=50%&amp;gt;터미널3에 rosrun turtlesim turtle_teleop_key를 실행한다. 이를 실행하면 터미널에서 방향키로 거북이를 움직일 수 있게 된다.그 후, 터미널2에서 rosnode list를 다시 실행시켜보면 다른 것들이 추가되어 있다.rqt_graph를 살펴보기 위해 터미널4에 rosrun rqt_graph rqt_graph를 실행시켜본다.그림을 보면 teleop_turtle(퍼블리셔 노드) 에서 /turtle/cmd_vel(토픽)이 /turtlesim(서브스크라이버 노드)로 전송되고 있는 것을 확인해볼 수 있다. 그리고 다시 터미널4에 rostopic list를 실행하면 어떤 토픽이 발생(전송)되고 있는지 확인할 수 있다.그 중 1개를 선택하여 어떤 값인지 확인이 가능하다. 또한, rostopic list -v를 통해 퍼블리셔 노드와 서브스크라이버 노드를 더 자세히 볼 수 있다.이 때, 토픽을 직접 지정해줄 수도 있다.#rosrun {퍼블리셔} {1회} {토픽이름} {메시지 타입} -- {메시지 내용} =&amp;gt; 1회 z방향으로 1.8만큼 twist(돈다)rosrun pub -1 /turtle/cmd_vel geometry_msgs/Twist -- &#39;[2.0,0.0,0.0]&#39; &#39;[0.0,0.0,1.8]&#39;# -r 은 발행 주기(Hz) =&amp;gt; 초당 1회 =&amp;gt; 1초당 1회rosrun pub /turtle/cmd_vel geometry_msgs/Twist -r 1 -- &#39;[2.0,0.0,0.0]&#39; &#39;[0.0,0.0,1.8]&#39;Reference ROS 위키 참고 블로그1 참고 블로그2" }, { "title": "Coding Test[Python] - 연습문제", "url": "/posts/codingtestpra/", "categories": "Classlog, coding test review", "tags": "coding test, practice", "date": "2022-01-11 13:00:00 +0900", "snippet": "프로그래머스에 올라와 있는 코딩테스트를 진행하면서 공부한 내용 및 막혔던 부분들에 대해 리뷰하고자 합니다. 저는 거의 대부분을 python을 사용하였습니다.이 글은 python을 통한 스택/큐에 대해 리뷰하고 다른 것들을 참고하시려면 아래 링크를 참고해주세요 스택/큐 힙 해시 정렬 완전탐색 카카오Overview스택스택(stack)은 데이터 구조 중 하나로 데이터 삽입은 push, 데이터 추출은 pop이라 한다. LIFO(Last In First Out)는 후입선출 구조, 즉 후에 들어온 것이 먼저 나가는 방식이다. 스택은 방문기록 뒤로가기, 역순 문자열, 실행 취소 등에 활용된다.파이썬에서는 push와 pop을 각각 append, pop 으로 되어 있다.큐반대로 큐(queue)는 데이터 구조 중 하나로 데이터 삽입은 인큐(enqueue), 제거는 디큐(deque)라고 한다. FIFO(First In First Out) 선입선출 구조, 즉 먼저 들어온 것이 먼저 나가는 방식이다. 큐는 은행 업무, 프린터 인쇄 대기열, 캐시 등에 활용된다.큐는 너비 우선 탐색(BFS)에 주로 사용된다.큐 연산을 구현하는 방법으로 3가지 정도가 있다. list 자료형 사용 Collections 모듈의 deque 사용 queue 모듈의 Queue 클래스 사용나의 경우 list를 많이 사용하기 때문에 list를 위주로 리뷰할 것이다. list 자료형 사용의 경우 append, pop, insert를 많이 사용한다. 이 방법의 단점은 성능이 비교적 좋지 않다. 참고 블로그INDEX 124나라의 숫자 가장 큰 정사각형 찾기 올바른 괄호 다음 큰 숫자 땅따먹기 숫자의 표현 최댓값과 최솟값 최솟값 만들기 피보나치 수 행렬의 곱셈 JadenCase 문자열 만들기 N개의 최소공배수124나라의 숫자참고 코드def solution(n): answer = &#39;&#39; k = [&#39;1&#39;,&#39;2&#39;,&#39;4&#39;] while n &amp;gt; 0: n -= 1 answer = k[n%3] + answer n //= 3 print(answer, n) return answer입력값 〉 13출력 〉 1 411 1111 0입력값 〉 12출력 〉4 344 0str형으로 하는 것은 생각했으나 좀 더 간단하게 푸는 방법을 몰랐다. 저 코드에 의하면 일단 124의 나라이므로 k를 선언한 후, 나의 경우 for문으로 0부터 시작했다. 그렇게 되면 너무 많은 경우의 수가 생겼다. while을 통해 n을 3으로 나눈 나머지에 대해 k[n%3]을 넣고, 문자이므로 그냥 더하여 문자열을 계속 추가해준다. 나는 1의 자리부터 채울 생각을 했지만, 앞에서부터 채워넣어나가니 편안한 듯하다.가장 큰 정사각형 찾기나의 전체 코드올바른 괄호나의 전체 코드def solution(s): k=0 if s[0] == &quot;)&quot;: return False for i in s: if i == &quot;(&quot;: k += 1 else: if k &amp;gt; 0: k -= 1 return k==0올바른 괄호 쌍이 되기 위해서는 “(“이 먼저 나온 후 “)”가 나와야 한다. 따라서 “(“가 나오면 +1을 하고 앞에 “(“가 있다는 의미인 k가 &amp;gt;0인 상태에서만 “)”가 나왔을 때 -1을 해주어 닫혔다는 것을 표현해준다. 이렇게 하면 대부분의 입력값이 통과되지만, 예외적으로 “)))()”가 될 때는 올바르지 않지만, 올바르다고 잘못 판단되어진다. 따라서 예외적으로 “)”가 먼저 나왔을 때는 바로 false을 리턴한다.다음 큰 숫자나의 전체 코드def solution(n): answer = 0 n2 = (bin(n))[2:] print(n2, type(n2)) c1 = 0 for i in n2: if i == &#39;1&#39;: c1 += 1 k = n while True: c2 = 0 k+=1 k2 = (bin(k))[2:] for i in k2: if i == &#39;1&#39;: c2 += 1 if c1 == c2: break return kbin이라는 함수는 2진수로 바꿔주는 함수이다. 이를 실행하면 0b + 2진수코드를 받을 수 있다. 그래서 [2:]를 해주어 앞의 0b를 제거한다. 그 후 이 2진수에서 1의 갯수를 찾기 위해 c1 변수를 통해 갯수를 구한다. 그 후 n보다 더 큰 숫자를 찾기 위해 무한 while문을 사용하여 1씩 더해가며 찾는다. 그렇게 구해진 k들의 2진수값에서 1의 갯수가 맞는 가장 작은 수를 찾게 되면 반복문을 멈추고 리턴한다.bin, oct, hex 2진수2진수는 bin함수로 구하고, 추출하게 되면 맨 앞에 0b가 붙는다.&amp;gt;&amp;gt;&amp;gt; bin(42)0b101010다시 원래대로 되돌리기 위해서는 str()이나 int(2진수,2)를 사용하면 된다. 원래 사실 int인수 뒤에는 디폴트 값으로 10이 되어 있다. 즉, 10진수값으로 추출하겠다는 의미다.&amp;gt;&amp;gt;&amp;gt; str(0b101010)42&amp;gt;&amp;gt;&amp;gt; int(0b101010,2)42 8진수8진수는 잘 사용하지 않지만, oct함수로 구하고, 추출하면 0o가 붙는다.&amp;gt;&amp;gt;&amp;gt; oct(42) 0o52&amp;gt;&amp;gt;&amp;gt; str(0o52)42&amp;gt;&amp;gt;&amp;gt; int(0o52,8)42 16진수&amp;gt;&amp;gt;&amp;gt; hex(42)0x2a 참고 블로그땅따먹기##숫자의 표현나의 전체 코드def solution(n): answer = 0 for i in range(n): c = i ing = 0 while ing &amp;lt; n: c+=1 ing = ing + c if ing == n: answer += 1 return answer이중 반복문을 사용하여 돌렸지만, 이는 다른 사람의 코드를 참고했을 때 1줄로 표현이 가능하다.def solution(n): return len([i for i in range(1,n+1,2) if n % i == 0])이는 등차수열 합 공식을 사용한 것이라 한다.*참고 블로그최댓값과 최솟값나의 전체 코드def solution(s): answer = &#39;&#39; s = list(map(int,s.split(&#39; &#39;))) return str(min(s)) + &quot; &quot; + str(max(s))입력이 ‘ ‘로 숫자들이 구분되어 있어 ‘ ‘로 구분한 후 이를 간단하게 int list로 만들기 위해 list(map())을 사용했다. 이 것에 대해 최솟값과 최댓값을 다시 문자열로 바꾸어 리턴한다.최솟값 만들기def solution(a,b): return sum([a*b for a,b in zip(sorted(a), sorted(b,reverse=True))])최댓값과 최솟값을 이어 곱해주면 가장 작은 수가 될 것이라 생각했다. 그래서 이를 코드화 한 것이다.피보나치 수def solution(n): s = [0,1] for i in range(1,n): s.append(s[i-1] + s[i]) return s[-1] % 1234567피보나치 수는 F(n) = F(n-1) + F(n-2) 의 식을 가지고 있다. 따라서 이를 계속 append해주면서 공식을 만들었다.만든 것들에서 가장 뒤의 값에 1234567(이건 왜 하는건지 모름..)을 나누어주면 된다.JadenCase 문자열나의 전체 코드두 가지 방법이 있다. 하나는 다소 지저분해보이나 쉽게 떠올릴 수 있는 방법이고, 두번째는 함수를 활용하는 것이다.def solution(s): return &quot; &quot;.join([(i[0].upper() + i[1:].lower()) if i != &#39;&#39; else &#39;&#39; for i in s.split(&#39; &#39;)])def solution(s): return &#39; &#39;.join(i.capitalize() for i in s.split(&#39; &#39;))첫번째부터 보면 ‘ ‘단위로 분할한 후 빈 문자열이 아닐 경우 맨 앞 문자열은 대문자로, 다른 것은 소문자로 변경해준 후 list를 str형태로 바꾸어 출력한다.두번째는 capitalize()를 사용하는 것으로 이 함수는 첫번째 글자가 숫자인지 문자인지 구분하지 않는 title()과는 조금 더 세심?한 느낌이 있다. title()의 경우 앞에 숫자가 있으면 뒤의 문자열 첫번째꺼를 대문자로 변경하기 때문에 예외가 많지만, capitalize()는 숫자인지 문자인지 구분하여 맨 앞에 있는 글자만 대문자로 만든다.N개의 최소공배수나의 전체 코드def solution(arr): a = 1 for i in arr: a *= i print(a) c=1 while c != a: c+=1 z=0 for i in arr: if c % i == 0: z += 1 pass else: break if z == len(arr): return c조금 지저분하게 풀었다. 일단 모든 숫자를 다 곱한 후 그 숫자까지 반복을 하는데, 모든 arr의 원소들이 다 나눠지는 숫자가 되면, 즉 c%i==0이 모든 원소가 다 되는 상황(z=len(arr))이 되면 멈추고 이를 반환한다.이 때, gcd 함수를 사용할 수 있다.gcd파이썬의 최대공약수를 구하는 함수로 math라이브러리에 포함되어 있다. 최대공약수는 둘 이상의 정수의 공약수 중에서 가장 큰 것을 말한다.import gcd&amp;gt;&amp;gt;&amp;gt; math.gcd(66, 22, 11) 11 이를 활용해 다시 코드를 짠다면 다음과 같다.from math import gcddef solution(arr): answer = arr[0] for i in arr: answer = i * answer // gcd(i,answer) return answer동일하게 모든 list의 원소들을 다 곱해주는데, i와 answer의 최대공약수를 나누어준다. arr = [5,6,8,14]일 때, gcd(i,answer)를 매 반복마다 출력해보면 다음과 같다.[5, 6, 8, 14]출력 〉 i:5 answer:5 gcd(i,answer): 5i:6 answer:5 gcd(i,answer): 1i:8 answer:30 gcd(i,answer): 2i:14 answer:120 gcd(i,answer): 2최대공약수만을 나눈다는 것은 동일한 공약수를 1번만 곱하겠다는 의미와 같다. 즉, 8과 30이 있을 때 8을 곱할 건데 이전에 이미 2를 곱했었기 때문에 2를 중복으로 곱하지 않기 위해 나누어준다. lcm추가적으로 math.lcm함수는 최소공배수를 찾아주는 함수다. 따라서 저 문제는 lcm으로 풀면 한 줄이면 가능하다. 하지만 3.9버전부터 사용가능하다import mathdef solution(arr): return math.lcm(arr)+ 추가적으로 알아본 함수/표현zip(*iterable)zip(*list)를 사용하게 되면 행이 아닌 열 단위로 값을 볼 수 있다.&amp;gt;&amp;gt;&amp;gt; x = [[0,0,0],[1,1,1]]&amp;gt;&amp;gt;&amp;gt; for i in zip(*x): print(i)[[0,1],[0,1],[0,1]]range(n,0,-1)반복문을 n부터 0까지 반복하고자 할 때는 for i in range(n,0,-1)로 작성하면 된다.powpow(x,y)는 x의 y 제곱한 결과를 추출한다.filterfilter(함수, 반복가능한 자료형)은 반복 가능한 자료형이 함수에 입력됬을 때 반환 값이 참인 것만 묶어서 리턴해준다.def positive(x): return x &amp;gt; 0print(list(filter(positive, [1, -3, 2, 0, -5, 6])))positive함수에 반복가능한 자료형인 list[1,-3,2,0,-5,6]을 하나씩 집어넣어 참인 것만 묶기 때문에 1,2,6 만 추출할 수 있다.allall(x)은 반복 가능한 자료형 x를 인수로 받아 x의 요소가 모두 참이면 True, 하나라도 False이면 False를 추출한다." }, { "title": "ROS 개요 및 설치", "url": "/posts/ros1/", "categories": "Review, ROS", "tags": "ROS", "date": "2022-01-10 13:00:00 +0900", "snippet": "ROS 개요ROS란 로봇 응용프로그램을 개발할 때 필요한 오픈소스 로봇 운영체제라고 할 수 있다. 기본적으로 가볍게 설계되어 있어 main()으로 불러오지 않아도 되고, python과 c++로 구현되어 있다. 하드웨어 추상화 저수준 기기 제어 로보틱스에서 많이 사용되는 센싱 인식, 지도 작성 모션 플래닝등의 기능 구현 프로세스 사이간 메시지 전달, 패키지 관리 기능 제공 개발환경에 필요한 라이브러리와 다양한 개발과 디버깅 도구을 제공한다. 공동 작업 가능메타운영체제메타운영체제(Meta-Operating System)은 어플리케이션과 분산 컴퓨팅 자원간의 가상화 레이어로 분산 컴퓨팅 자원을 활용하여, 스케쥴링 및 로드, 감시, 에러 처리 등을 실행하는 시스템이다. 또, 다수의 하드웨어간의 데이터 송수신, 스케쥴링, 에러 처리 등 로봇 응용 소프트웨어를 위한 필수 기능들을 라이브러리 형태로 제공하고 있다. 이런 로봇 소프트웨어 프레임워크를 기반으로 다양한 목적의 응용 프로그램을 개발, 관리, 제공하고 있으며 유저들이 개발한 패키지 또한 유통하는 생태계를 갖추고 있다.ROS 용어 정리 노드 최소 단위의 실행 가능한 프로세서를 가리키는 용어로서 하나의 실행 가능한 프로그램이다. ROS에서는 최소한의 실행단위로 프로그램을 나누어 작업하게 되므로 각 노드는 메시지 통신으로 데이터를 주고 받는다. 퍼블리셔, 서브스크라이버, 서비스 서버, 서비스 클라이언트 마스터마스터는 노드와 노드 사이의 연결과 메시지 통신을 위한 네임 서버와 같은 역할을 한다. roscore가 실행 명령어이고, 마스터를 실행하면 각 노드의 이름을 등록하고 필요에 따라 정보를 받을 수 있다. 마스터가 없이는 노드간의 접속, 토픽과 서비스와 같은 메시지 통신을 할 수 없다. 패키지하나 이상의 노드, 노드 실행을 위한 정보 등을 묶어 놓는 것이다. 즉 ROS를 구성하는 기본 단위로, 하나 이상의 노드를 포함하거나 다른 패키지의 노드를 실행하기 위한 설정 파일들을 포함한다. 패키지의 묶음을 메타패키지라 하여 따로 분리한다. 메시지메시지를 통해 노드간의 데이터를 주고받게 된다. 메시지는 integer, floating, point, boolean와 같은 변수형태다. 또한, 메시지 안에 메시지를 품고 있는 간단한 데이터 구조 및 메시지들의 배열과 같은 구조도 사용할 수 있다. 퍼블리시 및 퍼블리셔 퍼블리시: 토픽의 내용에 해당하는 메시지 형태의 데이터를 송수신 퍼블리셔 노드: 퍼블리시를 수행하기 위해 토픽을 포함한 자신의 정보들을 마스터에 등록, 서브스크라이버에 메시지 전송 서브스크라이브 및 서브스크라이버 서브스크라이브: 토픽의 내용에 해당하는 메시지를 원하는 서브스크라이버 노드에 보냄 서브스크라이버 노드: 서브스크라이버를 하기 위해 마스터에 자신의 노드 정보 및 자신이 원하는 토픽을 등록하고, 자신이 원하는 토픽을 퍼블리시하는 퍼블리셔 노드의 정보를 마스터에게 직접 퍼블리셔 노드에 접속해서 메시지를 받는다. 토픽퍼블리셔 노드가 하나의 이야깃거리에 대해 토픽으로 마스터에 등록한 후 이야기거리에 대한 이야기를 메시지 형태로 퍼블리시한다. 서브스크라이버는 마스터에 등록된 토픽이름에 해당하는 퍼블리셔의 노드 정보를 받는다. 퍼블리셔 노드에서 서브스크라이버 노드로 메시지를 토픽으로 송수신한다. 하나의 토픽에 대해 복수의 퍼블리셔, 복스의 서브스크라이버도 가능하다. 서비스서비스 메시지 통신은 특정 목적의 작업 해당되는 서비스를 요청하는 서비스 클라이언트와 서비스 응답을 담당하는 서비스 서버간의 동적 양방향 서비스 메시지 통신을 말한다. 서비스 서버서비스 서버는 요청을 입력으로 받고 응답을 출력으로 하는 서비스 메시지 통신의 서버 역할을 말한다. 요청과 응답은 모두 메시지로 되어 있으며, 서비스 요청을 받으면 지정된 서비스를 수행한 다음 그 결과를 서비스 클라이언트에게 전달한다. 서비스 서버는 정해진 명령을 받아 수행하는 노드에 사용된다. 서비스 클라이언트서비스 클라이언트는 요청을 출력으로 하고, 응답을 입력으로 받는 서비스 메시지 통신의 클라이언트 역할을 말한다. 요청과 응답은 모두 메시지로 되어 있으며, 서비스 요청을 서비스 서버에 전달하고 그 결과값을 받는다. 서비스 클라이언트는 정해진 명령을 지시하고 결과 값을 받는 노드에 사용된다. 액션액션은 서비스처럼 양방향을 요구하나 요청 처리 후 응답까지 오랜시간이 걸리고, 중간 결과값이 필요한 경우에 사용되는 메시지통신 방식이다. 액션 파일은 서비스와 비슷하게 요청과 응답에 해당되는 목표와 결과가 있지만, 중간 결과값에 해당되는 피드백이 추가되었다. 액션 서버액션 서버는 액션 클라이언트로부터 목표를 입력으로 받고, 결과 및 피드백 값을 출력으로 하는 메시지 통신의 서버 역할을 한다. 액션 클라이언트로부터 목표값을 전달받은 후 지정된 실질적인 액션의 실행을 담당한다. 액션 클라이언트액션 클라이언트는 목표를 출력으로 하고, 액션 서버로부터 결과 및 피드백 값을 입력으로 받는 메시지 통신의 클라이언트 역할을 말한다. 액션 서버에게 목표를 전달하고 결과 및 피드백을 수신 받아 다음 지시를 내리거나 목표를 취소하는 역할을 한다. catkin캐킨은 ROS의 빌드 시스템을 말한다. ROS의 빌드 시스템은 기본적으로 CMake를 이용하고 있어서 패키지 폴더에 CMakeList.txt라는 파일에 빌드 환경을 기술하고 있다. 캐킨 빌드 시스템은 ROS와 관련된 빌드, 패키지 관리, 패키지 간의 읜존관계 등을 편리하게 사용할 수 있게 한다. roscoreroscore는 ROS마스터를 구동하는 명령어이다. 같은 네트워크라면 다른 컴퓨터에서 실행해도 된다. 단 멀티 roscore를 지원하는 특수한 경우를 제외하고는 같은 네트워크에서 하나만 구동된다. ROS를 구동하면 사용자가 정해놓은 ROS_MASTER_URI변수에 기재된 URI주소와 포트를 사용하게 된다. 사용자가 설정해놓지 않았다면 URI의 주소로 현재 로컬 IP를 사용하고 11311 포트를 이용한다. rosrunrosrun은 ROS의 기본 실행 명령어이다. 패키지에서 하나의 노드를 실행하는데 사용된다. 노드가 사용하는 URI주소와 포트는 현재 노드가 실행중인 컴퓨터에 저장된 ROS_HOSTNAME 환경변수 값을 URI주소로 사용하며, 포트는 임의의 고유값으로 설정된다. roslaunchrosrun은 하나의 노드를 실행하는 명령어라면, roslaunch는 여러 노드를 실행하는 개념이다. 이 명령어를 통해 하나 그 이상의 정해진 노드를 실행시킬 수 있다. 그 밖의 기능으로 노드를 실행할 때 패키지의 파라미터나 노드 이름, 변경, 노드 네임스페이스 설정, ROS_ROOT 및 ROS_PACKAGE_PATH 설정, 환경변수 설정 변경 등 많은 옵션을 갖춘 노드 실행에 특화된 ROS명령어이다. 노드 간의 통신 마스터 구동 ($roscore) 서브스크라이버 노드 구동 ($rosrun 패키지이름 노드이름) 퍼블리셔 노드 구동 ($rosrun 패키지이름 노드이름) 퍼블리셔 정보 알림 마스터는 서브스크라이버 노드에게 새로운 퍼블리셔 정보를 알린다. 퍼블리셔 노드에 접속 요청 마스터로부터 받은 퍼블리셔 정보를 이용하여 TCPROS 접속을 요청 서브스크라이버 노드에 접속 응답 접속 응답에 해당되는 자신의 TCP URI 주소와 포트번호를 전송 TCP 접속 TCPROS를 이용하여 퍼블리셔 노드와 직접 연결한다. 메시지 전송 (퍼블리셔)발행자 노드는 서브스크라이버 노드에게 메시지를 전송 (토픽) 토픽방식에서는 접속을 끊지 않는 이상 지속적으로 메시지를 전송한다. (연속성) 서비스 요청 및 응답 1회에 한해 접속, 서비스 요청 및 서비스 응답이 수행되고, 서로간의 접속을 끊는다. 서비스는 토픽과 달리 1회에 한해 접속하고, 서비스 요청 및 서비스 응답을 수행한 후 서로간의 접속을 끊게 된다. (1회성) ROS 패키지 설치일단 Ubuntu 버전별로 ROS 버전이 다르다. 따라서 자신의 버전에 맞는 버전을 다운받아야 한다. Ubuntu 14.04 &amp;gt; ROS indigo Ubuntu 16.04 &amp;gt; ROS Kinetic Ubuntu 18.04 &amp;gt; ROS melodic Ubuntu 20.04 &amp;gt; ROS noetic# 자신의 ubuntu 버전 확인하기ice@LAPTOP-FCNUC3SV: ~$lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.04.3 LTSRelease: 20.04Codename: focal ROS 패키지 소프트웨어를 설치할 수 있도록 컴퓨터를 설정sudo sh -c &#39;echo &quot;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main&quot; &amp;gt; /etc/apt/sources.list.d/ros-latest.list&#39; 키 설정sudo apt install curlcurl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add - 키 등록 후 업데이트sudo apt update 패키지 다운로드sudo apt install ros-noetic-desktop-install 사용 가능한 패키지 찾기apt search ros-noetic ROS 환경변수를 bash에 추가echo &quot;source /opt/ros/melodic/setup.bash&quot; &amp;gt;&amp;gt; ~/.bashrcsource ~/.bashrc 추가적인 패키지 설치sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential rosdep 초기화sudo rosdep initrosdep update 설치확인roscoreReference ROS 위키 참고 블로그1 참고 블로그2" }, { "title": "Coding Test[Python] - 완전탐색", "url": "/posts/codingtestsearch/", "categories": "Classlog, coding test review", "tags": "coding test, search", "date": "2022-01-10 13:00:00 +0900", "snippet": "프로그래머스에 올라와 있는 코딩테스트를 진행하면서 공부한 내용 및 막혔던 부분들에 대해 리뷰하고자 합니다. 저는 거의 대부분을 python을 사용하였습니다.이 글은 python을 통한 스택/큐에 대해 리뷰하고 다른 것들을 참고하시려면 아래 링크를 참고해주세요 스택/큐 힙 해시 정렬 연습문제 카카오Overview완전탐색완전탐색에서는 딱히 특정한 방법이 존재하는 것이 아니라 이것저것 시도해봐야 한다. 완전탐색 부분이 수학적인 부분들을 많이 생각해봐야 하는 것 같다. set&amp;gt;&amp;gt;&amp;gt;s = set()&amp;gt;&amp;gt;&amp;gt;s = set([1,2,3]){1,2,3}&amp;gt;&amp;gt;&amp;gt;s = set(&quot;Hello&quot;){&#39;e&#39;,&#39;H&#39;,&#39;l&#39;,&#39;o&#39;} 참고 블로그INDEX 모의고사 소수 찾기 카펫모의고사소수 찾기전체 코드from itertools import permutationsdef solution(n): a = set() for i in range(len(n)): a |= set(map(int, map(&quot;&quot;.join, permutations(list(n), i + 1)))) a -= set(range(0, 2)) for i in range(2, int(max(a) ** 0.5) + 1): a -= set(range(i * 2, max(a) + 1, i)) return len(a)이 문제는 혼자 풀지 못해서 다른 사람의 풀이를 보았다. 하지만, 그래도 이해가 되지 않았다. 일단 모르는 기호나 함수가 많았기 때문이다. 또한, for문으로 각각을 list에 넣지 않고, list(numbers)를 하면 바로 list로 만들어진다. 그렇기에 list(map(int, numbers))를 하게 되면 for문을 사용하지 않아도 된다.set집합(set)은 set()의 괄호 안에 리스트나 문자열 등을 입력할 수 있다. 또 set()과 같이 비어 있는 집합 자료형을 만들어볼 수도 있다.&amp;gt;&amp;gt;&amp;gt;s = set()&amp;gt;&amp;gt;&amp;gt;s = set([1,2,3])&amp;gt;&amp;gt;&amp;gt;li = list(s)&amp;gt;&amp;gt;&amp;gt;li[1,2,3]&amp;gt;&amp;gt;&amp;gt;tu = tuple(s)&amp;gt;&amp;gt;&amp;gt;tu(1,2,3)set은 중복을 허용하지 않기 때문에 자료형의 중복을 제거하기 위한 필터 역할로도 사용할 수 있다.set의 유용한 점 중 하나는 교집합, 합집합, 차집합을 구할 수 있다는 것이다.&amp;gt;&amp;gt;&amp;gt;s1 = set([1,2,3,4,5,6])&amp;gt;&amp;gt;&amp;gt;s2 = set([4,5,6,7,8,9]) 교집합&amp;gt;&amp;gt;&amp;gt;s1 &amp;amp; s2{4,5,6}&amp;gt;&amp;gt;&amp;gt; s1.intersection(s2){4,5,6}s2.intersection(s1)을 사용해도 결과는 같다. 합집합&amp;gt;&amp;gt;&amp;gt; s1 | s2{1,2,3,4,5,6,7,8,9}&amp;gt;&amp;gt;&amp;gt; s1.union(s2){1,2,3,4,5,6,7,8,9} 차집합&amp;gt;&amp;gt;&amp;gt; s1 - s2{1,2,3}&amp;gt;&amp;gt;&amp;gt; s2 - s1{7,8,9}&amp;gt;&amp;gt;&amp;gt; s1.difference(s2){1,2,3}&amp;gt;&amp;gt;&amp;gt; s2.difference(s2){7,8,9} 값 추가값을 추가할 때는 add를 사용한다.&amp;gt;&amp;gt;&amp;gt;s1=set([1,2,3])&amp;gt;&amp;gt;&amp;gt;s1.add(4)여러 개를 추가할 때는 update를 사용한다.&amp;gt;&amp;gt;&amp;gt;s1.update([4,5,6])제거할 때는 remove를 사용한다.&amp;gt;&amp;gt;&amp;gt;s1.remove(2) 참고 블로그|=|는 or 연산자이므로 |=는 |의 결과를 update한다는 뜻이다.&amp;gt;&amp;gt;&amp;gt; s1 = [1,2,3]&amp;gt;&amp;gt;&amp;gt; s2 = [4,5,6]# 결과만 추출&amp;gt;&amp;gt;&amp;gt; s1 | s2&amp;gt;&amp;gt;&amp;gt; s1[1,2,3]# 결과를 저장&amp;gt;&amp;gt;&amp;gt; s1 |= s2&amp;gt;&amp;gt;&amp;gt; s1[1,2,3,4,5,6] 참고 블로그permutationspython의 itertools를 사용하면 순열과 조합을 for문 없이 구현할 수 있다. 순열(permutation)순열이란 몇 개를 골라 순서를 고려해 나열한 경우의 수이다. 즉, 서로 다른 n개 중 r개를 골라 순서를 나열하는 것이고, 그래서 수학에서 nPr이라는 기호를 사용한다. 순열은 순서를 고려하는 것이기 때문에 [A,B,C] 리스트를 2개를 골라 나열한다면 [(A,B),(B,C),(B,A),(B,C),(C,A),(C,B)]가 나오게 된다.import itertools&amp;gt;&amp;gt;&amp;gt;array = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]&amp;gt;&amp;gt;&amp;gt;npr = itertools.permutations(array,2)&amp;gt;&amp;gt;&amp;gt;print(list(npr))[(&#39;A&#39;,&#39;B&#39;),(&#39;B&#39;,&#39;C&#39;),(&#39;B&#39;,&#39;A&#39;),(&#39;B&#39;,&#39;C&#39;),(&#39;C&#39;,&#39;A&#39;),(&#39;C&#39;,&#39;B&#39;)] 조합(combination )조합이란 순서를 고려하지 않고 나열한 경우의 수이다. 따라서 위에서 (‘A’,’B’)와 (‘B’,’A’)는 다른 경우지만, 여기서는 같은 경우로 판단한다. combination에서 c를 따 nCr의 기호를 사용한다.import itertools&amp;gt;&amp;gt;&amp;gt;array = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]&amp;gt;&amp;gt;&amp;gt;nCr = itertools.combinations(array,2)&amp;gt;&amp;gt;&amp;gt;print(list(nCr))[(&#39;A&#39;,&#39;B&#39;),(&#39;A&#39;,&#39;C&#39;),(&#39;B&#39;,&#39;C&#39;)]추가로 한가지 더하자면 조건에 맞게 각각을 추가하는 것보다 빼는 것이 시간적으로 더 짧다고 한다.소수 찾는 법소수인지 판별하기 위해서 간단하게 생각하면 2~N-1까지 해당 수를 나눠보고 어떠한 값도 나누어지지 않는다면 소수라고 생각할 수 있다. 이를 코드로 표현하게 되면 다음과 같다.for i in range(2,n+1): if n % i == 0: return Falsereturn True하지만 이 방법은 시간이 너무 오래걸릴 수 있다. 그렇다면 어떻게 하면 시간을 줄이면서 소수를 구할 수 있을까?약수의 특성을 활용하면 연산 횟수를 반으로 줄일 수 있다. 즉, 특정 수n의 약수를 나열해보았을 때 가운데 수를 기준으로 약수가 대칭된다. 16을 예로 들면 16의 약수는 [1,2,4,8,16]으로 4를 기준으로 대칭되고 있다. (1x16, 2x8, 4x4)이를 활용하여 가운데 값을 기준으로 한쪽만 연산하더라도 다른 쪽의 약수를 구할 수 있다. 중간값은 제곱근, 즉 루트를 취하여 구할 수 있으며, 이 값을 기준으로 왼쪽에 약수가 하나라도 존재하지 않는다면 오른쪽에도 약수가 존재하지 않는다는 말이다. 이를 코드로 표현하게 되면 range(2, int(n ** 0.5) + 1)이 될 것이다. ** 0.5는 제곱근을 뜻하며, sqrt()로 사용해도 무관하다.for i in range(2, int(n**0.5) + 1): if n % i == 0: return Falsereturn True 에라토스테네스의 체이때까지 1개의 수에 대해서만 구해보았다. 하지만 여러 수에 대해 소수 판별을 해줘야 한다면 어떻게 해야 할까?100~300 사이의 모든 소수를 구해야 한다면 위의 방법으로 하나하나 판별해야 한다면 너무 오래 걸릴 것이다. 이 상황에서 사용할 수 있는 알고리즘 중 하나가 에라토스테네스의 체 방식이다. 알고리즘 작동방식은 2~N까지의 범위가 담긴 배열을 만든다. 해당 배열 내의 가장 작은 수 i 부터, i의 배수들을 해당 배열에서 지워준다. 주어진 범위 내에서 i의 배수가 모두 지워지면 i 다음으로 작은 수의 배수를 같은 방식으로 배열에서 지워준다. 더 이상 반복할 수 없을 때까지 2,3번 과정을 반복한다.이를 코드화 하면 다음과 같다.for i in range(2, int(max(s)**0.5) +1): s -= set(range(i*2, max(s)+1,i)) print(i, i*2, max(s)+1, int(max(s)**0.5) + 1) print(set(range(i*2, max(s)+1, i)))입력값 &amp;gt; [71, 17, 7]출력 〉 2 4 72 9 {4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70}3 6 72 9 {6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69}4 8 72 9 {32, 64, 36, 68, 8, 40, 12, 44, 60, 16, 48, 20, 52, 24, 56, 28}5 10 72 9{65, 35, 70, 40, 10, 45, 15, 50, 20, 55, 25, 60, 30}6 12 72 9 {66, 36, 42, 12, 48, 18, 54, 24, 60, 30}7 14 72 9 {35, 70, 42, 14, 49, 21, 56, 28, 63}8 16 72 9 {32, 64, 40, 16, 48, 24, 56}가장 큰 값을 기준으로 모든 약수를 빼내면 다른 것들은 자동으로 만족하게 된다. 그래서 max(s)값인 71을 기준으로 소수 찾기를 진행한다.따라서 71^0.5 = 8.42 =&amp;gt; 8 + 1 = 9 까지 for문을 진행하며, 각 i마다 i는 소수이므로 i에 2를 곱한 후 i단위로 약수들을 다 제거해준다.카펫def solution(b, y): answer = [] yh=0;yw=0 for i in range(1,int(y**0.5)+1): k = y // i if y % i== 0: if 2*k + 2*i + 4 == b: yh = i; yw = k answer = [yw+2, yh+2] answer.sort(reverse=True) return answer약수와 비슷하게 가운데 값을 기준으로 약수를 다 구한 후 둘레에 대한 brown 값과 비교하여 맞다고 하면 그것이 yellow의 가로,세로길이가 될 것이다." }, { "title": "Coding Test[C++] - 완전탐색", "url": "/posts/codingtestcsearch/", "categories": "Classlog, coding test review", "tags": "coding test, search", "date": "2022-01-10 13:00:00 +0900", "snippet": "프로그래머스에 기재되어 있는 c++ coding test에 대해 리뷰하고자 합니다. 그 중에서도 연습문제에 해당하는 문제를 이 곳에 작성할 예정입니다. 다른 것을 참고하시려면 아래 링크를 클릭하시기 바랍니다. 해시 정렬 연습문제 카카오 블라인드 채용 문제overview완전탐색수학적으로 알고리즘을 생각한 후 그것을 코드로 표현하는 문제가 많은 듯하다.Index 모의고사 사전순 부분 문자열모의고사나의 전체 코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;algorithm&amp;gt;#include &amp;lt;cmath&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;vector&amp;lt;int&amp;gt; solution(vector&amp;lt;int&amp;gt; answers) { vector&amp;lt;int&amp;gt; answer; vector&amp;lt;int&amp;gt; f = {1,2,3,4,5}; vector&amp;lt;int&amp;gt; s = {2,1,2,3,2,4,2,5}; vector&amp;lt;int&amp;gt; t = {3,3,1,1,2,2,4,4,5,5}; vector&amp;lt;int&amp;gt; count(3); for(auto i=0;i&amp;lt;answers.size();i++) { if(answers[i] == f[i%f.size()]) count[0]++; if(answers[i] == s[i%s.size()]) count[1]++; if(answers[i] == t[i%t.size()]) count[2]++; } int count_max = *max_element(count.begin(), count.end()); for(int i=0;i&amp;lt;count.size();i++) { if(count[i] == count_max) answer.push_back(i+1); } return answer;}*max_element()max_element(start,end)는 vector 중에서 [start, end) 구간에서 가장 큰 값을 추출하는데, 이는 cout « endl; 에서 사용이 되기 때문에 포인터와 함께 써야 한다.cout &amp;lt;&amp;lt; max_element(count.begin(), count.end()) &amp;lt;&amp;lt; endl;int count_max = *max_element(count.begin(), count.end());사전순 부분문자열 string과 사전순의 특성을 사용하여 진행#include &amp;lt;iostream&amp;gt;#include &amp;lt;vector&amp;gt;using namespace std;// 테스트를 위한 변수 선언string ss{ &quot;xyb&quot; };// 미리 메모리 크기를 지정해서 변수 선언string answer{ &quot;&quot; };string solution(string ss, string answer){ // 출력값을 담을 변수 선언 string lists; for (auto s : ss) { // 에러를 방지하기 위해 비어 있는지를 체크하고, // 현재 값이 순서상 가장 맨 뒤가 될 때까지 맨 뒤 값들을 뺀다. while (!lists.empty() &amp;amp;&amp;amp; lists.back() &amp;lt; s) lists.pop_back(); // 처리가 완료된 리스트에 삽입 lists.push_back(s); } return lists;}int main(){ // 정답 확인을 위한 변수 선언 string correction; // 입력 인자를 통해 정답을 입력 cin &amp;gt;&amp;gt; correction; // 출력한 리스트와 정답이 같으면 correct, 아니면 No를 출력 if (solution(ss, answer) == correction) cout &amp;lt;&amp;lt; &quot;correct&quot; &amp;lt;&amp;lt; endl; else cout &amp;lt;&amp;lt; &quot;No&quot; &amp;lt;&amp;lt; endl;}이 문제에서는 특별한 함수나 익숙치 않은 메서드를 사용한 것이 아니라, 사전 순에 따라 알고리즘을 생각해내는 것이 중요했다." }, { "title": "Coding Test[Python] - 정렬", "url": "/posts/codingtestsort/", "categories": "Classlog, coding test review", "tags": "coding test, sorted", "date": "2022-01-09 13:00:00 +0900", "snippet": "프로그래머스에 올라와 있는 코딩테스트를 진행하면서 공부한 내용 및 막혔던 부분들에 대해 리뷰하고자 합니다.이 글은 python을 통한 힙에 대해 리뷰하고 다른 것들을 참고하시려면 아래 링크를 참고해주세요. 스택/큐 힙 해시 완전탐색 연습문제 카카오overview정렬list.sort()는 내림차순으로 정렬하고, list.sort(reverse=True)는 오름차순으로 정렬하는 것이다. list.sort(key=len)으로 하면, 길이를 기준으로 정렬할 수 있다.list.reverse()를 사용하게 되면 전체 list를 뒤집는다.위의 코드들은 출력만 한다. 결과를 반환하고자 한다면, sorted와 reversed를 사용해야 한다.&amp;gt;&amp;gt;&amp;gt; x = [1 ,11, 2, 3]&amp;gt;&amp;gt;&amp;gt; s = sorted(x)&amp;gt;&amp;gt;&amp;gt; s[1, 2, 3, 11]&amp;gt;&amp;gt;&amp;gt; r = reversed(x)&amp;gt;&amp;gt;&amp;gt; y&amp;lt;list_reverseiterator object at 0x1060c9fd0&amp;gt;&amp;gt;&amp;gt;&amp;gt; list(y)[3, 2, 11, 1]reversed를 하면 확인을 위해서 list로 한번 더 변형을 해야 한다.Index k번째수 가장 큰 수 H-IndexK번째 수나의 전체 코드def solution(array, commands): answer = [] for i in commands: start = i[0]; end = i[1] arr = array[start-1:end] arr = sorted(arr) answer.append(arr[i[2]-1]) #print(&quot;start: {}\\t end: {}\\t arr: {}\\t answer: {}&quot;.format(start,end,arr,answer)) return answer입력값 〉 [1, 5, 2, 6, 3, 7, 4], [[2, 5, 3], [4, 4, 1], [1, 7, 3]]출력 〉 start: 2 end: 5 arr: [2, 3, 5, 6] answer: [5]start: 4 end: 4 arr: [6] answer: [5, 6]start: 1 end: 7 arr: [1, 2, 3, 4, 5, 6, 7] answer: [5, 6, 3]가장 큰 수참조한 코드def solution(numbers): answer = &#39;&#39; numbers = list(map(str,numbers)) numbers.sort(key= lambda x:x*4, reverse=True) return str(int(&#39;&#39;.join(numbers)))나의 경우 for문을 중복해서 사용했다. 이는 시간 초과를 야기했고, 26점을 받았다. 그래서 다른 사람의 풀이를 보았는데, map이라는 함수와 key를 적절히 사용한 것을 보았다. 생각치도 못했다. 계속 x*4를 만들어주고, 그를 통해 정렬한 후 원래 숫자에 해당하는 길이만큼 잘라주었는데, 반례가 너무 많았다.mapmap은 리스트의 요소를 지정된 함수로 처리해주는 함수다. 원본 리스트를 변경하지 않고 새 리스트를 생성하게 된다. list(map(함수,리스트)) tuple(map(함수,튜플))&amp;gt;&amp;gt;&amp;gt; a = [1.2, 2.5, 3.7, 4.6]&amp;gt;&amp;gt;&amp;gt; a = list(map(int, a))&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4]매번 for문을 반복하면서 요소를 변경하지 않아도 되는 편한 함수다. 참고 블로그H-index나의 전체 코드def solution(citations): answer = 0 maxs = 0 citations.sort() for h in range(max(citations)+1): up = [h &amp;lt;= i for i in citations] count = up.count(True) if h &amp;lt;= count: maxs = h else: return h print(h,up,count,maxs) return maxs입력값 〉 [10, 8, 5, 4, 3]출력 〉 0 [True, True, True, True, True] 5 01 [True, True, True, True, True] 5 12 [True, True, True, True, True] 5 23 [True, True, True, True, True] 5 34 [False, True, True, True, True] 4 45 [False, False, True, True, True] 3 46 [False, False, False, True, True] 2 47 [False, False, False, True, True] 2 48 [False, False, False, True, True] 2 49 [False, False, False, False, True] 1 410 [False, False, False, False, True] 1 4문제를 이해하기만 하면 풀기 쉬운 문제다." }, { "title": "Coding Test [C++] - 정렬", "url": "/posts/codingtestcsort/", "categories": "Classlog, coding test review", "tags": "coding test, sorted", "date": "2022-01-09 13:00:00 +0900", "snippet": "프로그래머스에 기재되어 있는 c++ coding test에 대해 리뷰하고자 합니다. 그 중에서도 연습문제에 해당하는 문제를 이 곳에 작성할 예정입니다. 다른 것을 참고하시려면 아래 링크를 클릭하시기 바랍니다. 해시 연습문제 완전탐색 카카오 블라인드 채용 문제overviewIndex k번째 수k번째수#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;#include &amp;lt;algorithm&amp;gt;using namespace std;bool compare(const int&amp;amp; a, const int&amp;amp; b) { return a &amp;lt; b;}vector&amp;lt;int&amp;gt; solution(vector&amp;lt;int&amp;gt; array, vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; commands) { vector&amp;lt;int&amp;gt; answer; for (auto c : commands) { vector&amp;lt;int&amp;gt; arr(c[1] - c[0] + 1); // 특정 범위의 부분을 복사하기 위해 copy함수 사용, copy(원본 시작지점, 원본 끝지점, 복사될 시작지점); copy(array.begin() + c[0] - 1, array.begin() + c[1], arr.begin()); // 정렬을 위해 sort함수 사용, 정확하게 파악하기 위해 compare함수 선언해보았습니다. sort(정렬 시작지점, 정렬 끝지점, 정렬 타입); sort(arr.begin(), arr.end(), compare); // 정렬된 백터에서 c[2] -1 번째 요소를 정답 vector에 삽입 answer.push_back(arr[c[2] - 1]); } return answer;}int main() { vector&amp;lt;int&amp;gt; array { 1, 5, 2, 6, 3, 7, 4 }; vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; commands{ {2, 5, 3}, { 4, 4, 1 }, { 1, 7, 3 } }; cout &amp;lt;&amp;lt; solution(array,commands);}입력값 〉 [1, 5, 2, 6, 3, 7, 4], [[2, 5, 3], [4, 4, 1], [1, 7, 3]]출력 〉 [5,6,3]vector 함수vector을 정렬하고 원하는 값들을 추출한다.다양한 함수들은 연습문제에 적혀있으니 참고하고, 거기에 없는 부분들만 체크하고자 한다. vector.remove_if(bool)bool predicate(int num){ return num&amp;gt;=100 &amp;amp;&amp;amp; num&amp;lt;=200;}vector.remove_if(predicate) 참고 블로그가장 큰 수#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;algorithm&amp;gt;using namespace std;bool compare(string a, string b) { return (a + b &amp;gt; b + a);}string solution(vector&amp;lt;int&amp;gt; numbers) { string answer = &quot;&quot;; vector&amp;lt;string&amp;gt; str; for (int i : numbers) str.push_back(to_string(i)); sort(str.begin(), str.end(), compare); for (auto i=str.begin();i&amp;lt;str.end();i++) { answer += *i; } if(answer[0] == &#39;0&#39;){ answer = &#39;0&#39;; } return answer;}다른 사람의 코드를 참고했는데, sort에 bool 함수를 집어넣고, 개별로 서로 비교하여 정렬하는 것이 핵심인 것 같다.constconst는 값을 선언할 수 있도록 도와주는 키워드다. 즉, 값을 변경할 수 없게 한다. 따라서 const는 값을 한 번 설정하고, 그 값을 유지하면서 사용할 때 필요하다.포인터와 const를 함께 사용하게 되면, 상수를 가르키는 포인터(* ptr)가 가르키는 공간은 수정할 수 없는(const) 공간이지만, 상수 변수의 주소를 가르키는 포인터는 수정할 수 있다.int value = 5, value2 = 11;const int * ptr = &amp;amp;value;// *ptr = 10; // error! can&#39;t change const valuevalue = 10; // ok!std::cout &amp;lt;&amp;lt; value &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; *ptr &amp;lt;&amp;lt; std::endl; // 10 10 참고 블로그boolbool 변수 선언bool tr = true;bool fa = false;bool 함수 선언bool b(a,b) { if(a == b) return true; return false;}sortbool compare(string a, string b) { return (a + b &amp;gt; b + a);}sort(str.begin(), str.end(), compare);compare 함수를 만들어서 sort의 세번째 인자 값으로 넣게 되면, 해당 함수의 반환 값에 맞게 정렬한다. 즉, true가 되도록 정렬하겠다는 의미이다.* 포인터포인터는 특정 값을 저장하는 것이 아니라 메모리 주소를 저장하는 변수다.int value = 5;int * ptr = &amp;amp;value;ptr은 값으로 value 변수 값의 주소를 가지고 있다. 그러므로 ptr을 value 변수를 ‘가리키는’ 값이라고 할 수 있다.&amp;amp;to_stringto_string(숫자)를 하게 되면 숫자가 str로 변경된다.stoi()string을 int로 변환하기 위한 std::stoi()가 있다. 그 외에도 stol()(str to long long), stod()(str to double), stof()(str to float)등도 있다.int i = std::stoi(int_val);H-index나의 전체 코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;algorithm&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;int solution(vector&amp;lt;int&amp;gt; citations) { int answer = 0; vector&amp;lt;int&amp;gt; answ; sort(citations.begin(),citations.end(),greater&amp;lt;int&amp;gt;()); for(auto i : citations) cout &amp;lt;&amp;lt; i &amp;lt;&amp;lt; endl; for(int i=0;i&amp;lt;citations.size();i++) { cout &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; citations[i] &amp;lt;&amp;lt; endl; if(citations[i] &amp;lt;= i) return i; }}입력값 〉 [10, 8, 5, 4, 3]출력 〉 1085430 101 82 53 44 3정렬한 후 차례대로 비교했을 때 i가 증가한다는 것은 특정 값 이상인 논문이 i개 있다는 말과 같다. 즉 i가 3일때는 4보다 큰 값이 3개 있다는 것이고, i가 4일 때 3보다 큰 값이 4개 있으므로 i=4가 h의 최댓값이 된다.greater()sort는 기본적으로 오름차순으로 정렬한다. 내림차순으로 정렬하고자 하면, greater()를 사용하면 된다.sort(start,end,greater&amp;lt;int&amp;gt;()) 을 하게 되면 내림차순으로 정렬된다." }, { "title": "Coding Test[Python] - 해시", "url": "/posts/codingtesthash/", "categories": "Classlog, coding test review", "tags": "coding test, hash", "date": "2022-01-07 13:00:00 +0900", "snippet": "프로그래머스에 올라와 있는 코딩테스트를 진행하면서 공부한 내용 및 막혔던 부분들에 대해 리뷰하고자 합니다.이 글은 python을 통한 힙에 대해 리뷰하고 다른 것들을 참고하시려면 아래 링크를 참고해주세요 스택/큐 힙 정렬 완전탐색 연습문제 카카오overview해시해시는 dictionary라는 자료구조를 사용하는 방식을 말하는데, dict을 사용하는 이유는 리스트를 사용하지 못할 때list[‘a’]가 불가능하기 때문에 dict을 통해 dict[‘a’]를 찾을 수 있다. 빠른 탐색이 필요할 때딕셔너리는 리스트보다 탐색이 빠르다. 집계가 필요할 떄원소의 개수를 세는 문제에서 해시와 collections 모듈의 counter 클래스를 사용하면 빠르게 해결할 수 있다. Index 완주하지 못한 선수 전화번호 목록 위장 베스트앨범완주하지 못한 선수나의 전체 코드 - list 사용def solution(participant, completion): for i in participant: if not i in completion: return i elif participant.count(i) != completion.count(i): return idict 사용def solution(participant, completion): c = {c:i for i,c in enumerate(completion)} p = {p:i for i,p in enumerate(participant)} for i in p.keys(): if c.get(i,-1) == -1: return i elif participant.count(i) &amp;gt; 1 and participant.count(i) != completion.count(i): return i입력값 〉 [&quot;marina&quot;, &quot;josipa&quot;, &quot;nikola&quot;, &quot;vinko&quot;, &quot;filipa&quot;], [&quot;josipa&quot;, &quot;filipa&quot;, &quot;marina&quot;, &quot;nikola&quot;]출력 〉 dict p: {&#39;josipa&#39;: 0, &#39;filipa&#39;: 1, &#39;marina&#39;: 2, &#39;nikola&#39;: 3}return : &#39;vinko&#39;효율성이 4개 안되지만, 이보다 좋은 방법이 떠오르지 않았다.dictdictionary에서 사용하는 함수들로는 다음과 같다. dict.get(key, 특정 값)dict에서 원소를 가져오는데, 해당하는 값이 없을 경우 keyerror 대신 특정한 값을 리턴한다.dict = {&#39;가&#39;: 3, &#39;나&#39;: 1}dict.get(&#39;다&#39;, 0) # 0dict에 ‘다’라는 key가 없다면 0을 출력한다. del dict[key]dict에서 key에 해당하는 값을 삭제한다. dict.pop()함수를 사용해도 된다. 이때 dict.pop(&#39;가&#39;,2)와 같이 적게 되면 ‘가’ key를 삭제하고자 하는데 값이 없는 경우 2를 리턴한다. dict.items() / dict.keys() / dict.values()for문을 사용할 때 key-value를 동시에 반복하기 위해 사용할 수 있다. for key, value in dict.items(): print(key,value)하지만, key만 보고 싶을 때는 dict.keys()를 할 수 있다.dict.keys() # &#39;dict_keys([&#39;가&#39;,&#39;나&#39;])또한, value만 보고 싶을 때는 dict.values()를 할 수 있다.dict.values() # &#39;dict_values([3,1]) 참고 블로그try/except코드에서 에러가 발생했을 때, 에러를 핸들링하는 기능이다. try 블럭에서는 실행할 코드를 작성하고, 여기서 에러가 발생했을 때 except를 통해 예외 처리한다. 마지막에 항상 실행되는 문으로 finally도 함께 쓸 수 있다.value = [1,2]for i in range(3): try: sum = value[i] except Indexerror as err: print(&quot;indexerror: &quot;,err) except keyerror as err: print(&quot;keyerror: &quot;,err) finally: print(&quot;finish!&quot;, &quot; sum: &quot;, sum)finish! sum: 1finish! sum: 2indexerror: list index out of rangefinish! sum: 2value[0]과 value[1]은 값이 있지만, value[2]는 값이 없다. 따라서 indexerror가 발생했고, 그것을 예외 처리하여 indexerror: error 내용 으로 출력했고, finally를 매 반복마다 출력한 것을 볼 수 있다.전화번호 목록나의 전체 코드def solution(phoneBook): phoneBook.sort() for p1, p2 in zip(phoneBook, phoneBook[1:]): print(p1,p2) if p2.startswith(p1): return False return True12 2323 3434 4545 56정렬 후 바로 앞의 문자와만 비교하면 된다.위장나의 전체 코드def solution(clothes): answer = 1 b={} cloth = {n:k for n,k in clothes} for k,v in cloth.items(): print(&quot;k: {} \\t v: {} &quot;.format(k,v)) if v not in b: b[v] = 1 else: b[v] += 1 for k,v in b.items(): v += 1 answer *= v print(k,v) answer -= 1 return answer다른 사람의 풀이를 조금 참고 했는데, 나는 b라는 종류별 수량을 리스트로 또 만들어 거기서 1,2,3,… 개씩 빼면서 나머지의 원소들을 곱하는 식으로 계산하고, 완전 다 입었을 때와 개별로 입었을 때의 값을 더해주었다. 그 코드는 매우 지저분했으며, 예외사항이 너무 많았다.하지만, 입지 않는다는 경우의 수를 1씩 더 해주는 방식으로 경우의 수를 생각해본다면 결과는 똑같이 나온다.베스트앨범나의 전체 콛def solution(genres, plays): answer = [] sing = dict() for g,p in zip(genres, plays): if g in sing: sing[g] += p else: sing[g] = p sing = sorted(sing.items(), key=lambda x: x[1],reverse=True) sing = [s[0] for s in sing] print(sing) for i in sing: s = [] for k in range(len(plays)): try: if plays[genres.index(i,k)] not in s:s.append(plays[genres.index(i,k)]) except ValueError as err: print(err) s = sorted(s,reverse=True) s = s[:2] print(s) for j in s: answer.append(plays.index(j)) return answer입력값 〉 [&quot;classic&quot;, &quot;pop&quot;, &quot;classic&quot;, &quot;classic&quot;, &quot;pop&quot;], [500, 600, 150, 800, 2500]출력 〉 sing: [&#39;pop&#39;, &#39;classic&#39;]pop: [2500, 600]&#39;classic&#39; is not in listclassic: [800, 500]이 코드는 86.7/100 의 점수를 받았다. 먼저 조회수를 기준으로 정렬한 후 장르만 추출한다. 그 다음 각 장르마다 조회수를 추출하여 크기별로 정렬하고, 2개만 추출하기 때문에 2개까지만 리턴한다." }, { "title": "Coding Test[Python] - 힙", "url": "/posts/codingtestheapq/", "categories": "Classlog, coding test review", "tags": "coding test, heapq", "date": "2022-01-06 13:00:00 +0900", "snippet": "프로그래머스에 올라와 있는 코딩테스트를 진행하면서 공부한 내용 및 막혔던 부분들에 대해 리뷰하고자 합니다.이 글은 python을 통한 힙에 대해 리뷰하고 다른 것들을 참고하시려면 아래 링크를 참고해주세요 스택/큐 해시 정렬 완전탐색 연습문제 카카오overview힙(heap)힙 모듈은 이진 트리 기반의 최소 힙 자료구조를 제공한다. heapq을 사용하면 원소들이 항상 정렬된 상태로 추가되고 삭제되며, 가장 작은 값이 항상 0에 위치한다. 또한 내부적으로 모든 원소k는 항상 자식 원소들(2k+1, 2k+2)보다 크기가 작거나 같도록 원소가 추가되고 삭제된다.heapq 라이브러리를 사용하여 heapq.heappush/heapq.heappop 등을 사용하여 추가 및 삭제한다.Index 더 맵게 이중우선순위큐 디스크 컨트롤러더 맵게나의 전체 코드import heapq as hqdef solution(scoville, K): answer = 0 hq.heapify(scoville) while scoville[0] &amp;lt; K: if len(scoville) &amp;lt;= 1: break a = hq.heappop(scoville) b = hq.heappop(scoville) hq.heappush(scoville, a+2*b) #print(scoville) answer += 1 if scoville[0] &amp;lt; K: answer = -1 return answerheapq 리스트를 힙으로 변환전 scoville list [9, 8, 7, 6, 5]heapq.heapify(scoville)변환 후 scoville list [5, 6, 7, 9, 8]이처럼 기존의 리스트를 heap 배열에 맞게 변경할 수 있다. 힙에 원소 추가전 scoville list [5, 6, 7, 9, 8]heapq.heappush(scoville, 4)후 scoville list [4, 6, 5, 9, 8, 7]원소를 추가할 수 있는데, heap 배열에서는 최솟값이 항상 0에 위치하므로 맨 앞에 추가된 것을 볼 수 있다. 또한, 그에 맞게 heap 배열 형태로 재배치되었다. 힙에 원소 삭제전 scoville list [4, 6, 5, 9, 8, 7]heapq.heappop(scoville)후 scoville list [5, 6, 7, 9, 8] [응용] 최대 힙블로그에는 다르게 나와 있지만, 나의 아이디어로는 heappop은 최솟값을 삭제하는 것이니 list의 길이-1 만큼 pop을 시키면 최댓값만 남지 않을까하는 생각이다.heapq.heapify(scoville) while len(scoville) != 1: heapq.heappop(scoville) print(scoville)scoville: [1, 2, 3, 9, 10, 12]max value: [12] [응용] K번째 최솟값/최댓값위의 방법을 사용하여 k번째 최댓값을 찾으면 된다.k = 4maxs = 0while len(scoville) &amp;gt;= k: k_maxs = heapq.heappop(scoville)print(&#39;k번째 최댓값: &#39;, k_maxs)print(scoville)scoviile: [1, 2, 3, 9, 10, 12] k(4)번째 최댓값: 3scoville: [9, 12, 10]scoville의 길이가 k이상일 때까지만 실행, 즉 k보다 낮아지면 멈춘다는 것이다. 그렇다면 scoville의 길이가 3이 되면 멈추게 되고, 3번 반복했기 때문에 3번째 최솟값이자, 4번째 최댓값이 되는 것이다. [응용] 힙 정렬k = 0sort_list = []for i in range(len(scoviile)): k = heapq.heappop(scoville) sort_list.append(k)scoville heap list [5, 6, 7, 9, 8]scoville sort list [5, 6, 7, 8, 9] 참고 블로그이중우선순위큐나의 전체 코드import heapqdef solution(operations): answer = [] s = [] for i in operations: num = int(i.split(&#39; &#39;)[1]) #print(&quot;i: {} \\tanswer: {} &quot;.format(i, s)) if i.startswith(&quot;I&quot;): heapq.heappush(s,num) else: if len(s) == 0: pass elif num == -1: heapq.heappop(s) else: if len(s) == 1: heapq.heappop(s) else: s.remove(max(s)) if not s: answer=[0,0] else: answer.append(max(s)) answer.append(min(s)) return answer입력값 〉 [&quot;I 16&quot;, &quot;I -5643&quot;, &quot;D -1&quot;, &quot;D 1&quot;, &quot;D 1&quot;, &quot;I 123&quot;, &quot;D -1&quot;]출력 〉 i: I 16 answer: [] i: I -5643 answer: [16] i: D -1 answer: [-5643, 16] i: D 1 answer: [16] i: D 1 answer: [] i: I 123 answer: [] i: D -1 answer: [123] 일단 입력된 operation을 원소마다 판별을 하는데, “I 16”의 경우 str 형태이고, 각각을 비교해야 하므로 ‘ ‘를 기준으로 split하고 i가 “I”로 시작할 때와 D로 시작할 때로 경우를 나누고, D에서 1과 -1을 또 분리한다. 그래서 s가 빈 리스트라면 [0,0]을 그렇지 않다면 [최댓값, 최솟값] 을 리턴한다.디스크 컨트롤러" }, { "title": "Coding Test[Python] - 스택 / 큐", "url": "/posts/codingteststack/", "categories": "Classlog, coding test review", "tags": "coding test, push&pop", "date": "2022-01-05 13:00:00 +0900", "snippet": "프로그래머스에 올라와 있는 코딩테스트를 진행하면서 공부한 내용 및 막혔던 부분들에 대해 리뷰하고자 합니다. 저는 거의 대부분을 python을 사용하였습니다.이 글은 python을 통한 스택/큐에 대해 리뷰하고 다른 것들을 참고하시려면 아래 링크를 참고해주세요 힙 해시 정렬 완전탐색 연습문제 카카오Overview스택스택(stack)은 데이터 구조 중 하나로 데이터 삽입은 push, 데이터 추출은 pop이라 한다. LIFO(Last In First Out)는 후입선출 구조, 즉 후에 들어온 것이 먼저 나가는 방식이다. 스택은 방문기록 뒤로가기, 역순 문자열, 실행 취소 등에 활용된다.파이썬에서는 push와 pop을 각각 append, pop 으로 되어 있다.큐반대로 큐(queue)는 데이터 구조 중 하나로 데이터 삽입은 인큐(enqueue), 제거는 디큐(deque)라고 한다. FIFO(First In First Out) 선입선출 구조, 즉 먼저 들어온 것이 먼저 나가는 방식이다. 큐는 은행 업무, 프린터 인쇄 대기열, 캐시 등에 활용된다.큐는 너비 우선 탐색(BFS)에 주로 사용된다.큐 연산을 구현하는 방법으로 3가지 정도가 있다. list 자료형 사용 Collections 모듈의 deque 사용 queue 모듈의 Queue 클래스 사용나의 경우 list를 많이 사용하기 때문에 list를 위주로 리뷰할 것이다. list 자료형 사용의 경우 append, pop, insert를 많이 사용한다. 이 방법의 단점은 성능이 비교적 좋지 않다. 참고 블로그INDEX 기능개발 프린터 다리를 건너는 트럭 주식가격기능 개발나의 전체 코드def solution(pro, spe): answer = [] for e in range(100): c = [] for i,(p, s) in enumerate(zip(pro, spe)): if pro[i] &amp;lt; 100: pro[i] = p + s while len(pro) != 0 and pro[0] &amp;gt;= 100: c.append([pro.pop(0),spe.pop(0)]) #print(e,&#39;\\tpro:&#39;,pro,spe,&#39;\\tc:&#39;,c) if len(c) != 0: answer.append(len(c)) c = [] #print(k,&#39;\\n&#39;) if len(pro) == 0: return answer입력값 〉 [95, 90, 99, 99, 80, 99], [1, 1, 1, 1, 1, 1]출력 〉 0 pro: [96, 91, 100, 100, 81, 100] spe: [1, 1, 1, 1, 1, 1] c: []answer: [] 1 pro: [97, 92, 100, 100, 82, 100] spe: [1, 1, 1, 1, 1, 1] c: []answer: [] 2 pro: [98, 93, 100, 100, 83, 100] spe: [1, 1, 1, 1, 1, 1] c: []answer: [] 3 pro: [99, 94, 100, 100, 84, 100] spe: [1, 1, 1, 1, 1, 1] c: []answer: [] 4 pro: [95, 100, 100, 85, 100] spe: [1, 1, 1, 1, 1] c: [100]answer: [1] 5 pro: [96, 100, 100, 86, 100] spe: [1, 1, 1, 1, 1] c: []answer: [1] 6 pro: [97, 100, 100, 87, 100] spe: [1, 1, 1, 1, 1] c: []answer: [1] 7 pro: [98, 100, 100, 88, 100] spe: [1, 1, 1, 1, 1] c: []answer: [1] 8 pro: [99, 100, 100, 89, 100] spe: [1, 1, 1, 1, 1] c: []answer: [1] 9 pro: [90, 100] spe: [1, 1] c: [100, 100, 100]answer: [1, 3] . . .18 pro: [99, 100] spe: [1, 1] c: []answer: [1, 3] 19 pro: [] spe: [] c: [100, 100]answer: [1, 3, 2] 전에는 다소 비효율적인 코드를 작성했다. 하지만, zip+enumerate 와 pop을 사용하게 되면서 좀 더 쉽게 풀었다.poppop함수란 리스트 요소를 꺼내는 함수로, pop()으로만 사용하게 되면 맨 마지막 요소를 꺼내고 리스트에서 그 요소를 삭제한다.&amp;gt;&amp;gt;&amp;gt; a = [1,2,3]&amp;gt;&amp;gt;&amp;gt; a.pop()3&amp;gt;&amp;gt;&amp;gt; a[1, 2]pop(x) 를 사용하게 되면 x번째에 있는 요소를 꺼내겠다는 것이다.&amp;gt;&amp;gt;&amp;gt; a = [1,2,3]&amp;gt;&amp;gt;&amp;gt; a.pop(1)2&amp;gt;&amp;gt;&amp;gt; a[1, 3] [응용]&amp;gt;&amp;gt;&amp;gt; while len(pro) != 0 and pro[0] &amp;gt;= 100: c.append([pro.pop(0),spe.pop(0)]) print(c)[100][100,120]...while과 함께 사용하여 조건에 만족할 동안 맨 앞에 있는 요소를 계속 추출하고, 다른 list에 입력시킨다.pop과 반대로 push가 있는데 나의 경우 push는 잘 사용하지 않고, append를 많이 사용한다.프린터priorities 각각의 숫자와 위치를 구분하기 위해 문자를 추가하려고 했는데, 사실은 그냥 숫자를 부여해주면 된다. dict나 tuple 다 상관없지만, 이를 구분해주는 방법이 for문이 아니라 enumerate를 사용했으면 더 편했다. 그러나 위치를 나타내는 방법이 문자이면 dict가 편하지만, 숫자면 tuple이 더 간편하다.예를 들어, 내가 찾고자 하는 것이 apple이라 하면 dict[‘apple’] 을 통해 맞는 위치를 찾으면 되지만, 숫자라면 서로 구분이 되지 않기 때문에, tuple(1)을 통해 묶어준 튜플에서 알맞는 값을 불러온다. 하지만 나는 풀 당시 dict만을 생각하고 풀었기 떄문에 어려움을 겪었다.또한, 하나의 원소로 리스트 각각의 값과 비교하는 방법을 몰라서 어렵게 했지만, any()라는 함수를 사용하면 된다.나의 전체 코드def solution(priorities, location): answer = 0 process = [(i,p) for i,p in enumerate(priorities)] while True: testp = process.pop(0) if any(testp[1] &amp;lt; p[1] for p in process): process.append(testp) else: answer += 1 if testp[0] == location: return answer입력값 〉 [2, 1, 3, 2], 2출력 〉 process: [(0, 2), (1, 1), (2, 3), (3, 2)]testprocess: (0, 2)testprocess: (1, 1)testprocess: (2, 3)도저히 너무 길게만 짜져서 다른 사람 풀이를 참고하여 만들었다.enumeratefor i,k in enumerate(list): 의 형태로 사용하는데, 순서와 리스트의 값을 전달해주는 기능을 한다. 리스트 이외에 튜플, 문자열 등도 가능하다.&amp;gt;&amp;gt;&amp;gt; process = [(i,p) for i,p in enumerate(priorities)]&amp;gt;&amp;gt;&amp;gt; print(process)process: [(0, 2), (1, 1), (2, 3), (3, 2)]이와 같은 형태로 출력이 된다. 즉 i는 0,1,2,3… p는 순서대로 list의 각 요소이다. 참고 블로그: https://wayhome25.github.io/python/2017/02/24/py-07-for-loop/any()any(iterablevalue)는 전달받은 자료형의 element 중 하나라도 True일 경우 True를 돌려주고, 그렇지 않은 경우 False를 돌려준다. 빈 자료일 경우도 False를 반환한다.&amp;gt;&amp;gt;&amp;gt; a = [True,False,True] &amp;gt;&amp;gt;&amp;gt; any(a) True 참고 블로그startwith()any()와 비슷하게 True or False 를 반환해주는 함수가 있다. 대소문자를 구분하고, 인자값에 있는 문자열이 string에 있으면 True, 없으면 False를 반환한다. 인자값으로는 tuple만 사용가능하다. 따라서 tuple안의 요소 중 하나라도 string에서 시작하는 문자열과 동일하다면 True를 반환한다.string = &quot;hello startswith&quot;print(string.startswith(&quot;hello&quot;))&amp;gt;&amp;gt;&amp;gt; True 참고 블로그다리를 지나는 트럭이 문제의 경우 선입선출이므로 큐 문제에 해당한다. list 자료형을 사용할 것이다.def solution(bridge_length, weight, truck_weights): arrive = [] ing = [] truck = [[t,0] for t in truck_weights] w = 0 sec = 0 while len(arrive) != len(truck_weights): for i in range(len(ing)): ing[i][1] += 1 if len(ing) != 0 and ing[0][1] &amp;gt;= bridge_length: w -= ing[0][0] arrive.append(ing.pop(0)[0]) if len(truck) != 0 and w + truck[0][0] &amp;lt;= weight and len(ing) &amp;lt;= bridge_length-1: w = w + truck[0][0] ing.append(truck.pop(0)) sec += 1 #print(&quot;truck: {}\\nw: {}\\ning: {}\\t arrive: {}\\nsec: {}\\n&quot;.format(truck,w,ing,arrive, sec)) return sec입력값 〉 2, 10, [7, 4, 5, 6]출력 〉 truck: [[4, 1], [5, 1], [6, 1]]w: 7ing: [[7, 1]] arrive: []sec: 1truck: [[4, 1], [5, 1], [6, 1]]w: 7ing: [[7, 2]] arrive: []sec: 2truck: [[5, 1], [6, 1]]w: 4ing: [[4, 1]] arrive: [7]sec: 3truck: [[6, 1]]w: 9ing: [[4, 2], [5, 1]] arrive: [7]sec: 4truck: [[6, 1]]w: 5ing: [[5, 2]] arrive: [7, 4]sec: 5truck: []w: 6ing: [[6, 1]] arrive: [7, 4, 5]sec: 6truck: []w: 6ing: [[6, 2]] arrive: [7, 4, 5]sec: 7truck: []w: 0ing: [] arrive: [7, 4, 5, 6]sec: 8일단 지나가는 중인 리스트, 도착 리스트를 따로 구성하여 조건에 만족할 때 해당 리스트로 저장한다. w는 다리 위의 트럭들의 무게, sec는 초에 해당한다. 각 조건에 맞게 if문을 구성하였고, 해당 truck이 모두 지나갈 때까지 이므로 while을 사용했다.주식가격이 문제는 지문이 조금 이해하기 어려웠다. 내가 이해하기로는 초 단위로 주어진 prices일 때 구매를 한다고 가정할 때, 산 가격보다 더 떨어지지 않는 기간이 얼마인가를 출력해야 하는 것 같다.def solution(prices): answer = [0]*len(prices) for i in range(len(prices)): k = prices[i] for j in prices[i+1:]: answer[i] += 1 print(k,j) if k &amp;gt; j: break return answer입력값 〉 [2, 2, 5, 8, 13, 1]출력 〉 k: 1 - answer: [4]k: 2 - answer: [4, 3]k: 3 - answer: [4, 3, 1]k: 2 - answer: [4, 3, 1, 1]k: 3 - answer: [4, 3, 1, 1, 0]정확성은 통과했지만 효율성은 통과하지 못했다. 아마 for문이 중첩되어 그런 듯하다." }, { "title": "Coding Test [C++] - 연습문제", "url": "/posts/codingtestcpra/", "categories": "Classlog, coding test review", "tags": "coding test, practice", "date": "2022-01-05 13:00:00 +0900", "snippet": "프로그래머스에 기재되어 있는 c++ coding test에 대해 리뷰하고자 합니다. 그 중에서도 연습문제에 해당하는 문제를 이 곳에 작성할 예정입니다. 다른 것을 참고하시려면 아래 링크를 클릭하시기 바랍니다. 해시 정렬 완전탐색 카카오 블라인드 채용 문제Index 더 맵게 이중우선순위큐 디스크 컨트롤러2016년나의 전체 코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;string solution(int a, int b) { string answer = &quot;&quot;; vector&amp;lt;string&amp;gt; week = {&quot;THU&quot;,&quot;FRI&quot;,&quot;SAT&quot;,&quot;SUN&quot;,&quot;MON&quot;,&quot;TUE&quot;,&quot;WED&quot;}; int days[] = {31,29,31,30,31,30,31,31,30,31,30,31}; int day = 0; for(int i=0; i&amp;lt;a-1; i++){ day += days[i]; } day += b; int c = day%7; answer = week[c]; return answer;}2016년의 경우 1월1일이 금요일이다. 따라서 b=1=day의 경우 FRI 가 출력되어야 한다. 금요일을 시작으로 5/24일까지 날짜를 다 더해서 7로 나눈 후 나머지를 구한다.vector containervector 컨테이너는 자동으로 메모리가 할당되는 배열이다. 이 구조는 push_back, pop_back을 통해 맨 뒤에 삽입/제거를 사용할 수 있다.vector&amp;lt;int&amp;gt; v2(v1);이 형태는 v2는 v1 vector를 복사하여 생성한다는 것이다.vector&amp;lt;int&amp;gt; v(5, 2);2로 초기화된 5개의 원소를 가지는 vector v를 생성한다.vector&amp;lt;string&amp;gt; v;string으로 된 vector 컨테이너 v를 생성한다.vector에서 사용하는 함수 v.assign(3,2)2의 값으로 3개의 원소 할당 v.at(idx)v에서의 idx번째 원소를 참조. v[idx]도 사용가능하나 안전하게 사용가능 v.front() / v.back()첫번째 / 마지막 원소를 참조 v.begin() / v.end()첫번째 / 마지막 원소를 가르킴 v.push_back(2) / v.pop_back()마지막 원소 뒤에 원소 2를 삽입 / 마지막 원소를 제거 v.resize(n) / v.resize(n,3)크기를 n으로 변경, n 이상의 원소는 default값인 0으로 초기화 / 3으로 지정하게 되면 0 대신 3으로 초기화 v.size()v의 원소의 갯수를 리턴 v2.swap(v1)v1와 v2의 원소와 capacity를 바꿔줌 v.insert(2,3,4)2번째 위치에 3개의 4값을 삽입, 즉 2번째 위치에다가 4값을 3개 추가한다는 것 v.erase(iter)iter가 가리키는 원소를 제거, 사이즈는 줄지만 메모리는 그대로 남음 v.empty()size가 비었으면 true를 리턴 참고 블로그 가운데 글자 가져오기나의 전체 코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;#include &amp;lt;cmath&amp;gt;using namespace std;string solution(string s) { string answer = &quot;&quot;; string c; int ceils = ceil(float(s.length())/2); ceils -= 1; for(int i=1;i&amp;lt;=ceils;i++) { s.erase(s.begin()); s.pop_back(); } cout &amp;lt;&amp;lt; ceils &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; answer &amp;lt;&amp;lt; endl; return s;}입력값 〉&quot;abcde&quot;출력 〉ceils: 2 s: cs의 크기를 2로 나누고, 그것을 올림한다. 그렇게 되면 ceils는 3이 되는데, 1뺀 값인 2번을 앞뒤로 뺀다. 이 때 s의 크기가 짝수이면 2로 나누면 정수가 되고, 이에 1을 뺀 값만큼 앞뒤로 빼면 중앙값이 나오게 된다. 하지만,,이는 1줄이면 바로 출력이 가능하다. return s.length()&amp;amp;1 ? s.substr(s.length()*0.5,1) : s.substr(s.length()*0.5-1,2);substrstring.substr(start,length)start는 탐색구간의 시작점, length는 탐색구간의 길이이므로 start ~ start+length 위치에 있는 문자열을 리턴인자로 음수를 넣으면 뒤에서부터 한다.같은 숫자는 싫어나의 전체 코드#include &amp;lt;vector&amp;gt;#include &amp;lt;iostream&amp;gt;using namespace std;vector&amp;lt;int&amp;gt; solution(vector&amp;lt;int&amp;gt; arr) { vector&amp;lt;int&amp;gt; answer; for(int i=0; i&amp;lt;arr.size();i++) { answer.push_back(arr[i]); if(i&amp;gt;0 &amp;amp;&amp;amp; arr[i-1] == arr[i]) { answer.pop_back(); } } return answer;}이 또한 1줄로 가능하다.arr.erase(unique(arr.begin(), arr.end()),arr.end());vector&amp;lt;int&amp;gt; answer = arr;uniqueunique는 연속된 중복 원소를 vector의 제일 뒷부분으로 보내버린다.vector&amp;lt;int&amp;gt; v1 1 1 2 2 2 3unique(v.begin(),v.end())1 2 3 2 2 2 3이처럼 중복된 값은 뒤로 보내는데, 문제는 중복된 값 그대로 보내는 것이 아니라 원래 그 자리에 있던 원소 값이 들어가게 된다. 즉, 중복된 값을 제거한 원소는 [1,2,3] 인데, 이 뒤에 자리를 보면 [2,2,2,3]이라는 원래 자리에 있던 원소가 들어가 있는 것을 볼 수 있다.우리는 중복된 값을 제거한 vector가 필요하기 때문에 필요 없는 뒷부분은 제거해준다.v.erase(unique(v.begin(), v.end()), v.end());중복된 값을 뒤로 보내준 뒤 반환되는 값은 vector의 중복된 값의 첫번째 위치로 반환된다. 따라서 이 부분부터 뒷부분을 다 제거한다. unique는 문자도 정렬하여 사용할 수 있다.vector&amp;lt;int&amp;gt; va a a b b b cv.erase(unique(v.begin(),v.end()),v.end())a b c 참고 블로그나누어 떨어지는 숫자 배열나의 코드#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;algorithm&amp;gt;using namespace std;vector&amp;lt;int&amp;gt; solution(vector&amp;lt;int&amp;gt; arr, int divisor) { vector&amp;lt;int&amp;gt; answer; for(int i=0;i&amp;lt;arr.size();i++) { if (arr.at(i) % divisor == 0) { answer.push_back(arr.at(i)); } } sort(answer.begin(),answer.end()); if(answer.size() == 0) { answer.push_back(-1); } return answer;}push_back()python의 append 역할을 하는 함수로 push_back을 사용할 수 있다.sort()sort를 할 때는 #include &amp;lt;algorithm&amp;gt;을 해줘야 한다.sort(배열의 포인터, 배열의 포인터 + 배열의 크기) 로 되어있지만, 대부분은 sort(vector.begin(), vector.end()) 로 사용한다. 즉 vector의 첫번째부터 끝까지 정렬한다.내림차순으로 하고 싶을 경우 sort(vector.begin(), vector.end(), desc) 로 사용한다.두 정수 사이의 합#include &amp;lt;string&amp;gt;#include &amp;lt;vector&amp;gt;using namespace std;long long solution(int a, int b) { long long answer = 0; if(a&amp;gt;b) { int n = a; a = b; b = n; } for(int i=a;i&amp;lt;=b;i++) { answer += i; } return answer;}long longlong long 은 int형 연산에서 초과되는 범위를 다룰 때 사용한다.변수 자료형에는 int, char, long long, float, double 등이 있다." }, { "title": "[논문 리뷰] MMDetection: Open MMLab Detection Toolbox and Benchmark", "url": "/posts/MMdetection/", "categories": "Review, Object Detection", "tags": "Object Detection, mmdetection", "date": "2021-12-22 13:00:00 +0900", "snippet": "MMDetection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요.Abstractmmdetection은 object detection과 instance segmentation을 다루는 유명하고 다양한 모델을 하나의 toolbox로 구현한 일종의 플랫폼이다. MMDetection은 training and inference 코드뿐만 아니라 200개 이상의 network 모델 weight를 제공한다. 코드는 pytorch를 사용하였다고 한다.1. Introduction주요 특징으로는 Modular design: 모델이 모듈화되어 있어 사용자화를 쉽게 할 수 있다. Support of multiple frameworks out of box: 인기있는 여러 프레임워크를 지원한다. High efficiency: 모든 BBox와 mask 연산은 GPU에서 동작한다. 그리고, training 속도가 다른 detectron, maskrcnn-benchmark, simpleDet 보다 더 빠르다. State of the art: 2018 COCO detection challenge에서 우승한 MMDet team의 코드를 토대로 만들었다.2. Supported FrameworksMMDetection 안에 구성된 모델로는 2.1 Single-stage Methods SSD(2015) RetinaNet(2017): focal loss를 통한 고성능 단일 detector GHM(2019): gradient harmonizing 메커니즘을 통한 단일 detector FCOS(2019): fully convolutional anchor-free detector FSAF(2019): feature selective anchor-free module detector 2.2 Two-stage Methods Fast R-CNN(2015) Faster R-CNN(2015) R-FCN(2016): fully convolutional detector Mask R-CNN(2017): instance segmentation method Grid R-CNN(2018) grid를 통한 위치화 메커니즘을 통한 segmentation Mask Scoring R-CNN(2019): IOU를 예측함으로써 Mask R-CNN 개선 Double-Head R-CNN(2019) 2.3 Multi-stage Methods Cascade R-CNN(2017) Hybrid Task Cascade(2019): multi branch를 통한 가장 높은 성능을 보인 모델 2.4 General Modules and Methods Mixed Precision Training(2018): 반정밀 floating point 숫자를 이용해 훈련 Soft NMS(2017) OHEM(2016): online hard sampling DCN(2017): 변형 가능한 convolution 과 ROI Pooling DCNv2(2018): Train from Scratch(2018): imageNet 대신 random initialization ScratchDet(2018): M2Det(2018): 더 효과적인 feature pyramid를 위한 새로운 feature pyramid GCNet(2019): Generalized Attention(2019) SyncBN Group Normalization(2018) Weight Standardization(2019) HRNet(2019): 고해상도 이미지를 학습하는 데 집중한 새로운 backbone Guided Anchoring(2019) Libra R-CNN(2019) 3. Architecture3.1 Model Representationbackbone: 입력 이미지를 특징맵으로 변형시켜주는 부분, ResNet50과 같이 FC Layer(fully connected layer)가 없는 형태를 가짐Neck: backbone과 head를 연결해주는 부분으로, backbone을 통해 생성된 특징맵을 정제하고 재구성해준다. 예를 들면 FPN(Feature Pyramid Network)와 같다.DenseHead(AnchorHead/AnchorFreeHead): anchorHead와 anchorFreeHead를 포함한 특징맵의 밀도 높은 위치에서 작동하는 요소RoIExtractor: 단일 또는 다중 특징맵으로부터 RoIPooling과 같은 연산을 통해 RoI-wise 특징들을 추출, 예를 들어 특정 수준의 특징 피라미드에 대한 RoI 특징은 singleRoIExtractor이 된다.RoIHead(BBoxHead/MaskHead): RoI 특징을 입력으로 받고, RoI-wise한 task-specific 예측을 하는 부분이다. bounding box 분류/regression, mask 예측과 같다.위의 과정들을 그림으로 나타내면 아래와 같다.3.2 Training Pipeline최소한의 파이프라인만 정의하고 나머지는 hooking 메커니즘을 통해 정의되도록 했다. 후킹 중에서 forward hook은 모델의 파라미터(weights)나 특징맵(feature map)을 신호 전달 중간에 가로채는 기법이라 할 수 있다. backward hook은 gradient exploding을 방지하기 위해 활용한다.또한, before_run, before_train_epoch, after_train_epoch 등의 다양한 시점을 정의하고 관찰했다.저자는 figure 2와 같이 다양한 시점으로 나누고, 사용자가 원하는대로 hook 설정을 변경할 수 있도록 했다. 이 figure2는 학습 과정이고, validation 과정은 평가 후크를 사용해 각 epoch 이후의 성능을 테스트 했기 때문에 보여주지 않았다. 아마 학습 과정과 비슷할 것이다.4. Benchmarks4.1 Experimental Settingdataset: MS COCO 2017 데이터셋을 사용했다.Implementation Details: 명시하지 않을 경우 default 값을 사용한다. image는 비율 변화 없이 최대 1333x800으로 resize한다. 학습에서는 총 16개의 batch size의 8개의 V100 GPU와 추론에서는 단일 V100 GPU를 사용했다. 학습 과정은 detectron과 같다. 1x, 2x는 12epochs, 24epochs이고, 20e는 cascade 모델에서의 20 epochs를 의미한다.Evaluation metrics: 0.5~0.95의 여러 IoU의 multiple IoU threshold 를 적용할 수 있는 COCO dataset에 대한 표준 평가 metrics를 선택한다. region proposal network(RPN)의 결과는 AR(Average Recall)로 평가했고, detection 결과는 mAP로 평가했다.4.2 Benchmarking ResultsMain results: SSD, RetinaNet, Faster R-CNN, Mask R-CNN의 method를 사용했고, ResNet-50, ResNet-101, ResNet-101-32x4d와 같이 다양한 backbone을 사용했다. method간의 bbox/mask AP에 대한 추론 속도를 figure 3를 통해 확인할 수 있다.Comparison with other codebases: Detectron, maskrcnn-benchmark, SimpleDet 등을 비교했다. 결과는 table 2에서 확인가능하다.위의 결과처럼 hybrid task cascade 모델이 가장 성능이 좋았다.5. Extensive Studies5.1 Regression Losses여러 가지의 loss를 적용해보았으며, loss weight를 증가시켜가며 성능을 비교했다. 간단하게 Smooth L1 Loss의 loss weight를 증가시켜봄으로써 0.5% 정도 향상되었다.5.2 Normalization LayersGPU 메모리를 작게 하기 위해 detection 학습에서는 batch size가 비교적 작게 만든다. BN은 대체로 CNN에 적용되는데, 이 때, 통계량을 정확하게 추정하려면 큰 batch size를 요구한다. 하지만 object detection의 경우 batch size가 다소 작고, 보통 pretrained backbone을 사용하기에 학습할 때 weight나 BN이 업데이트 되지 않는다. 이 predtrained backbone을 Frozen BN이라고 부른다.최근 개발된 SyncBN(Synchronized BN)이나 GN(Group Normalization) 은 좋은 효과를 보여주었다. SyncBN은 다수의 GPU를 사용하여 평균과 분산을 계산하고, GN은 group 별로 특징들에 대한 channel을 나누어 각각의 group별로 평균과 분산을 계산한다.그렇다면 각각의 normalization을 비교하고, 성능을 좋게 만들기 위해서는 어떻게 해야 할까?같은 method와 같은 ResNet-50-FPN을 사용하고 여기서 BN layer만 교체하면서 비교를 진행했다. 그 결과 BN layer를 업데이트하여도 성능에 큰 변화는 없었고, FPN이나 bbox/mask head를 추가해도 별다른 이점이 없었다. 하지만, bbox head를 2fc를 4conv-1fc로 바꾸고, normalization을 추가하면 1.5% 정도의 성능 향상을 보였다. 또한, 더 많은 conv 층을 추가할 때 더 좋은 성능을 보였다.5.3 Training scales이전 연구에서는 대체로 1000x600 이나 1333x800 의 이미지 사이즈를 선호했다. 우리도 마찬가지로 1333x800을 default scale로 사용했다. 하지만 모델의 강인함(robust)을 위해 multi scale을 사용하는 것이 좋을 것이다. 이전에도 이에 대한 방법을 아직 논의하지 않았다. multi scale을 훈련하기 위해 우리는 각 반복마다 무작위로 scale을 정하고, 그것에 대해 입력 이미지를 resize했다.resize 하는 방법으로는 value mode: 스케일셋을 미리 정해놓고 임의로 스케일을 선택한다. range mode: 스케일 범위를 미리 정해놓고 최솟값과 최댓값 사이의 스케일을 임의로 만든다.표를 보게 되면 mask rcnn에서 다양한 scale을 적용했다. 1333x[640:800:32]의 표기는 긴 방향은 1333으로 고정하고 짧은 방향을 {640,672,704,736,768,800} 중 무작위로 1개 선택하는데, 위의 경우 640~800 중 32 단위로 1개 정하는 것이다. 이는 value mode에 해당된다. 여기서 만약 1333x[640:800]이라면 range mode가 된다.표에서 볼 수 있듯 range mode가 value mode보다 아주 조금 더 좋은 성능을 보인다.5.4 Other Hyperparameter간단하게 smoothL1_beta와 allowed_border을 볼 수 있다.smoothL1_beta: 대부분의 detection method는 regression loss로서 smoothL1 loss를 사용한다.beta는 L1과 MSELoss에서 threshold를 뜻한다. 1/9를 기본으로 사용한다.allowed_border: RPN에서 특징맵의 각 위치에 미리 정의된 anchor가 생성된다. 이 때 이미지 경계를 넘어가는 anchor은 무시된다. 기본적으로 이를 0으로 잡는다. 하지만 우리는 이것을 무시하지 않을 때 더 좋은 성능을 보인다는 것을 알아냈다. 이를 무한대라 정의한다면, allowed_border을 무한대로 설정했을 때 0.6% 이상의 성능 향상을 보인다.neg_pos_ub: 우리는 positive와 negative anchor라는 새롭게 하이퍼파라미터를 정의했다. 훈련 중 positive anchor이 충분하지 않을 경우 고정된 수의 훈련 샘플을 보장하기 위해 더 많은 음성 표본을 추출한다. negative sample 대비 positive sample의 비율이 더 많아지지 않게 하기 위해 neg_pos_ub를 설정한다. 이를 무한대로 설정하면 앞서 언급한 과정을 거친다. 3 또는 5로 설정한다는 것은 positive sample의 최대 3배 또는 5배까지는 negative sample을 허용하겠다는 뜻이다.Reference thesis: MMDetection:Open MMLab Detection Toolbox and Benchmark, https://arxiv.org/pdf/1906.07155.pdf github URL: https://github.com/open-mmlab/mmdetection Tutorial: 참고 블로그: https://wordbe.tistory.com/entry/MMDetection-%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-%EB%B0%8F-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84 " }, { "title": "병변 검출 AI 경진대회", "url": "/posts/lesioncontest/", "categories": "Review, DACON", "tags": "DACON, Object Detection", "date": "2021-12-20 13:00:00 +0900", "snippet": "결과114/250 (상위 46%)코드https://colab.research.google.com/drive/10vwjU62GcwKWN9ehU-01lORoJAP8WOj8#scrollTo=OGkiaInaZSLc복기custom dataset을 사용하고, 나머지는 baseline을 참고하여 구축했다. 앙상블을 하지 않고, fold, TTA를 하지 않은 것, model을 yolov5와 같은 성능이 좋은 것이 있음에도 다른 것을 고집한 것, baseline을 딴 후, yolov5에 대한 코드 공유가 있었는데, 따라했어야 했다. 데이터가 너무 많아 오래걸린다면 model을 일단 학습시키기 위해 학습 데이터를 줄이거나 모델을 바꾸거나 하는 것이 좋지 않았나 싶다.학습 시간데이터가 너무 많아서 학습을 하고 파일로 출력하는데 너무 오랜 시간이 걸렸다. 이를 해결하는 방법을 공부해봐야 할 것 같다.방법으로 parameter이나 weight 등의 시간이 오래걸리는 요소들을 파일로 저장/불러오기하면 시간이 단축되지 않을까여러 번 돌려야 할 필요도 없고, 런타임이 끊기더라도 다시 불러오기만 하면 되니까GPU 공간gpu가 부족할 경우 해결 방법으로는 batch size를 줄이면 된다.CUSTOM custom dataset custom transformCustom DatasetCustom TransformCustom Model대회때는 pretrained model을 사용하더라도 자기만의 model을 만들 줄 알면서 사용하는 것과 몰라서 사용하는 것에는 큰 차이가 있다. 따라서 custom model을 공부한다.Shell코드가 많을 경우 파일을 나눠서 사용해야 할 것이다. detect.py model.py 등과 같은 기능별 파일을 만들어 수행하는 방법을 공부한다.Model상위 랭커분들이 yolov5 + MMdetection을 사용했기에 이를 사용하여 detection 하는 방법을 공부한다.앙상블약 10개의 모델을 앙상블하는 모습을 볼 수 있으므로 앙상블하는 방법을 공부한다. BBox를 앙상블하는 방법은 Weighted Boxes Fusion를 사용한다.이 때, 모델 학습 + TTA(test time augmentation) 도 함께 앙상블한다.COCO dataset Tranfer대회 데이터 format에서 COCO 데이터 format으로 변형해야 한다. COCO라 함은 id, image_id, area, bbox, iscrowd 이와 같은 것들이다.fold splitKfold를 사용하여 적용해봐야 한다. 따라서 Kfold를 사용하여 모델을 구축하는 방법을 공부한다.MMDetection / yolov5MMdetection 와 yolov5를 실제로 적용하는 방법을 공부해야 한다. yolov5를 공부했지만 다시해야 할 것이고, MMDetection은 이제 공부를 시작하자." }, { "title": " CS231N chapter 14 - Reinforcement Learning ", "url": "/posts/cs231n14/", "categories": "Classlog, CS231N", "tags": "CS231N, reinforcement-learning", "date": "2021-11-09 13:00:00 +0900", "snippet": " 13강 리뷰 1) PixelRNN/CNN 2) VAE Autoencoders Variational Autoencoders 3) GAN우리는 지금까지 supervised learning을 배웠다. 지도 학습이란 데이터 x와 레이블인 y가 있고, x를 y에 매핑하는 함수를 학습하는게 목표인 학습이다. 가령 분류(classification)이 이에 해당한다.그리고 지난 강의에서 비지도 학습(unsupervised learning)을 배웠다. 데이터는 있는데 레이블이 없는 경우다. 데이터에 내재된 구조를 학습하는 것이 목표인 학습이다. 가령 생성모델(generative models)가 이에 해당한다.오늘 강의를 대충 소개하면 overview What is Reinforcement Learning? Markov Decision Processes(MDP) MDP는 강화학습 문제의 수식체계(formalism)이다. 강화학습의 대표적인 방법 Q-Learning Policy Gradients Reinforcement learning강화학습에는 에이전트(agent)가 있다. 환경(environment)에서 행동(action)을 취하는 주체다. 에이전트는 행동에 따른 적절한 보상(rewards)을 받는다. 즉, 에이전트는 환경에서 행동을 취하여 적절한 보상을 받는 주체다.강화학습은 에이전트의 보상을 최대화할 수 있는 행동이 무엇인지를 학습한다. 예를 들어, 고양이가 있고, 빨간색 버튼을 누르면 간식이 나오는 기계가 있다고 가정해보자. 고양이는 어떻게 해야 간식이 나오는지 모를 것이다. 아무거나 다 시도해보다가 버튼을 눌렀을 때 간식이 나오는 것을 확인한다. 이 때, 고양이(agent)는 간식이 나오는 기계(environment)의 버튼을 누르면(action) 간식 나온다는(rewards) 것을 학습한 것이다.예시로, “cart-pole” 문제가 있다. 이 문제의 목표는 움직이는 카트(cart)와 카트 위에 매달려있는 막대기(pole)의 균형을 유지하는 것이다. 상태(state)에는 현재 시스템이 기술되어 있다. 막대기의 각, 각속도, 카트의 위치 등이 있을 것이고, 에이전트는 카트를 수평으로 미는 행동을 취할 수 있다. 카트를 밀면서 동시에 막대기의 균형을 잘 유지해야 한다. 막대기가 균형을 잘 맞춰 제대로 서 있으면 환겨으로부터 보상을 받을 수 있다.고전적인 RL(Robot Locomotion = 로봇 보행) 에 대한 예시도 살펴보자. 사진을 보면 휴머노이드 로봇과 개미 로봇 모델이 있다. 로봇이 앞으로 나아가도록 하는 것이 목표다. 이 문제에서 상태(state)는 로봇의 모든 관절들의 각과 위치다. 에이전트가 취할 수 있는 행동(action)은 각 관절들에 가해지는 토크다. 이 문제에서 하고 싶은 것은 로봇이 앞으로 전진하는 것이다. 앞으로 가면 보상을 받고, 휴머노이드는 바로 서있어도 보상을 받는다.RL을 통해 게임 문제도 풀 수 있다. 여기 아타리 게임(atrari games)가 있다. 깊은 강화학습이 아주 큰 성과를 이룬 종목이다. 아타리 게임은 가능한 가장 높은 점수로 게임을 마치는 것이 목적이다. 에이전트가 게임 플리에어가 되어 게임을 진행한다. 위의 사진처럼 우리가 게임할 때 보이는 화면 그 자체가 상태(state)이다. 에이전트의 행동은 우리가 게임 플레이할 때처럼 위아래/좌우로 움직일 수 있다. 매 스텝마다 게임 점수를 확득할 수도, 잃을 수도 있다. 게임 종료 시점까지 점수를 최대화하는 것이 목표다.마지막으로 바둑이 있다. 딥마인드의 알파고가 세계 최고 바둑기사가 된 것이다. 바둑에서는 게임을 이기는 것이 목표다. 바둑을 둘 수 있는 모든 자리가 상태(state)이다. 행동은 다음 수를 두는 것이고, 게임을 이겼을 경우에만 보상이 주어진다.Markov Decision Process강화학습을 수학적으로 나타내면 다음과 같다.앞서 본 것처럼 환경(environment)은 에이전트(agent)에게 상태(state)를 부여한다. 그러면 에이전트는 행동을 취한다. MDP(Markov Decision Process)를 통해 강화학습 문제를 수식화시킬 수 있다.MDP는 Markov property를 만족한다. Markov Property란 현재 상태만으로 전체 상태를 나타내는 성질이다. 그리고 MDP는 몇 가지 속성으로 정의할 수 있는데, S: 가능한 상태들의 집합 A: 가능한 행동들의 집합 R: (state, action)쌍이 주어졌을 때 받게되는 보상의 분포 - (state, action)이 보상으로 매핑되는 함수 P: 전이확률(transition probability) - (state, action)쌍이 주어졌을 때 전이될 다음 상태에 대한 분포 γ: 보상을 받는 시간에 대해 우리가 얼마나 중요하게 생각할 것인가작동 방식은 다음과 같다. 처음 time step인 t=0이다. 환경은 초기 상태 분포인 p(s_0)에서 s_0를 샘플링한다. t=0에서부터 완료 상태가 될 때까지 반복한다. 에이전트가 행동 a_t를 선택 환경은 어떤 분포로부터 보상을 샘플링(보상은 우리의 상태와 우리가 택한 행동이 주어졌을 때의 보상) 환경은 다음 어떤 분포에서 상태 s_t+1을 샘플링 에이전트는 보상과 다음 상태를 받는다. 에피소드가 종료될 때까지 이를 반복한다. 또한, 정책(polity) π를 정의할 수 있다. 정책은 각 상태에서 에이전트가 어떤 행동을 취할지를 명시해주는 기능을 수행한다. 정책은 확률적인 값일 수도 있고, 명확한 값일 수도 있다.우리의 목적은 최적의 정책 π*을 찾는 것이다. 즉, 누적 보상(cumulative discounted reward)를 최대화시키는 것이다.보상에는 미래에 얻을 보상도 포함되는데, 이 보상은 discount factor에 의해 할인된 보상으로 얻게 된다.간단한 MDP 예제가 있다. 격자로 된 grid world가 있다. 이 곳에서 테스크를 수행할 것이므로 이 것이 상태이자 환경이다. 여기서 격자 중 어디로든 이동할 수 있다. 우리는 상태에 따라 행동을 취할 수 있다. 행동은 상/하/좌/우로 움직이는 것이다. 우리는 한 번 움직일 때마다 음의 보상을 받게 된다. 가령 r = -1이 될 수도 있다.우리의 목표는 회식으로 칠해진 종료 상태를 최소한의 행동으로 도달하는 것이다. 종료 상태에 도달하는 시간이 길수록 음의 보상은 점점 쌓인다.먼저 random policy에서는 기본적으로 어떤 방향으로 움직이든 무작위로 방향을 결정한다. 모든 방향이 동일한 확률을 갖는다.하지만, 일련의 학습을 거쳐 얻게될 optimal policy의 경우 점점 더 종료 상태에 가까워지도록 만드는 적절한 방향을 선택해서 행동을 취하게 된다. 종료 상태 바로 주변에 위치하는 경우라면 종료 상태로의 방향으로 이동하도록 하는 것이다. 종료 상태와 먼 곳에 있더라도 가장 가깝게 이동할 수 있는 방향으로 이동한다.이렇게 MDP를 정의하고 나면 최적의 정책인 π*를 찾아야 한다. 최적의 정책은 보상의 합을 최대화시킨다. 최적의 정책은 우리가 어떤 상태에 있더라도 그 상황에서 보상을 최대화시킬 수 있는 행동을 알려준다.Q-LearningMDP에서 발생하는 무작위성(randomness)는 어떻게 다뤄야 할까?초기 상태를 샘플링할 시 무작위성이 있고, 전이 확률 분포의 경우에도 다음 상태는 확률적이다. 이를 위해서는 보상의 합에 대한 기댓값을 최대화시키면 된다.수식적으로 보면, 최적의 정책 π*는 정책 π에 대한 미래의 보상들의 합의 기댓값을 최대화시키는 것이다.초기 상태(s_0)는 어떤 상태 분포를 따르고, 우리가 취하는 행동은 어떤 상태가 주어졌을 때 정책이 가지는 분포로부터 샘플링된다. 또한, 다음 상태는 전이 확률 분포로부터 샘플링된다.사용하게 될 정의들을 먼저 살펴보자.우리가 정책을 따라 무언가를 수행하게 되면 결국은 모든 에피소드마다 어떤 경로를 얻게 될 것이다. 초기 상태인 s_0,a_0,r_0부터 시작해서 s_1,a_1,r_1 … s_2,a_2,r_2 이런 식으로 나아갈 것이다. 그렇게 되면 우리가 얻을 수 있는 상태(s), 행동(a), 보상(r)들의 하나의 경로가 생기게 된다.임의의 상태 s에 대한 가치함수는 상태 s와 정책 π가 주어졌을 때 누적 보상의 기댓값이다.그렇다면 (state, action)쌍이 얼마나 좋은지 알 수 있는 방법은 Q-value function을 통해 정의할 수 있다. Q-value function은 정책 π, 행동 a, 상태 s가 주어졌을 때 받을 수 있는 누적 보상의 기댓값이다. 최적의 Q-value function인 Q*은 (state, action) 쌍으로부터 얻을 수 잇는 누적 보상의 기댓값의 최대값이다.강화학습에서 중요한 요소 중 하나인 벨만 방정식(bellman equation)을 살펴보도록 하자.우선 최적의 정책으로부터 나온 Q-value function인 Q*가 있다고 할 때, Q*는 벨만 방정식을 만족한다. 이는 어떤 (s,a)이 주어지던 간에 현재 (s,a)에서 받을 수 있는 r 과 에피소드가 종료될 s’까지의 보상을 더한 값이다.여기에서는 우리가 이미 최적의 정책을 알고 있기 때문에 s’에서 우리가 할 수 있는 최상의 행동을 취할 수가 있다. s’에서의 Q*의 값은 우리가 현재 상태에서 취할 수 있는 모든 행동중에 Q*(s’,a’)를 최대화시키는 값이 된다. 이를 통해 최적의 Q값을 얻는다.우리는 어떤 상태인지에 대한 무작위성이 존재하므로 기댓값을 취한다. 또한, Q*를 통해 특정 상태에서의 최상의 행동을 취할 수 있는 최적의 정책을 구할 수 있다. Q*는 어떤 행동을 취했을 때 미래에 받을 보상의 최대치다. 그러므로 우리는 그저 Q*에 대한 정책을 따라 행동을 취하기만 하면 최상의 보상을 받을 수 있다.최적의 정책을 구하는 방법은 value iteration algorithm이다. 반복적인 업데이트를 위해 벨만 방정식을 사용할 것이다. 벨만 방정식을 통해 각 스텝마다 Q*를 조금씩 최적화시킨다. 수학적으로 보면 Q_i의 i가 무한대일 때, 최적의 Q*로 수렴한다.하지만 여기서 문제가 있다. 이 방법은 scalable하지 않다는 것이다. 반복적으로 업데이트하기 위해서는 모든 (state, action)마다 Q(s)를 계산해야 한다. 예를 들어, 아타리 게임의 경우 스크린에 보이는 모든 픽셀이 상태가 된다. 이 경우 상태 공간이 매우 크며 기본적으로 전체 상태 공간을 계산하는 것은 불가능하다.그래서 우선 neural network를 사용하여 함수 Q(s,a)를 근사시킨다. 앞서 배웠듯 복잡한 함수를 추정하고 싶을 때는 neural network를 사용했다.여기에서는 행동 가치 함수를 추정하기 위한 함수 근사를 이용할 것이다. 함수 근사로는 deep neural network를 사용할 것이고, 이를 deep Q-learning이라 한다.deep Q-Learning은 강화학습하면 빠지지 않고 등장하는 방법이다. 여기에서는 함수 파라미터 ϴ가 있다. ϴsms neural network의 가중치다.Q-function은 벨만 방정식을 만족해야 한다. 그렇다면 해야할 일은 neural network로 근사시킨 Q-function을 학습시켜 벨만 방정식의 에러를 최소화시켜야 한다. 손실 함수는 q(s,a)가 목적 함수와 얼마나 멀리 떨어져 있는지 측정한다. 여기 보이는 y_i가 바로 앞서 살펴본 벨만 방정식이다.forward pass에서는 손실 함수를 계산한다. 손실이 최소로 만든다. backward pass에서는 계산한 솔실을 기반으로 파라미터 ϴ를 업데이트한다.이런식으로 반복적인 업데이트를 통해 Q-function이 타겟과 가까워지도록 학습시킨다.deep Q-learning이 적용된 아타리 게임을 살펴보자. 게임의 목적은 동일하게 최대한 높은 점수를 획득하는 것이다. 상태는 게임의 픽셀이 그대로 사용되고, 행동도 상/하/좌/우로 동일하다. 게임이 진행됨에 따라 매 time-step마다 점수가 늘어나거나 줄어듬에 따라 보상을 얻는다. 스코어를 기반으로 전체 누적 보상을 계산할 수 있다.Q-function에 사용한 네ㅡ워크는 다음과 같이 생겼다. Q-network는 가중치 ϴ를 가진다. 네트워크의 입력은 상태 s, 즉 현재 게임 스크린의 픽셀들이다. 실제로는 4프레임 정도 누적시켜 사용한다. 입력을 grayscale로 변환, downsampling, cropping 등의 전처리 과정을 거친다. 전처리를 거친 입력은 4프레임씩 묶어 84x84x4의 형태가 된다.그 다음으로는 컨볼루션, FC-layer들로 구성된다. FC-layer의 출력 벡터는 네트워크의 입력인 상태가 주어졌을 때 각 행동의 Q-value이다. 가령 네 가지 행동이 존재하면 출력도 4차원이다. 이는 현재 상태 s_t와 여기에 존재하는 행동들 a_1,a_2,a_3,a_4에 대한 Q값들이다. 각 행동들마다 하나의 스칼라 값인 Q를 얻는다.참고로 행동의 수는 아타리 게임의 종류에 따라 4~18가지로 변할 수 있다.이런 네트워크의 장점은 한 번의 forward pass만으로 현재 상태에 해당하는 모든 함수에 대한 Q-value를 계산할 수 있다. 현재 상태의 각 행동들에 각각 Q-value가 존재할텐데 현재 상태를 네트워크의 입력으로 넣어주기만 하면 모든 Q-value를 한번에 forward pass로 계산하는 것이다.이 네트워크를 학습시키기 위해서는 손실 함수가 필요하다.neural netowork로 근사시킨 함수도 벨만 방정식을 만족해야 한다. 따라서 네트워크의 출력인 Q-value가 타겟 값과 가까워지도록 반복적으로 학습시켜야 한다.여기 알아야 할 개념이 있다. experience replay는 Q-network에서 발생할 수 있는 문제들을 다룬다. 첫번째로, Q-network를 학습시킬 때 하나의 배치에서 시간적으로 연속적인 샘플들로 학습하면 안좋다. 그렇게 되면 모든 샘플들이 상관관계를 가지게 되어 비효율적이다.두번째로, 현재 Q-network 파라미터를 생각해보면 네트워크는 우리가 어떤 행동을 해야할지에 대한 정책을 결정한다는 것은 우리가 다음 샘플들도 결정하게 된다는 의미다. 이는 학습에 안좋은 영향을 미칠 수 있다. 예를 들어, 현재 상태에서 왼쪽으로 이동하는 것이 보상을 최대화하는 행동이라고 하면, 결국 다음 샘플들도 전부 왼쪽에서 발생할 수 있는 것들로만 편향된다.이 두 가지를 해결하기 위해 experience replay라는 방법을 사용한다.이 방법은 replay memory를 이용한다. replay memory에는 (상태, 행동, 보상, 다음상태)로 구성된 전이 테이블이 있다. 게임 에피소드를 플레이하면서 더 많은 경험을 얻음에 따라 전이 테이블을 지속적으로 업데이트시킨다. 여기에는 replay memory에서의 임의의 미니배치를 이용하여 Q-network를 학습시킨다. 연속적인 샘플을 사용하는 대신 전이 테이블에서 임의로 샘플링된 샘플을 사용하는 것이다.이 방식을 통해 상관관계를 해결할 수 있다. 추가적인 이점으로 각각의 전이가 가중치 업데이트에 여러 차례 기여할 수 있다는 것이다. 전이 테이블로부터 샘플링을 하면 하나의 샘플도 여러번 뽑힐 수 있다. 이를 통해 데이터 효율이 훨씬 증가한다.이제 experience replay를 적용한 Q-learning 알고리즘을 살펴보자. replay memory를 초기화한다. memory 용량은 N으로 지정한다. 그리고 Q-network를 임의의 가중치로 초기화시킨다. 에피소드는 M번 진행할 것이다. 즉 학습을 M번 한다. 그리고 각 에피소드마다 상태를 초기화시켜야 한다. 상태는 게임 시작 픽셀이 된다. 앞서 언급한 입력 상태를 만들기 위한 전처리 과정을 진행하고, 게임이 진행중인 매 time-stpe마다 임의의 행동을 취한다. 이는 충분한 탐사를 위해 중요하다. 이를 통해 다양한 상태 공간을 샘플링한다. 낮은 확률로 임의의 행동을 취하거나 현재 정책을 따라 행동을 취한다. 즉, 대부분은 현재 상태에 적합하다고 판단되는 행동을 취하지만, 가끔씩은 임의의 행동을 선택한다. 이렇게 행동 a_t를 취하면 보상(r_t)와 다음 상태(s_t+1)를 얻는다. 그 후 전이(transition)(s_t,a_t,r_t,s_t+1)을 replay memory에 저장한다. 이제 네트워크를 학습할 것이다. 학습에는 experience replay를 이용할 것이다. replay memory에서 임의의 미니배치 전이들을 샘플링한 다음 이를 이용해 업데이트한다.게임을 진행하는 동안 experience replay를 이용하여 미니배치를 샘플링하고, 이를 통해 Q-network를 학습시킨다.google deepmind에서 atari 게임을 q-learning으로 학습시키는 영상의 주소는 참고용으로 넣었다.지금까지 Q-learning을 살펴보았다. Q-learning에 문제가 존재하는데, 그것은 Q-funciton이 매우매우 복잡하다는 것이다. Q-learning에서는 모든 (state, action) 쌍들을 학습해야만 한다. 그런데 만약, 로봇이 어떤 물체를 손에 쥐는 문제를 풀어야 한다고 가정할 때, 이는 아주 고차원의 상태 공간이다. 또, 로봇의 모든 관절의 위치와 각도가 이룰 수 있는 모든 경우의 수를 생각해볼 수 있을 것이다.따라서 이 모든 (s,a)를 학습시키는 것은 아주 어려운 문제다. 반대로 정책은 간단해질 수 있다. “손을 움켜쥐는 것”과 같이 말이다. 그렇다면 손가락을 특정 방향으로만 움직이도록 하면 된다.Policy Gradients그렇다면 정책 자체를 학습시킬 수 있을까?그것이 가능해진다면 여러 정책들 가운데 최고의 정책을 찾아낼 수 있다. 정책을 결정하기에 앞서 Q-value를 추정하는 과정을 거치지 않고도 말이다. 이러한 접근 방식이 policy gradients 이다.이를 수식적으로 매개변수화된 정책을 정의해보자. 정책들은 가중치 ϴ에 의해 매개변수화된다.각 정책에 대해 정책의 값을 정의하면 사진 속 아래 식과 같다. J(ϴ)는 미래에 받을 보상들의 누적 합의 기댓값으로 나타낼 수 있다. 이는 우리가 지금까지 사용했던 보상과 동일하다.이런 상황에서 우리가 하고 싶은 것은 최적의 정책인 ϴ*를 찾는 것인데, 이는 argmaxJ(ϴ)로 찾을 수 있다.이를 통해 보상의 기댓값을 최대로 하는 정책 파라미터를 찾으면 된다. policy parameter에 대해 gradient ascent를 수행하면 된다. ascent를 통해 parameter를 연속적으로 업데이트하면 된다.좀 더 구체적으로 살펴보자REINFORCE 알고리즘을 보면, 경로에 대한 미래 보상의 기댓값으로 나타낼 수 있다.이를 위해서 경로를 샘플링해야 하는데, 이는 앞서 배운 s_0,a_0,r_0,s_1 등이 된다. 이들은 어떤 정책 π_ϴ을 따라 결정될 것이다. 그러면 각 경로에 대해 보상을 계산할 수 있고, 그 보상은 우리가 어떤 경로를 따라 얻을 수 있는 누적 보상이 될 것이다.따라서 정책인 π_ϴ의 값은 샘플링된 경로로부터 받게 될 보상의 기댓값이 될 것이다.gradient ascent를 수행해야 한다. J(ϴ)를 미분하면 위의 식과 같다. 하지만 여기서 문제가 있다. 미분을 했지만, 이 값은 계산할 수가 없다. p가 ϴ에 종속되어 있는 상황에서 기댓값 안에 gradient가 있으면 문제가 될 수 있다. 위의 식을 보면 p(τ;ϴ)에 대한 gradient를 구해야 하는데 τ에 대한 적분을 계산하기 어렵다.이를 해결하기 위해 분모분자에 p(τ;ϴ)를 곱해준다. 그리고 log(p)의 gradient는 (1/p)*p의 gradient와 같기 때문에 바꿀 수 있다.이렇게 바꾸고 나서 원래의 gradient식에 대입하게 되면, log(p)에 대한 gradient를 모든 경로에 대한 확률과 곱하는 꼴이 되고, 이를 τ에 대해서 적분하는 꼴이 되기 때문에, 이는 다시 경로 τ에 대한 기댓값의 형식으로 바꿀 수 있다.처음에는 기댓값에 대한 gradient를 계산하려 했지만, 이제는 gradient에 대한 기댓값으로 바꾼 셈이다.이와 같은 방법을 통해 gradient를 추정하기 위해 경로들을 샘플링하여 사용할 수 있게 되었다. 이로 인해 monte carlo 샘플링을 할 수 있다.좀 더 자세히 살펴보자.p(τ;ϴ)를 전이확률을 모른 채 계산할 수 있을까?우선 p(τ)는 어떤 경로에 대한 확률이다. 이는 현재 (state, action)이 주어졌을 때, 다음에 얻게될 모든 상태에 대해 전이확률과 정책 π로부터 얻은 행동에 대한 확률의 곱의 형태로 이루어진다. 이를 모두 곱하면 경로에 대한 확률을 얻어낼 수 있다.그렇다면 log(p;τ)의 경우를 보면 앞서 곱했던 것들이 모두 합의 형태로 바뀌게 될 것이다.log(p)를 θ에 대해 미분하는 경우 첫번째항인 전이확률은 θ와 무관하다. 두번째 항인 logπ_θ(a_t|s_t)만이 θ와 관련이 있다. 따라서 gradient를 계산할 때 전이확률은 필요하지 않다. 따라서 우리는 어떤 경로 τ에 대해서도 gradient를 기반으로 J(θ)를 추정할 수 있다.지금까지는 하나의 경로만 고려했었지만 기댓값을 계산하기 위해서는 여러 개의 경로를 샘플링할 수도 있다.gradient계산을 위해 어떤 경로로부터 얻은 보상이 크다면, 즉 일련의 행돌들이 잘 수행한 것이라면 일련의 행동들을 할 확률을 높혀준다. 그 행동이 좋은 선택이었다는 것을 알려주는 것이다.반면 어떤 경로에 대한 보상이 낮다면 해당 확률을 낮춘다. 이런 행동이 좋지 않은 행동이었으며 따라서 그 경로가 샘플링되지 않게 하기 위해서다. 여기 수식을 보면 π(a s)는 우리가 취한 행동들에 대한 likelihood이다. 파라미터를 조정하기 위해 gradient를 사용할 것이고, gradient는 likelihood를 높히기 위해 어떻게 해야 하는지를 알려준다. 이것은 우리가 받게될 보상, 즉 행동들이 얼마나 좋았는지에 대한 gradient를 통해 파라미터를 정하는 것을 의미한다. 어떤 경로가 좋았다면, 그 경로에 포함되었던 모든 행동이 좋다고 판단한다. 기댓값에 의해 이 모든 것들이 averages out된다. averages out을 통해 unbiased estimator를 얻을 수 있고, 충분히 많은 샘플링을 한다면 gradient를 잘 이용해서 정확하고 좋은 estimator을 얻읋 수 있다.이 방법이 좋은 이유는 gradient를 잘 계산한다면 손실 함수를 작게 만들 수 있고, 정책 파라미터 θ에 대한 local optimum을 구할 수 있기 때문이다.하지만, 여기서도 문제가 존재한다. 문제의 원인은 높은 분산이다. 왜냐하면 신뢰할당문제(credit assignment)가 아주 어렵기 때문이다. 일단 보상을 받았으면 해당 경로의 모든 행동들이 좋았다는 정보만 알려줄 것이다. 하지만 우리는 구체적으로 어떤 행동이 최선이었는지를 알고 싶지만, 이 정보는 average out된다. 그래서 구체적으로 어떤 행동이 좋았는지 알 수가 없기에 좋은 추정를 위해서는 샘플링을 충분히 해야만 한다.이 문제는 결국 분산을 줄이고, 추정의 성능을 높이기 위해서는 어떻게 해야 하는지에 대한 질문으로 귀결된다.분산을 줄이는 것은 policy gradient에서 아주 중요하다. 샘플링을 더 적게 하면서도 추정의 성능을 높일 수 있어서다.Variance Reduction그래서 이를 해결할 수 있는 방법들을 살펴보고자 한다.첫번째 아이디어는 해당 상태로부터 받을 미래의 보상만을 고려하여 어떤 행동을 취할 확률을 키워주는 방법이다. 이 방법은 해당 경로에서 얻을 수 있는 전체 보상을 고려하는 대신 현재부터 종료 시점까지 얻을 수 있는 보상의 합을 고려하는 것이다. 이 방법이 의도하는 바는 어떤 행동이 발생시키는 미래의 보상이 얼마나 클지를 고려하겠다는 말이다.두번째 방법은 지연된 보상에 대해 할인률을 적용하는 것이다. 수식을 보면 할인률이 추가되어 있다. 할인율이 의미하는 바는 당장 받을 수 있는 보상과 조금 더 늦게 받은 보상간의 차이를 구별하는 것이다. 어떤 행동이 좋은 행동인지 아닌지를 해당 행동에 가까운 곳에서 찾는다. 나중에 수행하는 행동에 대해서는 가중치를 조금 낮춘다. Baseline경로에서부터 계산된 값을 그대로 사용하는 것은 문제가 있다. 그런 값들 자체가 반드시 의미가 있는 값은 아닐 수 있기 때문이다. 보상이 모두 양수이기만 해도 행동들에 대한 확률이 계속 커지기만 할 수도 있다. 나름 의미가 있을 수 있지만, 정말 중요한 것은 얻은 보상이 우리가 얻을 것이라 예상했던 것보다 더 좋은지, 아닌지를 판단해야 한다. 이를 위해 baseline function을 사용한다.baseline 함수가 말하는 것은 해당 상태에서 우리가 얼마만큼의 보상을 원하는지이다. 그렇게 되면 확률을 키우거나 줄이는 보상 수식이 바뀌지만, 이를 수식에 적용하면 미래에 얻을 보상들의 합을 특정 기준이 되는 값(baseline)에서 값을 빼주는 형태가 되고, 이를 통해 우리가 기대했던 것에 비해 보상이 어떤지를 확인할 수 있다.baseline을 어떻게 선택하면 좋을까?가장 단순한 것은 에피소드를 수행하는 학습 과정에서 지금까지 봤던 모든 경로들에 대해 보상이 어땠는지에 대한 평균을 낸다. 이를 통해 현재 보상에 대해 좋은지, 나쁜지 알 수 있다.위의 variance reduction 방법들을 vanilla REINFORCE라고 한다. 할인율을 적용하여 미래에 받을 보상을 누적시키고 여기에 단순한 baseline을 추가한다.baseline의 주요 아이디어와 더 좋은 baseline을 선택하는 방법에 대해 살펴보자.우선 더 좋은 baseline이란 어떤 것일까?우리는 어떤 행동이 그 상태에서의 기댓값보다 좋은 경우에는 해당 상태에서 그 행동을 수행할 확률이 크길 원한다. 그럼 그 상태로부터 기대할 수 있는 값이라 하면, 앞서 Q-learning에서 배운 value function을 이용할 수 있다.어떤 상태 s에서 어떤 행동을 취했을 때 어떤 조건을 만족하면 그 행동이 좋았다고 판단한다. 그 조건은 어떤 상태에서 특정 행동을 했을 때 얻을 수 있는 Q-value가 그 상태에서 얻을 수 있는 미래의 받을 누적 보상들의 기댓값이라는 가치함수보다 더 큰 경우를 의미한다. 이는 그 행동이 우리가 선택하지 않은 다른 행동들보다 더 좋았다는 것을 의미한다. 반대로 그 차이 값이 음수거나 작은 경우 안 좋은 행동을 취했다는 것을 의미할 것이다.이를 추정(estimator) 수식에 적용해보면, 수식 자체는 같지만, 의미하는 바는 현재 행동이 얼마나 좋은 행동이었는지를 해당 상태에서의 Q-function과 value function의 차이를 통해 나타낸다.하지만 우리가 지금까지 살펴본 REINFORCE 알고리즘에서는 Q-function과 value function을 구하지 않았다.그러나, 우리는 policy gradient와 Q-learning을 조합해서 모델을 학습시킬 수 있다.여기 actor가 policy이고, critic이 Q-function이라 하고, 이들은 어떤 상태가 얼마나 좋은지, 그리고 상태에서의 어떤 행동이 얼마나 좋았는지를 말해준다.actor-critic 알고리즘에서 actor는 어떤 행동을 할지 결정한다. 그리고 critic은 그 행동이 얼마나 좋았으며, 어떤 식으로 조절해 나가야 하는지를 알려준다.기존의 Q-learning에서는 모든 (상태, 행동)쌍에 대한 Q-value를 학습해야만 했지만, 여기서는 policy가 만들어낸 (상태, 행동)쌍에 대해서만 학습시키면 된다.왼쪽 사진의 식을 보면 Q(s,a) - V(s)를 보상함수로 나타낸다. 즉, 보상함수는 어떤 행동을 했을 때 얼마나 많은 보상이 주어지는지를 나타낸다. 행동이 예상했던 것 보다 얼마나 더 좋은지를 나타내는 것이다.직접 알고리즘을 보면, policy 파라미터인 θ와 critic 파라미터인 π를 초기화시킨다. 매 학습마다 현재의 정책을 기반으로 M개의 경로를 샘플링한다. policy에 기반해서 경로인 (s_0,a_0,r_0,s_1…)를 뽑아내는 것이다. 각 경로마다 보상 함수에 대한 gradient를 계산하고 이를 전부 누적시킨다. critic 파라미터인 π를 학습시키기 위해 가치 함수를 학습시켜야 한다. 보상 함수를 최소화시키는 것과 같다. 이를 통해 가치 함수가 벨만 방정식에 근사하도록 학습시킨다. 이런 식으로 policy function(actor)과 critic function을 반복적으로 학습시킨다. gradient도 반복적으로 업데이트시킨다.REINFORCE를 활용한 몇 가지 예제를 살펴보자우선 RAM(Recurrent Attention Model)이 있다. 우선 hard attention에 대해 말하고자 하는데, image classifiction과 관련있는 이미지 클래스를 분류하는 문제이나 이미지의 일련의 glimpses(순간)를 가지고만 예측해야 한다. 이미지 전체가 아닌 지역적인 부분만 본다는 것이다. 어떤 부분을 볼지를 선택할 수 있다.이런 식의 접근 이유는 우리가 복잡한 이미지를 볼 때의 방식을 본땄다. 또한, 이런 식의 지역적인 부분만 살펴보면 이미지 전체를 처리할 필요가 없기에 계산을 절약할 수 있다. 그리고, 이 방법은 실제로 classification 성능을 높혀주기도 한다. 이 방법을 사용하면 필요없는 부분을 무시할 수 있기 때문이다.이 문제를 강화학습 수식으로 살펴보자.우선 상태(state)는 지금까지 관찰한 glimpses이다. 우리가 지금까지 얻어낸 정보라고 할 수 있다. 행동(action)은 다음에 이미지 내에 어떤 부분을 볼지를 선택하는 것이다. 실제로는 다음 스텝에서 보고 싶은 고정된 사이즈의 glimpse의 중간 x-y좌표가 될 수 있다.강화학습에서 분류 문제를 풀 때 보상은 최종 스텝에서 1인데, 이미지가 올바르게 분류되면 1, 그렇지 않으면 0이다.이 문제에서 강화학습이 필요한 이유는 이미지에서 glimpse를 뽑아내는 것은 미분이 불가능한 연산이기 때문이다. 어떻게 glimpse를 얻어낼 것인지를 REINFORCE를 통해 학습한다.누적된 glimpses가 모델에 주어지고, 여기에서는 상태를 모델링하기 위해 RNN을 이용한다. 그리고 policy 파라미터를 이용해 다음 action을 선택하게 된다.전체 과정을 보게 되면 입력 이미지가 들어온다. 이미지에서 glimpse를 추출한다. 추출된 glimpse는 neural network를 통과한다.(모델은 태스크에 따라 달라질 수 있다.) RNN을 이용해서 지금까지 있었던 glimpses를(state) 전부 결합시켜 준다.neural network의 출력은 x-y 좌표다. 실제로는 출력 밧이 행동에 대한 분포의 형태이고, 이 분포는 가우시안 분포를 따르며 결국 출력 값은 분포의 평균이 될 것이다. 평균과 분산을 모두 출력하는 경우도 있고, 분산은 고정된 값으로 설정하기도 한다.이 분포로부터 특정 x,y 위치를 샘플링하여 x-y좌표를 이용해서 다음 glimpse를 얻어낸다. 이것을 반복하여 여러 개의 glimpse를 누적시켜 RNN을 이용하여 policy를 모델링한다. RNN 모델은 다음 위치의 glimpse에 대한 분포를 추출한다.이 과정을 6~8번 반복한다. 마지막에는 분류를 위해 softmax를 통해 각 클래스 확률 분포를 출력한다.hard attention이라는 방법은 다양한 컴퓨터 비전에 사용된다. 계산 효율이 좋고, 이미지 내의 불필요한 정보들을 무시할 수 있기 때문이다.지금까지는 policy gradient를 활용한 hard attention의 예시를 보았다. 이제는 policy gradient를 이용한 방법을 살펴보자바둑을 학습시키는 모델이다. Deepmind에는 알파고라는 바둑을 두는 에이전트가 있다. 알파고에는 지도학습과 강화학습이 섞여 있다. 또한 monte carlo tree search와 같은 방식과 deep RL 방법도 섞여 있다.작동 방식 입력 벡터를 만들어야 한다. 바둑판과 바둑 돌의 위치와 같은 요소들을 특징화시켜 알파고의 입력으로 사용한다. 이는 상태를 설계하는 아주 좋은 방법이다. 성능 향상을 위해 바둑 돌의 색에 따라 채널을 따로 만들기도 했다. 네트워크를 학습시킨다. 네트워크는 프로 바둑기사의 기보를 지도학습으로 학습시킨다. 바둑판의 현재 상태가 주어지면 행동을 어떻게 취할지를 결정한다. policy network를 초기화한다. policy network는 바둑판의 상태를 입력으로 받아서 어떤 수를 둬야하는지를 반환한다. 지금까지의 policy gradient는 이런 식으로 동작한다. 알파고는 임의의 이전 반복에서의 자기 자신과 대국을 두며 학습을 진행한다. 스스로 대국을 둬서 이기면 보상 1을 받고, 지면 -1의 보상을 받는다. 앞서 배운 critic network를 통해 value network도 학습해야 한다.알파고가 다음 수를 어디에 둬야 할지는 value function과 MCTS로 계산된 값의 조합으로 결정된다.Summary policy gradient =&amp;gt; gradient descent/ascent를 통해 policy 파라미터를 업데이트 높은 분산을 처리하기 위해 Q-learning을 적용한다.Reference http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf" }, { "title": " CS231N chapter 13 - Generative Models ", "url": "/posts/cs231n13/", "categories": "Classlog, CS231N", "tags": "CS231N, Unsupervised-learing, pixelRNN/CNN, VAE, GAN", "date": "2021-11-05 13:00:00 +0900", "snippet": " 12강 리뷰 1) Dimensionality Reduction 2) Occlusion Experiment 3) Gradient Ascent 4) Texture Synthesis 5) Style Transfer지금까지는 지도학습(supervised learning)에 대해 배웠다. 지도 학습에는 데이터 x, 레이블 y가 있다. 지도학습의 목적은 데이터 x를 레이블 y에 매핑시키는 것이다. 레이블은 다양한 형태를 띈다.예를 들어 분류(classification) 문제의 예를 들어보자면 이미지가 입력이고, 출력은 클래스 레이블 y이다.object detection 문제에서의 출력은 각 객체들의 bounding box를, segmentation은 모든 픽셀마다 각 픽셀이 속하는 카테고리를, image captioning은 한 문장일 것이다.Unsupervised learning비지도 학습에서는 레이블이 없는 학습 데이터만 가지고 데이터에 숨어있는 기본적인 구조를 학습시켜야 한다. 비지도 학습의 예로는 군집화(clustering)이 있다. 군집화의 목표는 일정 metric을 가지고 유사한 데이터들끼리 묶어주는 것이다.가령 k-means clustering이 있다. 또는 차원 축소(dimensionality reduction)이 있다. 차원 축소를 통해 학습 데이터가 가장 많이 퍼져있는 축을 찾아낸다. 이 방법은 데이터의 차원을 감소시키는데 사용한다. 가령 위와 같이 3차원 데이터가 있을 때 이 데이터의 두 개의 축을 찾아낸다. 2차원으로 축소시키기 위해서다.비지도 학습의 또 다른 예는 데이터의 feature representation을 학습하는 것이다.앞서 분류 문제와 같은 supervised loss를 이용한 feature representation을 배웠다. 여기서는 분류를 위한 레이블이 있고 softmax loss를 사용한다. 그렇게 네트워크를 학습시켜 FC layer의 특징을 데이터의 feature representation으로 사용할 수 있다.비지도 학습의 경우 feature representation을 활용한 AE(Autoencoders)가 있다. AE의 loss는 입력 데이터를 얼마나 잘 재구성했느냐인데, 이를 이용해서 특징들을 학습시킬 수 있다. AE를 사용하면 추가적인 레이블 없이도 feature representation을 학습시킬 수 있다.비지도 학습의 예에는 분포 추정(density estimation)이 있다. 이는 데이터가 가진 기본적인 분포를 추정하는 방법이다.예를 들어 오른쪽 맨 위의 1차원 점들이 있다. 이 점들의 분포를 가우시안으로 추정한다. 또 하단의 예제는 2차원 데이터의 분포를 추정한 것이다. 점들이 더 많이 밀집되어 있는 곳의 분포가 더 크도록 이들의 분포를 적절히 모델링할 수 있다.지도/비지도 학습의 차이점을 요약해보면. 지도학습의 경우 레이블을 통해 x에서 y로의 매핑을 학습한다. 비지도학습의 경우 레이블이 없는 대신 데이터의 숨겨진 구조를 학습한다. 예를 들어 군집화, 변화의 중심 축, 데이터의 밀도추정 등이 있다.비지도 학습의 장점은 레이블이 필요하지 않기 때문에 데이터를 아주 많이 모을 수 있다. 데이터에 대한 비용이 적어지는 것이다.Generative models비지도학습(unsupervised learning)의 일종인 생성 모델(generative model)을 다루고자 한다.생성모델은 크게 pixelRNN and PixelCNN, Variational Autoencoders(VAE), Generative Adversarial Networks(GAN) 이 있다.생성 모델의 목적은 동일한 분포에서 새로운 샘플들을 생성해내는 것이다. 분포 P_data로부터 나온 학습 데이터들이 있다. 우리가 하고자 하는 것은 P_model을 학습시키는 것이다. p_model이 p_data와 같은 데이터를 생성하도록 만든다. 이를 위해서는 p_model과 p_data가 유사해야 할 것이다.생성 모델에서는 분포 추정을 다뤄야 한다.앞서 말했듯이 학습 데이터의 근본이 되는 분포를 추정해야 한다. 이는 비교사 학습의 핵심 문제이기도 하다. 분포 추정에는 여러 가지 전략이 있다. 하나는 생성 모델 p_model의 분포가 어떨지를 명시적으로 정의해주는 경우다.또는 간접적인 방법도 있다. 모델이 p_model에서 샘플을 만들어내도록 학습시키는 것은 동일하지만, 이번에는 p_model의 분포를 정의하지 않는다.생성 모델이 중요한 이유는 생성 모델을 통해 많은 것들을 할 수 있기 때문이다. 데이터 분포로부터 사실적인 샘플들을 생성해낼 수만 있으면 이를 이용해 아주 많은 것들을 할 수 있다.위의 사진은 생성 모델을 통해 생성된 샘플들이다. 생성 모델을 이미지에 적용하면 초해상도(super resolution) 이나 색입힘(colorization)과 같은 테스크를 적용할 수 있다.colorization의 에를 보면 지갑의 밑그림만 그려놓으면 생성 모델이 색을 채워줘서 지갑이 실제로 어떻게 생겼을지 알 수 있다.생성 모델은 강화 학습을 이용한 시뮬레이션이나 플래닝(planning)을 위한 시계열 데이터 생성에도 이용할 수 있다.생성 모델을 학습하면 latent representation을 추정해볼 수 있다. 데이터의 잠재적인 특징(latent features)을 잘 학습시켜 놓으면 추후 다른 테스크에도 아주 유용하게 쓸 수 있다.생성 모델의 종류로는 다음과 같은 taxonomy로 분류해볼 수 있다. 앞서 설명한 명시적/간접적 분포 모델에서 서브 카테고리로 더 쪼갤 수 있다.여기서 세가지 모델만 배울 것이다. PixelRNN/CNN과 VAE(variational autoencoders) 이 두가지 모델은 명시적 분포 모델에 속한다. pixelRNN/CNN은 계산 가능한 확률 모델 사용(tractable density)에 속하고, VAE는 근사적 밀도추정(Approximate density)에 속한다. 마지막으로 generative adversarial networks(GAN)에 대해 배울 것이다. GAN의 경우 간접적인 분포 추정(implicit density)에 속한다.PixelRNN/CNNpixelRNN/CNN은 fully visible brief networks의 일종이다. 밀도를 명시적으로 정의하는 모델이다.이미지 데이터 x가 있고, 이 x에 대한 likelihood인 p(x)를 모델링한 것이다. 이 모델의 경우 chain rule로 likelihood인 p(x)를 1차원 분포들간의 곱의 형태로 분해한다. 이렇게 분해하면 픽셀 x_i에 대해 각각 p(x_i|conditions)를 정의할 수 있다. conditions는 이전의 모든 픽셀 x1부터 x_(i-1)이다. 따라서 이미지 내 모든 픽셀에 대한 likelihood는 모든 픽셀의 likelihood의 곱의 형태와 같다.이제는 모델을 학습시키기 위해 학습 데이터의 likelihood를 최대화시킨다. 픽셀 값에 대한 분포를 보면 p(x_i|이전의 모든 픽셀)이다.복잡한 변환(transformation)을 수행할 때는 neural network를 사용해왔기에 이런 복잡한 분포를 표현하기 위해서 신경망을 사용한다. 여기서 문제는 픽셀들의 순서는 어떻게 할지를 생각해봐야 한다. 분포 p(현재 픽셀|모든 이전 픽셀)에서 모든 이전 픽셀이 의미하는 바는 앞으로 살펴볼 것이다.pixelRNN은 기본적으로 이 문제를 풀기 위해 고안된 방법이다. 우선 좌상단 코너에 있는 픽셀부터 생성한다. 위의 사진에 보이는 그리드가 이미지의 픽셀로 볼 수 있고, 화살표 방향으로의 연결성을 기반으로 순차적으로 픽셀을 생성한다. 이런 방향성을 기반으로 픽셀들간의 종속성을 RNN 중 LSTM을 사용한다.이 방법은 잘 동작하지만 순차적 생성 방식때문에 아주 느리다는 단점이 있다. 예를 들어 새로운 이미지를 생성한다고 하면 여러번의 feed forward를 거쳐야 한다. 모든 픽셀이 생성될 때까지 반복적으로 네트워크를 수행해야 하는 것이다.pixelRNN 후 pixelCNN이라는 또 다른 모델이 있다. pixelCNN는 pixelRNN과 거의 동일하다. 왼쪽 코너부터 새로운 이미지를 생성한다. 차이점은 모든 종속성을 고려하여 모델링하는 RNN과 달리 CNN으로 모델링한다. 이전의 모든 픽셀을 고려하는 대신 픽셀을 생성할 때 특정 픽셀만을 고려하는 것이다.또한, CNN은 출력 값을 softmax loss를 계산한다. 예기에서는 레이블이 0-255가 될 것이다. 우리는 학습 데이터로 likelihood가 최대화하도록 학습시킬 수 있다.이렇게 픽셀을 생성하는 과정에서 각 픽셀 값은 정답(Ground Truth)를 가지고 있을 것이다. 이 정답값은 0-255사이의 분류 문제를 풀기 위한 레이블이라고 볼 수 있다. 따라서 softmax loss로 학습시킬 수 있는 것이다. 이런 학습 과정은 동일하게 likelihood를 최대화한다.위의 분류 레이블은 loss를 계산할 때 쓴다.pixelCNN이 pixelRNN보다 훨씬 빠르다. 왜냐하면 Train time에서는 모든 데이터에 대해 학습 데이터의 likelihood를 최대화하는 것이기 때문이다. 학습 데이터는 이미 우리가 알고 있는 값이므로 학습 과정을 병렬화할 수 있다. 그러나 새로운 이미지를 생성해야 하는 test time에서는 여전히 코너에서부터 시작해야 하고 생성 방법에 대한 새로운 제안이 없기에 이전 픽셀부터 시작해야 하지만, 모든 픽셀을 고려하는 것이 아니기 때문에 빠를 것이다.첫 픽셀의 분포에 따라 민감할 수 있다. 처음 픽셀이 모든 분포에 영향을 미치기 때문이다. 초기 픽셀의 분포를 선택하는 방법은 training time에서 학습 데이터의 픽셀 값을 가져온다. test time에서 uniform distribution을 사용할 수 있고, 첫 픽셀만 가져올 수도 있다. 초기 값만 설정하면 나머지는 네트워크가 알아서 한다. Q. 한번에 모든 픽셀을 예측하는 대신 chain rule 방식으로 이를 정의하는 방법은 있는가? =&amp;gt; chain rule은 계산 가능한 확률모델을 기반으로 likelihood를 직접 최적화시킬 수 있도록 도와준다.이는 pixelRNN/CNN으로 생성한 이미지들이다.요약하자면, pixelRNN/CNN은 likelihood p(x)를 명시적으로 계산하는 방법이다. 우리가 최적화시킬 수 있는 분포(밀도)를 명시적으로 정의한다.이렇게 분포를 명시적으로 정의하는 경우의 추가적인 장점은 측정 공식(evaluation metric)이 존재한다는 것이다. 우리가 데이터를 통해 계산할 수 있는 likelihood를 이용하면 생성된 샘플이 얼마나 잘 만들어졌는지를 평가할 수 있다.pixelRNN/CNN의 가장 큰 단점은 생성 과정이 순차적이기 때문에 상당히 느리다는 것이다.하지만, pixelRNN/CNN은 음성생성(audio generation)에도 사용될 수 있다. 음성생성의 경우에도 느리다는 단점은 여전히 존재한다.Variational AutoEncoders(VAE)지금까지 살펴본 pixelCNN은 여기 있는 정의처럼 계산이 가능한 확률모델을 기반으로 한다. 이를 기반으로 학습 데이터의 likelihood를 직접 최적화한다.하지만 VAE의 경우 직접 계산이 불가능한 확률 모델을 정의한다.이제 추가적인 잠재 변수(latent variable) z를 모델링한다.VAE에서는 data likelihood p(x)가 적줍 형태를 띄고 있다. 가능한 모든 z값에 대한 기댓값을 구하는 방식이다. 문제는 이 식을 직접 최적화할 수 없다. 대신 이 likelihood p(x)의 lower bound를 구해서 최적화시켜야 한다.VAE의 배경을 살펴보기 위해 AE를 먼저 살펴봐야 한다. AutoEncodersautoencoders(AE)라는 비지도 학습 모델과 관련이 있다. 우선 autoencoders는 데이터 생성이 목적이 아니다. AE는 레이블되지 않은 학습 데이터로부터 저차원의 feature representation을 학습하기 위한 비지도 학습 방법이다. 위의 사진을 보면 입력 데이터 x가 있다고 하고, 어떤 특징 z를 학습하길 원한다. encoder는 입력 데이터 x를 특징 z로 변환하는 매핑 함수의 역할을 한다. encoder는 다양한 방법으로 설계할 수 있다. 일반적으로는 neural network를 사용한다.autoencoder는 예전부터 linear + nonlinearity를 이용한 모델을 사용했고, 이후 FC-Layer를 사용한 더 깊은 네트워크와 CNN을 사용하게 되었다.일반적으로 z는 x보다 작다. 이로 인해 기본적으로 AE를 통해 차원 축소의 효과를 기대할 수 있다. 차원 축소를 하는 이유는 z가 데이터 x의 중요한 요소들이 담겨 있는 특징들을 학습하길 원하기 때문이다.그렇다면 어떻게 feature representation을 학습할 수 있는가?AE는 원본을 다시 복원하는데 사용될 수 있는 특징들을 학습하는 방식을 취한다. AE의 과정을 보면 입력 데이터 x가 있고, encoder는 x를 더 낮은 차원의 z로 매핑시킨다. z는 encoder의 출력이다.z는 두번째 네트워크인 decoder의 입력으로 사용된다. decoder의 출력은 입력 x와 동일한 차원이고 x와 유사해야 한다. 즉 원본 데이터를 복원하는 것이다.decoder와 encoder는 동일한 구조를 지니며 대칭적이어야 한다. 대게는 CNN으로 구성한다.전체 과정을 보면 입력 데이터 x를 encoder에 통과시킨다. encoder는 4 layer CNN이 될 수 있을 것이다. encoder를 거쳐 특징 z를 얻으면 z를 decoder에 통과시킨다. decoder는 upconv 일 수 있다.CNN모델로 AE를 설계했을 때 encoder는 conv net, decoder는 upconv net인 이유는 encoder는 고차원 입력 x를 받아 저차원 특징 z로 변환하는 반면 decoder는 저차원 특징 z를 고차원으로 복원해야 하기 때문이다.loss로는 L2 loss function을 사용하는데, 복원된 이미지의 픽셀 값과 입력 이미지의 픽셀 값이 서로 같은지를 비교하는 것이다.중요한 것은 AE에서는 학습과정에서 추가적인 레이블을 필요로 하지 않는다는 것이다. 우리가 가진 데이터가 레이블이 없는 데이터여도 가능하다는 것이고, 그것으로 loss를 구할 수 있다.사실 decoder는 training time에서 입력을 복원해서 loss 함수를 계산하는 용도로만 사용된다. 따라서 특징 z가 나오면 추가적인 분류기를 붙힌다.레이블이 필요하지 않은 AE는 데이터가 부족한 지도 학습 모델의 초기 가중치로 이용할 수 있는 것이다. 소량의 데이터로 학습할 경우 과적합(overfitting)과 같은 다양한 문제가 발생한다.AE를 통해 학습 데이터의 변형(variation)을 잘 포착해낼 수 있다. 즉 잠재 변수인 벡터 z가 학습 데이터의 variation을 잘 가지고 있다. VAE는 AE와는 관점이 조금 다르다. VAE는 새로운 데이터를 생성할 것이고, 이를 위해 모델로부터 데이터를 샘플링할 것이다.VAE에서는 학습 데이터 xi가 있다. i는 1 ~ N이다. 이 학습 데이터는 잠재 변수 z에 의해 생성된다고 가정해보자. z는 어떤 벡터고, z의 각 요소들은 데이터의 변동 요소들을 잘 포착해내고 있을 것이다.즉, 벡터 z가 다양한 종류의 속성들을 담고 있다. 예를 들어 얼굴을 생성한다고 하면, z는 생성된 얼굴이 웃고있는지, 눈썹의 위치, 머리의 방향 등이 있을 것이다. 이런 것들이 학습될 수 있는 잠재된 요소다.생성 과정에서는 z에 대한 사전정보로부터 샘플링이 진행된다. 속성을 담기 위해서는 속성들이 어떤 분포를 가지는지에 대한 사전정보를 정의해야 한다. 가령 z에 대한 사전정보로 가우시안 분포를 선택할 수 있다. distribution, p로부터 샘플링하여 데이터 x를 생성해낸다.파라미터에는 사진에 보이는 distribution, theta* 과 prior 등이 있다. 생성모델이 새로운 데이터를 잘 생성하게 하려면 true parameter, theta* 을 잘 추정해야 한다.그렇다면 모델을 어떻게 설계할까?prior인 p(z)는 단순한 가우시안으로 정한다. 하지만 conditional distribution, p(x|z)는 다르다. 우리는 p(x|z)를 가지고 이미지를 생성해야 한다. 대체로 p(x|z)는 neural network로 모델링한다.이렇게 설계된 네트워크를 decoder network라고 한다. decoder는 잠재 변수z를 받아서 이미지로 디코딩하는 역할을 한다.모델들의 파라미터를 추정하기 위해서는 모델을 학습시켜야 할 것이다. 위의 pixelRNN/CNN의 전략을 떠올려보자면 모델 파라미터가 학습 데이터의 likelihood를 최대화하도록 학습시키는 것이다. VAE의 경우 잠재 변수 z가 있다. p(x)는 모든 z에 대한 기댓값(likelihood)이다. p(x)를 최대화시키려고 할 때, pixelRNN처럼 gradient를 계산하며 최대화하지 못한다.더 자세하게 들어가서 likelihood, p(x)의 첫번째 항은 prior, p(z)다. p(z)는 가우시안일 것이고, 이는 계산이 가능하다. p(x|z)는 우리가 정의한 decoder neural network의 출력이다. 따라서 z가 주어지기만 하면 p(x|z)는 계산이 가능하다. 그렇다면 문제는 모든 z에 대해 p(x|z)를 계산하고 싶지만 계산할 수 없다는 것이다.해결책으로는 decoder network가 p(x|z)와 추가적으로 encoder network를 정의하는 것이다. encoder는 q(z|x)이고, encoder를 이용해서 입력 x를 z로 인코딩할 것이다. 이 encoder를 통해 p(z|x)를 근사시킨다.이와 같이 p(x|z) 대신 p(z|x)를 정의하면 계산이 가능해진다. p(z|x)를 근사시키면 data likelihood의 최솟값(lower bound)를 구할 수 있고, 이를 통해 최적화가 가능해진다.더 구체적으로 살펴보자. VAE에서 우리가 하고싶은 것은 데이터의 확률론적 생성(probabilistic generation)모델을 만드는 것이다. autoencoder에서는 encoder에서 입력 x를 받아 z를 만들고 decoder에서 z를 받아 다시 이미지를 생성했다. VAE도 기본적으로 encoder-decoder 구조다. 추가적으로 확률론적 의미를 가미한다.우선 파라미터 ∅를 가진 encoder network q(z|x)를 살펴보자. encoder의 출력은 평균과 공분산으로, μ(z|x)와 ∑(z|x)이다. 이 두개를 평균하여 z를 구한다.decoder의 입력으로 z일 것이고, 출력으로는 다시 μ(x|z)와 ∑(x|z)가 된다. 이를 평균내어 출력한다. p(x|z)에서 x의 차원은 입력 x와 동일하다.z|x와 x|z를 얻으려면 이들의 분포로부터 샘플링해야 한다. 따라서 encoder/decoder network는 각각 z와 x에 대한 분포를 생성해야 한다.data likelihood를 보게 되면 likelihood, p(x)에 log가 취해져 있는 것을 볼 수 있다. 거기에 z에 대한 기댓값(expectation)을 취한다. expectation을 취할 수 있는 이유는 p(x)가 z에 독립적이기 때문이다.이 식을 확장해서 베이즈 룰(Bayes’ Rule)를 적용해보자. 그 다음 분모분자에 각각 상수인 q(z|x)를 곱해준다. 그리고 이 식을 3개 항으로 나눈다. 첫번째 항은 E(logp(x|z))이고, 나머지 두 개는 KL 항이다. 간단하게 설명하자면 KL divergence(차이)는 분포간의 거리를 측정하는 척도로, 두 분포가 얼마나 가까운지를 알려준다. 첫번째 KL는 q(z|x)와 p(z)가 얼마나 가까운지를 나타내는 것이다.p(x|z)는 decoder network다. 이 첫 번째 항은 샘플링을 통해서 계산할 수 있다. 이런 샘플링 과정에서 미분이 가능하도록 하는 “re-parametrization trick”이라는 기법이 있다.두 번째 항을 보게 되면 두 가우시안 분포 간의 KL divergence다. 우선 q(z|x)는 encoder에서 발생하는 분포로 평균/공분산을 가지는 가우시간 분포다. 그리고 prior, p(z)도 가우시안이다. 참고로 KL divergence에서 두 개의 분포가 모두 가우시안이면 closed form solution으로 풀 수 있다.마지막으로 세 번째 항은 q(z|x)와 p(z|x)간의 KL이다. 앞서 p(z|x)는 계산할 수 없는 항이었다. p(z|x)를 계산하지 못하여 q로 근사시킨 것이다. 하지만 여기서 KL이란 두 분포간의 거리다. 즉 항상 0보다 크다는 것이다.따라서 앞의 두 항만 가지고 gradient를 이용해 최적화시키면 된다.VAE를 학습시키는 것은 최솟값을 최대화시키도록 최적화하는 것이다. 다시 말해 data likelihood의 최솟값을 최적화시키는 것이다. 최솟값을 최대화시키기 위해서는 파라미터 θ와 ∅를 구해야 한다.최솟값 항을 보면 첫번째 항은 모든 샘플 z에 대한 기댓값(expectation)이다. z는 encoder의 출력이다. encoder로 z를 샘플링한 후, 모든 z에 대해 p(x|z)의 기댓값을 구한다. 즉 복원(reconstruction)을 의미한다. 첫번째 항이 크다는 것은 likelihood p(x|z)가 크다는 것이고, 이것은 데이터가 잘 복원되고 있다는 것을 의미한다.두번째 항을 보면 KL divergence가 작아야 한다는 것이다. 우리가 근사시킨 분포(q)와 prior의 분포(p)가 최대한 가까워야 한다. 즉, 잠재 변수 z의 분포가 prior 분포(가우시안)와 유사해야 한다.이렇게 구한 최솟값을 통해 VAE를 학습시키는 과정을 살펴보자. 우리는 이 lower bound를 최대화하길 원한다.우선 forward pass에서 입력 데이터 x가 minibatch 크기로 있을 것이고, encoder를 통과하면 q(z|x)를 얻을 수 있다. q(z|x)는 KL divergence를 계산할 때 이용할 수 있다. q(z|x) 분포로부터 잠재변수 z를 샘플링한다. 샘플링한 z를 decoder에 통과시킨다. decoder network의 출력은 p(x|z)에 대한 평균과 분산이다. 이 분포를 바탕으로 샘플링하여 x^를 구한다.training time에서는 log(p(x|z))가 최대가 되도록 학습시킨다.이렇게 모든 항은 미분이 가능하기 때문에 backpropagation을 할 수 있다. gradient를 계산하여 encoder/decoder의 파라미터 θ와 ∅를 최대화한다.VAE를 학습시키고 나면 데이터 생성시에는 decoder network만 필요하다. 앞서 training time에서는 z를 p(z|x)에 대해 샘플링했다. 하지만 생성과정에서는 prior(가우시안)에서 샘플링한다. 이를 바탕으로 데이터 x^을 샘플링한다.VAE를 MNIST로 학습시키고 샘플들을 생성시킨 결과다. 변수 z가 데이터의 잠재적인 속성들을 나타낸다. 오른쪽이 2차원 z분포에서 vary z1 / vary z2 의 조합으로 생성된 이미지다. z1과 z2의 값이 변함에 따라 이미지도 아주 부드럽게 변하고 잇는 것을 볼 수 있다. z는 독립적이고 이로 인해 각 차원마다 독립적인 해석이 가능한 요소들이 있다.얼굴 이미지의 예로 vary z1은 위 아래로 웃음의 정도를 나타낸다. vary z2는 머리의 위치가 바뀌는 것을 나타낸다. 이를 통해 VAE를 학습시킨 결과 z라는 변수가 해석 가능하고 아주 다양한 의미를 가지는 요소들이 인코딩될 수 있다는 것을 알 수 있다.앞서 학습시킨 encoder network에 새로운 입력 x를 넣어 z 공간으로 매핑시키면, 이 특징벡터를 classification이나 다른 테스크에 사용할 수 있다.VAE는 이미지들을 잘 생성해내지만, 단점으로는 원본에 비해 흐리다는 점이다. 특히 얼굴 이미지는 더더욱 그렇다.VAE는 autoencoders의 확률론적 변형 버전이다. AE는 x를 받아 z를 만들고 다시 x를 복원하지만, VAE는 데이터를 생성해내기 위해 분포와 샘플링 개념이 추가되었다. 또, 계산할 수 없는 분포를 다루기 위해 최솟값을 계산했다. Variational은 계산할 수 없는 형태를 계산할 수 있도록 근사시키는 방법을 의미한다. 따라서 VAE가 된다.VAE의 장점은 생성 모델에 대한 원칙적 접근 방법이라는 점과 모델에서 q(z|x)를 추론한다는 점이다. q(z|x)는 다른 테스크에서도 유용한 방법이 될 수 있다. 단점으로는 likelihood의 최솟값을 계산하는 것이다. 이렇게 되면 섬세하지 못할 수 있다.Generative Adversarial Networks(GAN)앞서 살펴본 pixelRNN/CNN은 계산 가능한 확률분포를 가정했다. 이를 이용해서 학습 데이터의 likelihood를 최적화시켰다.반면 VAE는 잠재 변수 z를 두고 생성 과정을 정의했다. 잠재 변수 z는 많은 이점이 있지만, VAE는 계산할 수 없는 확률분포를 가정하기 때문에 likelihood를 직접 최적화시키지 못하고 최솟값을 구해 그것을 최적화시켰다.하지만, “확률분포를 직접 모델링하지 않아도 되지 않을까”에 대한 의문으로 시작되어 GAN에서는 확률분포를 모델링하지 않는다. GAN에서는 2 player game 이라는 방식으로 학습 분포를 학습한다. GAN에서는 복잡한 고차원 학습 분포로부터 샘플링하는 것이다.하지만, 우리가 가진 분포가 매우 복잡(neural network)하기 때문에 분포에서 직접 샘플을 만들어내는 과정이 없다. 따라서 우선 gaussian random noise같은 더 단순한 분포를 활용할 것이다. 단순한 분포에서 우리가 원하는 학습 분포로 변환하는 함수를 배워야 한다.GAN에서는 입력으로 random noise 벡터(z)를 받는다. 벡터의 차원 수를 직접 명시해준다. 그리고 입력 z가 생성 네트워크를 통과하면 학습 분포로부터 직접 샘플링된 값을 출력한다. 따라서 모든 random noise 입력이 학습 분포의 샘플에 매핑되길 원한다.two player game을 살펴보자. 하나는 generator이고, 다른 하나는 discriminator(식별자)이다. generator는 사실적인 이미지를 생성하여 discriminator를 속이는 것이 목표이다. 식별자는 입력 이미지가 실제인지 거짓인지 구별하는 것이 목표이다.식별자는 이미지가 generator가 만든 가짜 이미지는인지 아닌지를 가능한 잘 구분해 내야 한다.random noise가 generator의 입력으로 들어가고, generator는 가짜 이미지를 생성한다. 식별자는 실제/가짜 이미지를 구별할 줄 알아야 한다. 그래서 식별자의 출력은 이미지가 진짜/가짜인지 이다.즉, 아주 좋은 generative model을 만들기 위해서는 discriminator가 아주 잘 학습되어 진짜인지 가짜인지 잘 구별해야 하고, 그러려면 generator가 더 실제같은 가짜 이미지를 만들 수 있어야 한다.minmax objective function는 generator network의 파라미터인 G를 최소화시킨다. 또, discriminator network의 파라미터인 D는 최대화시킨다. 이 함수에서 말하는 것은 데이터에 대한 Expectation(logD(x))이다. log D(x)는 실제 데이터(x)가 데이터 분포 p_data에 속할 확률(likelihood)이다.두번째 항을 보면 p(z)를 따르는 z에 대한 기댓값에서 z ~p(z)의 의미는 generator에서 샘플링한다는 의미다. 그리고 D(G(z))는 생성된 가짜 이미지(G(z))에 대한 식별자의 출력이다. D(x)는 실제 데이터이므로 값이1 이면 좋다. 반면 D(F(x))는 가짜 데이터에 대한 것이므로 0일수록 좋다.그리고 식별자의입장에서 objective function을 최대화시킨다는 것은 진짜인지 가짜인지를 더 잘 구별해낸다는 의미다. 반면 generator는 objective function이 작을수록 좋으므로 D(G(z))가 1에 가까울수록 좋다.D(G(z))가 1에 가까우면 1-D(G(z))는 값이 작아진다. 작아진다는 것은 가짜 이미지를 진짜라고 잘못 분류한다는 의미다. 다시 말해 generator가 진짜 같은 이미지를 잘 만들고 있다는 것이다.GAN은 기본적으로 비지도 학습에 해당한다. 즉, 레이블이 필요하지 않다. generator에서 생성된 데이터의 레이블은 가짜 이미지라는 의미에서 0이다. 그렇다면 실제 이미지의 경우 1에 해당한다. 이 레이블은 식별자의 loss함수에서 사용된다.학습을 시킬 때는 generator를 학습시키는 경우에는 식별자를 고정시키고 backprop를 진행하고, 식별자를 학습시키는 경우에는 generator를 고정시킨 후 backprop를 진행한다.GAN을 학습시키려면 generator와 discriminator를 번갈아가면서 학습시킨다. 식별자의 경우 objective function이 최대가 되는 theta를 학습시키기 위해 gradient ascent를 이용하고, generator는 반대로 gradient descent를 이용한다. descent를 통해 파라미터 theta_G를 학습시켜서 objective function이 최소가 되도록 한다.따라서 GAN은 two player, minmax 를 이용해서 discriminator/generator를 번갈아가면서 학습시킨다.여기서 중요한 것은 실제로는 generator의 objective function이 학습이 잘 안된다. 그 이유는 loss 곡선을 보면 알 수 있는데, 오른쪽 아래의 그래프가 D(G(x))의 loss 곡선이다. generator의 경우 1-D(G(x))의 값이 높을수록 좋다. loss가 최소가 되길 원하는데, loss의 길울기는 오른쪽으로 갈수록 점점 커진다. 즉, D(G(x))가 1에 가까울수록 기울기도 커진다는 것이다.이는 generator가 식별자를 잘 속이고 있으면 gradient도 점점 더 커진다. 반대로 generator가 아직 잘 학습되지 않는 경우라면 식별자가 쉽게 구분할 수 있는 상태로 x축 상의 0에 가까울 것이다. 이 지점은 gradient가 상대적으로 평평하다.이는, gradient가 generator가 생성을 이미 잘 하고 있는 지역에 몰려있다는 것이다. 우리가 원하는 것은 샘플이 좋지 않을수록 gradient가 높아야 한다. 따라서 generator가 학습이 어렵다.그렇기에 gradient 개선을 위해 objective function을 조금 변경해야 한다. generator에서도 gradient ascent를 사용해준다. 앞의 수식에서는 식별자가 정답을 잘 맞출 확률(likelihood)를 최소화시키는 방법이었다면, 이제는 반대로 식별자가 틀릴 확률을 최대화시키는 것이다. 이를 통해 objective function을 log(D(G(x)))를 최대화시키는 것으로 구현할 수 있다. 오른쪽 밑 그래프의 초록색 그래프에서 음수만 붙여주면 된다.요약하면, 학습은 반복으로 진행될 것이고, 학습 순서는 우선 식별자를 먼저 조금 학습시킨 후 generator를 학습시키는 방식이다.k번만큼 식별자를 학습시키고, noise prior z, p(z)에서 미니배치만큼 샘플링한다. 그리고 학습 데이터 x에서 실제 샘플을 미니배치만큼 샘플링한다. 샘플링한 노이즈를 generator에 통과시키면 가짜 이미지가 생성될 것이다. 그러면 미니배치만큼의 가짜 이미지와 미니배치만큼의 진짜 이미지가 존재하게 된다.식별자의 gradient를 계산할 때 이렇게 준비한 진짜/가짜 이미지를 사용한다. 그 후 식별자의 파라미터를 업데이트한다.이 작업이 끝나면 generator를 학습시킨다. 또 다시 noise prior, p(z)에서 노이즈를 샘플링한다. 그 노이즈를 generator에 통과시키고 generator를 최적화시킨다.이런식으로 반복적으로 번갈아 학습한다.GAN의 안정적인 학습을 위한 방법 중 대표적인 예로 “Wasserstein GAN”이 있다.원래의 GAN은 fully connected network를 사용한다. 그러나 기술이 발전함에 따라 FC layer 대신 CNN을 적용시켰다. 그 모델 이름이 DCGAN이다.전체적인 내용을 보면 입력 노이즈 벡터 z가 있고, z를 위와 같은 과정을 통해 변환 출력한다.그 결과 위와 같이 다양한 가구들이 잘 생성된 것을 볼 수 있다.여기에서는 2개의 랜덤 노이즈 벡터z 2개를 잡고, 그 사이를 보간한 것을 볼 수 있다. 위의 사진은 두 개의 랜덤 노이즈 z를 보간해서 이미지를 생성한 결과다. 양 끝단이 z이고, 이를 보간하며 자연스럽게 연결시킨다.다른 방식으로 접근해보자. 벡터 z를 가지고 벡터 연산을 한다. 이 실험에서는 웃고 있는 여성의 이미지와 웃고 있지 않은 여성/남성의 사진도 뽑는다. 그리고 뽑은 벡터 z 들에 각 평균을 취한다.그 후 3개를 벡터 연산하면 웃는 남성이 만들 수 있다.Referencehttp://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf" }, { "title": " CS231N chapter 12 - Visualizing and Understanding ", "url": "/posts/cs231n12/", "categories": "Classlog, CS231N", "tags": "CS231N, Visualizing, occlusion-experiment, Texture-synthesis, style-transfer", "date": "2021-11-03 13:00:00 +0900", "snippet": " 11강 리뷰 1) detection 2) Classification + Localization 3) segmentation instance segmentation , semantic segmentation What`s going on inside ConvNets?ConvNet 안에서 어떤 일이 일어나고 있는지 살펴보고자 한다. 이를 알기 위해서 주로 Visualizing 기법을 활용한다. 즉, 딥러닝 내부를 시각화해서 살펴보는 것이다.CNN의 입력은 이미지고, 출력은 class score 형태로 나온다. score 뿐만 아니라 bounding box의 위치, labeled pixel와 같은 형태의 출력도 있다.alexNet을 예시로 살펴보면, 첫번째 conv layer에는 많은 필터(64개)들이 있다. conv filter는 sliding window로 이미지를 돈다. image와 weight의 내적이 첫번째 layer의 출력이 된다.이 필터를 단순히 시각화시키는 것만으로도 이 필터가 이미지에서 무엇을 찾고 있는지 알아낼 수 있다. 11x11x3 필터가 총 64개 이므로 64개의 11x11 이미지를 시각화시킬 수 있다.edge(흰/검)나 보색(oppising colors)(초록/분홍 등) 성분이 주로 검출되는 것을 볼 수 있다.어떤 모델/데이터를 사용하든 첫번째 레이어는 이런 식으로 생겼다.하지만 layer가 깊어질수록 해석하기 어려워 시각화를 통해 정보를 얻어내기가 쉽지 않다.다음 layer의 가중치를 시각화한 것들은 전 layer의 출력을 가지고 최대화시키는 패턴에 대한 정보이기 때문이다.CNN의 마지막 layer는 1000개의 class score를 출력한다. 이는 학습 데이터의 predicted score를 의미한다.마지막 hidden layer를 시각화해서 어떤 일이 일어나는지 알아보는 것도 한 방법이 될 수 있다. 여기서는 각 이미지에서 나온 4096dim 특징 벡터를 저장한다.Nearest Neighbor시각화하는 방법 중 하나는 nearest neighbor을 이용하는 것이다. 픽셀 단위로 사용하면 정확도가 높지 않을 수 있지만, CNN 모델에서 마지막 hidden layer의 4096dim 특징 벡터들에 nearest neighbor을 적용하면 객체의 방향이나 위치가 서로 다르더라도 올바르게 분류하고 있음을 볼 수 있다.Dimensionality Reduction차원 축소(dimensionality reduction)의 관점으로 시각화할 수도 있다. Principle Component AnalysisPCA는 4096dim 과 같은 고차원 특징벡터들을 2dim으로 압축시키는 기법이다. 이 방법을 통해 특징 공간을 조금 더 직접적으로 시각화할 수 있다. t-distributed stochastic neighbor embeddingst-SNE이라는 알고리즘은 PCA보다 조금 더 파워풀하다. 많은 사람들이 특징공간을 시각화하기 위해서 더 사용하는 방법이다.MNIST를 t-SNE dimensionality reduction 를 통해 시각화한 모습이다. MNIST는 0부터 9까지로 이루어진 손글씨 숫자 데이터셋이다. MNIST의 각 이미지는 gray scale 28x28이고, t-SNE는 MNIST의 28x28 dim 데이터를 입력으로 받는다.(raw pixels) 그리고 2dim으로 압축한다.이런 식으로 자연스럽게 군집화된 모습을 볼 수 있다. 각 군집은 MNIST의 각 숫자를 의미한다.이미지에서 4096dim을 2dim으로 군집화시킨 후 차원 축소한 모습이다.한 이미지당 서로 다른 세 가지 정보가 존재한다. 우선 (픽셀로 된) 원본 이미지가 있다. 그리고 4096dim 벡터가 있고, t-SNE를 이용해 2dim벡터로 변환시킨 값이 있다. 결국 원본 이미지를 CNN으로 4096dim으로 줄이고 이를 다시 t-SNE을 통해 2dim으로 줄였다고 할 수 있다.다시 alexNet로 돌아가서 alexNet의 conv5의 특징은 128x13x13 dim tensor이다. 이 tensor는 128개의 13x13 2 dim grid로 볼 수 있다. 따라서 이 13x13 특징맵을 gray scale 이미지로 시각화해볼 수 있다.사람의 얼굴을 CNN에 넣게되면 특징맵이 얼굴에 활성화된 것을 볼 수 있다. Q. 검은색은 “Dead ReLU”인가? Dead ReLU는 모든 학습 데이터셋에 대해서 DEAD(활성화되지 않음)을 의미한다. 하지만 이 예시에서는 특정 입력에 대해서 활성화되지 않을 뿐이다.이 초록색 박스 하나가 activation map이다. 이처럼 activation을 시각화해도 해석이 가능할 수 있다.Maximally Activating Patchesvisualizing intermediate feature의 다른 방법으로 iuput image의 특정 patch를 시각화한 것이다.Occlusion experiment입력의 어떤 부분이 분류를 결정짓는 근거가 되는지에 대한 실험이다.입력 이미지의 일부를 가리고 가린 부분을 데이터셋의 평균 값으로 채운다. 그런 후 이미지를 네트워크에 통과시키고 네트워크가 이 이미지를 예측한 확률(결과)을 기록한다. 이 과정을 occlusion patch 를 이동시키면서 같은 과정을 반복한다.오른쪽 히트맵은 이미지를 가린 patch의 위치에 따른 네트워크의 예측 확률의 변화를 의미한다. patch의 위치에 따라 스코어가 크게 변화한다면 그 부분이 분류를 결정짓는 중요한 부분일 수 있다.위의 사진 오른쪽 부분에 occlusion 실험을 수행한 세 가지 예시가 있다.빨간색 지역은 확률 값이 낮고, 노란색 지역은 확률 값이 높음을 의미한다. go-kart를 보면 앞쪽의 kart를 가릴 경우 kart에 대한 확률이 많이 감소됨을 볼 수 있다. 이를 통해 네트워크가 분류를 결정할 때 실제로 kart를 많이 고려한다는 사실을 알 수 있다.위의 방법들이 성능을 좌우하는 큰 작업은 아닐 수 있으나 네트워크가 무엇을 하고 있는지 이해하는 도구로서 사용할 수 있다. 결국 이런 시각화는 네트워크의 성능을 높히려는 목적이 아니라 이해가 목적이다.Saliency Map입력 이미지가 들어와 이 이미지를 “개”라고 예측했을 때, 우리가 알고싶은 것은 네트워크가 픽셀들을 보고 이미지를 “개”라고 분류했는지다. 앞서 occlusion 방법과는 조금 다른 접근법을 취한다. 즉, 어떤 픽셀이 “개”라고 분류하는데 있어서 가장 필요한지를 알 수 있는 방법이다.입력 이미지의 각 픽셀들에 대해 예측한 class score의 gradient를 계산하고 절대값을 취한다.위의 사진을 보면 네트워크가 이미지에서 어떤 픽셀을 찾고 있는지 볼 수 있다. 개의 위치가 있는 픽셀이 활성화 되어 있음을 알 수 있다.saliency map은 semantic segmentation에도 사용할 수 있다. segmentation label없이도 grabcut 이라는 알고리즘을 사용해 segmentation mask를 씌울 수 있다. 하지만 잘 되지는 않는다. supervision을 가지고 학습을 시키는 네트워크에 비해 성능이 좋지 않다.Guided backpropagation이제는 class scor이 아니라 네트워크 중간 뉴런을 고른다. 그리고 입력 이미지의 어떤 부분이 내가 선택한 중간 뉴런의 값에 영향을 주는지를 찾는다.이 경우에도 saliency map을 만들어볼 수 있다. 이미지의 각 픽셀에 대해 class score의 gradient를 계산하는 것이 아닌 입력 이미지의 각 픽셀에 대한 네트워크 중간 뉴런의 gradient를 계산한다.이를 통해 어떤 픽셀이 해당 뉴런에 영향을 주는지 알 수 있다. 이 경우 평범한 back propagation을 사용한다. 대신 backprop시에 ReLU를 통과할 때, 조금의 트릭(변형)을 가미한다고 해서 “guided backpropagation”이라 한다.ReLU의 gradient가 양수면 그대로 통과하고, 음수면 backprop를 하지 않는다. 이로 인해 전체 네트워크가 실제 gradient를 이용하는 것이 아닌 양의 부호 gradient만 사용한다.위의 방법들은 입력 이미지에 대한 영향을 보는 것이기에 입력 이미지에 의존하지 않는 방법이 있는지 살펴보고자 한다.입력 이미지가 아닌 다른 이미지로 해당 뉴런을 활성화시킬 수 있을까?Gradient Ascent그에 대한 해답으로 gradient ascent를 제시할 수 있다.우리는 지금까지 loss를 최소화시켜 네트워크를 학습시키기 위해 gradient decent를 사용했다. 하지만 여기서는 네트워크의 가중치들을 전부 고정시킨다. 그 후 gradient ascent를 통해 중간 뉴런 혹은 class score를 최대화시키는 이미지의 픽셀들을 만들어낸다.gradient ascent는 가중치를 최적화하는 것이 아닌 뉴런 또는 class score가 최대화될 수 있도록 입력 이미지의 픽셀 값을 바꿔주는 방법이다. 이 방법을 위해서는 regularization term이 필요하다. 원래는 과적합을 방지하기 위해서 사용하는데, 이 경우 생성된 이미지가 특정 네트워크의 특성에 완전히 과적합되는 것을 방지하기 위해 사용된다.regularization term을 추가함으로써 우리는 생성된 이미지가 두 가지 특성을 따르길 원한다. 하나는 이미지가 특정 뉴런의 값을 최대화시키는 방향으로 생성, 다른 하나는 이미지가 자연스러워 보여야 한다는 것이다. regularization term을 통해 생성된 이미지가 자연스럽도록 강제한다.먼저 초기 이미지가 필요하다. 이를 zeros, uniform, noise 등으로 초기화한다. 그 후 이미지를 네트워크에 통과시키고 한 뉴런의 score를 계산한다. 그리고 이미지의 각 픽셀에 대한 해당 뉴런 score의 gradient를 계산하여 backprop를 수행한다. 그 다음 gradient ascent를 통해 이미지 픽셀 자체를 업데이트 한다. 해당 스코어를 최대화시키는 것이다.이 과정을 반복하면 이미지가 만들어진다.여기에서는 단순하게 생성된 이미지에 대한 L2 norm을 계산하여 더해준다. norm을 수행해주지 않을 경우 이미지가 아무것도 활성화되지 않은 것처럼 보일 수 있다. norm을 통해 이미지가 조금 더 자연스럽게 생성될수 있도록 해준다.덤벨, 컵, 달마시안 등이 생성되었는데, 달마시안을 보면 검/흰 반점 무늬를 볼 수 있다. 레몬의 경우 노란색도 보인다.이 시각화 방법으로 실제 색상을 시각화하는 것은 매우 까다롭다.norm + 최적화 과정에 이미지에 주기적으로 가우시안 블러를 적용하고, 주기적으로 값이 작은 픽셀들은 모두 0으로 만들면 더 인상적인 이미지를 얻을 수 있다. 낮은 기울기의 픽셀 값과 gradient가 작은 값들을 모두 0으로 만드는 것이다. 이는 일종의 projected gradient descent라고 볼 수 있다.가우시안 블러는 이미지를 주기적으로 매핑시키는 smooth 연산이다. 이를 통해 더 깔끔하게 만들 수 있다.각 layer가 어떤 부분을 찾고 있는지를 볼 수 있다. Multi-faceted각 클래스마다 클러스터링 알고리즘을 수행한다. 한 클래스 내 서로 다른 모드들끼리 다시 한번 클래스가 나뉜다. 나뉜 모드들과 가까운 곳으로 초기회홰주는 것이다. 이 방법을 통해 multimodality를 다룰 수 있다.예를 들어 8개의 이미지가 있다고 할 때, 이 8개 모두 식료품점에 대한 것이다. 위의 4개 이미지는 클로즈업한 사진들이다. 하단의 4개는 사람들이 식료품점을 돌아다니는 모습인 듯하다. 이 또한 식료품점으로 라벨링된다. 두 가지가 서로 다르지만 많은 클래스들이 이와 같이 multimodality를 가지고 있다.이미지를 생성할 때 multymodality를 명시하면 더 좋은 결과를 얻을 수 있다.이미지를 더 완전하게 생성하기 위해 사전 정보를 이용할 수도 있다.이 이미지들 모두 ImageNet의 특정 클래스를 최대화하는 이미지를 생성한 것이다. 입력 이미지의 픽셀을 곧장 최적화하는 대신 FC6을 최적화한다. 이를 위해 feature inversion network 등을 사용한다. 하지만 여기서는 설명하지 않겠다.그러나, 요점은 이런 이미지 생성 문제에서 사전 지식(prior)를 추가하면 아주 리얼한 이미지를 만들 수 있다.이미지 픽셀의 gradient를 이용해 이미지를 합성하는 방법은 강력하다. 이 방법으로 할 수 있는 것이 바로 네트워크를 속이는 이미지(fooling image)를 만드는 것이다.일단 아무 이미지를 하나 골라 네트워크가 이 사진을 다른 클래스로 분류하도록 미지리를 살짝 바꾼다. 그러면 네트워크는 이 이미지를 다른 클래스로 할 것이다. 우리에게는 다를 바 없지만 네트워크는 완전 다른 분류를 한다.이 모든 것들을 한 이유는 중간 뉴런을 이해하는 것이 최종 클래스 분류를 이해하는데 어떻게 도움을 줄 수 있는가에 대한 해답이다. 이런 시각화 기법들은 왜 딥러닝이 분류 문제를 잘 푸는지에 대한 의문에서 시작되었다. 딥러닝 모델이 아무렇게나 분류하는 것이 아니라 의미있는 행동을 통해 분류한다는 것을 증명하기 위함이다.이미지에 gradient를 업데이트하는 방식으로 가능한 아이디어가 하나 더 있다.DeepDreamDeepDream의 목적은 재미있는 이미지를 만드는 것이다. 부가적으로 모델이 이미지의 어떤 특징들을 찾고 있는지를 짐작할 수 있다. deepderam에서는 입력 이미지를 CNN의 중간 레이어 정도를 통과시킨다. 그 후 backprop를 하며 이미지를 업데이트한다. 네트워크에 의해 검출된 해당 이미지의 특징들을 증폭시키려는 것이다.여기에는 여러 트릭이 존재한다. 첫번째는 gradient를 계산하기 앞서 이미지를 조금씩 움직이는 것이다(jitter). 원본 이미지를 그대로 통과하지 않고 두 픽셀정도 이동시킨다. 이는 regularizer 역할을 함으로써 자연스럽고 부드러운 이미지를 만들게 한다. 그리고 L1 norm도 들어간다. 이는 이미지 합성 문제에서 아주 유용한 트릭이다. 그리고 픽셀 값을 한 번 클리핑(clipping)해주기도 한다. 이미지라면 값이 0~255 사이에 있어야만 한다. 이는 일종의 projected gradient decent인데, 실제 이미지가 존재할 수 있는 공간으로 매핑시키는 방법이다.def Ojbective_L2(dst): dst.diff[:] = dst.datadef make_step(net, step_size = 1.5, end=&#39;inception_4c/output&#39;, jitter=32, clip=True, objective=objective_L2): &#39;&#39;&#39;Basic gradient ascent step&#39;&#39;&#39; src = net.blobs[&#39;data&#39;] # input image is stored in Net&#39;s data blob dst = net.blobs[end] ox, oy = np.random.randint(-jitter, jitter+1, 2) # jitter image src.data[0] np.rool(np.roll(src.data[0], ox, -1), oy, -2) # apply jitter shift net.forward(end=end) objective(dst) # specify the optimization objective net.backward(start=end) g= src.diff[0] # apply normalized ascent step to the input image src.data[:] += step_size/np.abs(g).mean() * g # Li normalize gradients src.data[0] = np.roll(src.data[0], -ox, -1), -oy, -2) # unshift image # jitter image if clip: bias = net.transformer.mean[&#39;data&#39;] src.data[:] = np.clip[src.data, -bias, 255-bias] # clip pixel values이렇게 하늘 이미지를 가지고 알고리즘을 수행시키면 이처럼 아주 재미있는 결과를 볼 수 있다.Feature Inversion이 방법 또한 네트워크의 다양한 layer에서 입력 이미지의 어떤 요소들을 포착하고 있는지를 볼 수 있게 해준다.어떤 이미지가 있고, 이 이미지를 네트워크에 통과시킨 후 특징맵(activation map)을 저장한다. 그 후 특징맵만 가지고 이미지를 재구성한다. 해당 레이어의 특징 벡터로부터 이미지를 재구성해보면, 이미지의 어떤 정보가 특징 벡터에서 포착되는지를 알 수 있다. 이 방법에서 또한 regularizaer를 추가한 gradient ascent를 이용한다. score를 최대화시키는 것 대신 특징 벡터간의 거리를 최소화하는 것이다.기존에 계산해놓은 특징 벡터와 새롭게 생성한 이미지로 계산한 특징벡터 간의 거리를 측정한다. 여기서는 regularizer을 total vatication을 사용한다. total variation regularizer은 상하좌우 인접 픽셀 간의 차이에 대한 패널티를 부여한다. 이는 생성된 이미지가 자연스러운 이미지가 되도록 해준다.feature inversion을 통한 시각화 예제는 위와 같다. 왼쪽이 원본 이미지다. 이 이미지를 VGG-16에 통과시킨다. 특징맵을 기록하고, 기록된 특징맵과 부합하도록 하는 새로운 이미지를 합성한다.다양한 layer를 이용해 합성한 이미지를통해 얼마나 많은 정보들이 저장되어 있는지 짐작할 수 있다.relu2_2를 보면 기존의 이미지 정보가 거의 담겨있다. relu4_3이나 relu5_1과 같이 더 깊은 곳을 가보면 이미지의 공간적 구조는 잘 유지되어 있으나 디테일이 많이 날아간 것을 볼 수 있다. 네트워크가 깊어질수록 feature를 잃어간다.이제까지 style transfer을 배우기 위한 준비단계를 배웠다. style transfer과 feature inversion외에도 texture 합성과 관련된 문제도 살펴보고자 한다.Texture Synthesis여기 보이는 비늘 무늬와 같은 입력 패턴(texture) patch가 있을 때 동일한 패턴(texture)의 더 큰 patch를 생성한다.nearest neighbor를 통한 패턴 합성(texture synthesis)도 좋은 방법이다. 이 방법에는 신경망을 사용하지 않고, scan line을 따라서 한 픽셀씩 이미지를 생성해낸다. 현재 생성해야 할 픽셀 주변에 이미 생성된 픽셀을 살핀 후, 입력 패치에서 가장 가까운 픽셀을 계산하여 입력 패치로부터 한 픽셀을 복사해 넣는 방식이다.기본적으로 패턴 합성(texture synthesis)은 신경망 없이도 할 수 있다. 하지만 좀 복잡한 패턴에서는 상황이 다르다.단순하게 입력 패치를 복사하는 방식은 잘 동작하지 않는다. 신경망을 활용해서 패턴 합성하는 방법은 앞서 살펴본 특징맵을 이용한 gradient ascent 방법과 상당히 유사하다.Neural Texture Synthesisnerual texture synthesis 를 구현하기 위해 gram matrix라는 개념을 이용한다.위의 사진과 같이 자갈 사진이 있다고 했을 때, 이 사진을 네트워크에 통과시킨 후 네트워크의 특정 layer에서 특징맵(activation map)을 가져온다. 이 가져온 특징 맵의 크기는 C x H x W 다. H x W 그리드는 공간 정보를 가지고 있고, C는 차원을 의미한다. HxW의 크기를 가진 C차원의 특징 벡터는 해당 지점에 존재하는 이미지의 특징을 담고 있다. 이 특징맵을 통해 입력 이미지의 텍스트특징맵에서 서로 다른 두 개의 특징 벡터(빨강, 파랑)를 뽑아낸다. 각 특징 열 벡터는 C차원 벡터다. 이 두 벡터를 외적해서 C x C 행렬을 만든다. 이 C x C 행렬의 (i,j) 번째 요소의 값이 크다는 것은 두 입력 벡터의 i번째, j번째 요소 모두 크다는 것을 의미한다.이를 통해 서로 다른 공간에서 동시에 활성화되는 특징이 무엇인지 포착해낼 수 있다. 이 과정을 H x W 그리드에 전부 수행해서 평균을 계산해서 C x C gram matrix를 얻는다. 이 결과를 입력 이미지의 텍스처를 기술하는 텍스처 기술자로 사용한다.gram matrix의 흥미로운 점은 공간 정보는 모두 날려버린다는 것이다. 이미지의 각 지점에 해당하는 값들을 모두 평균화하기 때문이다. 공간 정보를 날린 대신 특징들 간의 동시 발생을 볼 수 있다.때문에 gram matrix는 texture 기술자(descriptor)로 적합하다. 계산은 C x H x W 차원의 3차원 텍서를 C x (HW)로 바꾼 후 한번에 계산하면 된다.gram matrix 대신 공분산 행렬을 사용해도 되지만 공분산 행렬은 계산하는 비용이 너무 크다. 따라서 패턴 합성에서는 gram matrix를 더 많이 쓴다.이미지를 직접 생성해보도록 하자. 이는 gradient ascent procedure와 유사한 과정을 거친다.입력 이미지의 특징맵 전체를 재구성하기보다 gram matrix를 재구성하도록 한다.이 때, 많은 사람들이 pretrained model로 VGG를 사용한다. 이미지를 VGG에 통과시키고 다양한 layer에서 gram matrix를 계산한다. 그리고 생성해야 할 이미지를 랜덤으로 초기화시키고, 그 다음 과정부터는 gradient ascent와 유사하다.다시 이미지를 VGG에 통과시키고, 여러 layer에서 gram matrix를 계산하고, 원본 이미지와 생성된 이미지의 gram matrix간의 차이를 L2 norm을 이용해 loss를 계산한다. 그 후 loss를 backprop를 통해 생성된 이미지의 픽셀에 대한 gradient를 계산한다. gradient ascent를 통해 이미지의 픽셀을 조금씩 업데이트한다. 이 단계를 여러번 반복한다.이 과정을 거치면 결국 입력 texture과 유사한 texture를 만들 수 있다.맨 위 네 가지의 이미지가 있고, 맨 아래 gram matrix를 이용한 패턴 합성을 볼 수 있다. 얕은 layer에서의 결과를 보면 색상은 유지되지만 공간적 구조는 잘 살리지 못한다. layer가 깊어질수록 이미지의 공간적 구조를 잘 구현한 것을 볼 수 있다.보통 loss를 어느 layer에서 계산하는가 하면, 일반적으로 gram matrix를 다양한 layer에서 계산하고 가중 합을 통해 최종 loss를 구한다.이 gram matrix를 예술 작품에 적용해보자생성된 이미지를 보면 예술작품의 아주 흥미로운 부분들을 재구성해내는 경향을 볼 수 있다.texture synthesis와 feature inversion을 조합하면 더 흥미로운 일이 발생한다. 이 아이디어가 바로 style transfer이다.Neural Style Transferstyle transfer에서는 입력이 두가지다. content image는 네트워크에게 우리의 최종 이미지가 어떻게 생겼으면 좋겠는지를 알려준다. style image는 최종 이미지의 패턴이 어떻게 생겼는지 알려준다. 최종 이미지는 content image의 특징 재구성(feature reconstruction) loss도 최소화하고, style image의 gram matrix loss도 최소화하는 방식으로 최적화하여 생성한다.이 두가지 loss를 동시에 활용하면 style image스러운 content image가 생성된다.네트워크에 content와 style 이미지를 네트워크에 통과시키고 gram matrix와 feature map을 계산한다. 최종 출력 이미지는 랜덤 노이즈로 초기화시킨다. forward/backward를 반복하여 계산하고 gradient ascent를 이용해서 이미지를 업데이트한다.수 백번 정도 반복하면 좋은 결과를 얻을 수 있다. 코드style transfer은 deepdream에 비해 이미지를 생성할 때 컨트롤할 만한 것들이 더 많다. deepdream의 경우 네트워크를 반복하면 특정 개체가 이미지 전체에 퍼질 뿐이다. style transfer의 경우 style image를 다양하게 할 수도 있고, 하이퍼파라미터도 자유롭게 조정가능하다. loss의 가중치를 조절하면 어디에 더 집중해서 만들지를 조절할 수 있다. 또 다른 하이퍼파라미터로 style image를 resize해서 넣어주면 다른 이미지의 결과를 도출해낼 수도 있다.또 여러 style image를 가지고 style transfer할 수도 있다. 동시에 여러 style loss의 gram matrix를 계산하는 것이다.또, style transfer 과 deepdream을 조합해볼 수도 있다. content loss + style loss + deepdream loss를 조합한다. 위의 사진은 개 달팽이가 사방에 퍼져있는 van gogh 그림이다.Problem of Neural Style Transferstyle stransfer 알고리즘의 단점은 아주 느리다는 것이다. 이런 이미지를 만들기 위해서 backward/forward를 아주 많이 반복해야 한다. 고해상도 이미지를 만드는 것은 계산량이 엄청나기 때문에 엄청난 메모리와 계산량을 필요로 한다.Fast Style Transfer위의 방법에 대한 해결책이라 하면 style transfer을 위한 또 다른 네트워크를 학습시키는 것이다.이 방법은 합성하고자 하는 이미지의 최적화를 전부 수행하는 것이 아니라 content image만을 입력으로 받아서 결과를 출력할 수 있는 단일 네트워크를 학습시키는 방법이다.이 네트워크의 학습 시에는 content/style loss를 동시에 학습시키고 네트워크의 가중치를 업데이트한다. 학습은 오래걸리지만 한 번 학습시키고 나면 이미지를 네트워크에 통과시키면 결과가 바로 나올 수 있다.이 네트워크가 효율적이어서 좋은 GPU로 하면 네 가지 스타일을 동시에 돌려볼 수 있다.이 네트워크는 segmentation 네트워크와도 비슷하다. semantic segmentation에서는 다운 샘플링을 여러 번 하고 transposed conv로 업샘플링한다. semantic segmentation과 다른 점은 출력이 RGB라는 것이고, 네트워크 중간에 batch norm이 들어간다.위의 네트워크들은 네트워크 하나당 하나의 style transfer 밖에 할 수 없다. 하지만 최근 논문에 의하면 하나의 네트워크로 다양한 style을 생성해낼 수 있는 방법이 있다고 한다. 이 네트워크는 실시간으로 동작할 수 있다. 서로 다른 style을 학습시키면 이 네가지 style을 섞을수도 있다." }, { "title": "random seed 설정", "url": "/posts/randomseed/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, random seed", "date": "2021-10-28 13:00:00 +0900", "snippet": "Random Seedseed() 괄호 안에 들어가는 숫자는 무슨 의미일까?seed value 숫자 자체는 중요하지 않고 서로 다른 시드를 사용하면 서로 다른 난수를 생성한다는 점만 알면 된다.random seed를 고정한다는 말은 동일한 셋트의 난수를 생성할 수 있게 하는 것이다.np.random.seed(0)np.random.rand(5)pytorch random seed딥러닝에서 초기화할때 random num을 사용한다. 이 때, 실험할 때마다 다른 값이 되면, 다른 결과값이 나타날 수 있다.그렇기 때문에 실험을 동일하게 진행하기 위해서는 동일한 난수의 사용이 필요하다.pytorch에서는 random seed를 고정하기 위해 manual_seed 함수를 사용한다.benchmark는 input size가 변하지 않을 때 사용하면 더 빠른 런타임을 사용할 수 있다. 하지만 그렇지 않을 경우 False로 설정하는 것이 좋다.import torchtorch.manual_seed(1)torch.rand(5)seed = 42os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)random.seed(seed)np.random.seed(seed)torch.manual_seed(seed)torch.cuda.manual_seed(seed)torch.cuda.manual_seed_all(seed)torch.backends.cudnn.benchmark = FalseReference " }, { "title": "overfitting 방지(prevent)하는 방법", "url": "/posts/overfitting/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, overfitting", "date": "2021-10-28 13:00:00 +0900", "snippet": "overfitting모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않는다. 이는 모델이 학습 데이터를 불필요할 정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있다.overfitting을 막는 방법에는 데이터 양 늘리기데이터 양이 적을 경우 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하므로 과적합이 잘 발생한다. 데이터가 적을 경우 data augmentation을 통해 양을 늘릴 수 있다. 모델 복잡도 줄이기은닉층의 수나 매개변수의 수 등을 줄이는 것이다. 가중치 규제(regularization) 적용하기복잡한 모델을 좀 더 간단하게 하는 방법으로는 가중치 규제(regularization)이 있다.Regularizationweight_decay를 설정하는 것이 regularization을 설정하는 것이다.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)weightedrandomsamplersampler = torch.utils.data.sampler.WeightedRandomSampler(weights, batch_size)Reference https://wikidocs.net/61374 https://ichi.pro/ko/pytorch-basics-saempeulling-saempeulleo-46244616519466" }, { "title": "pytorch Ensemble", "url": "/posts/ensemble/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, ensemble", "date": "2021-10-28 13:00:00 +0900", "snippet": "Ensemble 앙상블 학습이란?앙상블 학습(ensemble learning)은 여러 개의 분류기를 생성하고, 그 예측을 결합하는 것이다.강력한 하나의 모델을 사용하는 대신 약한 모델 여러 개를 조합하여 정확한 예측할 수 있다.import torch.nn as nnimport torchclass MyEnsemble(nn.Module): def __init__(self, modelA, modelB, modelC, output): super(MyEnsemble, self).__init__() self.modelA = modelA self.modelB = modelB self.modelC = modelC self.fc1 = nn.Linear(num_classes, output) def forward(self,x): out1 = self.modelA(x) out2 = self.modelB(x) out3 = self.modelC(x) out = out1 + out2 + out3 x = torch.softmax(x, dim=1) return xdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)import torchvision.models as modelsdensenet161 = models.densenet161(pretrained=True).to(device)resnet101 = models.resnet101(pretrained=True).to(device)googlenet = models.googlenet(pretrained=True).to(device)densenet161.classifier = nn.Linear(in_features=densenet161.classifier.in_features, out_features=num_classes)resnet101.fc = nn.Linear(in_features=resnet101.fc.in_features, out_features=num_classes)googlenet.fc = nn.Linear(in_features=googlenet.fc.in_features, out_features=num_classes)densenet161 = densenet161.to(device)resnet101 = resnet101.to(device)googlenet = googlenet.to(device)densenet161, history, best_epoch = train_and_val(densenet161, criterion, optimizer, dataloader[&#39;train&#39;], dataloader[&#39;valid&#39;], 2)densenet = [densenet161, history, best_epoch]resnet101, history, best_epoch = train_and_val(resnet101, criterion, optimizer, dataloader[&#39;train&#39;], dataloader[&#39;valid&#39;], 2)resnet = [resnet101, history, best_epoch]googlenet, history, best_epoch = train_and_val(googlenet, criterion, optimizer, dataloader[&#39;train&#39;], dataloader[&#39;valid&#39;], 2)googlenet = [googlenet, history, best_epoch]ensemble = MyEnsemble(densenet[1], resnet[1], googlenet[1], 10)Reference http://www.dinnopartners.com/__trashed-4/" }, { "title": "Custom augmentation class 정의해보기", "url": "/posts/customaugmentation/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, custom augmentation", "date": "2021-10-28 13:00:00 +0900", "snippet": "Data Augmentationdata augmentation 방법은 크게 “기하학 변환”과 “태스크 기반” 확장으로 나뉜다.기하적 변환은 크기조절, 반전, 자르기, 회전, 이동, 윈도우 분할 등이 있다.태스크 기반 확장은 데이터를 합성 시 분류에 대한 척도를 고려하여 학습을 수행한다. 물체 인식에 대해 자르기(crop)와 반전(filp), 자르기와 색 조절은 학습 데이터 확장에, 크기 변환과 다각도관점 변환 등이 있다.이것들의 함수를 직접 정의해보고자 한다.기본 image와 label 지정하기import wgetimport torchfrom PIL import Imageimport numpy as npimport matplotlib.image as imgimport matplotlib.pyplot as pltimport torchvision.transforms.functional as TF!wget https://i.stack.imgur.com/LW5Fv.png -O dog.jpgimage = Image.open(&quot;dog.jpg&quot;)image = np.array(image)xmin, xmax = (1,2)red_color = (255,0,0)label = (1000,450)drow = cv2.rectangle(image, (label[0]-550,label[1]-400),(label[0],label[1]),red_color,3)image = Image.from_array(image)plt.imshow(drow)plt.show()1. Resize이미지를 resize 해줌과 동시에 label도 갱신해줘야 한다.def resize_img_label(image, label=(0.,0.), target_size = (256,256)): w_orig, h_orig = image.size w_target, h_target = target_size cx, cy = label image_new = TF.resize(image, target_size) label_new = cx/w_orig*w_target, cy/h_orig*h_target return image_new, label_newimage_new, label_new = resize_img_label(image,label)plt.imshow(image_new)plt.show()2. horizontally Flipdef random_hflip(image, label): w, h = image.size x, y = label image = TF.hflip(image) label = w-x, y return image, labelimage_new, label_new = random_hflip(image,label)plt.imshow(image_new)plt.show()3. Vertically Flipdef random_vflip(image, label): w, h = image.size x, y = label image = TF.vflip(image) label = x, w-y return image, labelimage_new, label_new = random_vflip(image,label)plt.imshow(image_new)plt.show()4. Shift or translate imagedef random_shift(image, label, max_translate=(0.2,0.2)): w,h = image.size max_t_w, max_t_h = max_translate cx, cy = label trans_coef = np.random.rand() * 2 - 1 w_t = int(trans_coef * max_t_w * w) h_t = int(trans_coef * max_t_h * h) image = TF.affine(image, translate=(w_t, h_t), shear=0, angle=0, scale=1) label = cx + w_t, cy + h_t return image, labelimage_new, label_new = random_shift(image,label)plt.imshow(image_new)plt.show() Reference https://deep-learning-study.tistory.com/494" }, { "title": "KFold Cross Validation 과 StratifiedKFold", "url": "/posts/Kfold/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, kfold, stratifiedkfold", "date": "2021-10-28 13:00:00 +0900", "snippet": "KFold Cross Validation교차검증이란 훈련 시 과적합을 막기 위해 사용하는 것이다. 교차검증은 훈련 데이터 셋을 학습 데이터와 검증 데이터 셋으로 분할하고, 훈련 데이터와 검증 데이터 셋을 교차하면서 검증한다.학습셋과 검증셋을 나눠 반복해서 검증한다. k만큼의 폴드 셋으로 나누어 k번 반복한다. 그렇게 되면 최종 평가는 k만큼의 길이를 갖게 된다. 이를 평균 내어 평가한다.평균을 구하는 방법으로는 모든 fold에 대해 epoch의 평균 절대 오차인 MAE(mean absolute error)의 오차 평균을 구한다. Validation MAE(y축), epochs(x축)을 갖는 그래프를 그려보고 그래프의 흐름을 보면 어디서 과대적합이 일어났는지 체크할 수 있다.딥러닝에서 kfold를 사용하게 될 경우 데이터 검증 셋의 분할에 따른 검증 결과를 보고 어떤 데이터 셋이 과적합을 일으키는지 볼 수 있다. 이를 통해 과적합을 막을 수 있다. 검증 점수를 보고 epoch을 어떻게 구성해야 할지 알 수 있다. kfold를 쓰면 어느 지점에서 학습이 덜 되는지 알 수 있어 파라미터 값을 조절할 수 있다.사용 하는 모듈 종류1. Sklearn.model_selection.KFold일단 k개의 fold로 나누고 index를 반환해보자.from sklearn.model_selection import KFoldimport numpy as npx = np.arange(16).reshape((8,-1))y = np.arange(8).reshape((-1,1))kf = KFold(n_splits = 4, shuffle = False)for train_index, test_index in kf.split(x): print(&quot;train: &quot;, train_index, &quot;test: &quot;, test_index) x_train, x_test = x[train_index], x[test_index] y_train, y_test = y[train_index], y[test_index]2. sklearn.model_selection.StratifiedKFold데이터가 편향되어 있을 경우 단순 Kfold 교차검증을 사용하면 성능 평가가 좋게 나오지 않을 수 있다. 이럴 때 stratifiedkfold를 사용한다.stratifiedKFold 함수는 매개변수로 n_splits, shuffle, random_state를 가진다. n_splits는 몇개로 분할할지를 정하는 매개변수이고, shuffle의 기본값 false 대신 true를 넣으면 fold를 나누기 전에 무작위로 섞는다. 그 후, cross_val_score 함수의 cv매개변수에 넣으면 된다. 일반적으로 회귀에는 kfold cross validation을 사용하고, 분류에는 stratifiedkfold를 사용한다.또, cross_val_score 함수는 kfold의 매개변수를 제어할 수 없으므로 따로 kfold 객체를 만들고 매개변수를 조정한 다음 cross_val_score의 cv매개변수에 넣어야 한다.y가 0,0,1,2,1,0,0,0,0,1,2,2 이다. 숫자를 어떤 label이라 생각하고 분포를 살펴보면0이 6, 1이 3, 2가 3, 즉 2:1:1 분포를 가지고 있다. 그렇다면 fold들도 각각 label0, label1, label2를 2:1:1로 가져야 한다.위의 이미지와 같이 label의 분포가 모두 같음을 볼 수 있다.from sklearn.model_selection import StratifiedKFoldx = np.arange(12*2).reshape((12,-1))y = np.array([0,0,1,2,1,0,0,0,0,1,2,2])skf = StratifiedKFold(n_splits=3)for train, test in skf.split(x, y): print(train, test)3. Cross_val_scoreCross_val_score 함수는 교차 검증을 더 편리하게 해주는 것이다. kfold과정을 한꺼번에 수행한다.args로는 cross_val_score(estimator,x,y,scoring,cv,n_jobs=1, verbose=0,fit_params,pre_dipatch) x: image 데이터셋 y: label 데이터셋 scoring: 예측 성능 평가 지표 cv: 교차 검증 폴드 수수행 후 반환 값은 scoring 측정값을 배열 형태로 한다.from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import cross_val_score, cross_validatefrom sklearn.datasets import load_irisimport numpy as npiris = load_iris()dt = DecisionTreeClassifier(random_state = 156)features = iris.datalabel = iris.targetscores = cross_val_score(dt, features, label, scoring=&#39;accuracy&#39;, cv=3)print(&quot;\\n교차 검증별 정확도: &quot;, np.round(scores, 4))print(&quot;평균 검증 정확도: &quot;, np.round(np.mean(scores),4))iris 데이터를 통한 kfoldfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import KFoldimport numpy as npiris = load_iris()data = iris.datalabel = iris.targetdt= DecisionTreeClassifier()# 5개의 Fold로 분리하고 세트별 정확도를 담을 리스트 객체 생성kfold = KFold(n_splits = 5)cv_accuracy = []n_iter = 0# split()를 호출하면 fold별 학습용, 검증용 테스트의 행 인덱스를 array로 반환for train_index, test_index in kfold.split(data): x_train, x_test = data[train_index], data[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt.fit(x_train, y_train) pred = dt.predict(x_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred),4) print(n_iter,&quot;번쨰: &quot;, test_index,&quot;\\n&quot;) cv_accuracy.append(accuracy)print(&quot;평균 검증 정확도: &quot;, np.mean(cv_accuracy))stratified kfold불균형한 분포도를 가진 레이블 데이터 집합을 위한 kfold 방식레이블이 1:100000의 비율을 가진 데이터라면 학습/테스트 셋에도 1:100000의 비율로 들어가 있어 제대로 예측하기가 힘들다. 이때 사용하는 것이 stratified kfold이다. 레이블의 분포를 균등하게 해주고 kfold를 진행한다.그러나, iris 데이터에서는 레이블 value가 모두 동일하여 비교가 되지 않는다.import pandas as pdfrom sklearn.datasets import load_irisfrom sklearn.model_selection import KFoldfrom sklearn.model_selection import StratifiedKFoldiris = load_iris()data = iris.datalabel = iris.targetdt= DecisionTreeClassifier(random_state = 156)df = pd.DataFrame(data=iris.data, columns = iris.feature_names)df[&#39;label&#39;]=iris.targetdf[&#39;label&#39;].value_counts()skfold = StratifiedKFold(n_splits=3)n_iter = 0cv_accuracy = []for train_index, test_index in skfold.split(df, df[&#39;label&#39;]): n_iter += 1 label_train = df[&#39;label&#39;].iloc[train_index] label_test = df[&#39;label&#39;].iloc[test_index] print(&quot;교차검증: &quot;,n_iter) print(&quot;학습 레이블 분포도: \\n&quot;, label_train.value_counts()) print(&quot;검증 레이블 분포도: \\n&quot;, label_test.value_counts())from sklearn.model_selection import StratifiedKFoldiris = load_iris()data = iris.datalabel = iris.targetdt= DecisionTreeClassifier(random_state = 156)skfold = StratifiedKFold(n_splits=3)n_iter = 0cv_accuracy = []for train_index, test_index in skfold.split(data,label): x_train, x_test = data[train_index], data[test_index] y_train, y_test = label[train_index], label[test_index] dt.fit(x_train, y_train) pred = dt.predict(x_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred),4) train_size = x_train.shape[0] test_size = x_test.shape[0] print(&quot;\\n교차검증 정확도: {} 학습데이터 크기: {} 검증 데이터 크기: {}&quot;.format(accuracy, train_size, test_size)) print(&#39;검증 셋 인덱스: &#39;,test_index) cv_accuracy.append(accuracy)print(&quot;\\n교차 검증별 정확도: &quot;, np.round(cv_accuracy,4))print(&quot;평균 검증 정확도: &quot;, np.mean(cv_accuracy))image data에 kfold 적용해보기&#39;&#39;&#39; 기본 구조 &#39;&#39;&#39;stfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state=800)for idx, (train_idx, valid_idx) in enumerate(stfold.split(train_data ,train_data[&quot;digit&quot;])): train_data = total_data.iloc[train_idx] valid_data = total_data.iloc[valid_idx] train_dataset = dataset(train_data, tranforms[&quot;train&quot;]) valid_dataset = dataset(valid_data, tranforms[&quot;val&quot;]) train_dataloader = DataLoader(train_dataset, batch_size = bs, shuffle=True, drop_last = False) valid_dataloader = DataLoader(valid_dataset, batch_size = bs, shuffle=False, drop_last = False) for epoch in range(epoch+1): model.train() model.eval()이 때의 val 비율은 k가 5이므로 4:1, 즉 20%caltech data&#39;경로, label을 담은 csv 파일 만들기&#39;cal_dir = &#39;/content/drive/MyDrive/data/caltech_10&#39;train_dir = cal_dir + &#39;/train&#39;test_dir = cal_dir + &#39;/test&#39;classes = []image_label = []cn = 0train_folders = glob(train_dir + &#39;/*&#39;)for train_folder in train_folders: image_paths = glob(train_folder + &#39;/*&#39;) for image_path in image_paths: image = image_path.split(&#39;/&#39;)[-2:] image = image[0]+&quot;/&quot;+image[1] image_label.append([image, cn]) cn += 1 train_folder = train_folder.split(&#39;/&#39;)[-1] classes.append(train_folder)image_label = pd.DataFrame(image_label, columns=[&#39;label&#39;, &#39;class_number&#39;])image_label.to_csv(&#39;/content/drive/MyDrive/data/caltech_10/train/train_label.csv&#39;, index=False)pd.read_csv(&quot;/content/drive/MyDrive/data/caltech_10/train/train_label.csv&quot;)val_dir = cal_dir + &#39;/valid&#39;cn = 0classes = []image_label = []val_folders = glob(val_dir + &#39;/*&#39;)for val_folder in val_folders: image_paths = glob(val_folder + &#39;/*&#39;) for image_path in image_paths: image = image_path.split(&#39;/&#39;)[-2:] image = image[0]+&quot;/&quot;+image[1] image_label.append([image, cn]) cn += 1 val_folder = val_folder.split(&#39;/&#39;)[-1] classes.append(val_folder)image_label = pd.DataFrame(image_label, columns=[&#39;label&#39;, &#39;class_number&#39;])image_label.to_csv(&#39;/content/drive/MyDrive/data/caltech_10/valid/valid_label.csv&#39;, index=False)pd.read_csv(&quot;/content/drive/MyDrive/data/caltech_10/valid/valid_label.csv&quot;)CustomDataset&#39;&#39;&#39; caltech_10 data &#39;&#39;&#39;import torch, torchvisionfrom torchvision import datasets, models, transformsimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoaderimport timefrom torchsummary import summaryimport numpy as npimport matplotlib.pyplot as pltimport osfrom glob import globimport pandas as pdfrom PIL import Imageimport randomfrom tqdm.notebook import tqdmimport warningswarnings.filterwarnings(&#39;ignore&#39;)from PIL import Imagefrom torch.utils.data import Dataset, DataLoaderfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import KFoldfrom sklearn.model_selection import StratifiedKFolddevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)seed = 42os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)random.seed(seed)np.random.seed(seed)torch.manual_seed(seed)torch.cuda.manual_seed(seed)torch.cuda.manual_seed_all(seed)torch.backends.cudnn.benchmark = Falsecal_dir = &#39;/content/drive/MyDrive/data/caltech_10&#39;train_dir = cal_dir + &#39;/train&#39;valid_dir = cal_dir + &#39;/valid&#39;test_dir = cal_dir + &#39;/test&#39;train_csv = train_dir + &#39;/train_label.csv&#39;valid_csv = valid_dir + &#39;/valid_label.csv&#39;batch_size = 32epochs = 20num_classes = len(os.listdir(train_dir))transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 지금은 채널이 3이라 3개를 적지만, gray scale일 경우 ((0.5),(0.5)) 형태로 작성해야 함 ]), &#39;valid&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), &#39;test&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])}class CustomImageDataset(Dataset): def __init__(self, csv_file, root_dir, transform = None): self.label_dir = pd.read_csv(csv_file) self.img_dir = root_dir self.transform = transform def __len__(self): return len(self.label_dir) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.label_dir.iloc[idx,0]) image = Image.open(img_path).convert(&#39;RGB&#39;) label = self.label_dir.iloc[idx,1] if self.transform: image = self.transform(image) return image, label dataset = { &#39;train&#39;: CustomImageDataset(train_csv, train_dir,transforms[&#39;train&#39;]), &#39;valid&#39;: CustomImageDataset(valid_csv, valid_dir,transforms[&#39;valid&#39;])}dataloader = { &#39;train&#39;: DataLoader(dataset[&#39;train&#39;], batch_size = batch_size, shuffle = True), &#39;valid&#39;: DataLoader(dataset[&#39;valid&#39;], batch_size = batch_size, shuffle = False)}for x_train, y_train in dataloader[&#39;train&#39;]: print(&quot;x_train {} \\ny_train {}\\n\\n&quot;.format(x_train.size(), y_train.size())) breakimport torchvision.models as modelsmodel = models.densenet121(pretrained = True).to(device) num_ftrs = model.classifier.in_features model.fc = nn.Linear(num_ftrs, num_classes) model = model.to(device)criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.0001)&#39;&#39;&#39; train_valid 함수 정의&#39;&#39;&#39;def train_and_val(model, criterion, optimizer, train_data, valid_data, epochs=25): best_loss = 5 best_epoch = None history = [] for epoch in tqdm(range(epochs)): epoch_start = time.time() model.train() train_loss = 0.0 train_acc = 0.0 valid_loss = 0.0 valid_acc = 0.0 correct = 0 for i, (images, labels) in enumerate(train_data): images = images.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_loss += loss.item() predictions = outputs.max(1, keepdim = True)[1] correct += predictions.eq(labels.data.view_as(predictions)).sum().item() train_loss /= (len(train_data.dataset) / batch_size) train_acc = 100. * correct / len(train_data.dataset) correct = 0 with torch.no_grad(): model.eval() for i, (images, labels) in enumerate(valid_data): images = images.to(device) labels = labels.to(device) outputs = model(images) valid_loss += criterion(outputs, labels).item() predictions = outputs.max(1, keepdim = True)[1] correct += predictions.eq(labels.data.view_as(predictions)).sum().item() valid_loss /= (len(valid_data.dataset) / batch_size) valid_acc = 100. * correct / len(valid_data.dataset) if valid_loss &amp;lt; best_loss: best_loss = valid_loss best_epoch = epoch print(&quot;best_loss: {:.4f} \\n best_epoch: {}&quot;.format(best_loss, best_epoch)) history.append([train_loss,valid_loss, train_acc,valid_acc]) epoch_end = time.time() print(&quot;Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.2f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.2f}%, Time: {:.4f}s&quot;.format(epoch, train_loss, train_acc, valid_loss, valid_acc, epoch_end-epoch_start)) return model, history, best_epochmodel, history, best_epoch = train_and_val(model, criterion, optimizer, dataloader[&#39;train&#39;], dataloader[&#39;valid&#39;], epochs=10)CustomDataset + stratifiedKFoldcustomdataset의 args를 살짝 바꿔야 한다.class CustomDataset(Dataset): def __init__(self, data, transform = None): self.data_dir = data self.transform = transform def __getitem__(self, idx): img_path = os.path.join(train_dir, self.data_dir.iloc[idx,0]) image = Image.open(img_path).convert(&#39;RGB&#39;) label = self.data_dir.iloc[idx,1] if self.transform: image = self.transform(image) return image, label def __len__(self): return len(self.data_dir.iloc[:,1])train_csv_data = pd.read_csv(train_csv)stfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=seed)for idx, (train_idx, valid_idx) in enumerate(stfold.split(train_csv_data, train_csv_data.iloc[:,1])): train_data = train_csv_data.iloc[train_idx] valid_data = train_csv_data.iloc[valid_idx] stf_dataset = { &#39;train&#39;: CustomDataset(train_data, transforms[&#39;train&#39;]), &#39;valid&#39;: CustomDataset(valid_data, transforms[&#39;valid&#39;]) } stf_dataloader = { &#39;train&#39;: DataLoader(stf_dataset[&#39;train&#39;], batch_size = batch_size, shuffle = True), &#39;valid&#39;: DataLoader(stf_dataset[&#39;valid&#39;], batch_size = batch_size, shuffle = False) } #print(&quot;학습 레이블 분포도: \\n&quot;, train_data.iloc[:,1].value_counts()) #print(&quot;검증 레이블 분포도: \\n&quot;, valid_data.iloc[:,1].value_counts(),&quot;\\n\\n&quot;) model, history, best_epoch = train_and_val(model, criterion, optimizer, stf_dataloader[&#39;train&#39;], stf_dataloader[&#39;valid&#39;], epochs=5)stratifiedKFold를 적용할 경우 98~99% 정도의 valid acc를 볼 수 있다.Reference " }, { "title": "Torch Save and Load, data를 plot해보기", "url": "/posts/torchsaveload/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, Torch save, plot", "date": "2021-10-24 13:00:00 +0900", "snippet": "기본 구조기본 구조는 다음과 같다. 자세한 내용은 참고 사이트를 참고하길 바란다.Save기본적으로 모델을 저장하는 일반적인 방법은 내부 상태 사전(internal state dictionary)를 직렬화(serialize)하는 것이다.torch.save(model.state_dic(), &quot;model.pth&quot;)print(&quot;save pytorch model state to model.pth&quot;)Load모델을 불러오는 과정은 모델 구조를 다시 만들고 상태 사전을 모델에 불러와야 한다.model = NeuralNetwork()model.load_state_dict(torch.load(&quot;model.pth&quot;))이 모델을 통해 예측을 진행한다.model.eval()x, y = test_data[0],test_data[0]with torch.no_grad(): pred = model(x) predicted, actual = classes[pred[0].argmax(0), classes[y]] print(&quot;predicted: {}, actual: {}&quot;.format(predicted, actual))실제 코드에 적용해보기image classification에서의 best loss와 best accuracy에 대한 model을 저장하고 평가하고자 한다.#%env CUDA_VISIBLE_DEVICES=2import torch, torchvisionfrom torchvision import datasets, models, transformsimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoaderimport timefrom torchsummary import summaryimport numpy as npimport matplotlib.pyplot as pltimport osfrom PIL import Imagefrom tqdm.notebook import tqdmimport warningswarnings.filterwarnings(&#39;ignore&#39;)# Applying Transforms to the Data - normalizationimage_transforms = { &#39;train&#39;: transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), # 랜덤 크기로 자른 후 잘라낸 이미지의 크기는 256x256로 조정 transforms.RandomRotation(degrees=15), # 영상을 -15~15도 범위에서 랜덤 각도로 회전 transforms.RandomHorizontalFlip(), # 50% 확률로 이미지를 수평으로 랜덤하게 뒤집는다. transforms.CenterCrop(size=224), # 중앙에서 224x224 이미지를 자른다. transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 3채널 tensor을 취하며 각 채널의 입력 평균과 표준 편차로 각 채널을 정규화 ]), &#39;valid&#39;: transforms.Compose([ # 검증 및 테스트 데이터의 경우 resizedcrop과 rotalrotaion, horizontalfilp을 수행하지 않는다. transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), &#39;test&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])}device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# Load the Data# Set train and valid directory pathsdataset = &#39;/content/drive/MyDrive/data/caltech_10/&#39;train_directory = os.path.join(dataset, &#39;train&#39;)valid_directory = os.path.join(dataset, &#39;valid&#39;)# Batch sizeBATCH_SIZE = 32# Number of classesnum_classes = len(os.listdir(valid_directory)) #10#2#257 bear, chimp...print(&#39;num_class :&#39;, num_classes)# Load Data from foldersdata = { &#39;train&#39;: datasets.ImageFolder(root=train_directory, transform=image_transforms[&#39;train&#39;]), &#39;valid&#39;: datasets.ImageFolder(root=valid_directory, transform=image_transforms[&#39;valid&#39;]))}# Get a mapping of the indices to the class names, in order to see the output classes of the test images.idx_to_class = {v: k for k, v in data[&#39;train&#39;].class_to_idx.items()} # class_to_idx 는 데이터 셋의 클래스 매핑 레이블을 반환print(idx_to_class)# Size of Data, to be used for calculating Average Loss and Accuracytrain_data_size = len(data[&#39;train&#39;])valid_data_size = len(data[&#39;valid&#39;])print(train_data_size, valid_data_size)# Create iterators for the Data loaded using DataLoader moduletrain_data_loader = DataLoader(data[&#39;train&#39;], batch_size=BATCH_SIZE, shuffle=True)valid_data_loader = DataLoader(data[&#39;valid&#39;], batch_size=BATCH_SIZE, shuffle=True)def train_and_val(model, criterion, optimizer, epochs=25): best_loss = 5 best_epoch = None history = [] for epoch in tqdm(range(epochs)): epoch_start = time.time() model.train() train_loss = 0.0 train_acc = 0.0 valid_loss = 0.0 valid_acc = 0.0 correct = 0 for i, (images, labels) in enumerate(train_data_loader): images = images.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_loss += loss.item() predictions = outputs.max(1, keepdim = True)[1] correct += predictions.eq(labels.data.view_as(predictions)).sum().item() train_loss /= (len(train_data_loader.dataset) / BATCH_SIZE) train_acc = 100. * correct / len(train_data_loader.dataset) correct = 0 with torch.no_grad(): model.eval() for i, (images, labels) in enumerate(valid_data_loader): images = images.to(device) labels = labels.to(device) outputs = model(images) valid_loss += criterion(outputs, labels).item() predictions = outputs.max(1, keepdim = True)[1] correct += predictions.eq(labels.data.view_as(predictions)).sum().item() valid_loss /= (len(valid_data_loader.dataset) / BATCH_SIZE) valid_acc = 100. * correct / len(valid_data_loader.dataset) if valid_loss &amp;lt; best_loss: best_loss = valid_loss best_epoch = epoch print(&quot;best_loss: {:.4f} \\n best_epoch: {}&quot;.format(best_loss, best_epoch)) history.append([train_loss,valid_loss, train_acc,valid_acc]) epoch_end = time.time() print(&quot;Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.2f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.2f}%, Time: {:.4f}s&quot;.format(epoch, train_loss, train_acc, valid_loss, valid_acc, epoch_end-epoch_start)) return model, history, best_epoch&#39;&#39;&#39; pretrained model &#39;&#39;&#39;import torchvision.models as modelsmodel = models.resnet18(pretrained = False).to(device) num_ftrs = model.fc.in_features model.fc = nn.Linear(num_ftrs, num_classes) model = model.to(device)criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)train_model, history, best_epoch = train_and_val(model, criterion, optimizer)torch.save(model, dataset+&#39;_model_&#39;+str(best_epoch)+&#39;.pt&#39;)가져온 history를 통해 그래프 그리기&#39;&#39;&#39; loss 그래프 &#39;&#39;&#39;history = np.array(history)plt.plot(history[:,0:2])plt.legend([&#39;Tr Loss&#39;, &#39;Val Loss&#39;])plt.xlabel(&#39;Epoch Number&#39;)plt.ylabel(&#39;Loss&#39;)#plt.ylim(0,1)plt.savefig(dataset+&#39;_loss_curve.png&#39;)plt.show()&#39;&#39;&#39; accuracy 그래프 &#39;&#39;&#39;plt.plot(history[:,2:4])plt.legend([&#39;Tr Accuracy&#39;, &#39;Val Accuracy&#39;])plt.xlabel(&#39;Epoch Number&#39;)plt.ylabel(&#39;Accuracy&#39;)#plt.ylim(0,1)plt.savefig(dataset+&#39;_accuracy_curve.png&#39;)plt.show()단일 이미지를 best model로 예측해보기아래는 topk를 사용해보았다. 이는 topk에 대한 결과값을 받을 수 있다. 아래의 경우 k를 3으로 지정하였기에 확률이 가장 높은 3개에 대한 라벨 확률을 확인 할 수 있다.def predict(model, test_image_name): test_image = Image.open(test_image_name) plt.imshow(test_image) transform = image_transforms[&#39;test&#39;] test_image_tensor = transform(test_image) test_image_tensor = test_image_tensor.view(1,3,224,224).cuda() with torch.no_grad(): model.eval() out = model(test_image_tensor) ps = torch.exp(out) topk, topclass = ps.topk(3, dim=1) # argmax와 비슷하게 top-k에 대한 결과 값을 받는다. score = topk.cpu().numpy()[0][0] for i in range(3): print(&quot;Prediction&quot;, i+1, &quot;:&quot;, idx_to_class[topclass.cpu().numpy()[0][i]], &quot;, Score: &quot;, topk.cpu().numpy()[0][i])import wget!wget https://cdn.pixabay.com/photo/2018/10/01/12/28/skunk-3716043_1280.jpg -O skunk.jpgmodel = torch.load(&quot;{}_model_{}.pt&quot;.format(dataset, best_epoch))predict(model, &#39;skunk.jpg&#39;)Reference https://tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html" }, { "title": "TensorBoard 사용해보기", "url": "/posts/TensorBoard/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, TensorBoard", "date": "2021-10-21 13:00:00 +0900", "snippet": " 기본 모델 구조# importsimport matplotlib.pyplot as pltimport numpy as npimport torchimport torchvisionimport torchvision.transforms as transformsimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optim# transformstransform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])# datasetstrainset = torchvision.datasets.FashionMNIST(&#39;./data&#39;, download=True, train=True, transform=transform)testset = torchvision.datasets.FashionMNIST(&#39;./data&#39;, download=True, train=False, transform=transform)# dataloaderstrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)# 분류 결과를 위한 상수classes = (&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle Boot&#39;)class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 4 * 4, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)1. TensorBoard 설정from torch.utils.tensorboard import SummaryWriter# 기본 &#39;log_dir&#39;은 &quot;runs&quot;이며, 여기서는 더 구체적으로 지정한다.writer = SummaryWriter(&#39;runs/fashion_mnist_experiment_1&#39;) # run/fashion_mnist_experiment_1 폴더를 생성한다.2. TensorBoard에 기록이제 TensorBoard에 이미지(구체적으로는 make_grid를 사용하여 그리드)를 나타내보자.# 이미지를 보여주기 위한 함수# (아래 `plot_classes_preds` 함수에서 사용)def matplotlib_imshow(img, one_channel=False): if one_channel: img = img.mean(dim=0) img = img / 2 + 0.5 # unnormalize npimg = img.numpy() if one_channel: plt.imshow(npimg, cmap=&quot;Greys&quot;) else: plt.imshow(np.transpose(npimg, (1, 2, 0)))# 임의의 학습 이미지를 가져온다.dataiter = iter(trainloader) # iter = for 반복과 비슷한 것images, labels = dataiter.next() # 반복가능한 객체의 다음 요소를 반환# 이미지 그리드를 만든다img_grid = torchvision.utils.make_grid(images)# 이미지를 보여준다.matplotlib_imshow(img_grid, one_channel = True)# tensorboard에 기록writer.add_image(&#39;four_fashion_mnist_images&#39;, img_grid)%load_ext tensorboard%tensorboard --logdir=runs3. TensorBoard를 사용하여 모델 살표보기(inspect)복잡한 모델 구조를 시각화하는 기능Net을 더블클릭하여 펼쳐보면 모델을 구성하는 개별 연산들에 대해 자세히 볼 수 있다.tensorboard는 이미지 데이터와 같은 고차원 데이터를 저차원 공간에 시각화하는데 매우 편리한 기능을 제공한다.writer.add_graph(net, images)writer.close()4. TensorBoard에 Projector 추가하기add_embedding 메소드를 통해 고차원 데이터의 저차원 표현을 시각화할 수 있다.import tensorflow as tfimport tensorboard as tbtf.io.gfile = tb.compat.tensorflow_stub.io.gfiledef select_n_random(data, labels, n=100): &#39;&#39;&#39; 데이터셋에서 n개의 임의의 데이터포인트(datapoint)와 그에 해당하는 라벨을 선택합니다 &#39;&#39;&#39; assert len(data) == len(labels) perm = torch.randperm(len(data)) return data[perm][:n], labels[perm][:n]# 임의의 이미지들과 정답(target) 인덱스를 선택합니다images, labels = select_n_random(trainset.data, trainset.targets)# 각 이미지의 분류 라벨(class label)을 가져옵니다class_labels = [classes[lab] for lab in labels]# 임베딩(embedding) 내역을 기록합니다features = images.view(-1, 28 * 28)writer.add_embedding(features, metadata=class_labels, label_img=images.unsqueeze(1))writer.close()이를 통해 projector 탭에서 각각은 784차원인 100개의 이미지가 3차원 공간에 투사된 것을 볼 수 있다.또한, 이것은 대화식으로 클릭하고 드래그하여 3차원으로 투영된 것을 회전할 수 있다.마지막으로 시각화를 더 편히 볼 수 있는 팁으로는 좌측 상단에서 “color by:label”을 선택하고, 야간모드를 활성화하면 이미지 배경이 흰색이 되어 더 편하게 볼 수 있다.4. TensorBoard로 모델 학습 추적데이터를 충분히 살펴보았으므로 이제 학습 과정부터 시작하여 TensorBoard가 어떻게 모델 학습과 평가를 더 명확히 추적할 수 있는지 살펴보고자 한다.이전 예제에서는 단순히 모델 학습 중 손실(running loss)을 2000번 반복할 때마다 출력하기만 했다. 이제는 tensorboard에 학습 중 손실을 기록하는 것 대신에 plot_classes_predcs함수를 통해 모델의 예측 결과를 함께 볼 수 있도록 하겠다.def images_to_probs(net, images): &#39;&#39;&#39; 학습된 신경망과 이미지 목록으로부터 예측 결과 및 확률을 생성합니다 &#39;&#39;&#39; output = net(images) # convert output probabilities to predicted class _, preds_tensor = torch.max(output, 1) preds = np.squeeze(preds_tensor.numpy()) return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]def plot_classes_preds(net, images, labels): &#39;&#39;&#39; 학습된 신경망과 배치로부터 가져온 이미지 / 라벨을 사용하여 matplotlib Figure를 생성합니다. 이는 신경망의 예측 결과 / 확률과 함께 정답을 보여주며, 예측 결과가 맞았는지 여부에 따라 색을 다르게 표시합니다. &quot;images_to_probs&quot; 함수를 사용합니다. &#39;&#39;&#39; preds, probs = images_to_probs(net, images) # 배치에서 이미지를 가져와 예측 결과 / 정답과 함께 표시(plot)합니다 fig = plt.figure(figsize=(12, 48)) for idx in np.arange(4): ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[]) matplotlib_imshow(images[idx], one_channel=True) ax.set_title(&quot;{0}, {1:.1f}%\\n(label: {2})&quot;.format( classes[preds[idx]], probs[idx] * 100.0, classes[labels[idx]]), color=(&quot;green&quot; if preds[idx]==labels[idx].item() else &quot;red&quot;)) return fig마지막으로 이전 튜토리얼과 동일한 모델 학습 코드에서 1000배치마다 콘솔에 출력하는 대신에 tensorboard에 결과를 기록하도록 하여 학습해보자. 이는 add_scalar함수를 사용한다.또한, 학습을 진행하면서 배치에 포함된 4개의 이미지에 대한 모델의 예측 결과와 정답을 비교(versus)하여 보여주는 이미지를 생성하도록 한다.running_loss = 0.0for epoch in range(1): for i, data in enumerate(trainloader): # [inputs, labels]의 목록인 data로부터 입력을 받은 후; inputs, labels = data # 변화도(Gradient) 매개변수를 0으로 만들고 optimizer.zero_grad() # 순전파 + 역전파 + 최적화를 한 후 outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 1000 == 999: # 매 1000 미니배치마다... # ...학습 중 손실(running loss)을 기록하고 writer.add_scalar(&#39;training loss&#39;, running_loss / 1000, epoch * len(trainloader) + i) # ...무작위 미니배치(mini-batch)에 대한 모델의 예측 결과를 보여주도록 # Matplotlib Figure를 기록합니다 writer.add_figure(&#39;predictions vs. actuals&#39;, plot_classes_preds(net, inputs, labels), global_step=epoch * len(trainloader) + i) running_loss = 0.0print(&#39;Finished Training&#39;)이렇게 하면 scalars 탭에서 15000번 반복할 때의 손실을 확인할 수 있다.또한, 학습 과정 전반에 걸쳐 임의의 배치에 대한 모델의 예측 결과를 확인할 수 있다. “images”탭에서 스크롤을 내려 “예측 vs 정답(predictions Vs actuals)” 시각화 부분에서 이 내용을 볼 수 있다.5. TensorBoard로 학습된 모델 평가이전에는 모델이 학습 완료된 후에 각 분류별 정확도를 살펴보았다. 이제느 tensorboard를 사용하여 각 분류별 정밀도-재현율 곡선을 그릴 수 있다.# 1. 예측 확률을 test_size x num_classes 텐서로 가져옵니다# 2. 예측 결과를 test_size 텐서로 가져옵니다# 실행하는데 10초 이하 소요class_probs = []class_label = []with torch.no_grad(): for data in testloader: images, labels = data output = net(images) class_probs_batch = [F.softmax(el, dim=0) for el in output] class_probs.append(class_probs_batch) class_label.append(labels)test_probs = torch.cat([torch.stack(batch) for batch in class_probs])test_label = torch.cat(class_label)# 헬퍼 함수def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0): &#39;&#39;&#39; 0부터 9까지의 &quot;class_index&quot;를 가져온 후 해당 정밀도-재현율(precision-recall) 곡선을 그립니다 &#39;&#39;&#39; tensorboard_truth = test_label == class_index tensorboard_probs = test_probs[:, class_index] writer.add_pr_curve(classes[class_index], tensorboard_truth, tensorboard_probs, global_step=global_step) writer.close()# 모든 정밀도-재현율(precision-recall; pr) 곡선을 그립니다for i in range(len(classes)): add_pr_curve_tensorboard(i, test_probs, test_preds)PR Curves 탭에서 각 분류별 정밀도-재현율 곡선을 볼 수 있다.Reference https://tutorials.pytorch.kr/intermediate/tensorboard_tutorial.html" }, { "title": "Custom Dataset, Dataloader, Transform", "url": "/posts/Dataset/", "categories": "Classlog, Pytorch Tutorial", "tags": "pytorch tutorial, CustomDataset, CustomDataloader, CustomTransforms", "date": "2021-10-21 13:00:00 +0900", "snippet": "1. landmark가 있는 Custom Dataset일반적이지 않은 데이터셋으로부터 데이터를 읽어오고 전처리하는 튜토리얼dataset download url : https://download.pytorch.org/tutorial/faces.zip이 데이터셋은 landmark가 있는 데이터셋이다.Dataset 클래스len(dataset) 에서 호츨되는 len 은 데이터셋의 크기를 리턴해야 한다.dataset[i] 에서 호출되는 getitem 은 i번쨰 샘플을 찾는데 사용된다.__init__을 사용하여 CSV 파일 안에 있는 데이터를 읽지만, __getitem__을 이용해서 이미지를 판독해야 한다. 이 방법은 모든 이미지를 메모리에 저장하지 않고 필요할때마다 읽기 때문에 메모리에 효율적이다.데이터 샘플은 {‘image’: image, ‘landmarks’: landmarks}의 사전 형태를 갖는다.from __future__ import print_function, divisionimport osimport torchimport pandas as pdfrom skimage import io, transformfrom skimage.transform import rescale, resizeimport numpy as npimport matplotlib.pyplot as pltfrom torch.utils.data import Dataset, DataLoaderfrom torchvision import transforms, utilsfrom PIL import Imageimport cv2import warningswarnings.filterwarnings(&quot;ignore&quot;)plt.ion() # 반응형 모드class FaceLandmarksDataset(Dataset): &quot;&quot;&quot; Face Landmarks dataset &quot;&quot;&quot; def __init__(self, csv_file, root_dir, transform = None): &quot;&quot;&quot; Args: csv_file (string): csv 파일 경로 root_dir (string): 모든 이미지가 존재하는 디렉토리 경로 transform (callable): 샘플에 적용할 optional transform &quot;&quot;&quot; self.landmarks_frame = pd.read_csv(csv_file) # csv_file을 불러온다. self.root_dir = root_dir # 모든 이미지가 존재하는 디렉토리 self.transform = transform def __len__(self): return len(self.landmarks_frame) # 데이터의 개수, 즉 image 개수라고 할 수 있다. def __getitem__(self, idx): if torch.is_tensor(idx): idx = idx.tolist() # idx가 tensor이면 list로 바꾼다. img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) # root_dir 안의 csv_file에서 모든 이미지를 불러와 할당 image = io.imread(img_name) # 이미지 읽기 landmarks = self.landmarks_frame.iloc[idx, 1:] # csv_file에서 각 idx 마다의 데이터들을 다 불러온다. landmarks = np.array([landmarks]) # reshape를 위해 list가 아닌 numpy landmarks = landmarks.astype(&#39;float&#39;).reshape(-1,2) # (k,2) 배열로 변환 sample = {&#39;image&#39;: image, &#39;landmarks&#39;: landmarks} # 각 image와 landmarks(keypoint)를 연결 if self.transform: sample = self.transform(sample) return sample&#39;&#39;&#39; landmark를 보기 위한 함수 &#39;&#39;&#39;def show_landmarks(image, landmarks): &quot;&quot;&quot;Show image with landmarks&quot;&quot;&quot; &quot;&quot;&quot; 랜드마크(landmark)와 이미지를 보여줍니다. &quot;&quot;&quot; plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker=&#39;.&#39;, c=&#39;r&#39;) plt.pause(0.001) # 갱신이 되도록 잠시 멈춥니다.face_dataset = FaceLandmarksDataset(csv_file = &#39;/content/drive/MyDrive/data/faces/face_landmarks.csv&#39;, root_dir = &#39;/content/drive/MyDrive/data/faces/&#39;) # __init__ 에 대한 args들을 받으면 len, getitem을 계산해놓고, 추후에 len이나 dataset[i]를 실행하면 리턴해줌fig = plt.figure()for i in range(len(face_dataset)): sample = face_dataset[i] # i만큼 idx를 지정 print(&quot;number: {} \\t image shape: {} \\t label shape: {} \\n len image: {} \\t len label: {}&quot; .format(i, sample[&#39;image&#39;].shape, sample[&#39;landmarks&#39;].shape, len(sample[&#39;image&#39;]),len(sample[&#39;landmarks&#39;]))) &#39;&#39;&#39; image - 1개의 이미지에 대해 324 row(높이) x 215 col(넓이) x 3 dim(RGB) label - 총 68개의 landmarks(keypoint) 68 row x 2 col &#39;&#39;&#39; ax = plt.subplot(1, 4, i+1) plt.tight_layout() ax.set_title(&#39;Sample #{}&#39;.format(i)) ax.axis(&#39;off&#39;) show_landmarks(**sample) # **sample = io.imread(os.path.join(&#39;/content/drive/MyDrive/data/faces&#39;, img_name)),landmarks if i == 3: plt.show() breakQ. 왜 landmarks shape이 (68,2)가 되어야 하지? 데이터가 columns를 보면 x,y가 존재한다. 따라서 1개의 landmark(keypoint)를 표현하기 위해 x,y값을 묶어야 한다. =&amp;gt; 묶어서 보았을 때 이미지 상에 총 68개 keypoint가 존재한다.Q. image와 label의 size를 맞춰야 하는 것 아닌가? 지금 출력하고 있는 것은 각각의 이미지의 height x weight x depth 이다. 따라서 총 이미지 크기가 아니기 때문에 맞지 않았던 것이다. 총 images 들의 len과 labels 들의 len은 맞는다.Transform 클래스샘플들을 data augmentation 시킨다. rescale: 이미지의 크기를 조절 randomCrop: 무작위 자르기, data augmentation ToTensor: numpy이미지를 torch이미지로 변경( 축 변환 )클래스를 작성할 때는 call 함수를 구현해야 한다. 필요하다면 init 함수도 구현해야 한다.class Rescale(object): &#39;&#39;&#39; 주어진 사이즈로 샘플 크기 조정 Args: output size(tuple or int): 원하는 사이즈 값 tuple인 경우 해당 tuple이 결과물의 크기가 되고, int라면 비율을 유지하면서 길이가 작은 쪽이 output size가 된다. &#39;&#39;&#39; def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) # assert = 뒤의 조건이 True 가 아니면 에러를 발생시킨다. output size가 int나 tuple 타입이면 true self.output_size = output_size def __call__(self,sample): image, landmarks = sample[&#39;image&#39;], sample[&#39;landmarks&#39;] h, w = image.shape[:2] # 각 image.shape는 [h,w,c] 로 구성되어 있다. 따라서 2번째까지만 추출 if isinstance(self.output_size, int): # args - output_size가 int라면 if h &amp;gt; w: # h/w 비율을 곱하여 new_h가 더 큰 값이 되도록 new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: # tuple이라면 그대로 출력 new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) # 크기는 int로 출력해야 되기에 img = cv2.resize(image, (new_h, new_w)) # image를 내가 원하는 사이즈의 값으로 크기 조정 # 강의에서는 transform.resize 로 되어있는데 실행되지 않아 cv2로 바꾸어 실행해보았다. # print(&quot;landmarks data: {} \\nlandmarks shape: {} \\nnew_w data: {} \\nnew_h data: {}&quot;.format(landmarks, landmarks.shape, new_w, new_h) ) landmarks = landmarks * [ new_w / w, new_h / h ] return {&#39;image&#39;: img, &#39;landmarks&#39;: landmarks}class RandomCrop(object): &#39;&#39;&#39; 샘플데이터를 무작위로 자름 Args: output_size (tuple or int): 줄이고자 하는 크기로 int면 정사각형, tuple이라면 지정된 크기로 &#39;&#39;&#39; def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): # int self.output_size = (output_size, output_size) # 그대로 else: # tuple assert len(output_size) == 2 # size가 2일때만 self.output_size = output_size def __call__(self, sample): image, landmarks = sample[&#39;image&#39;], sample[&#39;landmarks&#39;] h, w = image.shape[:2] # 256 이라고 하면 new_h, new_w = self.output_size # new = 224,224 top = np.random.randint(0, h - new_h) # (0 ~ (256 - 224)) 값 사이의 random추출 left = np.random.randint(0, w - new_w) # 원본을 넘게 자르도록 설정되지 않도록 하기 위함 image = image[top: top + new_h, # image의 높이를 새로 설정한 크기와 random하게 추출한 값을 더함 left: left + new_w] # 원본보다는 무조건 작거나 같다. landmarks = landmarks - [left,top] # keypoint의 위치도 left,top을 뺌 return {&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}class ToTensor(object): &#39;&#39;&#39; numpy array를 tensor(torch)로 변환 &#39;&#39;&#39; def __call__(self,sample): image, landmarks = sample[&#39;image&#39;], sample[&#39;landmarks&#39;] # swap color axis ( numpy image: HxWxC =&amp;gt; torch image: CxHxW ) image = image.transpose((2,0,1)) return {&#39;image&#39;:torch.from_numpy(image), &#39;landmarks&#39;:torch.from_numpy(landmarks)}transform = transforms.Compose([Rescale(256), RandomCrop(224)]) # output_size = 256,256 # output_size = 224,224fig = plt.figure()sample = face_dataset[60]for i in range(1): image_transforms = transform(sample) ax = plt.subplot(1,3,i+1) plt.tight_layout() ax.set_title(type(transform).__name__) show_landmarks(**image_transforms)plt.show()transformed_dataset = FaceLandmarksDataset(csv_file = &#39;/content/drive/MyDrive/data/faces/face_landmarks.csv&#39;, root_dir = &#39;/content/drive/MyDrive/data/faces/&#39;, transform=transforms.Compose([ Rescale(256), RandomCrop(224), ToTensor() ]))Q. landmark에다 new_w/w,new_h를 왜 곱하는걸까 landmarks data가 각 keypoint의 좌표를 찍어놓은 것이기 때문에 landmark 데이터에도 비율만큼 곱해줘야 맞는 위치를 찍을 수 있다.Q. crop 할 때 왜 224가 output_size가 되야 할텐데 top+new_w를 하면 224가 무조건 나오는 건 아니지 않나. 이름만 output_size인가 일단 음,, 실행이 되지 않음Dataloader 클래스torch.utils.data.DataLoader를 사용하면 반복자(iterator)로서 많은 기능을 해준다.dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=0)# 배치하는 과정을 보여주는 함수입니다.def show_landmarks_batch(sample_batched): &quot;&quot;&quot;Show image with landmarks for a batch of samples.&quot;&quot;&quot; images_batch, landmarks_batch = \\ sample_batched[&#39;image&#39;], sample_batched[&#39;landmarks&#39;] batch_size = len(images_batch) im_size = images_batch.size(2) grid_border_size = 2 grid = utils.make_grid(images_batch) plt.imshow(grid.numpy().transpose((1, 2, 0))) for i in range(batch_size): plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size, landmarks_batch[i, :, 1].numpy() + grid_border_size, s=10, marker=&#39;.&#39;, c=&#39;r&#39;) plt.title(&#39;Batch from dataloader&#39;)for i, batched_sample in enumerate(dataloader): print(i, batched_sample[&#39;image&#39;].size(), batched_sample[&#39;landmarks&#39;].size()) if i == 3: plt.figure() show_landmarks_batch(batched_sample) plt.axis(&#39;off&#39;) plt.ioff() plt.show() breakcolab 실행 코드2. Fashion-MNIST를 활용한 Custom Datasetdataset은 샘플(image)과 정답(label)을 저장하고, dataloader은 dataset을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감싼다.fashionMNIST를 통해 benchmark 하고자한다.Custom Dataset사용자 정의 dataset 클래스는 반드시 3개 함수(init, len, getitem)를 구현해야 하므로, img_dir을 통해 이미지가 있는 디렉토리를, annotations_file을 통해 label이 있는 csv을 불러온다.import osimport pandas as pdimport torchfrom torch.utils.data import Datasetfrom torchvision import datasetsfrom torchvision.transforms import ToTensorfrom torchvision.io import read_imageimport matplotlib.pyplot as plt&#39;&#39;&#39;대부분의 데이터 label.csv는 image1.jpg , 0image2.jpg , 1...image9.jpg , 8&#39;&#39;&#39;class CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform = None, target_transform=None): &#39;&#39;&#39; __init__함수는 dataset객체가 생성될 때 한번만 실행된다. 여기에서 이미지와 label 파일이 포함된 디렉토리와 transform을 초기화한다. transform - image, target-transform - label &#39;&#39;&#39; self.img_labels = pd.read_csv(annotations_file) # init에서는 read_csv만 한다. self.img_dir = img_dir self.transform = transform self.target_transform = trarget_transform def __len__(self): &#39;&#39;&#39; __len__함수는 데이터셋의 샘플 개수를 반환 &#39;&#39;&#39; return len(self.img_labels) # label의 행 길이(갯수) row def __getitem__(self,idx): &#39;&#39;&#39; __getitem__함수는 주어진 인덱스 idx에 해당하는 샘플을 데이터셋에서 불러오고 반환한다. 인덱스를 기반으로, 디스크에서 이미지의 위치를 식별하고, read_image를 사용하여 이미지를 텐서로 변환, self.img_labels의 csv 데이터로부터 해당하는 label(정답)을 가져오고 변형함수를 호출하고, image와 label을 리턴 &#39;&#39;&#39; img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) # image 경로들 image = read_image(img_path) label = self.img.labels.iloc[idx, 1] # image에 맞는 label 지정 if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, labeldataloaderfrom torch.utils.data import DataLoadertraining_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform = ToTensor())validation_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform = ToTensor())train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)val_dataloader = DataLoader(validation_data, batch_size=64, shuffle=True)Data 시각화train_features, train_labels = next(iter(train_dataloader))print(f&quot;Feature batch shape: {train_features.size()}&quot;)print(f&quot;Labels batch shape: {train_labels.size()}&quot;)img = train_features[0].squeeze() # [64,1,28,28] 을 [28,28]로 펼쳐야 plt 할 수 있다.print(f&quot;img shape: {img.size()}&quot;)label = train_labels[0]plt.imshow(img, cmap=&quot;gray&quot;)plt.showTransformfashionMNIST 특징(image)들은 PIL image 형식이며 정답(label)은 int이다. 학습을 위해 정규화를 통한 이미지와 원핫으로 부호화된 텐서 형태의 label이 필요하다.ToTensor는 PIL image나 Numpy ndarray를 floatTensor로 변환하고, 이미지의 픽셀의 크기 값을 [0.,1.]범위로 비례하여 조정한다.Lambda 를 통해 정수를 원-핫으로 부호화된 텐서로 바꾸는 함수를 정의한다. 이 함수는 먼저(데이터셋 정답의 개수인)크기 10짜리 zero tensor를 만들고, scatter_를 호출하여 주어진 정답 y에 해당하는 인덱스에 value=1을 할당한다.import torchfrom torchvision import datasetstransform = ToTensor()target_transform = Lambda(lambda y: torch.zeros(10, dytype=torch.float).scatter_(0, torch.tensor(y), value=1))Instance segmentation dataset에서의 Custom Dataset객체 검출, 인스턴스 분할 및 키포인트 검출을 학습하기 위한 새로운 사용자 정의 데이터셋을 추가해보고자 한다.__getitem__ 메소드가 다음을 반환해야 한다. image: PIL이미지의 크기(H,W) target: 다음의 필드를 포함하는 사전 타입 boxes (FloatTensor[N,4]): N개의 bounding box 좌표가 [x0,y0,x1,y1] 형태를 가진다. x와 관련된 값의 범위는 0부터 w이고, y와 관련된 값의 범위는 0부터 H까지다. labels (int64Tensor[N]): 바운딩 박스마다의 라벨 정보, 0 은 배경 image_id (int64Tensor[1]): 이미지 구분자이다. 데이터셋의 모든 이미지 간에 고유한 값이어야 하며 평가 중에도 사용된다. area (Tensor[N]): 바운딩 박스의 면적, 면적은 평가 시 작음, 중간, 큰 박스 간의 점수를 내기 위한 기준이고, COCO 평가를 기준으로 한다. iscrowd (Uint8Tensor[N]): 이 값이 팜일 경우 평가에서 제외한다. (선택)masks (Uint8Tensor(N, H, W]): N개의 객체 마다의 분할 마스크 정보 (선택)keypoints (FloatTensor[N, K, 3]): N개의 객체마다의 키포인트 정보. 키포인트는 [x,y,visibility] 형태의 값이다. visibility 값이 0인 경우 키포인트는 보이지 않음을 의미한다. data augmentation의 경우 키포인트 좌우 반전의 개념은 데이터 표현에 따라 달라지며, 새로운 키포인트 표현에 대해 코드 부분을 수정해야 할 수도 있다. 모델을 위의 방법대로 리턴하면 학습과 평가 둘 다에 대해 동작한다.배경은 무조건 0이어야 하기 때문에, 클래스 분류가 개, 고양이를 분류하고자 할때는 0이 아닌 1과 2로 정의해야 한다.추가로 학습 중 가로 세로 비율 그룹화를 사용하려는 경우, 이미지의 넓이, 높이를 리턴할 수 있도록 get_height_and_width 메소드를 구현하기를 추천한다. 이를 구현하지 않은 경우 모든 데이터셋은 __getitem__를 통해 메모리에 이미지가 로드되어 사용자 정의 메소드를 제공하는 것보다 느릴 수 있다.폴더 구조는 다음과 같다.- PennFudanPed/ - PedMasks/ - FudanPend00001_mask.png - FudanPend00002_mask.png - ... - PNGImages/ - FudanPed00001.png - FudanPed00002.png - ...image - masks 예시&amp;lt;img src = https://tutorials.pytorch.kr/_static/img/tv_tutorial/tv_image01.png width=”40%”&amp;gt; &amp;lt;img src = https://tutorials.pytorch.kr/_static/img/tv_tutorial/tv_image02.png width=”40%”&amp;gt;각 이미지에 해당하는 분할 마스크가 존재한다.import osimport numpy as npimport torchfrom PIL import Imageclass PennFudanDataset(torch.utils.data.Dataset): def __init__(self, csv_file=None, root_dir, transforms=None): self.root_dir = root_dir self.transforms = transforms # 모든 이미지와 분할 마스크 정렬 self.imgs = list(sorted(os.listdir(os.path.join(root_dir, &quot;PNGImages&quot;)))) # image셋을 불러옴 self.masks = list(sorted(os.listdir(os.path.join(root_dir, &quot;PedMasks&quot;)))) # mask셋을 불러옴 # detection과 segmentation이므로 label에 대한 값은 가져오지 않는다. 정답에 대한 셋이 mask이다. def __getitem__(self, idx): # 이미지와 마스크를 읽어옵니다 img_path = os.path.join(self.root_dir, &quot;PNGImages&quot;, self.imgs[idx]) # 이미지 각각의 데이터를 가져옴 mask_path = os.path.join(self.root_dir, &quot;PedMasks&quot;, self.masks[idx]) img = Image.open(img_path).convert(&quot;RGB&quot;) # 분할 마스크는 RGB로 변환하지 않음을 유의하세요 # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다 mask = Image.open(mask_path) # PIL 이미지를 numpy 배열로 변환합니다 mask = np.array(mask) # 인스턴스들은 각기 다른 색들로 인코딩 되어야 한다. obj_ids = np.unique(mask) # 첫번째 id 는 배경이라 제거합니다 obj_ids = obj_ids[1:] # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눕니다. ??? masks = mask == obj_ids[:, None, None] # 각 마스크의 바운딩 박스 좌표를 얻습니다 num_objs = len(obj_ids) boxes = [] for i in range(num_objs): pos = np.where(masks[i]) # 각 masks에 대한 값들 xmin = np.min(pos[1]) # 중 각각의 x,y 좌표를 지정 xmax = np.max(pos[1]) ymin = np.min(pos[0]) ymax = np.max(pos[0]) boxes.append([xmin, ymin, xmax, ymax]) # 모든 것을 torch.Tensor 타입으로 변환합니다 boxes = torch.as_tensor(boxes, dtype=torch.float32) # 32와 64는 메모리 사용량을 나타내는 것 # 예제에서는 사람만을 분류하므로 객체는 1종류이다. labels = torch.ones((num_objs,), dtype=torch.int64) masks = torch.as_tensor(masks, dtype=torch.uint8) # as_tensor = tensor의 데이터타입이나 device를 바꾸는 방법, # .to()와 똑같은 기능, x.to(device, dtype=torch.float64) # device는 .cuda()로, 타입은 .half(), .float() 등으로 바꿀 수도 있음 image_id = torch.tensor([idx]) # 이미지 구분자 area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # 바운딩 박스의 면적, 평가 시 점수내는 기준 # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다 iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # 이 값이 참일 경우 평가에서 제외 target = {} target[&quot;boxes&quot;] = boxes target[&quot;labels&quot;] = labels target[&quot;masks&quot;] = masks target[&quot;image_id&quot;] = image_id target[&quot;area&quot;] = area target[&quot;iscrowd&quot;] = iscrowd if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.imgs)이 장에서는 custom dataset을 위주로 공부할 것이기에 모델 학습과 평가는 하지 않을 것이다. 더 궁금하다면 참고 사이트를 참고하길 바란다.Reference landmark dataset: https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html Fashion-MNIST datset: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html#id7 Instance segmentation: https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html" }, { "title": "2021 Ego-Vision 손동작 인식 경진대회", "url": "/posts/handdetection/", "categories": "Review, DACON", "tags": "DACON, keypoint, classification", "date": "2021-10-07 13:00:00 +0900", "snippet": "일단 제출은 하지 못했다.최소한의 기능만 가지고 제출을 목표로 코드를 구현했다.아직 customdataset을 만드는 방법을 잘 몰라 고생을 했다. 정답 label과 폴더의 개수가 맞지 않고, 내가 원하는 image와 label을 불러와 돌릴 수 없으니 제약되는 것이 많았다. keypoint를 추출하여 ±100 하여 image를 crop하는 방법도 있었으나, customdataset을 만들지 못하여 사용하지 않았다.그래서 train 폴더를 retrain폴더로 answer label 별로 재분류하여 model에 집어넣었다.많은 augmentation이 있지만, 일주일밖에 시간이 없었고, 제출만을 목표로 하였기 때문에 적용하지 않았다.valid도 추가하여 loss나 accuracy를 보며 수정해야 했지만, 이 또한 수행하지 못했다.epoch 자체가 train을 통해 loss를 줄여나가는 것이다. 따라서 test에 epoch을 사용하는 것이 아니라, 모든 test image를 다 돌려보기 위해서는 다른 함수가 필요하다.pytorch책에서 배운 evaluate 함수는 val을 위한 함수다. 따라서 test image에 대한 모델과 이미지는 따로 만들어야 한다. 이미지는 모든 이미지를 불러오는 코드를 구현해야 할 것이다.train은 val을 평가하는 것이기도 하다. 따라서 model parameter을 저장해야 한다.사실 train,val,test 함수는 별 차이 없다. 하지만 preprocessing을 어떻게 하냐를 많이 구현하고, train 후 plot하여 loss를 크게 변화시키는 데이터가 어떤 것인지 알아보는게 중요한 듯하다.아니면,, 여러가지 augmentation이나 kfold, ensemble 등의 방법을 통해 성능을 끌어올리는듯하다.5등껄보니 꽤 비슷했다. customdataset만 만들줄 알았다면 꽤 등수가 올랐을 것 같다.공부해야 할 것들따라서 이 대회를 통해 깨달은 나의 부족한 점이나 보완할 점은 Custom Dataset 만드는 방법 공부 validation 추가하고, model을 save하고 plot까지 augmentation 공부 일반적인 transfrom 말고도 많으니 다양한 aumentation을 적용하는 방법 공부 Rule-based Approach : Public-0.00670, Private score-0.00578 (아래에서 자세히 분석하겠지만 큰 효과를 봤습니다.) Flip Augmentation Random Margin Crop Image Random Affine &amp;amp; Random Perspective Augmentation OneCycleLR Scheduler WarmUpLR Scheduler Kfold 적용해보기 validation Kfold 등등 Ensembles 적용하는 방법 공부 seed 설정 방법 batch normalization 적용하는 방법 overfitting 막는 방법 weight_decay 는 regularization 하는 매개변수이다. 따라서 overfitting을 막기 위한 방법 중 하나로 weight_decay를 사용 먼저 egovision 코드로 한 후, 교통 수신호 동작 인식 ai경진대회도 참고하여 코드 공부하기URL https://github.com/dkssud8150/dacon-egovision/blob/main/egovision.ipynb https://dacon.io/competitions/official/235805/codeshare/3362?page=1&amp;amp;dtype=recent 다른 대회 참고 코드" }, { "title": " CS231N chapter 11 - Detection and Segmentation ", "url": "/posts/cs231n11/", "categories": "Classlog, CS231N", "tags": "CS231N, classification, detection, segmentation", "date": "2021-10-04 13:00:00 +0900", "snippet": " 10강 리뷰 1) Recurrent Neural Network language model 2) CNN + RNN = image captioning 3) LSTM Computer Vision Tasks다양한 computer vision task가 있다. semantic segmentation / classification+localization / object detection / instance segmentation 등이 있다.Semantic Segmentationsemantic segmentation은 입력은 이미지, 출력은 이미지의 모든 픽셀에 카테고리를 정하는 것이다. 그 픽셀이 어떤 물체에 해당하는지 결정한다.semantic segmentation은 개별 객체를 구분하지 않는다. 픽셀의 카테고리만 구분할 뿐이다.그에 반해 instance segmentation은 개별 객체까지 구분한다.segmentation초기 모델에서는 sliding window를 적용했다. 입력 이미지를 아주 작은 단위로 쪼개어 classification을 하는 것이다.이 방법은 비용이 엄청나게 크다. 모든 픽셀에 대해 작은 영역으로 쪼개고, 이 모든 영역을 forward/backward pass 하는 일은 상당히 비효율적이다.그래서 fully convolutional network를 사용해보았다. FC Layer을 없애고 convolution layer로 구성된 네트워크이다.이 네트워크의 출력 tensor는 C x H x W이다. c는 카테고리 수에 해당한다. 이 출력값은 입력 이미지의 모든 픽셀 값에 대해 classification score을 매긴 값이다.이 네트워크를 학습시키려면 모든 픽셀의 classification loss를 계산하고 평균 값을 취한다. 그리고 backpropagation을 수행한다.training data를 만드는 방법은 입력 이미지에 모든 픽셀에 대해 라벨링하면 된다. 사람이 툴을 만들기도 한다. 객체의 외관선만 그려주면 그 안을 채워넣는 식이다.또한, 손실 함수는 출력의 모든 픽셀에 corss entropy를 적용한다. 출력의 모든 픽셀과 실제 값(ground truth)와의 cross entropy를 계산한다. 이 값들을 모두 더하거나 평균화시켜 loss를 구한다.문제는 이 네트워크는 입력 이미지의 spatial size(공간적 사이즈)를 계속 유지시켜야 한다. 그래서 비용이 아주 크다.실제 네트워크의 형태는 대부분 이렇다. 특징맵을 downsampling &amp;amp; upsampling 한다.이미지 전체를 계속 convolution 시키기보다 max pooling/ stride convolution을 통해 특징맵을 downsampling 한다. 그 후 다시 입력 이미지의 해상도와 같도록 upsampling 한다. 이 방법을 사용하면 계산 효율이 더 좋아진다.upsampling의 방법으로 nearest neighbor 이나 bed of nails 가 있다.nearest neighbor upsampling의 경우는 해당하는 receptive field로 값을 그대로 복사한다. 주변의 값들을 복사하는 것이다.bed of nails upsampling 의 경우는 원래 값을 가져오고 나머지 공간에는 0을 집어넣는다.Max unpooling 이라는 방법도 있다. pooling과 연관을 지어 downsampling 시에 maxpooling에 사용했던 요소들을 기억하고 그 자리에 값들을 집어넣는 것이다.maxpooling을 하면서 특징맵의 공간 정보를 잃게 된다. 이 방법을 사용하면 공간정보 손실을 줄일 수 있기 때문에 더 좋은 결과를 얻을 수 있다.transpose convolution이라는 것도 있다. 앞서 배운 것(upsampling , max unpooling)들은 고정함수이기 때문에 학습하지는 않는다.하지만, transpose convolution은 어떤 방식으로 upsampling할지 학습을 할 수 있다.일반적인 3x3 convolution filter(stride=1,padding=1)은 입력도 출력도 4x4이다.하지만, strided convolution 의 경우 입력이 4x4이고, 출력이 2x2이다.이유는 3x3 필터에 대한 strided convolution은 한 픽셀씩 이동하면서 계산하지 않고, 두 픽셀씩 움직여야 한다. 따라서 stride=2인 strided convolution은 학습 가능한 방법으로 2배 downsampling한다.transpose convolution은 반대로 입력이 2x2 이고, 출력이 4x4다.여기서는 내적을 수행하지 않고, 우선 input 특징맵에서 값(빨간색)을 하나 선택한다. 이 하나의 값(스칼라)과 필터를 곱한 값을 3x3 영역에 넣는다.transpose convolution은 필터와 입력의 내적의 계산이 아니라 입력 값이 필터에 곱해지는 가중치 역할을 한다. 따라서 출력은 필터 * 입력(가중치) 가 된다.입력이 한칸씩 움직이면 출력은 2칸씩 움직인다. 이 때, 서로 겹치는 값이 존재할 경우 두 값을 더한다.transpose convolution의 구체적인 예시이다. 입력은 a,b 필터는 x,y,z 이다. 출력값을 계산해보면 오른쪽과 같은 형태가 된다.이름이 transpose인 이유를 설명하자면,왼쪽은 일반적인 convolution 이고, 오른쪽은 transpose convolution이다. 같은 행렬이지만, 오른쪽의 행렬을 transpose(전치)를 취한 후 곱 연산한다. 왼쪽은 stride 1 convolution이고, 오른쪽은 stride 1 transpose convolution인 것이다.왼쪽의 필터의 양 끝단의 0 은 padding으로 인한 값이다.하나 더 보자면, stride 1 transpose convolution을 보면 비슷하게 생겼다. padding 등을 고려하면 좀 달라지지만 기본적으로 비슷하다. 하지만 여기서 stride = 2로 설정되어 있다. 다른 점은 xyz가 2칸씩 움직인다.이 때, stride가 1을 초과하면 transpose convolution과 convolution이 같지 않게 된다. 아마 a가 a,b,c,d가 아닌 a,b만 나오기 때문이 아닐까 싶다.Classification + Localization이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체가 어디에 있는지 알기 위해 bounding box를 친다.object detection은 다중 객체를 인식할 때 사용되고, localization은 단일 객체를 분류할 때 자주 사용된다.localization의 기본 구조를 나타낸 것이다. 입력 이미지를 받아 네트워크에 넣고 출력 레이어 직전의 FC layer는 class score로 연결되어 카테고리를 정한다. 그리고 4개의 원소를 가진 vector와 연결된 FC layer가 하나 더 있다. 이 4개의 원소는 width/height/x/y로 bounding box의 위치를 나타내는 것이다.따라서 localization을 할 때는 2개의 출력 값을 가진다. 하나는 클래스 스코어, 하나는 bounding box 좌표이다. 이 때, 학습 이미지에는 카테고리 레이블과 해당 객체의 bounding box Ground Truth를 동시에 가지고 있어야 한다.그래서 loss도 두 개 존재한다. 스코어에 대한 loss는 softmax, bounding box에 대한 loss는 L2 loss로 구한 후 두개를 더해 최종적인 loss를 구한다. L2 대신 smooth L1을 더 많이 사용되긴 한다.이처럼 두 개의 loss를 합친 loss를 Multi-task loss라고 한다. gradient를 구하려면 네트워크 가중치들의 각각의 미분 값을 계산해야 한다. loss가 두 개이므로 미분 값도 두 개다.loss의 수식을 보면 두 loss의 가중치를 조절하는 하이퍼파라미터가 존재한다. 이 두 값을 조절하는 것은 매우 까다롭다. 그래서 loss 값이 아닌 다른 지표(MAP(mean average precision), model size, speed, num of parameters)를 통해 성능을 비교하는 것이 좋다.classification + localizatino은 human pose estimation에도 적용할 수 있다. 사람의 관절이 어디에 위치하는지 예측한다. 관절의 위치로 사람의 포즈를 정의한다. 정의한 관절의 위치를 GT와 비교하여 regression Loss를 구한다.Object Detectionobject detection은 엄청 많은 곳에 적용해볼 수 있는 방법이다. visual tracking, pose estimation, 등등 object detection 모델에 segmentation이나 tracking 모델을 더해서 사용할 수 있기 때문이다. 실제로 이런 방식을 사용하면 더 성능이 좋게 나온다.object detection을 할 때도 초기 모델에서는 sliding window를 많이 사용했다. 입력 이미지를 매우 작은 픽셀로 나누고 그 위를 window가 모든 픽셀을 읽어들이도록 한다.silding window의 문제는 어떻게 영역을 추출할지와 이미지에 object가 몇 개 존재하는지, 어디에 존재하는지 파악하지 못한다. object의 크기도 알수가 없다. 또, 작은 영역 하나마다 거대한 CNN을 통과시키면 계산량이 엄청나다.그래서 Region Proposals를 개발했다. 이 방법은 현재 딥러닝에 사용되지 않지만 전통적인 방식이다.object가 있을법한 2000개의 bounding box 후보군을 찾아내는데, 찾는 방법으로 selective search가 있다. selective search를 돌려 객체가 있을만한 2000개의 region proposal을 만들어내는 것이다. 찾아낸 region proposal을 CNN의 입력으로 한다.이 방법을 사용하면 silding window방식보다 계산량이 줄어든다. 하지만, 이 방법은 노이즈가 심하다. 대부분은 실제 객체가 아닐지라도 recall이 매우 높다.region proposal을 region of interest(ROI)라고도 한다. 실제로 ROI라는 말을 더 많이 사용한다.여기서 ROI의 사이즈가 각양각색이다. 추출된 ROI로 CNN Classification을 수행하기 위해서는 FC-layer의 특성에 의해 사이즈가 다 동일해야 한다. 따라서 고정된 사이즈로 크기를 맞춰준다. 이를 warp시킨다고 한다.R-CNNR-CNN의 경우 ROI들의 최종 classification에 SVM classifier을 사용하여 score를 분류한다.R-CNN은 Bounding Box를 보정해주기 위한 offset 값 4개도 예측한다.고정된 알고리즘인 selective search를 사용하기 때문에 region proposal은 학습되지 않는다.R-CNN의 문제점으로는 계산비용이 크고 느리다. 2000개의 ROI를 독립적으로 CNN에 넣기 때문이다. 그리고 CNN에서 나온 feature 들을 디스크에 저장하므로 용량이 엄청나다. training time/test time 둘다 매우 느리다.Fast R-CNN위의 문제들을 해결하고자 Fast R-CNN을 개발했다. 전체 알고리즘 입력 이미지를 미리 학습된 CNN을 통과시켜 feature map을 추출한다. selective search를 통해서 찾은 각각의 ROI에 대해 ROI Pooling을 진행하여 고정된 크기의 feature vector를 얻는다. feature vector는 fully connected layer들을 통과한 뒤, 두 개의 브랜치로 나뉘게 된다. 1) 하나의 브랜치는 softmax를 통과하여 해당 ROI가 어떤 물체인지 classification한다. SVM은 사용하지 않는다. 2) 나머지 브랜치는 bounding box regression을 통해서 selective search로 찾은 박스의 위치를 조정한다. R-CNN과 다른 점은 각 ROI마다 각각을 CNN에 넣지 않고, 입력 이미지에 CNN을 수행하여 고해상도 feature map을 얻고, 거기에 ROI를 적용시킨다. 이에 따라 여러 ROI가 서로 feature을 공유할 수 있게 되었다.feature map에서 가져온 ROI는 FC layer의 입력에 알맞게 크기를 조정해야 한다. 이를 위해 ROI pooling layer을 적용한다.feature map에서 가져온 ROI의 크기를 조정(wraped)한 후, FC layer의 입력으로 넣어 classification score과 regression box를 얻을 수 있다.R-CNN과의 차이는 CNN을 한 번만 통과시키기 때문에 계산이 줄어들고, SVM을 사용하지 않는다는 것이다.CNN을 한번만 통과시킨 뒤 그 feature map을 공유하는 것은 SPPNet에서 고안한 방법이다.fast R-CNN에서는 feature들을 서로 공유하기 때문에 계산이 엄청 빠르다. region proposal을 계산하는 시간이 대부분이다. faster R-CNN에서는 계산 시간을 줄이고자 했다.Faster R-CNN지금까지 사용되왔던 selective search대신 RPN(region proposal network)을 사용한다.입력 이미지 전체를 CNN에 넣어 feature map을 얻는다. 여기서 RPN을 적용시켜 region proposal을 예측한다. RPN 알고리즘 feature map을 input으로 받는다. feature map에 3x3 convolution을 256 또는 512 채널(depth)만큼 수행한다. convolution된 map을 입력으로 받아 classification과 bounding box regression 예측 값을 계산한다. 앞서 얻은 값들로 ROI를 구한다. 학습시에 RPN도 함께 학습시키는데, ground truth object(실제 객체)와 일정 threshold(한계 = 정도)이상 겹치는 proposal(제안)을 positive라 하고, 그 이하는 negative라고 한다.Faster R-CNN에서는 최종 classification loss/bounding box regression loss도 구하고, proposal에 대한 classification loss와 bounding box regression loss까지 총 4개를 구하고 이들에 대한 loss를 합쳐 최종 loss를 계산한다.위의 R-CNN 시리즈는 2-stage model들이다. 즉, classification과 bounding box regression을 각각 수행한다. 하지만 YOLO/SSD의 경우 1-stage model로 이 둘을 함께 수행한다. 따라서 2stage는 정확도는 좋지만, 속도가 느리고, 1stage의 경우는 속도는 빠르나 정확도가 다소 떨어진다.You Only Look Once / Single Shot DetectionYOLO(you only look once) 모델은 요즘 매우 핫한 모델로 2021년도에 개발된 yolov5 까지 있다. 이 두 모델은 거대한 CNN을 통과하면 모든 것을 담은 예측값이 한번에 나온다.region proposal 단계를 제거하고, bbox regression과 classification을 한 번에 수행하는 구조다.먼저 입력 이미지를 S X S 그리드 영역으로 나눈다. 각 그리드 영역에서 물체가 있을만한 영역에 해당하는 B개의 bounding box를 예측한다. 그 다음 해당 박스의 신뢰도를 나타내는 confidence를 계산한다. 또한, bounding box 안에 객체가 존재할 가능성을 의미하는 classification score도 계산한다.출력은 S x S x (5*B + C) [B: bounding box의 offset, C: 클래스 개수]의 3-dimension를 가지는 tensor이다. 이 값을 CNN으로 학습시킨다.base network(backbone network)에는 VGG, ResNet, MobileNet 등이 있다. 아키텍처에는 2stage인 R-CNN 시리즈와 1stage인 YOLO 등이 있다. 2stage와 1stage의 중간인 R-FCN도 있다. Dense Captioningobject detection + captioning 을 dense captioning으로 명명했다. 각 region에 대해 카테고리 대신 문장(caption)을 예측하는 것이다.데이터셋으로는 각region에 caption이 있는 데이터셋이 필요하다.이 모델을 end-to-end로 학습시켜 모든 것을 동시에 예측할 수 있도록 만들었다.네트워크에는 Region proposal을 사용했고, caption을 예측해야 하기에 softmax loss 대신 RNN Language model을 도입했다.Instance Segmentationsemantic segmentation과 object detection을 섞은 것이다. 이미지 내의 두 마리 개가 있을 때 2마리를 구분해야 한다. 또한 BBox 가 아닌 각 객체애 해당하는 segmentation mask를 예측해야 한다.Mask R-CNN 아키텍처가 instance segmentation에 속한다.입력 이미지는 CNN과 RPN을 거친다. 그 후 classification과 regression이 아닌 bbox마다의 segmentation mask를 예측한다. RPN으로 뽑은 ROI 영역 내에서 각각 semantic segmentation을 수행한다. feature map으로부터 ROI Pooling을 수행하면 두 갈래로 나뉜다. 첫번째는 각 region propasal이 어떤 카테고리에 속하는지 계산하고, region proposal의 좌표를 보정해주는 bbox regression도 예측한다. 다른 갈래는 각 픽셀마다 객체인지 아닌지 분류한다.Mask R-CNN에 pose estimation도 융합할 수 있다. 관절의 좌표를 예측하는 부분을 추가하면 된다. region proposal에 한 갈래를 추가해 region proposal 안의 객체의 관절 좌표를 예측하면 된다. Mask R-CNN 코드https://colab.research.google.com/drive/1dWg0nx7KEYGSH05heY2_z5hosHBK3EbP#scrollTo=iTL-OFlWNSWqReference http://cs231n.stanford.edu/2017/syllabus.html https://lsjsj92.tistory.com/416" }, { "title": " CS231N chapter 10 - Recurrent Neural Networks", "url": "/posts/cs231n10/", "categories": "Classlog, CS231N", "tags": "CS231N, RNN, LSTM, GRU", "date": "2021-10-03 13:00:00 +0900", "snippet": " 9강 리뷰 1) AlexNet 2) VGGNet filter 개수를 줄여 depth를 더 쌓음 3) GoogLeNet inception module, bottleneck layer 4) ResNet residual mapping Recurrent Neural Network지금까지 배운 아키텍쳐들은 ONE-TO-ONE(Vanilla Neural Network)의 모양이라고 할 수 있다. 이미지 또는 벡터를 입력으로 받아, 입력 하나가 hidden layer을 거쳐 하나의 출력을 내보내는 방식이다.classification문제라면 카테고리가 될 것이다.하지만, 머신러닝의 관점에서 생각해보면 모델이 다양한 입력을 처리할 수 있도록 유연해질 필요가 있다. 그렇기에 RNN은 네트워크가 다양한 입/출력을 다룰 수 있는 여지를 제공해준다. One to Many입력은 이미지와 같은 “단일 입력”이지만, 출력은 caption과 같은 “가변 출력”이다. caption에 따라 단어의 수가 천차 만별이다. Many to One입력이 문장과 같은 “가변 입력”이다. 이 문장의 감정을 분류한다. 긍정적인 문장인지 부정적인 문장인지 구별하는 것이다. Many to Manycomputer vision의 경우, 입력이 비디오라고 하면, 비디오에 따라 전체 프레임 수가 다양하다. 따라서 전체 비디오를 읽으려면 가변 길이의 입력을 받아야 한다.비디오를 입력받아서 비디오에 나타나는 action을 분류한다면, 이 경우는 입/출력 모두 가변이어야 한다.번역을 할 때도, 입력이 영어 문장이면 가변 입력이 될 것이고, 출력은 번역 결과인 문장이 되니 출력도 가변 출력이 된다.비디오의 매 프레임마다 classification을 해야 한다면 이 또한, 입/출력이 둘다 가변이 된다.Recurrent Neural Network는 가변 길이의 데이터를 다루기 위해 필요한 일반적인 방법이다.RNN은 위의 다양한 상황들에 대해 잘 처리할 수 있도록 해준다. Sequential Processing of Non-Sequence Data입/출력은 고정이지만, 연속적인 프로세싱이 요구되는 경우가 있다.위의 이미지가 있다고 치면, 이는 고정 입력이다. 그리고 이미지의 숫자가 몇인지 분류하는 문제이다. 입력 이미지의 정답을 전체를 보지 않고, 이미지의 여러 부분을 조금씩 살펴본다. 다 살펴본 후 숫자가 몇인지 판단한다.이처럼 입/출력이 고정된 길이라고 해도 가변 과정인 경우에도 RNN은 상당히 유용하다.그렇다면, RNN은 어떻게 동작하는 것일까?일반적으로 RNN은 작은 “Recurrent Core Cell”을 가지고 있다.입력 x가 RNN으로 들어가면 RNN에는 내부에 hidden state가 있을 것이다. hidden state는 RNN이 새로운 입력을 학습할 때마다 매번 업데이트된다.RNN이 매 단계마다 값을 출력하는 경우 RNN이 입력을 받는다. hidden state를 업데이트 출력값을 내보낸다.RNN의 구조를 수식적으로 나타낸다면 아래와 같다.초록색 RNN Block은 함수f를 통해 재귀적인 관계를 연산할 수 있도록 설계된다.파라미터 w를 가진 함수f가 있다고 할 때, 함수 f는 이전 상태의 hidden state인 h_t-1과 현재 상태의 입력인 x_t를 입력으로 받는다. 그리고 h_t를 출력한다. h_t는 다음 사애의 hidden state이다.다음 단계에서는 h_t와 x_t+1이 입력이 된다.RNN에서 출력값 y를 가지려면 h_t를 입력으로 하는 FC-Layer을 추가해야 한다.FC Layer은 매번 업데이트되는 hidden state(h_t)를 기반으로 출력 값을 결정한다. 이때, 함수 f와 파라미터 w는 항상 동일하다.단순한 vanilla RNN을 예시로 보자.이전 hidden state와 현재 입력을 받아서 다음 hidden state를 출력한다.좀 더 자세하게 보자면, 이전 hidden state인 h_t-1와 곱해지는 가중치 행렬 w_hh와 현재 입력 x_t와 곱해지는 가중치 행렬 w_xh이 있다. 그리고 비선형을 위해 tanh를 적용한다.이제 이를 통해 매 스텝마다 출력 y를 얻고자 한다. 이를 위해서는 h_t를 새로운 가중치 행렬 w_hy와 곱해준다. 매 스텝에 출력 y는 class score가 될 수 있을 것이다.RNN의 두 가지 방법의 해석 RNN이 hidden state를 가지며 재귀적으로 피드백한다는 것첫 스텝에서 initial hidden state인 h0가 있다. 대부분은 h0는 0으로 한다. 입력 x_t와 h0를 fw의 입력으로 받아 h1을 출력한다. 이 과정을 반복해 h2,h3,,, h_T가 될 것이다. 여기서 주의해야 할 것은 동일한 가중치 행렬 W를 매번 사용한다는 점이다. h와 x만 달라진다.그리고 위의 그림에서 y_t는 매 스텝의 class score이라 할 수 있다. y_t를 통해 실제 값과 y_t의 차이를 통해 loss 값을 구한다. 최종적인 loss는 각 개별 loss들의 합이 된다.모델을 학습시키기 위해 dL/dw를 구해야 한다.동일한 node를 여러 번 사용할 때 backward pass시에 dL/dw를 계산하려면 w의 gradient를 전부 더해준다. 따라서 RNN 모델의 backprop을 위한 행렬 w의 gradient를 구하려면 각 스텝에서의 w에 대한 gradient를 전부 계산한 뒤에 이 값을 모두 더해주면 된다.many to many가 아닌 many to one의 경우는 어떻게 될까?이 경우 최종 hidden state에서만 결과 값이 나온다.one to many의 경우에는고정 입력을 받지만, 가변 출력인 네트워크이다. 이 경우 대체적으로 고정 입력은 모델의 initial hidden state를 초기화시키는 용도로 사용한다.RNN의 경우는 모든 스텝에서 출력값을 가진다.sequence to sequence가변 입력(sequence)과 가변 출력(sequence)을 가지는 모델로 기계 번역에 사용될 수 있는 모델이다.many to one 과 one to many 모델의 결합이라 할 수 있다. 즉, encoder과 decoder 구조를 갖는다. encoder은 가변 입력을 받는다. english sentence가 될 수 있을 것이다.encoder의 final hidden state를 통해 전체 sentence를 요약한다. 가변 입력을 하나의 벡터로 요약한 것이다.반면, decoder은 가변 출력을 출력한다. 한국어로 번역된 문장을 출력한다고 할 수 있을 것이다. 가변 출력은 매 스텝 적절한 단어를 내뱉는다.output의 각 losses를 합쳐서 backpropagation을 수행한다.더 구체적인 예를 살펴보자. 대부분 RNN은 language modeling에 자주 사용된다. 즉, 어떻게 자연어를 만들어 내는가의 모델로서 사용된다. 예를 들어 문자(character)를 내뱉는 모델이라면 매 스텝 어떻게 문자를 생성해낼지를 해결해야 한다.문자열 시퀀스를 읽어들이고, 현재 문맥에서 다음 문자를 예측해야 하는 네트워크이다. 리스트 [h,e,l,o]가 있고, hello 라는 단어를 만들고자 한다. train time에서는 training sequence인 hello의 각 단어를 입력으로 넣어줘야 한다.hello가 입력 x_t가 된다. 우선 입력은 한 글자씩한다. 일반적으로 입력을 넣어주는 방법이 있는데, 우선 vocabulary는 총 h,e,l,o로 4가지이다. 각 글자는 하나의 벡터로 표현할 수 있다. 해당 글자의 위치에 해당하는 위치에만 1로 표시한다. 이 예제는 4개 글자이므로 4d 벡터로 표현가능하다.forward pass에서의 동작을 나타낸 것이다.우선 첫 스텝으로 입력 문자 h가 들어오면, 네트워크는 y_t를 출력한다. y_t는 어떤 문자가 h 다음에 나올 것 같은지를 예측한 값이다. 이 예제에서는 e를 예측해야 정답이다.softmax를 통해 이 예측값이 얼마나 정답에 가까운지 값을 출력할 것이고, 다음 스텝도 동일하게 진행된다.하지만 다른 점은, 이전 hidden state도 함께 계산이 이루어진다는 것이다. 이전 hidden state와 함께 새로운 hidden state를 만들어낸다.모델에 다양한 문장을 학습시키면 결국 모델은 이전 문장들의 문맥을 참고해서 다음 문자가 무엇일지를 예측한다.test time에서는 train time에 모델이 봤을 법한 문장을 모델 스스로 생성해내도록 한다.우선 모델에게 문장의 첫 글자만 제공하여 각 문자에 대한 스코어(output)를 얻는다. 스코어를 softmax 함수를 활용하여 확률분포로 만들고, 이를 이용해 다음 글자를 선택한다.이렇게 뽑힌 글자를 다음 스텝의 입력으로 넣어준다. 이 문자를 벡터화 할 것이고, 입력으로 넣어 네트워크의 두번째 출력을 만들어낸다.가장 높은 스코어가 아닌 확률분포를 사용하는 이유는 가장 높은 스코어를 사용한다고 해서 올바른 결과를 얻을 수 없다. 어떤 경우는 argmax probasbility만 사용할 수 있다. 하지만 확률분포로 샘플링하는 것이 모델에게 다양성을 줄 수 있다.또, test time에서 softmax vector 대신 one hot vector을 사용하게 되면 입력이 train time에서 보지 못한 입력 값을 주게 되면 대게 모델이 아무 기능을 못한다. 또, 실제의 vocabulary는 매우 크기 때문에, one hot 대신 softmax를 사용한다.실제로 one hot vectore를 sparse vectore 처리하는 방법도 있다. 큰 단어가 들어오면 연산량이 엄청나기 때문이다. 이런 모델의 경우 스텝마다 출력값이 존재한다. 이 출력값들의 loss를 계산해서 final loss를 얻는데 이를 backpropagation through time이라 한다.forward pass의 경우 전체 시퀀스가 끝날 때까지 출력값이 생긴다. 반대로 backward pass에서도 각 sequence를 통한 loss를 계산해야 한다. 이 경우 문장이 아주 길면 문제가 될 수 있다. gradient를 계산하려면 모든 것들을 다 거쳐야 하기 떄문이다.따라서 truncated backpropagation through time을 통해 backpropagation을 근사시키는 기법을 사용한다.비록 입력 sequence가 무한대일지라도 train time에 한 스텝을 100정도의 일정 단위로 자른다.100 스텝만 forward pass하고 이 서브 시퀀스의 loss를 계산한다. 그리고 gradient step을 진행한다. 이 과정을 반복한다.다음 batch의 forward pass를 계산할 때는 이전 hidden state를 이용한다. 하지만, gradient step은 현재 batch에서만 진행한다. Q. RNN이 Markov Assumption을 따르는가?그렇지 않다. RNN은 이전 hidden state를 계속해서 앞으로 가져오기 때문이다. hidden state를 조건으로 마르코비아 가정을 하고 있다면, hidden state가 미래를 예언하는 시퀀스의 우리가 필요로 하는 모든 것이기 때문에 시간을 통한 역전파를 시켜야 한다.RNN 코드RNN 코드min-char-rnn이라는 코드가 있다. 112 줄의 코드로 RNN 전체 과정을 구현했다. 여기서 truncated backporpagation을 사용한다.이 모델은 학습과정 동안에 시퀀스 데이터의 숨겨진 구조(latent structure)을 알아서 학습한다. hidden vector이 있고 이 vector이 계속 업데이트된다.vector이 어떤 의미를 하는 것인지 알아보기 위해 실험을 진행했다. hidden state는 아무 의미 없는 패턴이었지만, vector하나를 뽑은 다음 이 시퀀스를 다시 한번 forward시켜보았다.위의 각 색깔은 시퀀스를 읽는 동안 앞서 뽑은 hidden vector의 값을 의미한다. 대부분의 cell은 해석하기 어려웠다. 대부분은 다음에 어떤 문자가 와야할지 알아내기 위한 low level 정도였다.하지만, 따옴표를 찾는 벡터를 알아냈다. 처음에는 계속 off(파란색)이지만, 따옴표를 만나면 값이 켜진다(빨간색). 그리고는 따옴표가 닫히기 전까지 유지되다가 마침 따옴표를 만나면 다시 값이 꺼진다.또, 줄바꿈을 위해 현재 줄의 단어 갯수를 세는 듯해보이는 cell도 있었다. 처음에는 0으로 시작하다가 줄이 점점 길어지면 점점 빨간색으로 변했다. 줄바꿈이 이루어지면 다시 0으로 리셋되었다. 이를 통해, 모델이 언제 새로운 라인을 필요로 할지 지속적으로 추적하는 역할을 한 것이다.Image Captioning다시 이미지 모델로 돌아와, image captioning은 입력이 이미지이고 출력이 자연어로 된 caption이다.caption은 가변길이다. caption마다 다양한 시퀀스 길이를 가지고 있다. 여기에 RNN Language model이 아주 잘 어울린다.모델에 CNN이 들어있는데, CNN은 요약된 이미지 정보가 들어있는 vector를 출력한다. 이 vector은 RNN의 초기 step의 입력으로 들어간다. 그러면 RNN은 caption에 사용할 문자들을 하나씩 만들어낸다.그렇다면 test time에서는 어떻게 동작하는지 알아보자 입력 이미지를 받아 CNN의 입력으로 넣는다. 다만 softmax score을 사용하지 않고, 직전의 4096차원의 vector을 출력한다. 이 벡터는 전체 이미지 정보를 요약하는데 사용된다. RNN에 이 벡터를 초기 입력값으로 입력한다. 모델에게 “여기 이미지 정보가 있으니 이 조건에 맞는 문장을 만들어줘” 라고 시작해야 한다. 이전까지의 모델에서는 RNN 모델이 두 개의 가중치 행렬을 입력으로 받았다. 하나는 현재 스텝의 입력, 다른 하나는 이전 스텝의 hidden state였다. 이 둘을 조합하여 다음 hidden state를 얻었다. 하지만 이제는 이미지 정보도 더해줘야 한다. 가장 쉬운 방법이 세번째 가중치 행렬을 추가하는 것이다. 다음 hidden state를 계산할 때마다 모든 스텝에 이 이미지 정보를 추가한다. vocabulary의 모든 스코어들에 대한 분포를 계산한다. 여기서 vocabulary는 엄청 크다. 그 분포에서 샘플링하고 그 단어를 다음 스텝의 입력으로 다시 넣어준다. 모든 스텝이 종료되면 결국 한 문장이 만들어진다. train time에는 모든 caption의 종료지점에 end 토큰을 삽입한다. 네트워크가 학습하는 동안 스퀀스의 끝에 end토큰을 넣어야 한다는 것을 알려줘야 하기 때문이다. 학습이 끝나고 test time에는 모델이 문장 생성을 끝마치면 end 토큰을 샘플링한다. 이 모델은 완전한 supervised learning(지도학습)으로 학습시킨다. 이 모델을 학습시키기 위해서는 natural language model과 CNN을 동시에 backprop시켜야 한다.하지만, 위의 모델은 보지 못한 데이터에 대해서는 잘 동작하지 않는다.따라서 더 발전된 모델인 Attention이라는 모델을 개발했다. 이 모델은 caption을 생성할 때 이미지의 다양한 부분을 집중해서(attention) 볼 수 있다.간단하게만 보자면 CNN이 있는데, CNN으로 벡터 하나를 만드는게 아니라 각 벡터가 공간정보를 가지고 있는 grid of vector(LxD)을 만들어낸다.forward pass할 때, 매 스텝 vocabulary에서 샘플링을 할 때 모델이 이미지에서 보고싶은 위치에 대한 분포도 같이 만든다. 이미지의 각 위치에 대한 분포는 train time에 모델이 어느 위치를 봐야하는지에 대한 attention이라 할 수 있다.첫번째 hidden state(h0)는 이미지의 위치에 대한 분포를 계산한다. 그리고 분포(a1)를 다시 벡터 집합(LxD)와 연산하여 이미지 attention(z1)을 생성한다. 이 벡터는 RNN의 다음 스텝의 입력으로 들어간다.그러면 두 개의 출력(a2,d1)이 생성된다. d1은 vocabulary의 각 단어들의 분포, a2는 이미지 위치에 대한 분포이다.이 과정을 반복하면 매 스텝마다 값 두 개가 계속 만들어질 것이다.caption을 만들 때 마다 이미지 내에 다양한 곳들에 attention을 주는 것도 볼 수 있다.attention 모델로 caption을 생성하면 의미있는 부분에 집중되어 있다는 것을 알 수 있다.RNN + AttentionRNN+Attention 조합은 image captioning 뿐만 아니라 VQA(Visual Question Answering) 과 같은 곳에도 사용될 수 있다.VQA에서는 입력이 두 가지다. 하나는 이미지, 다른 하나는 이미지에 대한 질문이다.예를 들어, 첫번째 그림과 같이 Q: 트럭에 그려져 있는 멸종위기 동물은 무엇인가? 와 같이 Q와 이미지가 같이 입력으로 들어가야 한다. 그리고 모델은 네 개의 보기 중에서 정답을 맞춰야 한다.이 경우는 many to one의 경우에 해당된다. 모델은 자연어 문장(질문)을 입력으로 받아야 한다. 이는 RNN을 통해 RNN이 질문을 vector로 요약하고, 이미지 요약을 위해 CNN을 사용한다.간혹 VQA 문제를 풀기 위해 soft special attention 알고리즘을 적용하는 경우도 있다.이 예시를 보면 모델이 정답을 결정하기 위해 이미지에 대한 attention을 만들어낸다.Multilayer RNN이때까지는 RNN 레이어를 단일로 사용했다. hidden state도 하나 뿐이었다. 하지만 더 자주 보게 될 모델은 Multilayer RNN이다.3 layer RNN이 있다. 입력은 첫번째 RNN으로 들어가서 첫번째 hidden state를 만들어낸다. RNN 하나를 돌리면 hidden states 시퀀스(첫번째 초록색 줄)가 생긴다. 이렇게 만들어진 hidden state 시퀀스를 다른 RNN의 입력으로 넣어줄 수 있다. 그러면 RNN layer가 만들어내는 또 다른 hidden state 시퀀스가 생겨난다.이런식으로 RNN Layer을 쌓아올린다. 이렇게 하는 이유는 모델이 깊어질수록 다양한 문제들에 대해 성능이 좋아지기 때문이다. 대부분 2~4 layer RNN을 사용한다.RNN을 사용할 때 문제점이 있다.우리가 봐오던 일반적인(vanilla) RNN Cell이 있다. 입력은 현재 입력 x_t 와 이전 hidden state h_t-1이다. 이 두 입력을 쌓는다(stack).그리고 가중치 행렬 w와 행렬 곱연산을 하고 tanh를 씌워 다음 hidden state(h_t)를 만든다.이 아키텍처의 backward pass에서 gradient 계산과정은 어떻게 될까?우선 backward pass시 h_t에 대한 loss의 미분값을 얻는다. 그 다음 loss에 대한 h_t-1의 미분값을 계산하게 된다.backward pass는 빨간색 화살표를 따르게 될 것이다. 우선 gradient가 tanh gate를 탄 후 matmul gate를 통과한다.matmul gate의 backprop는 이 가중치 행렬 W을 곱하게 된다.이는 매번 cell을 하나 통과할 때마다 가중치 행렬의 일부를 곱하게 된다는 것을 의미한다.하지만 이는 우리가 h_0에 대한 gradient를 구하고자 한다면 결구 모든 RNN cell을 거쳐야 한다는 것이다. 즉, 아주 많은 가중치 행렬들이 개입하며 이는 안좋은 방법이다.가중치를 행렬이 아닌 스칼라로 생각해보자. 만약 곱해지는 값이 1보다 크면 점점 값이 커지고, 1보다 작다면 점점 작아져 0이 될 것이다. 이 두 상황이 일어나지 않으려면 곱해지는 값이 1인 경우밖에 없다. 실제로 1이 되기는 매우 어렵다.스칼라와 동일하게 행렬도 마찬가지다. 행렬의 특이값(singular value)가 엄청나게 크다면, 역시 h_0의 gradient는 엄청 커진다. 이를 exploding gradient problem이라고 한다. backprop시 레이어가 깊어질수록 gradient가 기하급수적으로 증가하는 현상이다.반대로 특이값이 1보다 작다면 기하급수적으로 작아져 vanishing gradient problem이 발생한다.그래서 사람들은 gradient clipping이라는 기법을 사용하곤 한다. gradient를 계산하고, gradient의 L2 norm이 임계값보다 큰 경우 gradient가 최대 임계값을 넘지 못하도록 조정해준다. 좋은 방법은 아니지만, 많은 사람들이 RNN학습에 활용한다.exploding을 막는데는 효과적이나, vanishing을 다루려면 좀 더 복잡한 RNN 아키텍처가 필요하다.Long Short Term Memory이는 LSTM(Long Short Term Memory)이 관한 것이다. LSTM은 vanishing과 exploding gradient 문제를 완화시키기 위해 디자인되었다. gradient가 잘 전달되도록 아키텍처 자체를 디자인 한 것이다.한 cell 당 1개의 hidden state만 가지던 vanilla RNN과 달리 Cell 당 두 개의 hidden state를 가진다. 하나는 Vanilla RNN에 있던 것과 유사한 개념인 h_t가 있고, cell state라고 하는 c_t라는 vector이 더 존재한다. c_t는 LSTM 내부에만 존재하여 밖에 노출되지 않는 변수이다.LSTM도 두 개의 입력(h_t-1, x_t)을 받는다. 그리고 i,f,o,g라는 4개의 gates를 계산한다. 이 gates를 c_t를 업데이트하는데 이용한다. 그리고 c_t로 다음 스텝의 hidden state를 업데이트한다.동작하는 방식으로는, 우선 이전 hidden state h_t와 입력 x_t를 입력으로 받는다. vanilla RNN의 경우 두 입력을 cancat하고 행렬곱 연산으로 hidden state를 직접 구했다.하지만, LSTM에서는 이전 hidden state와 입력을 받아 쌓는다. 그리고 네 개의 gate의 값을 계산하기 위한 커다란 가중치 행렬을 곱해준다. 각 gate의 출력은 hidden state의 크기와 동일하다. 다른 크기로 디자인할 수도 있긴 하다.gate의 출력 중 i는 input gate이다. i는 cell에서의 입력 x_t에 대한 가중치다. f는 forget gate로 이전 스텝의 cell의 정보를 얼마나 지울지에 대한 가중치다. o는 output gate로 cell state인 c_t를 얼마나 밖에 드러내 보일지에 대한 가중치다. g는 gate gate라 하는데, input cell을 얼마나 포함시킬지 결정하는 가중치다.중요한 것은 각 gate에 사용하는 non-linearity가 다 다르다는 점이다. input/forget/output gate는 sigmoid를 사용한다. gate의 값이 0~1 이라는 의미다. 반면 gate gate는 tanh를 사용한다. -1~+1 값을 갖기 위함이다.forget gate=0이면 이전 cell state를 잊는다는 것이고 1이라면 이전 cell state를 기억한다는 것이다.i=0 or 1이므로 이 cell state를 사용하고 싶으면 1, 쓰지 않으려면 0이 된다.c_t의 수식을 보면 이전 cell state(c_t-1)을 계속 기억할지 말지를 결정한다(f*c_t-1). 즉 cell state의 각 요소는 줄었다 늘었다 할 수 있다.cell state를 계산한 후에는 hidden state를 업데이트할 차례다. h_t는 실제로 밖에 보여지는 값이다. 그렇기에 cell state는 counters의 개념으로 최대 1 또는 -1씩 세는 것이다. 이 값이 tanh를 통과하여 output gate와 곱해진다.output gate 또한 sigmoid에서 나온 값으로 0~1 값을 가진다. output gate는 각 스텝에서 다음 hidden state를 계산할 때 cell state를 얼마나 노출시킬지를 결정한다.이는 LSTM 동작 다이어그램이다. 우선 cell state c_t-1과 hidden state h_t-1을 입력으로 받는다. 현재 입력 x_t도 있다. 이전 hidden state h_t-1과 현재 입력 x_t를 쌓는다. 가중치 행렬 w와 곱해서 4개의 gates를 만든다. forget gate를 이전 cell state c_t-1 과 곱한다. input/gate gate를 곱한 후 cell state 와 곱해서 다음 cell state를 만든다. c_t는 tanh를 거쳐 output gate와 곱해져 다음 hidden state(h_t)를 만든다.그렇다면 backward pass는 어떨까?앞서 배운 Vanilla RNN의 경우에는 가중치 행렬 W가 계속해서 곱해지는 문제가 있었다. 하지만 LSTM에서는 cell state의 gradient를 계산해주는 backward경로를 살펴보면 addition operation(+)가 있다. additino에서는 gradient가 그저 복사된다.따라서 gradient는 upstream gradient * forget gate 이다.이 특성으로 인해 Vanilla RNN보다 좋은 점이 두 가지 있다.첫번째로는 forget gate와 곱해지는 연산이 matrix multiple 이 아닌 element-wise이다.두번째로는 element wise multiplication을 통해 매 스텝 다른 값의 forget gate와 곱해질 수 있다는 것이다. 이를 통해 exploding/vanishing gradient 문제를 해결할 수 있다. 앞서 vaniila RNN의 경우는 동일한 가중치 행렬(h_t)만 계속 곱했다. 이때문에, exploding/vanishing 문제가 발생한 것이다.그리고, forget gate는 sigmoid의 출력이므로 0~1이다. 이는 forget gate를 반복적으로 곱한다고 했을 때 더 좋은 수치적 특성을 보일 수 있다.gradient 계산을 쭉 보면, 모델의 종단인 loss에서 가장 처음 cell state(c0)까지 방해가 별로 없다. Q. W를 업데이트해야 할텐데, W에 대한 gradient는 어떻게 되나?가중치 w에 대한 local gradient는 해당 스텝에서의 cell/hidden state로부터 전달된다. vanilla RNN의 경우 각 스텝의 가중치 행렬 w들이 서로 영향을 미쳤다. 하지만 LSTM의 경우 아주 긴 시퀀스가 있다고 했을때, w의 local gradient는 cell/hidden state로부터 흘러온다, lstm에서는 cell state가 gradient를 잘 전달해주기 때문에 w에 대한 local gradient도 훨씬 더 잘 전달된다. Q. non-linearity로 인한 vinishing gradient 문제는 어떻게 되나?f의 경우 출력이 0~1이니 항상 1보다 작아져서 gradient가 점점 감소할 수 있다. 이를 해결하기 위해 f의 bias를 양수로 초기와시키는 방법이 있다. 이를 위해 학습 초기에 forget gate의 값을 1에 가깝게 설정한다. 그렇게 학습이 진행되면 f의 bias가 적절한 자기 자리를 찾아간다. 물론 LSTM에서도 vanishing gradient문제가 있다. 하지만 vanilla RNN보다 덜 민감한 첫번째 이유는 매 스텝마다 f가 변하기 때문이고, 두번째는 LSTM에서는 element wise multiplication을 수행하기 때문이다.LSTM을 보면 ResNet과 유사하게 생겼다. ResNet에서도 backward pass에서 고속도로 역할인 identity connection이 유용했다.LSTM에서도 Cell state의 element wise multiple이 gradient 고속도로 역할을 수행한다.GRULSTM 다음으로 유명한 것이 GRU(Gated Recurrent Unit)이다. vanishing gradient를 해결하기 위한 element wise mult gate가 있다.Reference http://cs231n.stanford.edu/2017/syllabus.html https://velog.io/@cha-suyeon/CS231n-Lecture-10-%EA%B0%95%EC%9D%98-%EC%A0%95%EB%A6%AC" }, { "title": "CS231N chapter 9 - CNN Architectures", "url": "/posts/cs231n9/", "categories": "Classlog, CS231N", "tags": "CS231N, CNN-architecture", "date": "2021-09-30 13:00:00 +0900", "snippet": " 8강 리뷰 1) Tensorflow 2) Pytorch 3) static Vs dynamic graphCNN Architecture크게 AlexNet, VGGNet, GoogLeNet, ResNet 등이 있다.LeNetLeNet은 산업에 성공적으로 적용된 최초의 ConvNet이다.이미지를 입력으로 받아 stride = 1인 5x5 필터를 거치고 몇 개의 conv Layer과 pooling layer를 거친다. 끝단에 FC layer가 붙는다.간단한 모델이지만, 엄청난 성공을 거둔 모델이다.AlexNet2012년에 AlexNet이 나왔다. 최초의 Large scale CNN이다. ImageNet을 아주 잘 수행했다.LeNet와 유사하나 layer 수가 더 많아졌다.AlexNet의 전체 layer이다.AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ))기본적으로 conv- pool - normalization이 2번 반복되고, conv layer가 조금 더 붙고 (conv 3,4,5…), 그 뒤에 pooling layer(Maxpooling)가 있다. 마지막에는 FC layer가 몇 개 붙어있다.따라서, 5개의 conv layer과 3개의 FC layer로 구성되어 있다.기본적으로 imageNet으로 학습시킨 모델이므로 입력 크기는 227x227x3이다.(0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))첫 레이어인 conv1은 96개의 stride = 4인 11x11 필터가 존재한다. 그렇다면 출력은 (227-11)/4 + 1 = 55 x 55 x 96의 activiation map이 생성된다.또한, 입력 depth(channel)는 3이다. 따라서 첫번째 layer는 11x11x3 필터가 96개 있으므로, (11x11x3)x96 개의 파라미터를 가지고 있다.(2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)두번째 layer는 pooling layer이다.여기에는 stride = 2 인 3x3 필터가 있다. 따라서 이 레이어의 출력의 크기는 27x27x96이다. pooling layer은 depth가 변하지 않는다.pooling layer에는 파라미터가 존재하지 않으므로 0개다. 파라미터는 우리가 학습시키는 가중치다. conv layer에는 학습할 수 있는 가중치가 있지만, pooling layer는 학습시킬 파라미터가 존재하지 않는다.모든 레이어의 파라미터 사이즈와 갯수를 계산해보자. INPUT : [227 x 227 x 3] CONV1 : 96개의 11x11 filter at stride 4, padding 0 -&amp;gt; (227 - 11)/4 + 1 = 55 -&amp;gt; [55 x 55 x 96], p = (11x11x3)x96 MAX POOL1 : 3x3 filters at stride 2 -&amp;gt; (55 - 3)/2 + 1 = 27 -&amp;gt; [27 x 27 x 96], p=0 NORM1 : [27 x 27 x 96] CONV2 : 256 5x5 filters at stride 1, pad 2 -&amp;gt; (27 + 2*2 - 5)/1 + 1 = 27 -&amp;gt; [27 x 27 x 256], p=(5x5x96)x256 MAX POOL2 : 3x3 filters at stride 2 -&amp;gt; (27 - 3)/2 + 1 = 13 -&amp;gt; [13 x 13 x 256], p=0 NORM2 : [13 x 13 x 256] CONV3 : 384 3x3 filters at stride 1, pad 1 -&amp;gt; (13 + 2*1 - 3)/1 + 1 -&amp;gt; 13 -&amp;gt; [13 x 13 x 384], p=(3x3x256)x384 CONV4 : 384 3x3 filters at stride 1, pad 1 -&amp;gt; (13 + 2*1 - 3)/1 + 1 -&amp;gt; 13 -&amp;gt; [13 x 13 x 384], p=(3x3x384)x384 CONV5 : 256 3x3 filters at stride 1, pad 1 -&amp;gt; (13 + 2*1 - 3)/1 + 1 -&amp;gt; 13 -&amp;gt; [13 x 13 x 256], p=(3x3x384)x256 MAX POOL3 : 3x3 filters at stride 2 -&amp;gt; (13 -3)/2 + 1 = 6 -&amp;gt; [6 x 6 x 256], p=0 FC1 : p=4096 FC2 : p=4096 FC3 : p=1000사실 이것은 summary 함수를 통해 확인해볼 수 있다.---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 56, 56] 23,296 ReLU-2 [-1, 64, 56, 56] 0 MaxPool2d-3 [-1, 64, 27, 27] 0 Conv2d-4 [-1, 192, 27, 27] 307,392 ReLU-5 [-1, 192, 27, 27] 0 MaxPool2d-6 [-1, 192, 13, 13] 0 Conv2d-7 [-1, 384, 13, 13] 663,936 ReLU-8 [-1, 384, 13, 13] 0 Conv2d-9 [-1, 256, 13, 13] 884,992 ReLU-10 [-1, 256, 13, 13] 0 Conv2d-11 [-1, 256, 13, 13] 590,080 ReLU-12 [-1, 256, 13, 13] 0 MaxPool2d-13 [-1, 256, 6, 6] 0AdaptiveAvgPool2d-14 [-1, 256, 6, 6] 0 Dropout-15 [-1, 9216] 0 Linear-16 [-1, 4096] 37,752,832 ReLU-17 [-1, 4096] 0 Dropout-18 [-1, 4096] 0 Linear-19 [-1, 4096] 16,781,312 ReLU-20 [-1, 4096] 0 Linear-21 [-1, 1000] 4,097,000================================================================Total params: 61,100,840Trainable params: 61,100,840Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.59Forward/backward pass size (MB): 8.49Params size (MB): 233.08Estimated Total Size (MB): 242.16구조를 자세히보면 모델 끝에는 3개의 FC layer가 있다. 2개의 fc layer은 4096 node를 가지고, 마지막 layer은 FC layer은 softmax를 통과하여 1000개의 imageNet 클래스를 분류한다.ReLU, dropout을 사용하였고, batch size는 128이다. optimization으로는 SGD momentum을 사용했다. 초기의 lr은 1e-2이었다가 val accuracy가 올라가지 않는 지점에서는 학습이 종료되는 시점까지 1e-10까지 줄였다. weight decay를 사용했고, 마지막에는 모델 앙상블로 성능을 향상시켰다.이 당시에는 Batch Normalization이 존재하지 않았기 때문에, flip, jitter, colo norm 등의 data augmentation을 많이 사용했다.다른 Network와의 차이점이라 하면 모델이 두개로 나뉘어져 서로 교차한다는 점이다. 그 당시의 GPU의 메모리가 3GB뿐이었기에, 분산시켜 넣은 것이다. 즉, 각 GPU에의 Depth는 48이다.conv1,2,4,5 는 같은 gpu내에 있는 feature map만 사용하기 때문에 각 gpu는 48개의 feature map만 사용한다.그러나 conv3, FC6,7,8 은 이전 계층의 전체 feature map과 연결되어 있다. 이 layer 들은 gpu간의 통신을 하기 떄문에 전체 depth를 전부 가져올 수 있는 것이다.alexnet은 2012년 image classification benchmark에서 우승한 모델이다. 최초의 CNN 기반 우승 모델이고, CNN을 보편화시킨 모델이라 볼 수 있다.ZFNetZFNet은 2013년 imageNet challange에서 우승한 모델이다.alexnet과 레이어 수가 같고, 기본적인 구조가 같다. stride size, 필터 수 와 같은 하이퍼파라미터를 조절하여 개선한 모델이다.VGGNet2014년에는 엄청난 변화가 있었다. architecture도 변하고, 성능도 훨씬 향상되었다. 가장큰 차이점은 네트워크가 훨씬 깊어진 것이다. 이전의 8개 layer에서 19개의 layer와 22개의 layer로 늘어났다.2014년의 우승 모델은 GoogLeNet이고 VGGNet이 2등이었다. 하지만, 다른 트랙에서 VGGNet이 1위를 차지했다.VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (17): ReLU(inplace=True) (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (24): ReLU(inplace=True) (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (26): ReLU(inplace=True) (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (31): ReLU(inplace=True) (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (33): ReLU(inplace=True) (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (35): ReLU(inplace=True) (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) ))우선 VGGNet을 살펴보자면, 네트워크가 훨씬 더 깊어졌고, 더 작은 필터를 사용했다. 16~19개의 layer을 사용하고, 3x3필터만 사용했다. 주기적으로 pooling을 수행하였다. vggnet은 아주 간단하면서도 고급진 아키텍처이다.3x3 필터를 사용한 이유는 필터가 작으면 파라미터의 수가 작아진다. 따라서 큰 필터에 비해 layer를 조금 더 많이 쌓을 수 있다. 즉, depth를 더 키울 수 있는 것이다.그렇다면, stride = 1인 3x3 필터를 3 개 쌓을 때의 receptive field는 어떻게 될까?receptive field는 필터가 한번에 볼 수 있는 입력의 영역을 의미한다.첫번째 layer의 receptive field는 3x3이다. 두번째 layer의 경우 첫번째의 layer 출력의 3x3만큼 보고, 3x3중 각 사이드는 한 픽셀씩 더 볼 수 있다. 따라서 두번째 layer는 실제로 5x5 receptive field를 가진다. 세번째 layer는 두번째 layer의 3x3을 보게 되므로 입력 layer의 7x7을 보게 된다.따라서, 3x3 필터를 3개 쌓는 것은 하나의 7x7 필터를 사용하는 것과 같다.?????????실질적인 receptive field를 동일하게 가지면서 파라미터 개수를 줄일 수 있고, non-linearity를 늘릴 수 있기 때문에 3x3 필터가 이점을 가질 수 있는 것이다. 3x3 필터의 파라미터 수는 depth가 3이라는 가정하에, (filter 크기) x feature map depth = (3X3X3) X 3 = 81, 3개를 쌓아야 하므로 81x3 = 243 7x7 의 경우 7x7x3x3x1 = 441따라서 3x3필터를 3개 쓰는 것이 7x7 필터 1개 쓰는 것보다 개수가 적다.전체 구조와 파라미터 수는 다음과 같다.---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 227, 227] 1,792 ReLU-2 [-1, 64, 227, 227] 0 Conv2d-3 [-1, 64, 227, 227] 36,928 ReLU-4 [-1, 64, 227, 227] 0 MaxPool2d-5 [-1, 64, 113, 113] 0 Conv2d-6 [-1, 128, 113, 113] 73,856 ReLU-7 [-1, 128, 113, 113] 0 Conv2d-8 [-1, 128, 113, 113] 147,584 ReLU-9 [-1, 128, 113, 113] 0 MaxPool2d-10 [-1, 128, 56, 56] 0 Conv2d-11 [-1, 256, 56, 56] 295,168 ReLU-12 [-1, 256, 56, 56] 0 Conv2d-13 [-1, 256, 56, 56] 590,080 ReLU-14 [-1, 256, 56, 56] 0 Conv2d-15 [-1, 256, 56, 56] 590,080 ReLU-16 [-1, 256, 56, 56] 0 Conv2d-17 [-1, 256, 56, 56] 590,080 ReLU-18 [-1, 256, 56, 56] 0 MaxPool2d-19 [-1, 256, 28, 28] 0 Conv2d-20 [-1, 512, 28, 28] 1,180,160 ReLU-21 [-1, 512, 28, 28] 0 Conv2d-22 [-1, 512, 28, 28] 2,359,808 ReLU-23 [-1, 512, 28, 28] 0 Conv2d-24 [-1, 512, 28, 28] 2,359,808 ReLU-25 [-1, 512, 28, 28] 0 Conv2d-26 [-1, 512, 28, 28] 2,359,808 ReLU-27 [-1, 512, 28, 28] 0 MaxPool2d-28 [-1, 512, 14, 14] 0 Conv2d-29 [-1, 512, 14, 14] 2,359,808 ReLU-30 [-1, 512, 14, 14] 0 Conv2d-31 [-1, 512, 14, 14] 2,359,808 ReLU-32 [-1, 512, 14, 14] 0 Conv2d-33 [-1, 512, 14, 14] 2,359,808 ReLU-34 [-1, 512, 14, 14] 0 Conv2d-35 [-1, 512, 14, 14] 2,359,808 ReLU-36 [-1, 512, 14, 14] 0 MaxPool2d-37 [-1, 512, 7, 7] 0AdaptiveAvgPool2d-38 [-1, 512, 7, 7] 0 Linear-39 [-1, 4096] 102,764,544 ReLU-40 [-1, 4096] 0 Dropout-41 [-1, 4096] 0 Linear-42 [-1, 4096] 16,781,312 ReLU-43 [-1, 4096] 0 Dropout-44 [-1, 4096] 0 Linear-45 [-1, 1000] 4,097,000================================================================Total params: 143,667,240Trainable params: 143,667,240Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.59Forward/backward pass size (MB): 242.32Params size (MB): 548.05Estimated Total Size (MB): 790.96----------------------------------------------------------------VGG19를 불러왔다. forward/backward pass 시 필요한 메모리양이 242MB로 다른 아키텍처에 비해 많은 편이다.전체 파라미터의 경우 alexnet보다 훨씬 많다. 가장 많은 파라미터를 사용하는 layer는 FC layer이다. FC layer가 fully connected(dense connection)되었기 때문이다. 최근 네트워크의 경우 파라미터를 줄이기 위해 FC layer를 없애기도 한다.동일하게 활성함수로 relu를 사용하고, fc layer에서 dropout을 사용했다. Q. 네트워크가 깊어질수록 layer의 필터 개수를 늘려야 하는가?(channel depth를 늘려야 하는지)=&amp;gt; 필수는 아니지만 depth를 늘리는 경우가 많은데, 계산량을 일정하게 유지시키기 위해서다. downsampling을 하면 Width와 Height가 작아져서 depth를 늘려도 부담이 적어진다.모델 성능을 향상시키기 위해 앙상블 기법을 사용했다.layer를 보면 conv3-64 라는 것이 있는데, 이는 64개 필터를 가진 3x3 conv 필터라는 뜻이다.마지막 FC7은 1000 class를 분류하기 위한 layer로 아주 좋은 feature represetation(특징 표현)을 가지고 있다. 다른 task에서도 능력이 뛰어나기에 vgg가 많이 사용된다.GoogLeNetGoogLeNet( (conv1): BasicConv2d( (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (conv2): BasicConv2d( (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (conv3): BasicConv2d( (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (inception3a): Inception( (branch1): BasicConv2d( (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (branch2): Sequential( (0): BasicConv2d( (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicConv2d( (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) ) (branch3): Sequential( (0): BasicConv2d( (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicConv2d( (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) ) (branch4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True) (1): BasicConv2d( (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (inception3b): Inception( (branch1): BasicConv2d( (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (branch2): Sequential( (0): BasicConv2d( (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicConv2d( (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) ) (branch3): Sequential( (0): BasicConv2d( (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicConv2d( (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) ) (branch4): Sequential( (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True) (1): BasicConv2d( (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True) ) ) )GoogLeNet은 총 22개의 layer을 가지고 있다.GoogLeNet에서 가장 중요한 것은 효율적인 계산에 관한 그들만의 특별한 관점이 있고, 높은 계산량을 아주 효율적으로 수행하도록 네트워크가 디자인되어 있다.GoogLeNet은 inception module을 여러 개 쌓아져 있다. 그리고 FC layer가 없다. 파라미터를 줄이기 위해서이다. 전체 파라미터가 6M 정도이다. 60M인 alexNet보다 훨씬 적은 양이다. 그럼에도 더욱 깊다.Inception module구글은 좋은 로컬 네트워크 typology를 디자인하고자 했고, network-in-network라는 개념으로 이를 구현했다.여기서 local network를 inceptino module이라 한다. 내부에는 동일한 입력을 받는 다양한 필터가 병렬로 존재하고, 이 layer의 입력을 받아서 다양한 conv 연산을 수행한다.중간의 그림을 보면 1x1, 3x3, 5x5 conv에 3x3 pooling도 있다. 각 layer에서 각각의 출력 값이 나오는데, 이 출력은 모두 depth방향으로 합쳐진다.(concatenate) 이런 방식으로 합치면 하나의 tensor로 출력이 결정되고, 이 하나의 출력을 다음 layer로 전달하는 것이다.지금까지는 다양한 연산을 수행하고 이를 하나로 합쳐준다는 단순한 방식(naive way)를 살펴보았다. naive way의 문제점계산 비용이 크다.128개의 1x1 필터, 192개의 3x3필터, 96개의 5x5 필터가 있다고 하자. 그리고 입력 크기는 28x28x256이라고 하자.stride를 조절하여 입/출력 간의 spatial dimension(receptive field)를 유지시킨다.이 경우 1x1 conv필터 하나당 28x28 feature map을 생성하게 되기 때문에, 1x1x128 conv의 출력은 28x28x128이 된다. 1x1 conv의 경우 입력 depth가 256이므로 동일하게 256인데, 128개의 필터 하나당 28x28 feature map을 생성하게 된다.똑같이 3x3x192 conv를 하면 28x28x192 가, 5x5x96 conv를 하면 28x28x96이 출력될 것이다.conv는 입력 depth인 256만 가지고 내적을 한다고 볼 수 있기 때문이다. 즉, conv는 (입력 데이터 크기 W x H) x (filter 개수) 의 출력값을 가진다.이 때, spatial dimention을 유지하기 위해 zero padding한다.3x3 pooling layer의 경우 입력 그대로 28x28x256으로 나온다.이를 다 합치면 28x28x(128+192+96+256) = 28x28x672 의 출력값이 된다. 결론적으로 dimention은 변하지 않았지만, depth가 엄청나게 커졌다.1x1 conv에서의 전체 연산량은 28x28x128x256으로 엄청 많다. 3x3/5x5 도 마찬가지일 것이다. 특히 pooling layer에서는 더더욱 많다. 총 연산을 합치면 850M 정도가 된다.레이어를 거칠수록 점점 더 늘어나게 된다. 이 문제를 해결하기 위해서 bottleneck layer를 이용한다.bottleneck layer는 conv연산을 시작하기 전 입력을 더 낮은 차원으로 보내는 방법이다.1x1 conv를 살펴보자. 1x1 conv는 내적을 수행한다. 그러면서 filter 개수를 활용해 depth만 줄일 수 있다. 입력의 depth를 더 낮은 차원으로 만드는 것이다.이를 input feature map들 간의 선형결합(linear combination) 이라고 하기도 한다.따라서, 3x3/5x5 conv 이전에 1x1 conv 64개를 추가한다. 또, pooling layer 이후에도 1x1 conv를 추가한다.1x1 conv를 bottleneck layer의 역할로 추가되는 것이다.이전과 비교했을 때, 1x1 conv 64 =&amp;gt; 28x28x64x1x1x256 1x1 conv 64 =&amp;gt; 28x28x64x1x1x256 1x1 conv 128 =&amp;gt; 28x28x128x1x1x256 3x3 conv 192 =&amp;gt; 28x28x192x3x3x64 5x5 conv 96 =&amp;gt; 28x28x96x5x5x64 1x1 conv 64 =&amp;gt; 28x28x64x1x1x256=&amp;gt; total 350M « 850M전과 확연히 차이나게 줄어들었다.자세하게 보자면, 입력은 28x28x256이고, 3x3 conv 앞쪽의 1x1 conv 출력은 28x28x64이다.따라서 28x28x64로 전의 28x28x256보다 줄었다.정리하면, 1x1 conv를 추가함으로써 계산량을 조절할 수 있다1x1 conv를 추가한다고 해서 데이터 변형이 일어나지 않는다.GoogLeNet의 앞단(초기 6개 layer)에는 일반적인 네트워크 구조다. 이때는 conv-pool를 반복한다.이후 inception module을 쌓는데, 모두 조금씩 다르다. 그리고 마지막에는 classifier 결과를 출력한다.아래 부분을 보면 추가적으로 줄기가 뻗어져 있다. 이들은 보조분류기(auxiliary classifier)이다. 이것들은 작은 미니 네트워크다. average pooling과 1x1 conv가 있고 FC layer도 몇개 붙고, softmax로 1000개의 imageNet class를 구분한다.이 부분에서도 loss를 계산한다. 네트워크 끝에서 뿐만 아니라 이 두곳에서도 계산하는 이유는 네트워크가 꽤 깊기 때문이다.보조분류기를 중간 layer에 달아주면 추가적인 gradient를 얻을 수 있고, 중간 layer의 학습도 도울 수 있다.googleNet 학습 시 보조 분류기에서 나온 loss의 모든 합의 평균을 계산한다.또, 보조분류기에서 추가적인 gradient를 얻는 이유는 네트워크가 엄청 깊은 경우 gradient 신호가 점점 작아지게 되고 결국 0에 가깝게 된다. 그래서 보조분류기를 이용해서 추가적인 gradient 신호를 추가한다.ResNetresnet은 152개의 layer를 가진 엄청나게 깊은 네트워크다. ResNet은 residual connections 라는 방식을 사용한다.맨 처음 생각한 방법은 conv-pool layer을 계속해서 깊게 쌓으면 성능이 좋아질까 라는 것이었다.하지만 성능은 좋이지지 않았다.네트워크가 깊어지게 되면 어떤 일이 발생할까? 20 layer Vs 56 layers기본적으로 네트워크가 깊어지면 파라미터가 엄청 많아지기 때문에 overfit이 발생할 것이다. overfit이 발생하면 test error는 높더라도, training error는 아주 낮아야 정상이다.하지만 56 layer을 보면 training error가 20 layer보다 안좋다.따라서 더 깊은 모델임에도 test 성능이 낮은 이유는 overfitting 문제가 아니라는 것을 알 수 있다.여기서 ResNet 저자들은 더 깊은 모델 학습 시 최적화(optimization) 가 더 어려워진다는 가설을 세웠다.모델이 더 깊다면 적어도 더 얕은 모델만큼은 성능이 나와야 정상아닌가 생각할 것이다.그래서 해결책으로 일단 더 얕은 모델의 가중치를 깊은 모델의 일부 레이어에 복사했다. 나머지 레이어는 identity mapping을 했다. identity mapping이란 input을 output으로 내보내는 것을 말한다.이와 같이 얕은 모델을 복사해왔기 때문에, 비슷한 성능이 나올 수 있다.이 방식을 녹여 모델을 만들고자 했다.ResNet의 아이디어는 단순히 layer를 쌓는 방법(direct mapping)이 아니라 residual mapping의 방법이었다.레이어가 직접 H(x)를 학습하기보다 이와 같이 H(x) - x를 학습할 수 있도록 만들어준다. 이를 위해 skip connection를 도입한다.오른쪽 고리 모양을 보면, skip connection은 가중치가 없으며 입력을 indentity mapping을 통해 그대로 출력단으로 보낸다.실제 레이어는 변화량(delta)만 학습하면 된다. 입력 x에 대한 잔차(residual)이라 할 수 있다. 즉, 원래의 출력인 H(x) 대신 입력 x를 그대로 가져와 H(x)와 x의 차이(변화량)인 F(x)만을 학습시킨다.그래서, 최종 출력 값은 input X + 변화량(residual) 이다.이 방법을 사용하면 학습이 더 쉬워진다. 가령 input = output 인 상황이라면 F(x)(residual) = 0 이므로 모든 가중치를 0 으로 만들어주면 된다.네트워크는 residual만 학습하면 되기 때문에 한층 더 쉬워졌다. 전체 full mapping을 학습하는 대신, residual mapping 만 학습하는 것이고, 출력 값도 결국엔 입력 x와 가까운 값이기 때문이다.이때, layer의 출력과 skip connection의 출력은 같은 차원이다. 같지 않더라도 depth-wise padding으로 차원을 맞춰줄 수 있다.사실 이것들은 ResNet 저자의 가설이었기에, 입증된 바는 없다. 그러나 실제로 ResNet을 사용할 때 성능이 더 좋아지기도 한다.전체 ResNet 구조를 나타낸 것이다. 아래는 직접 resnet을 불러온 결과를 가져와봤다.하나의 residual blocks는 두개의 3x3 conv layer로 이루어져 있다. 이렇게 구성해야 잘 동작한다. 이 Residual block을 아주 깊게 쌓는다.또한, 주기적으로 필터를 두배씩 늘리고, stride 2를 이용하여 downsampling한다.그리고 맨 처음에 7x7 conv-64를 붙였고, 네트워크 끝에는 FC layer가 없다. 그 대신 global average pooling layer를 사용한다. 맨 마지막에 존재하는 FC 1000 은 클래스 분류를 위한 노드이다.GAP(global average pooling layer)는 하나의 map 전체를 average pooling 하는 것을 말한다.ResNet 중에서도 가장 많이 사용되고 있는 모델은 resnet101 이다. 101은 모델의 depth(layer)를 말한다. 101말고도 34, 50 100, 152 등이 있다.depth가 50이상일 때 1x1 conv를 도입하여 depth를 줄이는 bottleneck layer를 도입한다. 이는 GoogLeNet에서 사용한 방법과 유사하다.bottleneck layer을 통해 3x3 conv의 계산량을 줄인다. 그리고 뒤에 다시 1x1 conv를 추가하여 depth를 다시 원래대로 늘린다.정리하자면 모든 conv layer 다음에 batch norm 을 사용한다. 초기화로는 Xavier을 사용하는데, 추가적으로 scaling factor(2로 나눔)를 더 수행한다. 이 방법은 SGD + momentum 에서 좋은 초기화 성능을 보인다. learning rate는 validation error가 줄어들지 않는 시점에서 조금씩 줄여주며 조절한다. minibatch size = 256 weight dacay 사용, 1e-5 dropout 사용 xResNet의 top-5 error은 3.6%로 인간의 성능(5%)보다 뛰어나다고 한다.아래는 ResNet 아키텍처를 직접 가져와봤다.&#39;&#39;&#39; ResNet101 structure &#39;&#39;&#39;ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) )ResNet과 관련된 최근 연구들 Identity Mapping in Deep Residual Networks이 논문에서는 ResNet의 블록 디자인을 향상시켰다. 그 방법으로 ResNet block path를 조절했다.새로운 구조는 direct path 간격을 늘려서 정보들이 앞으로 더욱 더 잘 전달되고 backprop도 더 잘 될 수 있도록 한 것이다. Wide Residual Networks기존의 ResNet 논문은 깊게 쌓는 것에 집중했지만, 사실 중요한 것은 depth가 아니라 residual이라 주장한 논문이다.residual connection이 있다면 네트워크가 굳이 더 깊게 필요가 없다고 주장한다.그래서 깊이 대신 residual block을 더 넓게 만들었다. 즉, conv layer의 필터를 더 많이 추가한 것이다.원래 F개의 filter가 존재했다면 여기서는 F * k개의 필터를 사용한다.그렇게 하여 50 layer만 있어도 152 layer의 resnet보다 성능이 좋다는 것을 입증했다.이 방법의 또 다른 이점은 계산 효율이 증가한 것이다. 왜냐하면 병렬화가 더 잘되기 때문이다.네트워크의 depth를 늘리는 것은 sequential의 증가, 즉 위아래로 늘어나는 것이라면, conv 필터를 늘리는 것은 width가 넓어지는 것이다. ResNeXtResNet의 저자의 논문으로 residual block의 width를 계속 연구한다. filter 수를 늘리는 것이다. 각 redisual block 내에 다중 병렬 경로를 추가한다. 이들은 pathway(경로)의 총 합을 cardinality(관계수)라고 불렀다.하나의 bottleneck ResNet block은 비교적 작지만 이런 얇은 block들이 병렬로 여러 개를 묶었다.layer를 병렬로 묶는다는 것에서 inception module과도 연관이 있다. Stochastic Depth이 논문의 주제는 depth이다. 네트워크가 깊어질수록 vanishing gradient가 발생한다. 뒤로 전달될수록 점점 grdient가 작아진다.그래서 train time에서 layer의 일부를 제거하여 short network를 만들어 training 능력을 약간 향상시키는 것이다.그 방법으로 일부 네트워크를 골라 identity connection으로 만들어버린다. dropout과 유사하다고 볼 수 있다. 동일하게 test time에서는 full layer를 사용한다.이제 좀 다른 아키텍처들을 살펴보고자 한다.Other Network architecture Network in Network (NiN)기본 아이디어는 MLP conv layer이다. 네트워크 안에 작은 네트워크를 삽입하는 것이다. 각 conv layer 안에 (MLP(Multi-layer perceptron) = FC layer)을 쌓는다.단순히 conv filter만 사용하지 않고, 좀더 복잡한 계층을 만들어서 feature map을 얻어보자는 아이디어다.NIN은 기본적으로 FC layer을 사용한다. 이를 1x1 conv layer라고도 한다.GoogLeNet이나 ResNet보다 먼저 bottleneck 개념을 정립했다. FractalNet이 논문에서는 residual connection이 필요없다고 주장한다. 그래서 FractalNet에서는 residual connection이 전혀 없다.그들은 오른쪽 그림과 같이 fractal(차원분열도형)한 모습이다.FactalNet에서는 shallow(얕은)/deep 경로를 출력에 모두 연결한다. 다양한 경로가 존재하지만 train time에는 dropout과 같이 일부 경로만을 이용해서 train한다. DenseNetdensely connected convolutional network, 즉 빼곡하게 연결된 convolutional network 이다.dense block을 사용하는데, 한 layer가 그 layer 하위의 모든 layer와 연결된다. 네트워크의 입력 이미지가 모든 layer의 입력으로 들어가는 것이다. 그 모든 layer의 출력이 각 layer의 출력과 concat(합침)된다.이 dense block을 통해 vanishing gradient 문제를 완화시킬 수 있다고 주장한다.그리고 dense connection은 feature을 더 잘 전달하고 더 잘 사용할 수 있게 해준다고 한다. 각 layer의 출력을 다른 layer에서도 여러번 사용하기 때문이다. SqueezeNet여기서는 fire module이라는 것을 도입했다. squeeze layer는 1x1 filter들로 구성되고, 출력 값이 1x1,3x3 filter들로 구성되는 expand layer의 입력이 된다. squeezeNet은 AlexNet 과 비슷한 accuracy를 보이지만 파라미터는 50배 더 적다.Comparing Model Complexity지금까지 배운 모델이거나 조금 변형된 모델들이다.incetion을 보면 V2,V3 등이 있는데 가장 좋은 모델은 당연히 V4이다. v4는 resnet + inception 모델이다.오른쪽 그래프를 보면 계산 복잡성이 추가하여 볼 수 있다. y축은 top-1 accuracy로 높을수록 좋다. x축은 연산량으로 오른쪽일수록 연산량이 많다. 원의 크기는 메모리 사용량이다.초록색 원은 VGGNet이다. 가장 효율성이 작다. 메모리는 크면서 계산량이 많다. 성능은 나쁘지 않다.파란색 원은 GoogLeNet인데, 가장 효율적인 네트워크다. 거의 왼쪽에 있으며 메모리 사용량도 작다.초기 AlexNet 모델은 accuracy가 낮다. 계산량은 작지만, 메모리 사용량이 비효율적이다.ResNet의 경우 적당한 효율성을 가지고 있다. 메모리 사용량과 계산량은 중간이지만, accuracy가 최상위에 있다.왼쪽 그래프틑 forward pass 시간이다. 단위는 ms인데, VGG가 제일 오래걸린다. 200ms, 즉 초당 5정도 처리한다.오른쪽은 전력소모량인데, 이는 논문을 참고하길 바란다.https://arxiv.org/abs/1605.07678Reference http://cs231n.stanford.edu/2017/syllabus.html https://velog.io/@guide333/%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-CS231n-9%EA%B0%95-Training-Neural-Networks-Part-2-2 https://velog.io/@cha-suyeon/CS231n-Lecture-9-%EA%B0%95%EC%9D%98-%EC%9A%94%EC%95%BD" }, { "title": "CS231N chapter 8 - Deep Learning Softwares", "url": "/posts/cs231n8/", "categories": "Classlog, CS231N", "tags": "CS231N, framework", "date": "2021-09-28 13:00:00 +0900", "snippet": " 7강 리뷰 1) Facier Optimization : 많이 사용되는 강력한 손실함수 최적화 알고리즘 SGD, Momentum, Nesterov, RMSProp, Adam 2) Regularization : 네트워크의 Train/Test error 간의 격차를 줄이기 위해 사용 (과적합시 이 값을 조정함 - 손실함수에 Regulaization term을 더해 함수를 일반화시킨다) dropout 3) Transfer Learning : 미리 학습된 모델을 불러와 데이터만 사용하여 학습시키는 방법 CPU vs GPU딥러닝을 진행할 때는 GPU를 사용하는 것이 좋다. GPU 종류 중에서는 NVIDIA 와 AMD가 있는데, AMD의 경우 딥러닝을 수행할 때 문제가 더 많이 발생할 수 있다.NVIDIA는 딥러닝에 적합한 하드웨어를 만들기 위해 공을 들였기 때문에, 딥러닝에서는 NVIDIA가 독점적으로 많이 사용된다.CPU는 기본적으로 GPU보다 core의 수가 적다. GPU의 경우 고성능 상업 GPU는 수천개의 코어를 가지고 있다. 코어 수를 가지고 직접적으로 비교하지는 않지만, 코어가 많다는 것은 어떤 일을 수행할 때 병렬로 수행하기 더 적합하다.또한, CPU는 대부분의 메모리를 RAM에서 끌어다 쓴다. 반면 GPU는 칩 안에 RAM이 내장되어 있다.따라서 GPU는 딥러닝 연산에 적합하다.GPU에서 실행되는 코드를 직접 작성할 수 있으나 상당히 어렵고 복잡하다. CUDA, OpenCL, Udacity 등이 있긴 하나 그 대신 편리한 라이브러리를 사용하여 구현하고, GPU를 통해 계산만 실행하면 된다.Deep Learning Frameworks딥러닝 프레임워크에는 Pytorch, TensorFlow, Caffe2, MXNet 등이 있지만, 가장 유명한 것은 tensorflow와 pytorch이다.프레임워크를 사용하는 이유는 computational graphs를 직접 만들지 않아도 된다. forward pass만 잘 구현해놓으면 backpropagation은 알아서 구성된다. GPU를 효율적으로 사용할 수 있다.여기 입력 x,y,z에 대한 computational graphs가 있다. 이를 numpy로 작성하면 위와 같은 코드가 될 것이다.그래프를 numpy로 작성한다면 backward도 직접 작성해야 한다. 이는 매우 까다로운 일이 될 것이다. 또, numpy는 GPU에서 동작하지 않는다. Tensorflow이를 tensorflow로 구현해보자.tensorflow 코드를 보면 numpy와 유사하다. 하지만 tensorflow에는 gradient를 계산해주는 코드가 존재한다.grad_x, grad_y, grad_z = tf.gradients(c, [x,y,z])또, tensorflow의 장점은 명령어 한 줄이면 CPU/GPU 전환이 가능하다는 것이다.with tf.device(&#39;/cpu:0&#39;): =&amp;gt; with tf.device(&#39;/gpu:0&#39;):CPU:0을 GPU:0으로 바꿔주기만 하면 된다. Pytorchpytorch로 구현해보자.비슷한 코드로 구성되어 있다.pytorch 또한 1줄이면 gradient를 계산할 수 있다.c.backward()GPU를 사용하기 위해서 .cuda()를 붙인다.각 프레임워크를 좀 더 깊게 살펴보자.Tensorflow&#39;&#39;&#39;tensorflow neural net&#39;&#39;&#39;import numpy as npimport tensorflow.compat.v1 as tftf.disable_v2_behavior() # placeholder 을 위해 사용#### computational graphs 구성N,D,H= 64,1000,100 # number of node, input_size, hidden_size&#39;&#39;&#39; 그래프의 입력 노드 생성, 그래프 밖에서 데이터를 넣어줄 수 있도록 해줌 =&amp;gt; session 시에 직접 값을 지정 그래프만 구성하고 실제적인 메모리할당은 일어나지 않는다. &#39;&#39;&#39;x = tf.placeholder(tf.float32,shape=(N,D)) # 입력데이터y = tf.placeholder(tf.float32,shape=(N,D)) # 입력데이터w1 = tf.placeholder(tf.float32,shape=(D,H)) # 가중치w2 = tf.placeholder(tf.float32,shape=(H,D)) # 가중치&#39;&#39;&#39; x와 w1 행렬곱 연산 후 maximum을 통한 ReLU 구현 &#39;&#39;&#39;h = tf.maximum(tf.matmul(x,w1),0) # x와 w1의 행렬곱 연산 -&amp;gt; max(,0)을 이용해 ReLU 함수 구현y_pred = tf.matmul(h,w2) # h와 w2의 행렬곱 연산 -&amp;gt; 최종 출력값 y_pred 계산&#39;&#39;&#39; L2 Euclidean &#39;&#39;&#39;diff = y_pred-y # 예측값과 실제 정답과의 차 loss = tf.reduce_mean(tf.reduce_sum(diff**2,axis=1)) # 예측값과 실제 정답값 y 사이의 유클리디안 거리를 계산&#39;&#39;&#39; gradient 함수를 통해 loss와, w1, w2의 gradient 계산. backprop 직접 구현할 필요가 없다. &#39;&#39;&#39;grad_w1,grad_w2 = tf.gradients(loss,[w1,w2])&#39;&#39;&#39; ㅡㅡㅡㅡㅡㅡㅡㅡ 그래프 구성 =&amp;gt; 아직까지 실제 계산이 이루어지지는 않았다. ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ&#39;&#39;&#39;#### Tensorflow session : 데이터 입력, 실제 그래프를 실행 및 계산with tf.device(&#39;/gpu:0&#39;): with tf.Session() as sess: &#39;&#39;&#39; 그래프에 들어갈 value 입력. tensorflow는 numpy를 지원한다. &#39;&#39;&#39; values={x:np.random.randn(N,D), w1: np.random.randn(D,H), w2:np.random.randn(H,D), y:np.random.randn(N,D)} learning_rate = 1e-5 &#39;&#39;&#39; 실제 그래프 실행하는 부분, 출력 값은 numpy array &#39;&#39;&#39; for iteration in range(50): # 50번 반복하여 네트워크를 학습한다. out=sess.run([loss,grad_w1,grad_w2],feed_dict=values) # 첫번째 인자를 통해 그래프의 출력으로 어떤 부분을 원하는지 볼 수 있다. =&amp;gt; loss, grad_w1, grad_w2 # feed_dict를 통해 실제 값을 전달 loss_val, grad_w1_val,grad_w2_val = out # output에는 loss와 gradient가 numpy array형태로 반환되어 있기에 각각에 반환한다. values[w1] -= learning_rate * grad_w1_val # 가중치 업데이트를 위해 gradient를 계산해서 수동으로 gradient diescent하고 있다. values[w2] -= learning_rate * grad_w2_val # 이 경우 forward pass에서 그래프가 실행될 때마다 가중치를 넣어줘야 한다.두 개의 fc layer + ReLU 네트워크가 있다. 그리고 손실함수로는 L2 Euclidean을 사용했다.위의 코드는 크게 2 stage로 나눌 수 있다. 1) computational graph 정의 2) 그래프 실행위의 코드는 GPU/CPU간의 데이터 교환은 엄청 느리고 비용도 크다.따라서 아래와 같이 코드를 수정하여 해결한다.import numpy as npimport tensorflow.compat.v1 as tftf.disable_v2_behavior()N,D,H= 64,1000,100#### 그래프 구성 x=tf.placeholder(tf.float32,shape=(N,D))y=tf.placeholder(tf.float32,shape=(N,D))&#39;&#39;&#39; variables로 변경, 변수가 그래프 내부에 있도록 해주는 함수. =&amp;gt; session 전에 값을 지정 tf.random_normal로 초기화 설정 &#39;&#39;&#39;w1=tf.Variable(tf.random_normal((D,H)))w2=tf.Variable(tf.random_normal((H,D)))h=tf.maximum(tf.matmul(x,w1),0)y_pred=tf.matmul(h,w2)diff=y_pred-yloss = tf.losses.mean_squared_error(y_pred, y) # L2 loss를 직접 구현하지 않고 함수를 불러와 계산 # 이전에는 loss를 계산하는 연산을 직접 만들었다. # loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))&#39;&#39;&#39; loss 계산, gradient 계산. backprop 직접구현 X &#39;&#39;&#39;grad_w1,grad_w2=tf.gradients(loss,[w1,w2])&#39;&#39;&#39; assign 함수를 통해 그래프 내에서 업뎃이 일어날 수 있도록 해줌 &#39;&#39;&#39;learning_rate=1e-5# tensorflow는 output에 필요한 연산만 수행한다. 그래서 w1와 w2를 업뎃하라고 명시적으로 넣어주어야 한다.# new_w1, new_w2를 직접적으로 session 안으로 추가해줄 수 있으나, # 사이즈가 큰 tensor의 경우 tensorflow가 직접 출력을 하는 것은 cpu/gpu간 데이터 전송이 필요하므로 좋지 않다.# 따라서 dummy node인 updates 변수를 만들어 그래프에 추가# update는 어떤 값이 아니라, 특수한 데이터 타입으로 assign operation(작업)을 수행하기 위한 함수 &#39;&#39;&#39;new_w1=w1.assign(w1-learning_rate* grad_w1)new_w2=w2.assign(w2-learning_rate* grad_w2) updates=tf.group(new_w1,new_w2)#### Tensorflow sessionwith tf.device(&#39;/gpu:0&#39;): with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 그래프 내부의 변수들을 초기화, 즉 ,w1,w2를 초기화 values={x:np.random.randn(N,D), # 데이터와 레이블만 넣어줌, 가중치는 그래프 내부에 항상 상주 y:np.random.randn(N,D),} # x,y도 가중치처럼 그래프에 넣어줘도 되지만, x,y가 mini-batch인 경우가 대부분이다. # 그렇다는 것은 매번 데이터가 바뀌므로 매번 데이터를 넣어줘야 한다. for i in range(50): loss_val, _ = sess.run([loss,updates],feed_dict=values) # 그래프를 실행시키면 loss와 dummy node 계산 # _ 는 원래 변수로 사용하던 값을 무시한다는 말이다. # updates 값은 받지 않고, loss 값만 출력placeholder을 써서 굳이 매번 가중치를 넣어줄 필요가 없다. 그 대신 variables를 선언해준다. variable은 computational graph안에 서식하는 변수이다.하지만 이 경우 그래프안에 살기 때문에, 우리가 tensorflow에게 어떻게 초기화시킬 것인지를 알려줘야 한다. 그래프 밖에 있다면 그래프에 넣어주기 전에 numpy를 통해 초기화(tf.random)시킬 수 있지만, 그래프 안에 있다면 초기화시킬 권한은 tensorflow에게 있다.이전 예제에서는 gradient를 계산하고 그래프 외부(session)에서 numpy로 가중치를 업데이트했다. 계산된 가중치를 다음 스텝에 다시 넣어주었다.그러나 그래프 안에서 하려면 그 연산 자체가 그래프에 포함되어야 한다. 이를 위해 assign함수를 이용하여 변수가 그래프 내에서 업데이트 되도록 한다.updates는 tensorflow의 트릭중 하나다. 그래프를 구성하는 동안에는 복잡한 객체를 반환하지만, session.run을 동작시키면 none을 반환(_)하게 된다.하지만, 사실 위의 방법은 좋은 방법은 아니다. 대신에 optimizer을 사용할 수 있다. assigns줄부터 updates줄까지를 아래와 같이 바꾸기만 하면 된다.optimizer = tf.train.GradientDescentOptimizer(learning_rate) # 이 외에도 Adam, RMSprop 등 optimization 알고리즘 사용 가능 # 그래프에 w1,w2의 gradient를 계산하는 노드도 알아서 추가하고, # w1,w2의 update opertation도 알아서 추가한다. # assign을 위한 grouping opertation도 추가된다.updates = optimizer.minimize(loss) # loss는 최소값을 찾아야 하므로 minimize입력과 가중치를 정의하고 행렬 곱연산으로 묶는 일이 번거롭기에 tf.layers로 간편하게 변경시킬 수 있다.N,D,H= 64,1000,100&#39;&#39;&#39; 그래프의 입력노드 생성 &#39;&#39;&#39;x=tf.placeholder(tf.float32,shape=(N,D))y=tf.placeholder(tf.float32,shape=(N,D))&#39;&#39;&#39; Xavier initializer로 초기화 &#39;&#39;&#39;init=tf.contrib.layers.xavier_initializer() # 초기화 방법 선언, 기존에는 tf.randomnormal로 일일이 초기화시켰다.&#39;&#39;&#39; w1, b2를 변수로 만들어주고 초기화 &#39;&#39;&#39;h=tf.layers.dense(inputs=x,units=H,activation=tf.nn.relu,kernel_initializer=init) # w1과 w2를 variables로 만들어주고, 그래프 내부에 적절한 shape으로 만들어준다. # activation을 통해 layer에 relu를 추가해준다. y_pred=tf.layers.dense(inputs=h,units=D,kernel_initializer=init) # 이전 h를 받아서 똑같이 수행한다. loss=tf.losses.mean_squared_error(y_pred,y)# optimizer을 이용해서 gradient를 계산하고 가중치를 업뎃할 수 있다optimizer=tf.train.GradientDescentOptimizer(1e-5)updates=optimizer.minimize(loss)## Tensorflow sessionwith tf.Session() as sess: sess.run(tf.global_variables_initializer()) values={x:np.random.randn(N,D), y:np.random.randn(N,D)} for t in range(50): loss_val=sess.run([loss,updates],feed_dict=values)위는 tf.contrib.layer과 tf.layer을 사용한 편리한 방법이다. 하지만 더 다양하고 향상된(high level) 라이브러리가 있다.그리고, 위의 코드들은 computational graph를 다루고 있다. 이는 low level이지만, 우리가 앞으로 다루어야 할 것들은 Neural Network인데, 이는 high level이다.그래서 tensorflow와 keras를 활용한 Network를 만들어보았다.from keras.models import Sequentialfrom keras.layers.core import Dense, Activationfrom keras.optimizers import SGDN, D, H = 64, 1000, 100model = Sequential() # 모델을 구성하기 위한 큰 틀과 같은 것model.add(Dense(input_dim=D, output_dim= H)) # 한 층 추가model.add(Activation(&#39;relu&#39;)) # 활성함수 추가model.add(Dense(input_dim=H, output_dim= D)) # 한 층 추가optimizer = SGD(lr = 1e0) # optimizer 선언model.compile(loss = &#39;mean_squared_error&#39;, optimizer = optimizer) # loss와 optimizer를 사용하여 그래프 생성x = np.random.randn(N, D) # 데이터 입력y = np.random.randn(N, D) # 데이터 입력history = model.fit(x, y, nb_epoch = 50, batch_size = N, verbose = 0) # model 50번 실행 tensorflow 기반 high level wrapper(함수? 라이브러리?) Keras https://keras.io/ TFLearn http://tflearn.org/ TensorLayer http://tensorlayer.readthedocs.io/en/latest/ tf.layers https://www.tensorflow.org/api_docs/python/tf/layers TF-Slim https://github.com/tensorflow/models/tree/master/inception/inception/slim tf.contrib.learn https://www.tensorflow.org/get_started/tflearn Pretty Tensor https://github.com/google/prettytensor tensorboard를 통해, tensorflow를 사용할 때 training 하는 동안 loss와 같은 통계값들을 시각화할 수 있다.Pytorchpytorch에는 3가지 중요한 요소들이 있다 tensor: imperative(명령형) array, gpu에서 사용가능, tensorflow의 numpy array와 같은 역할 명령형 언어 vs 선언형 언어의 차이 설명 참고 : https://blog.naver.com/66dlwjddbs66/221826016905 variable: 그래프의 노드, 그래프를 구성하고 gradient를 계산할 수 있음, tensorflow의 variable,placeholder과 같은 역할 module: Neural Network를 구성, tf.layer과 같은 역할아래는 pytorch tensor로 구성한 2-layer Network이다.import torchdtype=torch.cuda.FloatTensor # gpu에서 돌아가도록 데이터 타입을 변경 # gpu사용을 위해 FloatTensor 대신 cuda.FloatTensor로 변경N,D_in,H,D_out=64,1000,100,10&#39;&#39;&#39; random data 선언 &#39;&#39;&#39;x=torch.randn(N,D_in).type(dtype) y=torch.randn(N,D_out).type(dtype)w1=torch.randn(D_in,H).type(dtype)w2=torch.randn(H,D_out).type(dtype)learning_rate=1e-6for i in range(500): &#39;&#39;&#39; forward pass &#39;&#39;&#39; h = x.mm(w1) # mm : 행렬 곱 연산 함수 -&amp;gt; x와 w1을 행렬 곱 연산하여 h에 저장 h_relu = h.clamp(min=0) # clamp : min/max 를 통해 값을 바꾸는 함수, min = 0 이라면 0보다 작은 값들은 0으로 치환 -&amp;gt; relu 형태로 만듦 y_pred = h_relu.mm(w2) # h와 w2를 연산하여 pred로 출력 loss=(y_pred-y).pow(2).sum() # loss 계산, pow는 제곱 함수 -&amp;gt; L2 euclidean distance &#39;&#39;&#39; backward pass backward 직접 구현 - &amp;gt; gradient 계산 &#39;&#39;&#39; grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) # 행렬 곱 연산 grad_h[h&amp;gt;0] = 0 # relu grad_w1 = x.t().mm(grad_h) &#39;&#39;&#39; 가중치 업데이트 &#39;&#39;&#39; w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2따로 그래프를 구성하는 코드가 없다.아래는 variable을 사용하여 구현한 것이다.import torchfrom torch.autograd import VariableN,D_in,H,D_out=64,1000,100,10 # number of node, input_size, hidden_size, output_size&#39;&#39;&#39; variable은 computational graph를 만들고 이를 통해 gradient를 자동 계산하는 목적으로 사용 x는 variable, x.data는 tensor, x.grad는 variable, x.grad.data도 tensor x.grad.data는 gradient 정보를 담고 있다. &#39;&#39;&#39;x = Variable(torch.randn(N,D_in),requires_grad=False) # 이 값의 gradient를 계산할 것인지 지정할 수 있다.y = Variable(torch.randn(N,D_out),requires_grad=False) # 앞서 복잡한 gradient 계산 과정을 압축시킬 수 있음w1 = Variable(torch.randn(D_in,H),requires_grad=True) # 가중치에 대한 gradient만 계산w2 = Variable(torch.randn(H,D_out),requires_grad=True)learning_rate=1e-6for t in range(500): &#39;&#39;&#39; forward pass &#39;&#39;&#39; y_pred=x.mm(w1).clamp(min=0).mm(w2) # 출력값 계산 후 relu 연산 loss=(y_pred-y).pow(2).sum() # loss 계산 &#39;&#39;&#39; backward pass &#39;&#39;&#39; if w1.grad: w1.grad.data.zero_() # w1.grad 초기화 if w2.grad: w2.grad.data.zero_() # w2.grad 초기화 loss.backward() # gradient가 알아서 반환되어 backward 진행 &#39;&#39;&#39; w.grad.data를 이용해 가중치 업데이트 &#39;&#39;&#39; w1.data-=learning_rate* w1.grad.data w2.data-=learning_rate* w2.grad.data Tensorflow vs Pytorch먼저 tensorflow의 경우 그래프를 명시적으로 구성한 후 그래프를 돌린다. 즉 session 전에 그래프를 구성하고 session을 통해 계산한다.&#39;&#39;&#39; 그래프의 입력노드 생성 &#39;&#39;&#39;x=tf.placeholder(tf.float32,shape=(N,D))y=tf.placeholder(tf.float32,shape=(N,D))&#39;&#39;&#39; Xavier initializer로 초기화 &#39;&#39;&#39;init=tf.contrib.layers.xavier_initializer() &#39;&#39;&#39; w1, b2를 변수로 만들어주고 초기화 &#39;&#39;&#39;h=tf.layers.dense(inputs=x,units=H,activation=tf.nn.relu,kernel_initializer=init) y_pred=tf.layers.dense(inputs=h,units=D,kernel_initializer=init) loss=tf.losses.mean_squared_error(y_pred,y)반면 pytorch의 경우 forward pass 할 때마다 매번 그래프를 다시 구성한다.for t in range(500): &#39;&#39;&#39; forward pass &#39;&#39;&#39; y_pred=x.mm(w1).clamp(min=0).mm(w2) # 출력값 계산 후 relu 연산 loss=(y_pred-y).pow(2).sum() # loss 계산추가적으로 autograd를 직접 제작할 수 있다.하지만, 대부분의 경우 직접 구현하기보다, 원래 있는 모델을 사용하는 것이 좋다.pytorch에는 nn module이 존재한다. 이는 DNN 구성시 매우 많이 사용된다.import torchfrom torch.autograd import VariableN,D_in,H,D_out=64,1000,100,10learning_rate=1e-4x=Variable(torch.randn(N,D_in))y=Variable(torch.randn(N,D_out),requires_grad=False)model = torch.nn.Sequential( # layer 생성 torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H,D_out)) loss_fn=torch.nn.MSELoss(size_average = False) # common loss function, mean squared error loss(평균제곱오차)optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # optimizer를 위해 Adam 알고리즘 선언for t in range(500): &#39;&#39;&#39; forward pass, prediction, loss 계산 &#39;&#39;&#39; y_pred=model(x) # model에 x 넣고 prediction loss=loss_fn(y_pred,y) # prediction과 실제값 y를 통해 loss 계산 &#39;&#39;&#39; backward pass, gradient 계산 &#39;&#39;&#39; optimizer.zero_grad() # grad 초기화 loss.backward() # gradient 자동 계산 optimizer.step() # model parameter 업데이트 &#39;&#39;&#39; optimizer.step을 풀어쓰면 아래와 같다. for param in model.parameters(): param.data-=learning_rate*param.grad.data &#39;&#39;&#39;자신만의 모델을 구축해보자.여기 2-layer Network 예제가 있다.import torchfrom torch.autograd import Variablefrom torch.utils.data import TensorDataset, DataLoader&#39;&#39;&#39; 단일 모듈(class)로 model 생성 backward는 autograd가 알아서 해줌 &#39;&#39;&#39;class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): # layer 정의 super(TwoLayerNet, self).__init__() # super : 부모 클래스 함수의 메서드를 실행 가능하게 해줌, # __init__ : 우리는 객체(인스턴스) 생성시 매번 해주어야 하는 번거로운 작업을 간단하게 만들어줌 self.linear1=torch.nn.Linear(D_in,H) # Linear 구성 self.linear2=torch.nn.Linear(H,D_out) # Linear 구성 def forward(self,x): # forward pass 정의 h_relu=self.linear1(x).clamp(min=0) # 1층 layer 출력을 relu 연산 후 h_relu에 저장 y_pred=self.linear2(h_relu) # 2층 layer 출력을 y_pred 에 저장 return y_pred # y_pred 출력N,D_in,H,D_out=64,1000,100,10x=Variable(torch.randn(N,D_in))y=Variable(torch.randn(N,D_out),requires_grad=False)&#39;&#39;&#39; DataLoader가 minibatching, shuffling, multithreading 관리minibatch를 가져오는 작업들을 multi-threading을 통해 알아서 관리해준다. &#39;&#39;&#39;ds = TensorDataset(x, y) # 미리 정리된 TensorDateset을 불러옴loader = DataLoader(ds, batch_size=8) # dataset을 dataloader을 통해 loader함model = TwoLayerNet(D_in, H, D_out) # 정의했던 Net을 이용해 model을 불러옴criterion=torch.nn.MSELoss(size_average=False) optimizer=torch.optim.Adam(model.parameters(),lr=1e-4)for epoch in range(10): &#39;&#39;&#39; forward pass &#39;&#39;&#39; for x_batch, y_batch in loader: x_var, y_var=Variable(x),Variable(y) # mini-batch단위로 나뉘어진 loader를 각 epoch마다 x,y에 계속 입력시켜야 한다. y_pred=model(x_var) loss=criterion(y_pred,y_var) &#39;&#39;&#39; backward pass &#39;&#39;&#39; optimizer.zero_grad() loss.backward() optimizer.step()pretrained model을 불러와 사용할 수 있다.import torchimport torchvisionfrom torchsummary import summaryresnet101 = torchvision.models.resnet101(pretrained=True)&#39;&#39;&#39; 불러온 모델 구조 확인 방법 &#39;&#39;&#39;summary(resnet101, (3,224,224),device=&#39;cuda&#39;)print(resnet101)&amp;gt;&amp;gt;&amp;gt;---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 112, 112] 9,408 BatchNorm2d-2 [-1, 64, 112, 112] 128 ReLU-3 [-1, 64, 112, 112] 0 MaxPool2d-4 [-1, 64, 56, 56] 0 Conv2d-5 [-1, 64, 56, 56] 4,096 BatchNorm2d-6 [-1, 64, 56, 56] 128 ReLU-7 [-1, 64, 56, 56] 0 Conv2d-8 [-1, 64, 56, 56] 36,864 BatchNorm2d-9 [-1, 64, 56, 56] 128 ReLU-10 [-1, 64, 56, 56] 0 Conv2d-11 [-1, 256, 56, 56] 16,384 BatchNorm2d-12 [-1, 256, 56, 56] 512 Conv2d-13 [-1, 256, 56, 56] 16,384 BatchNorm2d-14 [-1, 256, 56, 56] 512 ReLU-15 [-1, 256, 56, 56] 0 Bottleneck-16 [-1, 256, 56, 56] 0tensorboard와 같이 pytorch에서도 visdom을 통해 loss에 대한 통계같은 값들을 시각화 할 수 있다.Static(tensorflow) Vs Dynamic(pytorch) graph먼저 tensorflow는 두 단계로 나눌 수 있다. 그래프를 구성하는 단계, 두번째는 그래프를 반복적으로 실행하는 단계이다.그래프가 단 하나만 고정적으로 존재하는 형태를 static computational graph라고 한다.반면 pytorch의 경우 매번 forward pass할 때마다 새로운 그래프를 구성한다.이를 dynamic computational graph라 한다.단순한 과정에서는 별 차이 없어보이지만, 차이점은 분명 존재한다.static graph는 한번 그래프를 구성해놓으면 학습시에 계속 똑같은 그래프를 재사용한다. 그렇다는 것은 그래프를 최적화시킬 수 있다는 것을 의미한다.일부 연산들을 합치거나 재배열시키는 등으로 가장 효율적으로 연산하도록 최적화시킬 수 있다는 것이다.처음에는 최적화작업이 오래걸리지만, 최적화한 그래프를 여러번 사용하기 때문에, 손해가 아니다.예를 들어, 여기 Conv와 Relu를 합쳐버릴 수 있다.반면 dynamic graph에서는 그래프를 최적화하기 어렵다.또 다른 차이점으로는 serialization에 관한 것이다.static graph를 사용한다고 해보자. 그래프를 한 번 구성해놓으면 메모리 내에 그 네트워크 구조를 가지고 있게 된다. 그렇다는 것은 그래프 자체, 네트워크 구조 자체를 disk에 저장할 수 있다. 이렇게 되면 전체 네트워크 구조를 파일 형태로 저장할 수 있다. 그러면 원본 코드 없이도 그래프를 다시 불러올 수 있다.python에서 불러올 수 있을 뿐만 아니라 c++에서도 불러 올 수 있다.반면 dynamic graph의 경우 그래프 구성과 그래프 실행하는 과정이 묶여 있다. 그렇기에 그래프를 다시 불러오기 위해서는 항상 원본 코드가 필요하다.하지만 dynamic은 대다수의 경우에 코드가 훨씬 깔끔하고 작성하기 더 쉽다.if문을 만들 때, pytorch는 단순하게 if문을 사용하면 된다.하지만 tensorflow의 경우 우선 그래프를 만들고 난 후 그래프 내에 조건부 연산을 정의하는 코드가 더 필요하다. 때문에, if문 대신 control flow 자체를 넣어줘야 한다. 그래프가 실행되기 전에 연산을 넣는 것이다.또한, 반복연산(loops)에서도 차이점이 있다.우리의 데이터는 다양한 사이즈일 수 있다. 데이터의 길이가 얼마인지 신경쓰지 않고 재귀 연산을 할 수 있다면 좋을 것이다.pytorch의 경우 기본적인 for문을 사용하면 된다.그러나 tensorflow에서는 그래프를 앞에서 미리 만들어줘야 하기 때문에, 그래프에 명시적으로 loop를 넣어줘야만 한다.특정 재귀적인 관계를 정의하기 위해 tf.fold1연산을 사용한다. fold는 dynamic graph처럼 만들어주기 위한 트릭이다.정리하자면static graph 그래프를 구성해놓으면 재사용이 가능, 즉 그래프를 최적화시킬 수 있다.ex) conv -&amp;gt; relu -&amp;gt; conv -&amp;gt; relu ==&amp;gt; conv+relu -&amp;gt; conv+relu 그래프를 구성 -&amp;gt; 네트워크 구조 자체를 disk에 저장 가능 -&amp;gt; 파일 형태로 네트워크 구조를 저장 가능 -&amp;gt; 원본 코드 없이 그래프 다시 불러오기가 가능 조건부 연산(if 문) 그래프를 따로 만들어야 함dynamic graph forward pass할때마다 그래프를 구성 -&amp;gt; 그래프를 최적화하기 어려움 모델을 다시 불러오기 위해서는 항상 원본 코드 필요 대신 코드가 더 깔끔하고 작성하기 쉬움그렇다면 어떤 상황에서 dynamic graph를 사용해야 할까?최근 나온 network인 image captioning이 있다. image captioning은 다양한 길이의 sequences(출력)를 다루기 위해 RNN을 이용한다.sequence는 입력 데이터에 따라서 다양하게 변할 수 있다. sequence의 크기에 따라 computational graph가 커질 수도, 작아질 수도 있다.따라서 image captioning은 dynamic graph를 이용할 일반적인 예시 중 하나다.또는, 자연어 처리 분야에서 문장을 파싱하는 문제에서 트리를 파싱하기 위해 recursive(반복)한 네트워크가 필요할 수 있다.이런 경우 layer의 sequence 구조를 이용하기 보다 graph나 tree 구조를 이용한다. 즉, 데이터에 따라 다양한 graph나 tree 구조를 가질 수 있다.이 때, tensorflow로 구성하려면 정말 복잡하고 까다롭다. 따라서 dynamic graph를 사용하는 것이 좋다.VQS를 다루는 neuro module(신경 모듈)이라는 연구가 있다. 이미지와 질문을 던지면 적절한 답을 해주는 것이다.질문에 따라 그 질문의 답을 위한 적절한 네트워크를 구성한다. 또, 질문이 물체 2개를 비교하는 것이라면 find, count와 compare을 수행해야 할 것이다.Caffe다른 딥러닝 프레임워크랑 다소 다르게 코드를 작성하지 않아도 네트워크를 학습시킬 수 있다.기존에 빌드된 바이너리를 호출하고 configuration 파일만 조금 수정하면 코드를 작성하지 않아도 데이터를 학습시킬 수 있다.training 과정 데이터를 HDF5 또는 LMDB포맷으로 변환시킨다. 코드 작성 대신 prototxt라는 텍스트파일을 만든다. 입력은 HDF5파일로 받는 부분도 있고 내적을 하는 부분도 있다. 이런 형식으로 그래프를 구성한다. 훈련시킨다.이런 방식은 네트워크의 규모가 커지면 상당히 보기 안좋다. 152layer 모델을 훈련시키려면 prototxt는 거의 7000줄이 된다.그래서 prototxt생성 대신 python 스크립트를 작성한다.caffe는 그래프를 하나하나 전부 다 기술해줘야 한다. 또, optimizer이나 solver을 정의하기 위해 또 다른 prototxt 파일을 작성해야 한다.caffe에도 pretrained model을 제공한다. alexnet,vgg,googlenet,resnet 등등이 있다.caffe는 산업이나 생산 개발에 많이 쓰인다.Caffe2caffe2는 caffe의 다음 버전이다.caffe2는 static graph를 사용한다. caffe와 같이 C++로 작성되어 있고, python 인터페이스도 제공한다.caffe와 차이점은 더이상 prototxt파일을 만들기 위해 python 스크립트를 작성할 필요가 없다는 것이다.Reference http://cs231n.stanford.edu/2017/syllabus.html https://blog.naver.com/asdjklfgh97/222157100126" }, { "title": "CS231N chapter 7 - Training Neural Network part 2", "url": "/posts/cs231n7/", "categories": "Classlog, CS231N", "tags": "CS231N, CNN", "date": "2021-09-23 13:00:00 +0900", "snippet": " 6강 리뷰 1) Activation Functions Sigmoid tanh ReLU leaky ReLU ELU Maxout 2) Data Preprocessing 3) Weight Initialization Xavier initialization 4) Batch Normalization 5) Babysitting Learning Process sanity check 6) Hyperparameter Optimization Cross Validation Training Neural Network - 2이번 강의에서 배울 내용은 Facier Optimization : 많이 사용되는 강력한 최적화 알고리즘(손실함수를 최적화) Regularization : 네트워크의 Train/Test error 간의 격차를 줄이기 위해 사용, 즉 과적합시에 이 값을 조정함(손실함수에 Regulaization term을 더해 함수를 일반화시킨다) Transfer Learning : 미리 학습된 모델을 불러와 데이터만 사용하여 학습시키는 방법Fancier optimizationNeural Network에서 가장 중요한 것이 최적화(optimization)이다. 네트워크의 가중치에 대한 손실함수를 정의해놓으면 이 손실함수는 가중치가 얼마나 좋은지 나쁜지를 알려준다.위의 사진에서 오른쪽 그림의 X/Y축은 두 개의 가중치를, 각 색은 loss 값을 의미한다. 가장 붉은 색인 지점이 가장 낮은 loss를 뜻하고 이에 대한 가중치를 찾는 것이 최적화의 목적이다.Stochastic Gradient Descent가장 간단한 최적화 알고리즘은 SGD이다. minibatch안의 데이터에서 loss를 계산하고 gradient의 반대 방향을 이용해서 파라미터 벡터를 업데이트한다.반대 방향인 이유는 손실 함수는 줄어드는 것이 올바른 방향이기 때문이다.업데이터를 반복하다보면 결국 빨간색 지역으로 수렴하게 될 것이고, loss가 낮아질 것이다.하지만 이 SGD에는 몇 가지 문제가 있다. 손실함수의 가중치 w1과 w2가 있다고 가정해보자.w1는 수평축으로 가중치가 변하고, w2는 수직축으로 가중치가 변한다면, 수평 축의 가중치는 변해도 loss가 아주 천천히 줄어들고, 수직 방향의 가중치 변화에 더 민감하다면 전체 loss는 매우 느리게 줄어든다.위의 그림은 SGD로 학습이 되는 과정을 보여준다.loss에 영향을 덜 주는 수평방향 차원의 가중치는 업데이트가 매우 느리게 진행되고, 수직방향 차원의 가중치는 비교적 빠르게 되므로 지그재그가 된다.우리가 보는 것은 2차원이지만, 훨씬 더 고차원 공간에서는 훨씬 더 빈번하게 발생한다.SGD의 또 다른 문제는 local minima와 saddle point와 관련된 문제다.이런 상황에서 SGD는 멈춰버린다. gradient가 0이 되기 때문이다. SGD는 gradient의 반대방향으로 이동하는 데, 0이 되면 이동이 안된다. 이런 경우가 local minima(locally falt) 에 해당한다.local minima는 아니지만, 증가하다가 감소하는 지역(saddle point)이 있다. 이 곳에서도 gradient는 0이 된다. 따라서 이런 상황에서도 학습이 멈추게 된다.saddle point 근처에서도 계산해보면 거의 0에 가깝기 때문에 업데이트가 아주 느리게 진행될 것이다.우리는 1차원의 예제만 보고 있지만 고차원 공간을 따져볼 때는 local minima보다 saddle point가 더 많아질 것이다. 1억개의 차원이라고 하면 사실 거의 모든 곳에서 발생한다고 봐도 무방하다.또 다른 문제가 있다. Stochastic gradient 는 확률적인 기울기란 뜻이다.손실 함수를 계산할 때는 엄청 많은 training set 각각의 loss를 전부 계산해야 한다. 이는 매우 어렵고 복잡하다. 그래서 실제로는 전체 training set가 아닌 minibatch의 데이터들만 가지고 실제 loss를 추정한다.이는 매번 정확한 gradient를 얻을 수 없다는 것을 의미한다. 그렇다는 것은 정확한 gradient를 얻고, 적절한 loss를 찾기 위해서는 오래 걸릴 수 밖에 없다.확률적 계산을 빼고 GD만 사용하면 되냐고 생각할 수 있겠지만, 이때도 같은 문제가 발생한다.noise 즉, gradient가 이리저리 돌아다니는 현상은 minibatch를 사용할 때만 발생하는 것이 아니라 full batch를 사용할 시에도 발생한다.SGD + Momentum이런 문제들을 해결 할 수 있는 간단한 방법은 SGD에 momentum term을 추가하는 것이다.왼쪽은 classic 버전의 SGD 이다. 오로지 gradient 방향으로만 움직인다.오른쪽은 SGD + momentum이다. gradient를 계산할 때 velocity를 이용하여 두 가지를 같이 고려한다.momentum의 비율을 나타내는 하이퍼파라미터인 rho를 추가하였다.velocity의 영향력을 rho의 비율로 맞춰주는데 보통 0.9와 같은 높은 값으로 맞춰준다.velocity에 일정 비율 rho를 곱해주고 현재 gradient를 더한다. 이를 통해 이제는 gradient vector 그대로의 방향이 아닌 velocity vector의 방향으로 나아가게 된다.이를 통해 많은 것을 해결할 수 있다.예를 들어 공이 경사면을 굴러온다고 해보자. 공이 떨어지면 속도는 점점 빨라진다. 이 공은 local minima에 도달해도 여전히 velocity를 가지고, saddle point에서도 여전히 velocity를 가지기 때문에 gradient가 0이어도 움직일 수가 있다.noise 문제도 해결할 수 있다. 위의 그림에서 검은색이 일반 SGD이고 파란색이 momentum SGD이다. momentum을 추가해서 velocity가 생기니 noise가 평균화되는 것을 볼 수 있다.업데이트가 빠르게 되지 않는 경우 즉, 지그재그로 움직이는 상황이었던 반면 momentum이 이 변동을 서로 상쇄시킬 수 있다. 이를 통해 loss에 민감한 수짓 방향의 변동은 줄여주고, 수평방향의 움직임을 가속화시킨다.velocity의 초기값은 항상 0 이다. 이는 하이퍼파라미터가 아니라 그저 0으로 두는 것이다.좀 더 직관적으로 보면 velocity는 이전 gradient들의 weighted sum이다.그리고 더 최근의 gradient에 가중치가 더 크게 부여된다. 매 스텝마다 이전 velocity에 rho=0.9 or 0.99를 곱하고 현재 gradient를 더해준다. 시간이 지날수록 이전의 gradient들은 영향력이 감소된다.momentum의 변형으로 nesterov momentum이라는 것이 있다. 계산하는 순서를 조금 바꾼 형태이다.현재 지점에서의 gradient를 계산한 뒤 velocity를 섞는 기본 SGD momentum에 비해nesterov는 우선 velocity 방향으로 움직인 후, 그 지점에서의 gradient를 계산하고, 다시 원점으로 돌아가서 둘을 합친다.nesterov momentum의 식은 다음과 같다.velocity를 업데이트하기 위해서 이전의 velocity와 (x + pv)에서의 gradient를 계산한다.그 후 앞서 계산한 velocity를 이용하여 업데이트한다.기존에는 loss와 gradient를 같은 점(xt)에서 구했다. 그러나 nesterov는 이 규칙을 조금 비틀었다.하지만 공식을 조금 변형하여 변수들을 적절히 잘 바꿔주면 loss와 gradient를 같은 점에서 계산할 수 있다.위의 공식을 보면, 첫번째 수식은 기존의 momentum과 동일하다. (xt+pvt)와 같이 velocity와 계산한 gradient를 일정 비율로 섞어준다.두번째 수식에서 아래 식을 보면 현재 점(xt)와 velocity(vt+1)를 더한다.거기에 (현재 velocity - 이전 velocity) * rho 를 더해준다. 이는 현재/이전의 velocity간의 에러보정 term 이다.momentum 방법은 velocity의 영향으로 인해 minima를 그냥 지나친다. 하지만 스스로 경로를 수정하여 결국 수렴한다.Nesterov 방법은 추가된 수식 때문에 momentum과 조금 다르게 움직인다. 일반 momentum에 비해 overshooting이 덜하다.Nesterov momentum은 convex optimization에서는 뛰어난 성능을 보이지만 neural network와 같이 non-convex optimization에서는 성능이 보장되지 않는다. Q. momentum의 경우 만일 minima가 엄청 좁고 깊은 곳이라면 오히려 지나치는 현상이 있을 수 있지 않나? =&amp;gt; 사실 좁고 깊은 minima는 좋은 모델이 아니다. 이는 더 심한 overfit을 불러오기 때문이다. 우리가 원하는 minima는 아주 평평한 minima, 즉 더 일반화를 잘하고 강인한 모델이다.AdaGrad이는 훈련 도중 계산되는 gradient를 활용하는 방법이다. 학습 도중 계산되는 gradient를 제곱해서 더해준다. 업데이트할 때 update term 앞에 다시 제곱항으로 나누어주어 step_size를 줄여준다.small dimension에서는 gradient의 제곱 값의 합은 작다. 이 작은 값이 나눠지므로 속도가 빨라진다.반대로 large dimension에서는 gradient가 크다. 따라서 큰 값이 나눠지므로 속도가 줄어든다.AdaGrad에는 문제가 하나 있다. update 동안 gradient의 제곱이 계속 더해지므로 grad_squared는 서서히 상승한다. 그렇게 되면 나눠지는 값이 점점 커지기 때문에 step_size가 점차 작아지게 된다.손실함수가 convex(볼록)한 경우에 step size가 점점 작아지는 것은 좋은 특징이다.하지만 non-convex인 경우 saddle point에 걸리면 학습이 멈춰질 수 있다.RMSPropRMSProp는 AdaGrad의 변형으로 앞서 언급한 step_size의 감소를 개선시켰다.RMSProp에서는 gradient 제곱 항은 그대로 사용하고 기존의 누적 값에 decay_rate를 곱해주고, 현재의 gradient의 제곱에 (1-decay rate)를 곱해서 더해준다.이 값은 momentum 수식과 유사하나 gradient 제곱을 계속 누적한다는 것이 차이점이다.RMSProp에서 쓰는 decay rate는 보통 0.9 or 0.99를 사용한다.dacay rate를 사용함으로써 점차 속도가 줄어드는 것을 해결하였다.momentum의 경우 overshoot한 뒤에 다시 돌아오는 형태지만, RMSProp는 각 차원마다의 상황에 맞도록 적절하게 귀적을 수정시킨다.일반적으로 Neural Network를 학습시킬 때 AdaGrad는 잘 사용하지 않는다.Adammomentum 과 AdaGrad/RMSProp 방식을 조합한 방법이다.Adam은 first moment 와 second moment를 이용해서 이전의 정보를 유지시킨다.first moment는 gradient의 가중 합이다. second moment는 AdaGrad나 RMSProp처럼 gradient의 제곱을 이용한다.Adam으로 업데이트하게 되면 first moment는 velocity를 담당한다. second moment는 sqrt 씌워져 나눠진다.이 수식에서 10e-7은 앞에서도 등장하는 것과 같이 second moment가 0일 경우 나눗셈이 오류가 난다. 이를 방지하게 위해 약간의 값을 더해주는 것이다.이 알고리즘에도 문제가 하나 존재한다.초기 step에서 second moment를 0으로 초기화한다. 1회 업데이트하고 나면 beta2는 decay_rate로 0.0 or 0.99이다. 그래서 second moment는 여전히 0에 가깝다.0에 가까운 값을 나눠주기 때문에 초기 step은 엄청 커지게 된다.이 때 중요한 것은 이 커진 step이 손실함수가 가파르기 때문이 아니라 0으로 초기화시켜서 발생하는 인공적인 현상이다.Adam의 문제를 해결하기 위해 보정하는 항(bias correction)을 추가한다.first/second moment를 계산한 후 현재 step에 맞는 적절한 unbias term을 계산해준다.Adam은 실제로 엄청 많이 사용되고 기본 알고리즘으로 사용되고 있다.특히 beta_1 = 0.9, beta_2 = 0.999로 설정하고 learning rate = 1e-3 or 1e-4로 설정해주면 거의 모든 architecture에서 잘 동작한다.그림과 같이 momentum과 RMS 를 섞어놓은 듯한 모습을 보인다. 약간의 overshoot와 RMS와 같이 각 차원의 상황에 따라 고려해서 step을 이동한다. Q. Adam이 해결하지 못한 문제는 무엇인가?=&amp;gt; 손실함수가 타원형일 경우 adam을 이용하면 각 차원마다 적절하게 속도를 높히고 줄이면서 step을 조절할 것이다. 하지만 이타원이 축 방향으로 정렬되지 않고 기울어져 있다면 다루기 힘들어질 것이다.이는 다른 알고리즘도 마찬가지다.지난 강의에서 봤듯이 learning rate는 중요하지만 잘 고르는 것은 어렵다. lr이 지나치게 높으면 솟구치고, 너무 낮으면 수렴하는데 오래걸린다.lr을 잘 고르기 위한 방법으로 Learning rate decay 전략을 사용할 수 있다.각각의 learning rate의 특성을 적절히 이용하는 것이다.처음에는 lr을 높게 설정한 다음 학습이 진행될수록 lr을 점점 낮추는 것이다.혹은 exponential decay 처럼 꾸준히 lr을 낮추는 방법도 있다.이와 같이 loss가 평평해지다가 다시 내려가고를 반복할 수도 있다.ResNet 논문에서는 step decay learning rate 전략을 사용한다.평평하다가 갑자기 내려가는 구간에서 learning rate를 낮춘다. 수렴을 잘하고 있는 상황에서 gradient는 점점 작아진다. 이 경우는 learning rate가 높아서 발생하기 때문에, learning rate를 낮춰서 지속적으로 loss가 내려갈 수 있도록 만든다.learning rate decay는 부가적인 하이퍼파라미터다. 일반적으로 학습 초기부터 이것을 고려하지는 않는다. cross-validation 할 때 lr과 decay 모두를 생각하면 너무 복잡해진다.그래서 일단은 decay없이 learning rate만을 사용해서 학습시켜본다. loss curve를 보다가 decay가 필요한 곳이 어딘지 관찰한다.사실 이 방법은 Adam 보다는 SGD momentum에서 더 자주 쓴다.위의 optimization 알고리즘들은 1차 미분을 활용한 방법이었다.빨간 점에서의 gradient를 계산하고, gradient 정보를 이용해서 손실함수를 선형함수로 근사시킨다. 이는 1차 테일러 근사에 해당한다.2차 근사 정보를 활용하는 방법도 있다.이는 2차 테일러 근사 함수가 될 것이고, 2차 근사를 활용하면 minima에 더 잘 근접할 수 있다. 이것이 2nd-order optimization의 기본 아이디어다.위의 예시를 다차원으로 확장시킨 것이 Newton step이다. 2차 미분값들로 된 행렬인 Hessian matrix를 계산하고 Hessian matrix의 역행렬을 이용하면 실제 손실함수의 2차 근사를 이용해 minima로 바로 이동할 수 있다.이론적으로 newton method에서는 learning rate는 필요없고 매 step마다 항상 minima를 향해 이동한다. 하지만 실제로는 2차 근사도 사실상 완벽하지 않기 때문에 learning rate가 필요하다.우리는 minima로 이동하는 것이 목적이지 minima의 방향으로 이동하는 것이 목적이 아니기 때문이다.불행하게도 newton method는 deep learning에는 사용할 수 없다. Hessian matrix는 N x N(N: network의 파라미터 수) 행렬이므로 N이 1억이면 1억의 제곱만큼 존재한다. 이를 메모리에 저장할 방법도 없고 역행렬 계산도 불가능하다.그래서 실제로는 quasi-Newton methods(BGFS) 를 사용한다.Hessian matrix를 그대로 사용하지 않고 근사시킨다. low-rank approximation(낮은 단계로의 근사)하는 방법이다.L-BFGS(limited memory BFGS)도 Hassian을 근사시켜서 사용하는 방법이다. 그러나 L-BFGS는 2차 근사가 stochastic case에서 잘 동작하지 않고, non-convex에도 적합하지 않기 때문에 DNN에서 잘 사용하지 않는다.하지만 full batch update가 가능하고 stochasticity가 적은 경우 L-BFGS가 좋은 선택이 될 수 있다.Regularization결론적으로 optimization 알고리즘들은 training error을 줄이고 손실함수를 최소화시키기 위한 역할을 수행한다.하지만 우리는 train error가 아닌 한번도 보지 못한 데이터에 대한 성능이 더 중요할 것이다. 즉, train/test error의 격차를 줄여야 한다.ensemble한번도 보지 못한 데이터에 대한 성능을 올리기 위한 방법으로 가장 빠르고 쉬운 길은 model ensemble이다.machine learning 에서는 종종 사용하는 기법이다. 모델을 하나만 학습시키지 않고 10개의 모델을 독립적으로 독립적으로 학습시킨다. 결과는 10개 모델 결과의 평균을 이용한다.모델이 늘어날수록 overfitting은 줄어들고 성능은 조금씩 향상된다.좀 더 창의적인 방법으로는 학습 도중 중간 모델들을 저장(snapshots)하고 ensemble을 사용하고, test time에는 여러 snapshot에서 나온 예측값들을 평균내서 사용하는 방법이 있다. 이 기법을 사용하면 모델을 한 번만 train시켜도 좋은 성능을 얻는다.모델마다 다양한 learning rate, model size, regularization 을 사용하여 ensemble할 수 있다.Polyak average또 다른 방법으로는 학습하는 동안에 파라미터의 exponentially decaying average를 계속 계산한다. 이를 통해 학습중인 네트워크의 smooth ensemble 효과를 얻을 수 있다.step마다 파라미터를 그대로 쓰지 않고 smoothly decaying average를 사용하는 방법이다.성능이 조금 향상될 수 있다. 시도해볼만한 방법이지만 실제로는 자주 사용하지는 않는다.위의 방법들은 ensemble 성능을 향상시키는 것들이었고, 단일 모델의 성능을 향상시키기 위해서는 어떻게 해야 할까?Regularization단일 모델의 성능을 올리는 방법은 Regularization 이다. regularization은 training data에서 overfitting 되는 것을 막아준다. 또 한번도 보지 못한 데이터에서의 성능을 향상시키는 방법이다.앞서 봤듯이 loss에 추가적인 항을 삽입하는 방법이 있다. 손실함수에서 기존의 항(앞 항)은 training data에 fit하려 하고, 다른 하나(뒷 항)는 regularization term이다.L2 regularization은 NN에 어울리지 않는다. 때문에 조금 다른 방법인 dropout을 사용한다.forward pass과정에서 임의로 일부 뉴런을 0으로 만든다. dropout은 forward pass iteration마다 모양이 계속 바뀐다. conv layer에서 일부 채널 자체를 dropout시킬수도 있다.dropout은 FC, conv layer 등 다양하게 쓰인다.keras.layers.Dense(1024, activation=&quot;relu&quot;)keras.layers.Dense(10,activation=&quot;softmax&quot;)keras.layers.Dropout(0.25)이와 같이 맨 마지막 단에 추가한다.dropout을 적용하면 특징들 간의 상호작용을 방지하기 때문에 성능이 좋아지고, 네트워크가 일부 feature에만 의존하지 못하게 만들어 다양한 feature을 고루 이용할 수 있도록 만든다.그렇기 때문에 overfitting도 방지시켜준다.최근 연구에서는 dropout이 앙상블 효과를 가져올 수 있다고 한다. dropout을 적용하게 되면 뉴런의 일부만 사용하는 또다른 네트워크를 학습할 수 있게 되는 것이다. 서로 파라미터를 공유하는 서브네트워크 앙상블을 동시에 학습시키는 것으로 볼 수 있다.dropout을 사용하게 되면 test time에는 Neural Network의 동작 방식이 변하게 된다. 기존 NN은 가중치 w와 입력 x에 대한 함수 f가 있다. dropou을 사용하면 NN에 random dropoutmask라는 임의의 입력값 z가 추가된다.하지만, test time에는 임의의 값을 넣는 것은 좋지 않다. 분류하는 문제에서 결과가 매번 다르게 나온다면 좋지 않기 때문이다. 따라서 test할 때는 적분을 통해 randomness를 없앤다.하지만 실제로는 이 적분을 다루기는 매우 어렵다. 그래서 z를 여러번 샘플링하여 적분을 근사시킨다.문제는, 이 방법도 마찬가지로 임의성을 없애지 못한다는 것이다.그래서 dropout을 진행할 때 일종의 locally cheap한 방법을 사용하여 이 적분식을 근사화시킨다.예를 들어 출력이 a이고 입력 x,y 가 있고, 가중치 w1,w2가 있는 뉴런이 있다고 하자. test 에서 a는 w1x + w2y 이다.이 네트워크에 dropout(p = 0.5)를 적용해서 train을 진행해보자.train time에서의 기댓값은 다음과 같이 계산할 수 있다.dropout mask에는 4가지 경우의 수((w1,w2),(w1,0),(0,w2),(0,0))가 존재하고, 그 값을 4개의 마스크에 대해 평균화시킨다.a의 기댓값 : E(a) = 1/4(w1*x+w2*y) + 1/4(w1*x+0*y) + 1/4(0*x+w2*y) + 1/4(0*x+0*y)= 1/2(w1*x+w2*y)이 부분에서 train/test 간의 기댓값이 서로 상이하다. train의 기댓값은 test의 절반밖에 안된다.따라서 이 둘의 기댓값을 같게 만들기 위해 네트워크 출력에 dropout 확률 p를 곱해준다.이 방법은 이전의 적분식을 쉽게 근사화할 수 있는 방법이기에 많이 사용된다.dropout에 대해 요약하자면 forward pass에 dropou을 추가시키는 것은 간단하다. 일부 노드를 무작위로 0으로 만들어줄 2줄이면 충분하다.그리고 test time에서는 그저 p 만 곱해주면 된다. inverted dropoutdropout을 역으로 게산하는 방법은 상당히 간단하고 효율적이다.test time에서는 계산 효율이 중요하므로 곱하기 연산 하나를 추가하는 것은 좋지 않다.따라서 test 시에는 그대로 사용하고 train time에서 p를 나눠준다. Q. dropout을 사용하면 train ime에서 gradient에는 어떤 일이 일어날까?=&amp;gt; dropout이 0으로 만들지 않는 노드에서만 backpropagation이 발생하고, 각 스텝마다 업데이트되는 파라미터의 수가 줄어든다. 따라서 dropou을 사용하면 전체 학습시간은 늘어나지만 모델이 수렴한 후에는 더 좋은 성능을 얻을 수 있다.dropout은 일반적인 regularization의 한 예다.regularization의 과정은 1. train time에는 네트워크에 임의성을 추가해 training data에 overfit하지 않도록 한다. 2. test time에서는 임의성을 근사화시켜 일반화한다.batch normalization도 이와 비슷한 동작을 한다.train time에서 BN은 하나의 데이터를 mini batch로 샘플링하여 매번 서로 다른 데이터들과 만나게 한다. 샘플링된 데이터에 대해서 이 데이터를 얼마나 어떻게 정규화시킬 것인지에 대한 확률성(stochasticity)이 존재했다.(γ,β)반면 test time에서는 정규화를 전체 데이터 단위로 수행함으로써 확률성을 평균화시킨다. 이 특성을 통해 regularization 역할을 수행한다.따라서 BN을 사용할 때는 dropout을 사용하지 않는다.Data Augmentationregularization 역할을 수행하는 또 다른 전략이다.기본적인 학습은 데이터와 레이블을 통해 매 스텝마다 CNN을 업데이트했다.하지만 그 대신 레이블은 그대로 놔둔 채 이미지를 무작위로 변환시켜 학습을 시킬 수 있다.이 방법을 통해 원본 이미지가 이닌 무작위로 변환된 이미지를 새롭게 학습할 수 있는 것이다.이미지를 상하좌우 반전이나 다양한 사이즈로 자른다. 이 이미지가 의미하는 바는 바뀌지 않는다.test time에서 임의성을 없애기 위해 각 모서리 4군데와 중앙에서 잘라낸 이미지, 이것들을 반전한 이미지를 사용한 성능과 원래의 성능과 비교하기도 한다.다른 방법으로는 색 변조를 하는 방법이 있고, 더 복잡한 방법으로는 PCA의 방향대로 이미지를 샘플링하여 color offset을 학습하기도 한다.이외에도 여러가지 방법들이 존재한다. 이와 같은 방법들을 사용하여 입력 데이터를 임의로 변환시켜 학습하면 regularization 효과를 얻어 성능을 향상시킬 수 있다. DropConnectdropout과 유사한 방법으로 activation이 아닌 weight matrix를 임의적으로 0으로 만들어주는 방법이다.dropout과 동작도 비슷하다. Franctional max pooling사람들이 자주 사용하지는 않지만 괜찮은 방법이다.보통 2x2 maxpooling 은 고정된 2x2 지역에서 연산을 수행한다.하지만 이 방법의 경우 pooling연산을 수행할 지역을 임의로 선정한다.train time에서는 임의의 region이 존재할 수 있다. 하지만 test에서는 임의성을 제거하기 위해 pooling region을 고정시켜 버리거나 여러개의 pooling region을 만들어 평균화시킨다.Stochastic Depth2016년에 나온 논문으로 깊은 네트워크가 있다고 가정해보자.train time시에 네트워크의 레이어를 random하게 drop한다. layer 중 일부가 제거된 채로 학습하는 것이다.test time에서는 전체 네트워크를 다 사용한다.획기적인 방법이나 dropout과 비슷한 효과를 보인다.Transfer Learning모델을 빠르게 학습시킬 수 있는 방법으로 미리 학습된 모델을 불러와 사용한다.imageNet에서 잘 학습된 features를 우리가 가진 작은 데이터셋에 적용시킨다.일반적인 과정은1) 가장 마지막의 FC layer은 최종 feature과 class score간의 연결이므로 이를 초기화시킨다.2) imageNet을 학습시킬 때는 4096 x 1000 차원의 행렬이다. 하지만 우리가 사용하는 것은 4096 x 10 이므로 바꿔준다.3) 가중치 행렬은 초기화시키고, 나머지 이전의 모든 레이어들의 가중치는 freeze시킨다.4) 이렇게 되면 linear classifier를 학습시키는 것과 같다. 마지막 레이어만 가지고 우리 데이터를 학습시키는 것이다.이 방법을 사용하면 아주 작은 데이터셋일지라도 아주 잘 동작하는 모델이 된다.만일 데이터가 조금 더 많다면 전체 네트워크를 fine-tuning할 수 있다.최종 레이어들을 학습시키고 나면, 네트워크의 일부만이 아닌 네트워크 전체의 학습을 진행할 수도 있다. 보통 기존의 learning rate보다 낮춰서 학습시킨다. 왜냐하면 기존의 가중치들이 이미 imagenet에 잘 학습되어 있고, 이 가중치들이 대게 잘 동작하기 때문이다.transfer learning을 수행함에 있어 위의 4가지 시나리오를 예상해볼 수 있다. 현재의 데이터셋이 ImageNet과 유사하지만 소량의 경우: 기존 모델의 마지막 레이어만 학습시키는 편이 좋다. 데이터가 조금 더 많고, imageNet과 유사한 경우: 모델 전체를 fine tuning X-ray / CAT scans 과 같은 의료영상처럼 ImageNet과 다르며 데이터가 그나마 많은 경우: 더 많은 레이어를 fine tuning 요즘에는 transfer learning은 일반적인 경우로 많이 사용되고 있다.따라서 자신의 task와 유사한 데이터셋으로 학습된 pretrained model을 확인하고 다운받는다.그리고 이 모델의 일부를 초기화시키고 자신의 데이터로 모델을 fine tune 한다.Reference http://cs231n.stanford.edu/2017/syllabus.html https://velog.io/@guide333/%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-CS231n-7%EA%B0%95-Training-Neural-Networks-Part-2-2" }, { "title": "CS231N chapter 6 - Training Neural Network part 1", "url": "/posts/cs231n6/", "categories": "Classlog, CS231N", "tags": "CS231N, CNN", "date": "2021-09-22 13:00:00 +0900", "snippet": " 5강 리뷰 1) History of CNN 2) CNN pooling padding stride 1x1 conv layer Training Neural NetworksNN 학습을 위해 필요한 기본 설정들에는 Activation Functions, Data Preprocessing, Weight Initialization, Batch Normailzation 등이 있다.또한 hyperparameter optimization, model ensemble, 그리고 학습이 잘 되고 있는지 확인하는 Training Dynamics도 있다.part 1에서는 activation function, data preprocessing, weight initialization, batch normailzation, learning precess, hyperparameter optimization을 배울 것이다.Activation FunctionFC/CNN 을 학습할 때, 데이터 입력이 들어오면 가중치와 곱하고 bias를 더한다.그 후 actviation 연산을 거치게 된다.(activation function)활성함수의 예시로는 다음과 같다.이번 강의에서는 다양한 활성함수와 그들간의 trade-off를 다뤄볼 것이다.sigmoid먼저 sigmoid를 살펴보자면, 1/(1+e^-x) 의 형태를 가지고 있다.입력을 받아서 그 입력을 [0,1] 사이의 값이 되도록 만든다. 입력의 값이 크면 sigmoid의 출력은 1에 가깝고, 입력값이 작으면 0에 가까워진다.sigmoid는 뉴런의 firing rate를 포화(saturation)시키는 것으로 해석할 수 있기에 과거에는 많이 사용되었지만, 크게 3가지의 문제점을 가지고 있어 지금은 잘 사용하지 않는다.sigmoid의 문제점 포화된 뉴런(0이나 1에 가까운 값을 내는 뉴런)이 gradient를 없앤다.(= vanishing gradient)sigmoid를 computational graph의 형태로 살펴보면, 데이터 x와 출력이 존재한다.backpropagation를 위해 gradient를 계산한다.먼저 (dL/dσ)가 있을 것이고, sigmoid gate를 지나 local sigmoid function의 gradient인 (dL/dx)를 구하기 위해 chain rule를 적용한다.즉, dL/dx = (dσ/dx) * (dL/dσ) 가 된다.x = -10 일 때의 gradient(dL/dx)는 그림에서 보듯이 0 이다. 그렇게 되면, 거의 0에 가까운 값들이 backpropagation 될 것이다. 때문에, 그 뒤에 전달되는 모든 gradient는 모두 죽어(=0)버린다.x = 0 일 때는 backprop가 잘 진행될 것이다.x = 10 일 때는 x = -10일 때와 같이 모든 gradient가 0 이 된다.따라서 x가 아주 크거나 아주 작다면 gradient가 계속 0으로 죽어버린다. sigmoid의 출력값이 0을 중심으로 하지 않는다.(not zero-centered)neural network에서, 입력값 x가 항상 양수일 경우를 가정해보자.x는 가중치 w와 곱해지고 활성함수를 통과할 것이다.wixi + b 가 활성함수 f를 통과하면 f(∑wixi + b)가 된다.항상 양수를 가정했으므로 sigmoid를 거친 값들은 항상 양수이다.backpropagation 시에 local gradient를 생각해보면, 우선 dL/df를 계산한다.w에 대한 gradient는 다음과 같다.dL/dw = dL/df * df/dw여기서 L은 loss 함수, f = w^T * x + b 이다. f 식을 통해df/dw = x 를 구할 수 있다.따라서 dL/dw = (dL/df) * x 이다.파라미터의 gradient는 입력값에 의해 영향을 받으며, 만약 입력값이 모두 양수라면 파라미터의 부호는 일정하게 된다.이렇게 되면 파라미터를 업데이트할 때 계속 증가하거나 계속 감소할 수 밖에 없기 때문에 매우 비효율적이다.w를 이차원적 예제로 생각해보자. w에 대한 두개의 축으로 이루어져 있다.전부 양수 또는 음수로 업데이트된다는 것은 gradient가 이동 가능한 방향은 두 방향뿐이다.파란선이 최적의 w 라고 한다면 빨간 화살표는 파란색 방향으로 내려갈 수가 없다. exp()의 계산 비용이 비싸다. (= 계산이 오래 걸린다)tanh그 다음으로 tanh 활성함수를 보고자 한다.sigmoid와 비슷하지만 범위가 [-1,1] 이라는 것이 차이점이다. 또한, zero-centered 형태이다.이를 통해 sigmoid의 두번째 문제를 해결할 수 있다. 하지만 여전히 gradient가 죽는다.ReLUReLU를 살펴보자.ReLU는 f(x) = max(0,x)의 형태를 가지고 있다.이 함수는 element-wise 연산을 수행하며 입력이 음수면 값이 모두 0이 되고, 양수면 입력값 그대로 출력한다.ReLU의 가장 큰 장점은 양의 값에서는 포화(saturation)되지 않는다.또한, 계산 효율이 아주 뛰어나다. ReLU는 단순히 max 연산이기 때문이다.실제 뉴런의 입/출력 값을 확인했을 때에도 sigmoid보다 ReLU가 더 가깝다.ReLU의 문제점은 zero-centered가 아니라는 것이다.또한, 음수의 경우 saturation된다. 즉, x&amp;lt;=0의 경우 gradient가 0이 된다.초록,빨강 선이 ReLU를 의미하고, DATA CLOUD는 training data이다.ReLU가 Data Cloud와 겹치지 않을 경우에는 dead ReLU가 발생할 수 있다.dead ReLU에서는 activate가 일어나지 않고 업데이트되지 않는다.반면 active ReLU는 일부는 active되고 일부는 active되지 않을 것이다.dead ReLU가 발생하는 경우는 2가지인데, 첫번째 경우는 가중치의 초기화가 잘못되어 가중치 평면이 data cloud와 멀리 떨어져 있을 때다.이때는 ReLU가 절대 활성화되지 않고, gradient가 업데이트되지 않는다.두번째는 흔한원인으로 learning rate가 지나치게 클 경우이다. 처음에 적절한 ReLU로 시작할 수 있어도 update를 지나치게 크게하면 갑자기 죽어버리는 경우가 생긴다.실제로 ReLU를 사용할 때는 10-20%가 dead ReLU가 되지만 이는 학습에 크게 영향을 주지 않는다.추가적으로, update시에 active ReLU가 될 가능성을 조금이라도 더 높여주기 위해서 ReLU를 초기화할 때 positive bias를 추가해주는 경우가 있다.하지만 이 방법은 잘 사용하지 않고 zero-bias를 사용한다.leaky ReLU / PReLU / ELUReLU를 약간 변형한 leaky ReLU 가 있다. negative에도 기울기를 살짝 주어 ReLU의 문제점인 saturation 되는 문제가 해결한다.여전히 계산도 효율적이다. 또한, 수렴도 빨리 할 수 있을 뿐더러 dead ReLU 현상도 더 이상 없다.다른 모델로 pararmetric ReLU 도 있다.negative space에 기울기가 있다는 점에서 Leaky ReLU와 유사하나 backpropagation가 α라는 파라미터로 결정된다.α는 정해놓는 것이 아니라 backprop로 학습시키는 파라미터이다.또 다른 모델로 ELU 라는 것이 있다. ReLU와 leaky ReLU의 중간 정도로 보면 된다. ReLU의 이점을 그대로 가져오면서 zero-mean에 가까운 출력값을 보인다.문제는 negative에서 기울기를 가지지 않고 saturation 된다. 또한, 복잡한 exp()를 계산해야 한다는 단점이 있다.장점으로는 좀 더 noise(잡음)에 강인하다. 이런 deactivation(불활성화)이 좀 더 강인함을 줄 수 있다고 주장한다.Maxout NeuronMaxout Neuron 은 입력을 받아드리는 특정한 기본 형식을 미리 정의하지 않고, α라는 파라미터를 추가한다. α는 backprop를 통해 학습된다.w1에 x를 내적한 값 + b1, w2에 x를 내적한 값 + b2 의 최댓값을 사용한다.Maxout은 ReLU와 leaky ReLU의 일반화된 형태이다.위의 두 개의 선형함수를 취하기 때문이다.이 때문에, 선형적인 형태를 띄지만 뉴련이 포화되지 않고 gradient도 죽지 않는다.그러나 뉴런당 파라미터가 2배가 된다는 단점이 있다.실제로는 ReLU를 가장 많이 사용하고 작동도 잘 된다.다만 ReLU를 사용하려면 learning rate를 아주 조심스럽게 결정해야 한다.Data Preprocessing이제는 실제 네트워크를 훈련시켜 볼 것이다.일반적으로 입력 데이터는 전처리를 해야 한다.가장 대표적인 전처리 과정은 zero-mean(데이터를 전체의 평균값으로 빼준다)으로 만들거나 normalize(데이터를 표준 편차로 나눈다)한다.zero-centered 하는 이유는 sigmoid함수에서 다룬 것과 같이 입력이 모두 양수/음수인 경우 모든 뉴런이 양/음수인 gradient를 갖기 때문이다. 하지만, 모두 양/음수인 gradient를 갖는다는 것은 좋은 최적화가 아니다.정규화를 하는 이유는 모든 차원이 동일한 범위 안에 있게 하여 전부 동등한 기여를 하도록 만드는 것이다.이미지의 경우 실제로는 전처리로 zero-centering 정도만 한다. 이미지는 이미 각 차원 간에 스케일이 어느정도 맞춰져 있기 때문에 정규화는 하지 않는다.또한, 이미지를 다룰 때는 굳이 입력을 더 낮은 차원으로 만들지 않는다.CNN에서는 원본 이미지 자체의 spatial 정보를 이용해서 이미지의 spatial structure을 얻을 수 있도록 한다.보통 입력이미지의 사이즈를 서로 맞춰주는데 네트워크에 들어가기 전에 평균값을 빼준다. 평균값은 미니배치 단위로 학습을 시킨다고 해도 평균은 train data 전체의 평균을 계산하여 사용한다.하지만, 채널마다 평균을 독립적으로 계산하는 경우도 있다.VGGNet의 경우 RGB별로 평균을 구하여 학습데이터와 테스트데이터의 각 채널에 뺀다. 이미지 전처리 zero-mean이 sigmoid의 문제점을 해결해주지는 않는다. 처음에는 zero-mean이기에 해결되지만 deep network에서의 구동이라면 점점 갈수록 non-zero-mean이 될 것이다.Weight Initialization2-layer Neural Network 의 경우에 가중치를 어떻게 초기화시켜야 하는지 알아보자.만약 모든 w=0이라면 동일한 w를 사용하므로 모든 뉴런이 같은 연산을 하게 된다. 따라서 출력값은 모두 같고 gradient도 모두 같게 될 것이다.따라서 0이 아닌 임의의 작은 값으로 초기화해야 한다.이 경우 초기 w를 표준정규분포(standard gaussian)에서 샘플링한다.좀 더 작은 값을 위해 0.01을 나눠 표준편차를 1e-2로 만든다.이런 식으로 모든 가중치를 임의의 값으로 초기화한다. 하지만 이 방법은 deep Network에서 문제가 발생한다.10개의 layer로 이루어진 network가 있고 layer당 500개의 뉴런이 있다고 하자.activation function으로 tanh를 사용하고 가중치를 임의의 작은 값으로 초기화 시킨다.데이터를 랜덤하게 생성하여 forward pass 시킨다.각 layer별 출력의 평균과 표준편차를 볼 수 있다. 평균은 tanh의 zero-centered 특성상 거의 0에 수렴한다. 표준편차를 보면 가파르게 줄어든다.밑의 그래프는 layer별 평균과 표준편차를 나타낸 것이다.w가 너무 작은 값들이라 출력값이 계속 수렴되다가 결국 0이 된다.backward pass로 넘어가서 gradient를 구해보자.각 layer의 입력값이 매우 작기에 입력값은 층을 지날수록 점점 0에 수렴한다.upstream gradient와 w에 대한 local gradient인 x를 서로 곱하여 gradient를 업데이트하는데, x가 너무 작으면 업데이트되는 gradient도 점점 작아지고 결국 0에 수렴하여 업데이트되지 않는다.그래서 w를 0.01 대신 1을 넣어 가중치 w를 초기화해보자.가중치가 크기 때문에 w*x를 tanh에 집어넣으면 값들이 saturation(포화)될 것이다.saturation되면 gradient는 0이 되어 가중치가 업데이트되지 않는다.출력이 항상 -1 또는 1이 된다.Xavier initialization합리적인 w를 찾을 수 있는 좋은 방법 중 하나로 standard gaussian에서 랜덤으로 뽑은 값을 입력의 수로 나누어 스케일링한다.기본적으로 Xavier initialization이 하는 일은 입/출력의 분산을 맞춰주는 것이다.입력의 수가 많으면 w가 작아지고, 입력의 수가 작으면 더 큰 값을 얻게 된다.각 layer의 입력이 unit gaussian이길 원하면 이런 형태의 초기화 기법을 사용할 수 있다.여기서 가정하는 것은 linear activation이 있다고 가정하는 것이다. tanh의 경우 tanh의 active 영역(gadient가 0이 아닌 곳은 선형적 형태를 띔)안에 있다고 가정하는 것이다.ReLU에서 실제로 ReLU의 특성때문에 반 밖에 작동하지 않기 때문에, Xavier initialization을 사용하면 잘 동작되지 않는다.그래서 입력데이터의 개수를 2로 나눠줘야 잘 동작된다고 한다.Batch Normalizationgaussian의 범위로 activation을 유지시키는 것에 관한 다른 아이디어가 있다. 레이어의 출력이 unit gaussian이길 바란다.따라서 강제로 layer의 출력을 unit gaussian(0~1 사이에 존재)로 만들어준다.즉, 각 layer에서 나온 batch 단위의 activation을 현재 batch에서 계산한 평균과 분산을 이용해 정규화함으로써 unit gaussian(평균이 0, 분산이 1 의 분포)으로 만든다.batch당 N개로 구성된 D차원인 학습 데이터 X가 있다고 하자.각 차원별로 실제 평균과 분산을 독립적으로 구한 후, 한 batch내에서 전부 계산해서 정규화한다. unit guassian범위를 가진 activation으로 만든다.batch normalization은 FC나 conv layer 직후, 활성함수 직전에 넣어준다. def forward(self,x): x = x.view(-1, 28*28) x = self.fc1(x) x = self.batch_norm1(x) x = F.relu(x) x = F.dropout(x, training = self.training, p = self.dropout_prob) # dropout은 학습 과정 속에서 랜덤으로 노드를 선택해 가중값이 업데이트되지 않도록 하지만 평가 과정 속에서는 모든 노드를 이용해 output을 계산하기 때문에 학습 상태와 검증 상태에서 다르게 적용돼야 한다. x = self.fc2(x) x = F.log_softmax(x, dim = 1) return x깊은 네트워크의 경우 각 layer의 가중치 w가 지속적으로 곱해져서 bad scaling effect가 발생하는데 정규화가 이를 상쇄시켜준다.Batch normalization은 입력의 스케일만 살짝 조정해주는 역할이기 때문에 FC나 Conv 어디든 적용할 수 있다.Conv layer에서는 normalization을 차원마다 하는 것이 아니라 같은 activation maps들은 같이 normalize해준다. 즉 activation map마다 평균과 분산을 하나씩만 구한다. CNN이 데이터의 spatial structure을 잘 유지하기를 원하기 때문이다.batch normalization에서는 입력값을 normalize 연산을 한 후, scaling/shifting 연산을 추가한다.이를 통해 normalize 된 값들을 γ(분산)는 scaling의 효과를, β(평균)는 shifting 연산하여 unit gaussian으로 정규화된 값을 다시 원상복구할 수 있도록 해준다. 즉 BN하기 전 값으로 돌아간다.이 때, γ와 β는 학습 가능한 파라미터이다. 학습시켜서 다시 indentity function(Y=X)가 되도록 하는 이유는 유연성을 얻기 위해서이다. unit gaussian이 필요하면 사용하지 않고, 아니라면 unit gaussian을 다시 scaling 하고 shifting 할 수 있다. Q. shift와 scale을 추가시키고 학습시키면 indentity mapping(원래 상태로 돌아옴)이 되서 BN이 사라지는 것이 아닌가?감마와 베타를 학습시키게 되면 indentity가 되지는 않는다. 일정량 변하긴 하지만 원래 상태로 돌아올 정도는 아니다. 때문에 여전히 효과를 볼 수 있다. BN 과정을 요약하자면 mini-batch에서의 평균, 분산을 계산 그것을 통해 normalize 다시 scaling, shifting 우리가 BN을 하는 이유는 layer의 입력을 normalization하기 위함이다. normalization하여 데이터를 gaussian distribution으로 만들어주는 것이다. BN의 장점으로는 batch normalization은 gradient의 흐름을 보다 원활하게 해주어 학습이 더 잘되도록 해준다. learning rate를 더 키울 수 있게 해준다. 초기화 기법을 다양하게 사용할 수 있게 해준다. 각 layer의 출력은 해당 데이터 하나 뿐만 아니라 batch 안에 존재하는 모든 데이터들(평균,분산)에 영향을 받기 때문에 regularization의 역할도 한다. 이는 오직 하나의 샘플에 대한 값이 아니라 batch 내의 모든 데이터가 하나로 묶일 수 있다. BN의 평균과 분산은 학습데이터에서 구한 것이므로 test time에서는 추가적인 계산을 하지 않고, training에서 계산한 평균과 분산을 test에도 사용한다.Babysitting the Learning Process지금까지는 네트워크를 설계하는 것에 대해 배웠다. 이제는 학습과정을 어떻게 모니터링하고 hyperparameter을 조절할 것인지 배워보자.data preprocessing우선 첫 단계는 데이터 전처리다. 전처리에는 zero-mean을 사용한다.Choose architecture그 후로는 architecture을 선택해야 한다.CNN, LSTM 등 다양하게 선택 가능하다.Sanity check네트워크를 초기화하고 forward pass를 하고난 후 loss를 구한다. 여기서 regularization은 0으로 준다.만약 softmax classifier를 사용할 때 가중치가 작으면 loss는 -log(1/N) (N:number of classes)이 되어야 한다.초기 loss가 정상이라면 regularization에 1e3으로 설저한다. 그 결과로 loss가 증가한다. 손실함수에 regularization term이 추가되기 때문(일반화되기 때문)이다.sanity check(Training)학습을 시작한다. 우선 데이터의 일부만 학습시킨다.데이터가 적으면 당연히 overfit되고 loss가 많이 줄어든다.이때는 regularization을 사용하지 않고 loss가 내려가는지만 확인한다.Training학습 시에 가장 중요하고, 가장 먼저 찾아야할 hyperparameter는 learning rate이다.전체 데이터셋에 regularization을 약간씩 주면서 적절한 learning rate를 찾아본다.처음에는 1e-6로 설정한다. 그 결과 loss가 잘 변하지 않는 것을 확인할 수 있다.loss가 잘 줄어들지 않는 가장 큰 요인은 learning rate가 지나치게 작은 경우다. 지나치게 작으면 gradient 업데이트가 충분히 일어나지 않게 되기 때문이다.loss가 잘 변하지 않음에도 training/calidation accuracy가 급상승하는 경우가 있다. 출력인 확률 값들이 퍼져있어서 loss는 잘 변하지 않지만 학습을 하고 있기에 이 확률 값들이 조금씩 옳은 방향으로 바뀌고 있다.가중치는 서서히 변하지만 accuracy는 가장 큰 값만 취하기 때문에 급상승할 수 있는 것이다.learning rate를 1e6으로 바꿔보자.cost가 nan이라 함은 cost가 발산한다는 것이다.이것은 learning rate가 너무 높을 경우 발생한다. 보통 learning rate는 1e-3~1e-5 사이의 값을 사용한다. 이 범위 사이의 값을 이용해서 cross-validation을 하여 learning rate가 적절한지 판단한다.Hyperparameter OptimizationCross-Validation하이퍼파라미터를 최적화시키는 방법에는 cross-validation이 있다.cross-validation은 training set으로 학습시키고 validation set으로 평가하는 방법이다.이는 3강에 잘 설명되어 있다.우선 coarse stage(first stage)에서는 넓은 범위에서 값을 골라낸다.epoch 몇 번 만으로도 현재 값이 잘 작동하는지 알 수 있다. 따라서 epoch을 적게 설정하여 Nan이 나오거나 loss가 줄지 않느지를 보면서 빠르게 찾는 것이 좋다.coarse가 끝나면 어느 범위가 잘 동작하는지 알 수 있다.fine stage(seconde stage)에서는 좀 더 좁은 범위를 설정하고 학습을 길게 시켜 최적의 값을 찾는다.Nan으로 발산하는 징조를 미리 감지할 수 있다. train동안 cost가 어떻게 변하는지 살펴봐야 한다. 이전의 cost보다 더 커진다면 학습이 잘못되고 있는 것이다.5 epoch을 돌며 coarse search하는 과정이다.여기서 확인할 것은 validation accuracy이다. 높은 val_acc에는 빨간색으로 표시해놨다.빨간색으로 표시해둔 지역이 바로 fine-stage를 시작할만한 범위가 된다.범위를 좁혀 reg는 0~10^-4, lr는 10^-4~10^-3로 설정한다.한 가지 중요한 점은 하이퍼파라미터 최적화시에는 log scale로 값을 주는 것이 좋다.파라미터 값을 샘플링할 때, (10^-3 ~ 10^-6)이 아닌 (-3~-6)와 같이 10의 차수 값만 샘플링해야 한다는 말이다.여기서 가장 좋은 acc인 빨간색 박스에 포함되는 learning rate는 전부 10e-4 사이에 존재한다. 이는 최적값이 범위(0~10e-4)의 경계부분에 존재한다. 최적의 값이 10e-5나 10e-6일 수도 있기 때문에 효율적으로 탐색할 수 없다.따라서, 최적의 값이 정한 범위의 중앙에 위치하도록 즉, (10e-2~10e-6)과 같이 범위를 잘 설정해주는 것이 중요하다.Grid Search하이퍼파라미터를 찾는 또 다른 방법으로 grid search라는 방법도 있다.하이퍼파라미터를 고정된 값과 간격으로 샘플링하는 것이다.하지만 이는 random search보다 좋지 않다.random search하는 경우 오른쪽에 있는 것과 같이 최적의 값을 찾을 수 있지만, 왼쪽의 경우 겹치는 grid가 많아 최적의 값을 찾기 어려운 상태다. 또한 grid search가 더 오래 걸린다.하이퍼파라미터에는 learning rate말고도 decay schedule, update type, regularization, network architecture 등이 있다.실제로 cross validation을 통한 하이퍼파라미터 최적화를 엄청 많이 돌려봐야 한다.cross validation으로 많은 하이퍼파라미터를 직접 돌려보고 어떤 값이 좋은지 판단해야 한다.loss가 발산하거나 가파르게 내려가다가 정체기가 생기면 learning rate가 높은 것이고, 너무 평평하면 너무 낮은 것이다.또한, loss가 평평하다가 갑자기 가파르게 내려가면 초기화의 문제다. backprop가 초기에는 잘 되지 않다가 학습이 진행되면서 회복되는 경우에 해당된다.train_acc와 va_acc가 큰 차이를 보인다면 overfitting일 것이다. 따라서 regularization의 강도를 높여야 한다.gap이 없다면 overfit되지 않은 것이기에 정확도를 높일 수 있는 충분한 여유가 존재한다.가중치의 크기 대비 가중치 업데이트의 비율을 지켜볼 필요도 있다.우선 파라미터의 norm을 구해서 가중치의 규모를 계산한다.업데이트 사이즈도 norm을 통해 구할 수 있고, 업데이트 사이즈를 통해 크게 업데이트되는지를 알 수 있다.가중치의 크기 대 가중치 업데이트 비율은 대략 0.001정도가 적당하다.이는 디버깅할 때 유용하다.Reference http://cs231n.stanford.edu/2017/syllabus.html https://velog.io/@guide333/%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-CS231n-6%EA%B0%95-Training-Neural-Networks-Part-1" }, { "title": "Motion Keypoint Detection AI Contest with HRNet &amp; YOLOv5", "url": "/posts/motionkeypoint/", "categories": "Review, DACON", "tags": "DACON, keypoint, Detection", "date": "2021-09-16 13:00:00 +0900", "snippet": "일단 yolo를 중점적으로 볼 예정이므로 test에 쓰이는 yolo만 보고 나중에 keypoint 를 공부한 후 hrnet 공부 후 볼 예정 정리 코드 Test with YOLOv5 Yolov5 + HRNet에르모팀 DACON 코드github정리[학습] HRNet 기반의 pose_HRNet model Learning rate = 1e-3, Optimizer = Adam train set : valid set = 0.8 : 0.2 Epoch = 20 , 14일 떄가 가장 결과가 좋았지만, mse loss가 7.4e-5보다 낮아지기 시작하면 score가 올라가기 시작해서 해당 지점에서 멈춤 Loss = gaussian -&amp;gt; heatmap mse input size = [640,480] , 사이즈가 클수록 성능이 좋음 sigma = 3.0 batch size = 8 shifting = True aid = false , 팔이 가려져서 예측을 못하는 경우가 있음 startify = True with direction , 각 사람별로 A,B,C,D,E 카메라로 촬영함. 그래서 카메라 별로 나누어 생각 use different joints weights = False , 손목 눈코입 발목이 가려지는 경우가 많아서 해당 케이스에 강한 weight를 줘봤는데 똑같음[전처리] 강한 augmentation 움직임에 대한 흐려짐 현상 해결을 위해 motionblur, blur, imagecompression, gaussianblur 사용 여러 색상에 대한 강인함을 위해 channelshuffle, huesaturation, rgbshift 사용 테스트 데이터의 경우 yolov5와 각 자세에 따라서 다른 가로 세로 비율을 주어 affine transformation 수행[후처리] Dark-pose 사용코드!pip install -U https://github.com/albu/albumentationsTest with YOLOv5이 코드의 경우 HRNet을 위주로 사용하고, Test의 경우에 yolov5 모델을 써서 사람을 detection한 다음에 해당 bbox의 중점을 활용해서 추론해줬음 yolov5를 통해서 사람의 위치를 뽑아내고, 해당 중점을 기준으로 900x900을 뽑아내었음 해당 bbox를 바로 사용하여 test했을때 결과가 그렇게 좋지 않아서 해당 bbox(900x900)의 중점만 활용해서 누워 있는 경우, 서 있는 경우, 앉아 있는 경우를 구분하여 따로 잘라내었음(test dataset에서 설명)!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txttest_df = pd.read_csv(os.path.join(main_dir,&#39;sample_submission.csv&#39;))yolov5 = torch.hub.load(&#39;ultralytics/yolov5&#39;,&#39;yolov5x&#39;,pretrained = True)yolov5.eval()test_data = {&#39;path&#39;:[],&#39;x1&#39;:[],&#39;y1&#39;:[],&#39;x2&#39;:[],&#39;y2&#39;:[]}total_test_imgs = []for i in range(len(test_df)): total_test_imgs.append(os.path.join(test_img_path, test_df.iloc[i, 0]))for idx, path in tqdm(enumerate(total_test_imgs)): w, h = 900, 900 offset = np.array([w//2, h//2]) img = cv2.imread(path)[:, :, ::-1] # img 읽어오기 centre = np.array(img.shape[:-1])//2 # center 좌표 x1,y1 = centre - offset x2,y2 = centre + offset with torch.no_grad(): cropped_img = img[x1:x2, y1:y2, :] results = yolo_v5([cropped_img]) cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB) try: for i in range(len(results.xyxy[0])): xyxy = results.xyxy[0][i].detach().cpu().numpy() cropped_centre = np.array([(xyxy[0]+xyxy[2])/2, (xyxy[1]+xyxy[3])/2], dtype=np.float32) box_w = (xyxy[2]-xyxy[0])/2 * 1.2 box_h = (xyxy[3]-xyxy[1])/2 * 1.2 new_x1 = np.clip(int(cropped_centre[0] - box_w), 0, img.shape[1]) new_x2 = np.clip(int(cropped_centre[0] + box_w), 0, img.shape[1]) new_y1 = np.clip(int(cropped_centre[1] - box_h), 0, img.shape[0]) new_y2 = np.clip(int(cropped_centre[1] + box_h), 0, img.shape[0]) if int(xyxy[-1]) == 0: new_x1 += y1 new_x2 += y1 new_y1 += x1 new_y2 += x1 test_data[&#39;path&#39;].append(path) test_data[&#39;x1&#39;].append(new_x1) test_data[&#39;y1&#39;].append(new_y1) test_data[&#39;x2&#39;].append(new_x2) test_data[&#39;y2&#39;].append(new_y2) except Exception as e: print(&quot;Skip&quot;)test_df = pd.DataFrame(data=test_data)test_df.to_csv(os.path.join(main_dir, &#39;test_bbox.csv&#39;), index=False)cfg = SingleModelTestConfig(input_size=[640, 480], target_type=&#39;gaussian&#39;)predictions = bbox_test(cfg, yaml_name=&#39;for_test.yaml&#39;, filp_test=True, debug=False)preds = []for prediction in predictions: row = [] for x, y in zip(prediction[:, 0], prediction[:, 1]): row.append(x) row.append(y) preds.append(row)preds = np.array(preds)# submission ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡsubmission_path = os.path.join(data_dir, &#39;sample_submission.csv&#39;)submission = pd.read_csv(submission_path)save_dir = os.path.join(main_dir, &quot;submissions&quot;)save_path = os.path.join(save_dir, &#39;submission.csv&#39;)submission.iloc[:, 1:] = predssubmission.to_csv(save_path, index=False)https://dacon.io/competitions/official/235701/codeshare/2490?page=1&amp;amp;dtype=recentHRNet + Detectron2https://dacon.io/competitions/official/235701/codeshare/2500?page=1&amp;amp;dtype=recent" }, { "title": "Landmark classification ", "url": "/posts/Landmark/", "categories": "Review, DACON", "tags": "DACON, classification", "date": "2021-09-16 13:00:00 +0900", "snippet": "# 라이브러리import pandas as pdimport numpy as npimport osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;import argparseimport torchfrom glob import globfrom PIL import Imagefrom tqdm import tqdmfrom torch.utils.data import Dataset, DataLoaderfrom torch import nn, optimfrom torch.nn import Conv2d, AdaptiveAvgPool2d, Linearimport torch.nn.functional as F# data 파일을 copy해서 옮기는 방법import zipfileimport osfrom tqdm import tqdmtoo_long = []zipf = os.listdir(&#39;/content/drive/My Drive/d/train_zip&#39;)zipf = sorted([os.path.join(&#39;/content/drive/My Drive/d/train_zip&#39;, z) for z in zipf])os.makedirs(&#39;train&#39;, exist_ok=True)for z in tqdm(zipf): try: # s = z.split(&#39;.&#39;)[0].split(&#39;/&#39;)[-1] zipfile.ZipFile(z).extractall() except: too_long += [z]from shutil import copyfilefor p in os.listdir(&#39;/content/drive/My Drive/d/648&#39;): copyfile(os.path.join(&#39;/content/drive/My Drive/d/648/&#39;, p) , os.path.join(&#39;/content/train/648/&#39;, p))a=0for i in os.listdir(&#39;train&#39;): a+=len(os.listdir(os.path.join(&#39;train&#39;, i)))# arguments# train_csv_exist, test_csv_exist는 glob.glob이 생각보다 시간을 많이 잡아먹어서 iteration 시간을 줄이기 위해 생성되는 파일입니다.# 이미 생성되어 있을 경우 train_csv_exist.csv 파일로 Dataset을 생성합니다.parser = argparse.ArgumentParser()parser.add_argument(&#39;--train_dir&#39;, dest=&#39;train_dir&#39;, default=&quot;./public/train/&quot;)parser.add_argument(&#39;--train_csv_dir&#39;, dest=&#39;train_csv_dir&#39;, default=&quot;./public/train.csv&quot;)parser.add_argument(&#39;--train_csv_exist_dir&#39;, dest=&#39;train_csv_exist_dir&#39;, default=&quot;./public/train_exist.csv&quot;)parser.add_argument(&#39;--test_dir&#39;, dest=&#39;test_dir&#39;, default=&quot;./public/test/&quot;)parser.add_argument(&#39;--test_csv_dir&#39;, dest=&#39;test_csv_dir&#39;, default=&quot;./public/sample_submission.csv&quot;)parser.add_argument(&#39;--test_csv_exist_dir&#39;, dest=&#39;test_csv_exist_dir&#39;, default=&quot;./public/sample_submission_exist.csv&quot;)parser.add_argument(&#39;--test_csv_submission_dir&#39;, dest=&#39;test_csv_submission_dir&#39;, default=&quot;./public/my_submission.csv&quot;)parser.add_argument(&#39;--model_dir&#39;, dest=&#39;model_dir&#39;, default=&quot;./ckpt/&quot;)parser.add_argument(&#39;--image_size&#39;, dest=&#39;image_size&#39;, type=int, default=256)parser.add_argument(&#39;--epochs&#39;, dest=&#39;epochs&#39;, type=int, default=100)parser.add_argument(&#39;--learning_rate&#39;, dest=&#39;learning_rate&#39;, type=float, default=0.001)parser.add_argument(&#39;--wd&#39;, dest=&#39;wd&#39;, type=float, default=1e-5)parser.add_argument(&#39;--batch_size&#39;, dest=&#39;batch_size&#39;, type=int, default=64)parser.add_argument(&#39;--train&#39;, dest=&#39;train&#39;, type=bool, default=True)parser.add_argument(&#39;--load_epoch&#39;, dest=&#39;load_epoch&#39;, type=int, default=29)args = parser.parse_args()# 경로 생성if not os.path.isdir(args.model_dir) : os.ma kedirs(args.model_dir)# 파이토치 agrs.Dataset 생성 for Train / Testclass TrainDataset(Dataset) : def __init__(self, args) : self.train_dir = args.train_dir self.train_csv_dir = args.train_csv_dir self.train_csv_exist_dir = args.train_csv_exist_dir self.args = args self.train_image = list() self.train_label = list() if not os.path.isfile(self.train_csv_exist_dir) : self.train_csv = pd.read_csv(self.train_csv_dir) self.train_csv_exist = self.train_csv.copy() self.load_full_data() self.train_csv_exist.to_csv(self.train_csv_exist_dir, index=False) else : self.load_exist_data() def load_full_data(self) : for i in tqdm(range(len(self.train_csv))) : filename = self.train_csv[&#39;id&#39;][i] fullpath = glob(self.train_dir + &quot;*/*/&quot; + filename.replace(&#39;[&#39;, &#39;[[]&#39;) + &quot;.JPG&quot;)[0] label = self.train_csv[&#39;landmark_id&#39;][i] self.train_csv_exist.loc[i,&#39;id&#39;] = fullpath self.train_image.append(fullpath) self.train_label.append(label) def load_exist_data(self) : self.train_csv_exist = pd.read_csv(self.train_csv_exist_dir) for i in tqdm(range(len(self.train_csv_exist))) : fullpath = self.train_csv_exist[&#39;id&#39;][i] label = self.train_csv_exist[&#39;landmark_id&#39;][i] self.train_image.append(fullpath) self.train_label.append(label) def __len__(self) : return len(self.train_image) def __getitem__(self, idx) : image = Image.open(self.train_image[idx]) image = image.resize((self.args.image_size, self.args.image_size)) image = np.array(image) / 255. image = np.transpose(image, axes=(2, 0, 1)) label = self.train_label[idx] return {&#39;image&#39; : image, &#39;label&#39; :label}class TestDataset(Dataset) : def __init__(self, args) : self.test_dir = args.test_dir self.test_csv_dir = args.test_csv_dir self.test_csv_exist_dir = args.test_csv_exist_dir self.args = args self.test_image = list() self.test_label = list() if not os.path.isfile(self.test_csv_exist_dir) : self.test_csv = pd.read_csv(self.test_csv_dir) self.test_csv_exist = self.test_csv.copy() self.load_full_data() self.test_csv_exist.to_csv(self.test_csv_exist_dir, index=False) else : self.load_exist_data() def load_full_data(self) : for i in tqdm(range(len(self.test_csv))) : filename = self.test_csv[&#39;id&#39;][i] fullpath = glob(self.test_dir + &quot;*/&quot; + filename.replace(&#39;[&#39;, &#39;[[]&#39;) + &quot;.JPG&quot;)[0] label = self.test_csv[&#39;id&#39;][i] self.test_csv_exist.loc[i,&#39;id&#39;] = fullpath self.test_image.append(fullpath) self.test_label.append(label) def load_exist_data(self) : self.test_csv_exist = pd.read_csv(self.test_csv_exist_dir) for i in tqdm(range(len(self.test_csv_exist))) : fullpath = self.test_csv_exist[&#39;id&#39;][i] label = self.test_csv_exist[&#39;id&#39;][i] self.test_image.append(fullpath) self.test_label.append(label) def __len__(self) : return len(self.test_image) def __getitem__(self, idx) : image = Image.open(self.test_image[idx]) image = image.resize((self.args.image_size, self.args.image_size)) image = np.array(image) / 255. image = np.transpose(image, axes=(2, 0, 1)) label = self.test_label[idx] return {&#39;image&#39; : image, &#39;label&#39; :label}# DataLoader 생성을 위한 collate_fndef collate_fn(batch) : image = [x[&#39;image&#39;] for x in batch] label = [x[&#39;label&#39;] for x in batch] return torch.tensor(image).float().cuda(), torch.tensor(label).long().cuda()def collate_fn_test(batch) : image = [x[&#39;image&#39;] for x in batch] label = [x[&#39;label&#39;] for x in batch] return torch.tensor(image).float().cuda(), label# Dataset, Dataloader 정의train_dataset = TrainDataset(args)test_dataset = TestDataset(args)train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_test)# glob.glob 사용하여 dataset 설정하는 방법class LandmarkDataset(Dataset): def __init__(self, mode: str = &#39;train&#39;, transforms: transforms = None): self.mode = mode self.image_ids = glob.glob(f&#39;./data/{mode}/**/**/*&#39;) if self.mode == &#39;train&#39;: with open(&#39;./data/train.csv&#39;) as f: labels = list(csv.reader(f))[1:] self.labels = {label[0]: int(label[1]) for label in labels} self.transforms = transforms def __len__(self): return len(self.image_ids) def __getitem__(self, index: int) -&amp;gt; Tuple[Tensor]: image = Image.open(self.image_ids[index]).convert(&#39;RGB&#39;) image_id = os.path.splitext(os.path.basename(self.image_ids[index]))[0] if self.transforms is not None: image = self.transforms(image) if self.mode == &#39;train&#39;: label = self.labels[image_id] return image, label else: return image_id, image# transformtransforms_train = transforms.Compose([ transforms.Resize((224, 224)), transforms.RandomHorizontalFlip(), transforms.RandomChoice([ transforms.ColorJitter(0.2, 0.2, 0.2, 0.2), transforms.RandomResizedCrop(224), transforms.RandomAffine( degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=15, resample=Image.BILINEAR) ]), transforms.ToTensor(), transforms.Normalize((0.4452, 0.4457, 0.4464), (0.2592, 0.2596, 0.2600)),])transforms_test = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.4452, 0.4457, 0.4464), (0.2592, 0.2596, 0.2600)),])# dataloadertrainset = LandmarkDataset(&#39;train&#39;, transforms_train)testset = LandmarkDataset(&#39;test&#39;, transforms_test)train_loader = DataLoader( trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)test_loader = DataLoader( testset, batch_size=16, shuffle=False, num_workers=num_workers)# Model# 여기서는 간단한 CNN 3개짜리 모델을 생성하였습니다.class Network(nn.Module) : def __init__(self) : super(Network, self).__init__() self.conv1 = Conv2d(3, 64, (3,3), (1,1), (1,1)) self.conv2 = Conv2d(64, 64, (3,3), (1,1), (1,1)) self.conv3 = Conv2d(64, 64, (3,3), (1,1), (1,1)) self.fc = Linear(64, 1049) def forward(self, x) : x = F.relu(self.conv1(x)) x = F.relu(self.conv2(x)) x = F.relu(self.conv3(x)) x = AdaptiveAvgPool2d(1)(x).squeeze() x = self.fc(x) return xmodel = Network()model.cuda()criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.wd)# args.Training# 매 epoch마다 ./ckpt 파일에 모델이 저장됩니다.# validation dataset 없이 모든 train data를 train하는 방식입니다.if args.train : model.train() for epoch in range(args.epochs) : epoch_loss = 0. for iter, (image, label) in enumerate(train_dataloader) : pred = model(image) loss = criterion(input=pred, target=label) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.detach().item() print(&#39;epoch : {0} step : [{1}/{2}] loss : {3}&#39;.format(epoch, iter, len(train_dataloader), loss.detach().item())) epoch_loss /= len(train_dataloader) print(&#39;\\nepoch : {0} epoch loss : {1}\\n&#39;.format(epoch, epoch_loss)) torch.save(model.state_dict(), args.model_dir + &quot;epoch_{0:03}.pth&quot;.format(epoch)) # 모든 epoch이 끝난 뒤 test 진행 model.eval() submission = pd.read_csv(args.test_csv_dir) for iter, (image, label) in enumerate(test_dataloader): pred = model(image) pred = nn.Softmax(dim=1)(pred) pred = pred.detach().cpu().numpy() landmark_id = np.argmax(pred, axis=1) confidence = pred[0,landmark_id] submission.loc[iter, &#39;landmark_id&#39;] = landmark_id submission.loc[iter, &#39;conf&#39;] = confidence submission.to_csv(args.test_csv_submission_dir, index=False)# Test# argument의 --train을 False로 두면 Test만 진행합니다.# Softmax로 confidence score를 계산하고, argmax로 class를 추정하여 csv 파일로 저장합니다.# 현재 batch=1로 불러와서 조금 느릴 수 있습니다.else : model.load_state_dict(torch.load(args.model_dir + &quot;epoch_{0:03}.pth&quot;.format(args.load_epoch))) model.eval() submission = pd.read_csv(args.test_csv_dir) for iter, (image, label) in enumerate(test_dataloader): pred = model(image) pred = nn.Softmax(dim=1)(pred) pred = pred.detach().cpu().numpy() landmark_id = np.argmax(pred, axis=1) confidence = pred[0,landmark_id] submission.loc[iter, &#39;landmark_id&#39;] = landmark_id submission.loc[iter, &#39;conf&#39;] = confidence submission.to_csv(args.test_csv_submission_dir, index=False)def train( model: nn.Module, data_loader: DataLoader, criterion: nn.Module, optimizer: optim, scheduler: optim.lr_scheduler, device: torch.device): epoch_size = len(trainset) // batch_size num_epochs = math.ceil(max_iter / epoch_size) iteration = 0 losses = AverageMeter() # scores = AverageMeter() # corrects = AverageMeter() # model.train() for epoch in range(num_epochs): if (epoch+1)*epoch_size &amp;lt; iteration: continue if iteration == max_iter: break correct = 0 for inputs, labels in data_loader: inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) loss = criterion(outputs, labels) confs, preds = torch.max(outputs.detach(), dim=1) optimizer.zero_grad() loss.backward() optimizer.step() losses.update(loss.item(), inputs.size(0)) scores.update(gap(preds, confs, labels), inputs.size(0)) corrects.update((preds == labels).float().sum(), input_size(0)) iteration += 1 if iteration % args.verbose_eval == 0: print(f&#39;[{epoch+1}/{iteration}] Loss: {losses.val:.4f}&#39; \\ f&#39; Acc: {corrects.val:.4f} GAP: {scores.val:.4f}&#39;) if iteration in [20000, 70000, 140000]: scheduler.step()def test(model: nn.Module, data_loader: DataLoader, device: torch.device): submission = pd.read_csv(&#39;./data/sample_submission.csv&#39;, index_col=&#39;id&#39;) model.eval() for image_id, inputs in data_loader: inputs = inputs.to(device) outputs = model(inputs) outputs = nn.Softmax(dim=1)(outputs) landmark_ids = np.argmax(outputs, axis=1) confidence = outputs[0, landmark_ids] submission.loc[image_id, &#39;landmark_id&#39;] = landmark_ids submission.loc[image_id, &#39;conf&#39;] = confidence submission.to_csv(&#39;submission.csv&#39;)# FocalLossimport torchfrom torch import nnimport mathclass FocalLoss(nn.Module): def __init__(self, gamma=0, eps=1e-7): super(FocalLoss, self).__init__() self.gamma = gamma #print(self.gamma) self.eps = eps self.ce = torch.nn.CrossEntropyLoss(reduction=&quot;none&quot;) def forward(self, input, target): logp = self.ce(input, target) p = torch.exp(-logp) loss = (1 - p) ** self.gamma * logp return loss.mean()import numpy as npimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchvision import transforms, datasetsif torch.cuda.is_available(): DEVICE = torch.device(&#39;cuda&#39;)else: DEVICE = torch.device(&#39;cpu&#39;)print(&#39;Using PyTorch version:&#39;, torch.__version__, &#39; Device:&#39;, DEVICE)BATCH_SIZE = 32EPOCHS = 10train_dataset = datasets.CIFAR10(root = &quot;../data/CIFAR_10&quot;, train = True, download = True, transform = transforms.Compose([ # 불러오는 이미지 데이터에 전처리 및 augmentation을 다양하게 적용할 때 이용하는 메서드 transforms.RandomHorizontalFlip(), # 해당 이미지를 50%의 확률로 좌우 반전하는 것을 의미 transforms.ToTensor(), # 0에서 1사이의 값으로 정규화하며 딥러닝 모델의 input으로 이용될 수 있도록 tensor 형태로 변환 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # totensor 형태로 전환된 이미지에 대해 또 다른 정규화를 진행하는 것을 의미 # 정규화를 진행할 때는 평균과 표준편차가 필요한데 rgb순으로 평균을 0.5씩 적용test_dataset = datasets.CIFAR10(root = &quot;../data/CIFAR_10&quot;, train = False, transform = transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)for (X_train, y_train) in train_loader: print(&#39;X_train:&#39;, X_train.size(), &#39;type:&#39;, X_train.type()) print(&#39;y_train:&#39;, y_train.size(), &#39;type:&#39;, y_train.type()) breakclass CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1) self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1) self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2) self.fc1 = nn.Linear(8 * 8 * 16, 64) self.fc2 = nn.Linear(64, 32) self.fc3 = nn.Linear(32, 10) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.pool(x) x = self.conv2(x) x = F.relu(x) x = self.pool(x) x = x.view(-1, 8 * 8 * 16) x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) x = F.log_softmax(x) return xmodel = CNN().to(DEVICE)optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)criterion = nn.CrossEntropyLoss()#ㅡㅡㅡ IMAGENET 데이터로 학습이 된 ResNet34 모델을 불러온 후 Fine Tuning 해보기 ㅡㅡㅡmodel = models.resnet34(pretrained = True) # pretrained = true는 imageNet 데이터를 잘 분류할 수 있도록 학습된 파라미터를 resnet34 모델에 적용해 불러오는 것을 의미num_ftrs = model.fc.in_features # CIFAR-10 데이터를 분류하기 위해 최종 output의 벡터를 10 크기로 설정해야 한다. # CIFAR-10 데이터의 클래스 종류는 10개이므로 각 클래스를 포현하는 원핫인코딩 값의 크기가 10이기 때문이다.model.fc = nn.Linear(num_ftrs, 10) model = model.cpu()optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) for epoch in range(1, EPOCHS + 1): # ImageNet 데이터에 학습이 완료된, 즉 학습을 통해 얻게 된 파라미터를 resnet34 모델의 초기 파라미터로 설정한 후 # CIFAR-10 이미지 데이터를 10개의 train(model, train_loader, optimizer, log_interval = 200) # 클래스로 분류할 수 있도록 기존 실습 내용과 동일한 환경으로 실험을 진행 test_loss, test_accuracy = evaluate(model, test_loader) print(&quot;\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n&quot;.format( epoch, test_loss, test_accuracy))# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡdef train(model, train_loader, optimizer, log_interval): model.train() for batch_idx, (image, label) in enumerate(train_loader): image = image.to(DEVICE) label = label.to(DEVICE) optimizer.zero_grad() output = model(image) loss = criterion(output, label) loss.backward() optimizer.step() if batch_idx % log_interval == 0: print(&quot;Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}&quot;.format( epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))def evaluate(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for image, label in test_loader: image = image.to(DEVICE) label = label.to(DEVICE) output = model(image) test_loss += criterion(output, label).item() prediction = output.max(1, keepdim = True)[1] correct += prediction.eq(label.view_as(prediction)).sum().item() test_loss /= (len(test_loader.dataset) / BATCH_SIZE) test_accuracy = 100. * correct / len(test_loader.dataset) return test_loss, test_accuracyfor epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, log_interval = 200) test_loss, test_accuracy = evaluate(model, test_loader) print(&quot;\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n&quot;.format( epoch, test_loss, test_accuracy))Reference https://dacon.io/competitions/official/235585/overview/description" }, { "title": "K-Fashion AI contest", "url": "/posts/KFashion/", "categories": "Review, DACON", "tags": "DACON, segmentation", "date": "2021-09-16 13:00:00 +0900", "snippet": "코드 공유가 거의 안되있으므로 베이스라인만 빠르게 따고 넘어가고 최신 segmentation 모델 사용해서 직접 만들어보기베이스라인 1,2,3, 평가 산식 구성하는 방법, coco api를 활용한 mask 도식, train/valid 데이터셋 분할 코드, 파이토치를 활용한 mask rcnnReference https://dacon.io/competitions/official/235672/overview/description" }, { "title": "CS231N chapter 5 - Convolutional Neural Networks", "url": "/posts/cs231n5/", "categories": "Classlog, CS231N", "tags": "CS231N, CNN", "date": "2021-09-14 13:00:00 +0900", "snippet": " 4강 리뷰 1) backpropagation 2) Calculation of the local gradient in computational graph. 3) Neural Network linear layer을 쌓고 그 사이에 비선형 layer를 추가하여 neural network를 만듦 History of Convolutional Neural Network이제 우리가 배울 것은 Convolutional layer이다. 이 layer은 Spatial Structure를 유지한다.1957년 최초로 perceptron을 구현했고, 여기서 가중치 w를 업데이트하는 방법이 처음 등장했다.1960년에 multi-layer perceptron network(MLP Network)를 발명했다.1986년에 backpropagation이 등장했고, 신경망의 학습이 시작되었다. 2006년에 deep learning의 학습 가능성이 거론되면서 전체 신경망을 backpropagation하거나 fine-tuning하는 식으로 진행되었다.model = models.resnet34(pretrained = True) 하지만 컴퓨터 속도와 용량 등의 환경적 요인에 의해 연구가 진행되기 어려웠다.그러다 2012년 imageNet 분류를 통해 에러를 극적으로 감소시키는 AlexNet이 처음 등장하면서 CNN이 알려지기 시작했다.AlexNet에서 처음으로 batch normalization을 사용했고, GPU를 2대 활용하여 대규모의 데이터를 활용하였다.CNN은 이미지 classification, 이미지 검색, 객체 탐지, segmentation, Lidar와 함께 자율주행, 얼굴 인식, 비디오 분석, 포즈 평가 등 다양하게 활용되고 있다.CNN Fully-Connected LayerFC layer에서 하는 일은 어떤 벡터를 가지고 연산을 하는 것이다. 우선 32x32x3 image를 input한다. 이 이미지를 길게 펴서 3072x1차원의 벡터로 만든다. 가중치 w를 곱한다. (W*x) - 이 예시에서는 w는 10x3072 행렬 activation, 즉 output을 얻는다. - 1x10 크기로 출력 될 것이다. 즉 class가 10개고, 그에 대한 각각의 score가 나올 것이다.Convolution layerconvolutional layer와 기존의 FC layer의 차이점은 conv layer는 기존의 구조를 그대로 보존시킨다는 것이다.이 작은 5x5x3 filter가 우리가 가진 가중치가 되는 것이다.filter의 각 w와 공간적 내적을 통해 숫자를 출력한다. 즉, 32x32x3의 입력 이미지에 5x5의 filter를 슬라이딩하면서 값을 추출하는 것이다.이때, 3이 의미하는 것은 RGB, 3채널을 뜻하고 depth라고 부른다. filter depth은 입력 이미지의 depth과 동일하게 부여해야 한다.convolution layer에서는 filter의 depth을 제외한 나머지의 크기(ex. 5x5)를 정할 수 있다.입력 이미지에 filter를 슬라이딩하여 내적을 구하는 것을 convolve한다고 한다.기본적으로 계산은 w^T * x + b (b: bias term) 의 식으로 내적을 계산한다.image가 5x5, filter이 3x3 일 경우 5x5 중에서 filter의 크기인 3x3크기만큼만 filter과 내적을 통해 1개의 결과값을 출력한다. 이를 이미지 좌측 상단부터 우측 하단까지 모든 픽셀을 슬라이딩하면서 계산을 하게 되는 것이다.위의 사진을 참고했을 때, 다음 계산은 [(4 1 2),(1 1 0),(2 1 0)] 과 filter의 내적이 된다.이 1개의 숫자가 나오는 계산식이 w^T * x + b이다.이 슬라이딩을 통해 얻은 값을 모으면 28x28x1의 이미지가 되는데, 이를 activation map이라 한다.출력된 activiation map의 크기는 filter의 크기와 숫자 그리고 슬라이딩을 어떻게 하느냐에 따라 달라진다.보통 convolution layer에서는 여러 개의 필터를 사용한다. 필터마다 다른 특징을 추출할 수 있기 때문이다.5x5x3의 또 다른 초록색 필터를 가져왔다. 이를 슬라이딩하여 동일하게 28x28x1의 activation map을 얻을 수 있다.6개의 필터를 사용하게 되면 총 6개의 map을 얻을 수 있다. 이렇게 나온 activation map을 모아 크기 28x28x6 의 map을 만들 수 있다.사이 사이에 ReLU와 같은 actiavtion function을 넣어 conv-ReLU가 반복시킬 수 있다. 또한, 중간 중간에 pooling layer도 추가하기도 한다.위의 그림은 32x32x3의 input image를 6개의 5x5x3 filter 6개를 각각 슬라이딩 하여 28x28x6의 activation map을 얻는다.이 activation map이 다시 input image가 되어 10개의 5x5x6 filter를 통해 24x24x10의 activation map을 얻는다.pooling layerpooling layer는 convolution시 downsampling을 할 때 이미지의 크기를 줄이면 데이터의 손실이 발생할 수 있다. 정보를 최대한 보존하면서 크기를 줄이기 위해 stride 값을 1로 지정하고 pooling을 통해 크기를 줄인다.또한 pooling을 하면 overfitting을 막을수도 있다. 입력 데이터에 과도하게 맞춰지는 경우를 방지하는 것이다.즉, 입력 데이터로 학습한 데이터만 정답으로 인식하고, 그 데이터의 회전, 특정 좌표로 이동, 자름 등의 이미지에 대해서는 유연하지 못한 것을 방지한다.pooling 중에서도 max pooling을 주로 사용하는데, max pooling 이란 pool size내에서 최댓값만 뽑아내는 것을 말한다.위의 그림은 2x2 max pooling 하는 과정이다. 첫번째 그림의 경우 1 2 0 1 중에서 가장 큰 2를 골라 추출한다.Pooling은 파라미터의 수를 줄이기 때문에 관리가 쉽고, 이미지의 차원을 공간적으로 줄여준다. 하지만 depth는 줄이지는 못한다.또한 pooling은 downsampling을 위해 사용하기 떄문에, pooling할 때 비슷한 역할인 padding하지는 않는다.padding필터를 적용할 때, 모서리에 있는 이미지 데이터의 정보는 중앙에 있는 정보보다 비교적으로 적게 연산되므로 이를 조절해주기 위한 방법이다.즉, 패딩은 출력 데이터의 공간적 크기를 조절해주기 위해 사용하는 파라미터다. 공간적 크기를 조절하여 모서리에 있는 데이터 정보에 대한 연산 수와 중앙에 있는 데이터 정보에 대한 연산 수를 얼추 맞춰 정보가 누락되지 않도록 한다.입력 데이터의 크기와 출력 데이터의 크기를 같게 해주는 것을 zero-padding이라 한다.zero-padding은 입력 이미지 바깥에 0으로 이루어진 pixel을 넣어주는 것이다.따라서 conv layer에서는 zero padding으로 출력 이미지의 크기와 데이터를 보존하고, 이미지의 크기를 줄이는 것은 pooling에서 진행한다.stride입력 데이터와 필터를 연산할 때 기본적으로 1칸씩 움직이면서 연산한다. 이를 stride 값이 1이라 할 수 있다.즉, stride는 입력 데이터에 필터를 적용할 때 이동할 간격을 조절해주는 파라미터다.pooling 과 stride는 기능은 비슷하나 stride가 조금 더 좋은 성능을 보이기 떄문에 stride를 더 많이 쓰는 추세다.stride = 2 일 때의 pooling을 진행하는 모습이다.여러 개의 layer을 쌓다보면 각 필터들이 계층적으로 학습한다.앞쪽 필터 즉, 입력층과 가까울수록 low-level feature을 학습한다. 뒤로 갈수록 점점 복잡해져 객체와 닮은 것들이 출력으로 나오는 것을 볼 수 있다.수많은 conv-relu-pooling layer들을 지나 마지막에는 3D conv output을 1차원으로 펴서 FC(fully connected) layer에 집어넣어 최종 스코어를 계산한다.activation map의 차원을 구하는 공식은 다음과 같다.NxNx3 input image, fxfx3 filter 일 경우 activation map 의 크기는 :[(N - f + 1)/stride size] x [(N - f + 1)/stride size]x number of filterex) N=6 , f=2, 5개 filter, stride=2 =&amp;gt; 3x3x5 의 activation map예를 들어 32x32x3 의 입력 이미지를 10개의 5x5 filter를 stride=1, padding=2 로 convolution 한다면output size는 32x32x10이다.padding=2 이므로 양쪽에 2씩 더한 후 [{(32 + 2*2)-5}/1] + 1 = 이므로 32가 된다.이 때 파라미터의 개수는5 * 5 * 3 + 1(bias) = 76개의 파라미터를 가지고, 필터의 개수 10을 곱하면 760개의 파라미터가 존재한다.보통 filter의 갯수는 2의 제곱수로 설정하고, stride는 1 or 2로 설정한다. 필터는 3x3 또는 5x5를 많이 사용한다.1x1 convolution layer이미지의 depth 크기를 줄이는 방법으로 1x1 convolution layer을 사용할 수 있다.위의 그림은 56x56x64의 입력이미지에 32개의 1x1 conv 필터를 적용한다.1x1 conv를 한다는 것은 1x1 만한 filter로 64개를 모두 내적한 다음 이것을 한 픽셀로 축소시킨다는 것이다.즉 1x1 conv = 1x1x64 filter가 된다. FC layer에서의 filter와 1x1 conv filter의 차이는 크기는 같을 수 있으나 FC layer는 고정된 크기를 입력이미지로 받지만, conv layer은 공간적으로 더 큰 입력 이미지를 받는다는 점이 다르다.이것을 32번을 수행하므로 차원은 줄어들지 않는 56x56이고, 32개 필터이므로 56x56x32 가 output size가 된다.1x1 convolution 은 depth를 줄여주는 역할을 한다.위의 경우처럼 56x56x64 였던 이미지에 1x1 conv를 k개 필터를 적용한다면 output은 56x56xk가 된다.Convolution layer Code추가로간단한 conv architecture을 가져와봤다.# keras CNNmodel = keras.Sequential([ keras.layers.Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), padding=&quot;same&quot;,activation=tf.nn.relu), # 32는 필터의 개수, kernel_size는 필터의 크기 keras.layers.MaxPool2D(pool_size(2,2), strides=(2,2), padding=&quot;same&quot;), # (pool_size: 윈도우 크기, strides:윈도우가 움직이고자 하는 거리, padding:데이터가 없는 부분에 zero-padding) keras.layers.Conv2D(64, kernel_size=(3,3), padding=&quot;same&quot;,activation=tf.nn.relu), keras.layers.MaxPool2D(pool_size(2,2), strides=(2,2), padding=&quot;same&quot;), keras.layers.Flatten(), keras.layers.Dense(1024, activation=&quot;relu&quot;) keras.layers.Dense(10,activation=&quot;softmax&quot;) keras.layers.Dropout(0.25)])# torch CNNclass CNN(nn.Module): # 마찬가지로 nn.module 클래스를 상속받는다. def __init__(self): super(CNN, self).__init__() self.conv1 = nn.Conv2d( # 2차원 이미지 데이터를 nn.conv2d메서드를 이용해 convolution연산을 하는 filter in_channels = 3, # filter의 크기는 상관없지만, 채널 수를 이미지의 채널 수와 동일하게 맞춰야 한다. 그래서 3으로 지정 out_channels = 8, # 설정해주는 filter개수만큼 depth가 정해진다. filter개수만큼 앞뒤로 쌓아 feature map을 형성하기 때문이다. kernel_size = 3, # filter의 크기를 정하는 부분이다. 스칼라 값으로 설정하려면 가로*세로 크기인 filter을 이용해야 한다. # 여기서 3x3 로 이용한다. 3x3 filter가 이미지 위를 9개의 픽셀 값과 filter 내에 있는 9개의 파라미터 값을 연산으로 진행 padding = 1) # 중앙에 비해 가장자리가 덜 연산되기에 테두리에 0을 채워 연산 횟수를 동일하게 맞춰주는 것 # 1로 설정하면 왼쪽에 1층, 오른쪽에 1층, 위 1층, 아래 1층으로 채움 self.conv2 = nn.Conv2d(in_channels = 8, # 앞에서 filter 수를 8로 했기에 입력을 8로 한다. out_channels = 16, # depth를 16으로 지정 kernel_size = 3, padding = 1) self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2) # convolution을 통해 feature map이 생성됐을 때, feture map을 부분적으로 이용한다. # convolution을 통해 다양한 수치가 생성되기 때문이다. maxpool2d는 2차원의 feature map 내에서 가장 큰 값만 이용 self.fc1 = nn.Linear(8 * 8 * 16, 64) # convolution을 하는 이유는 이미지 내 픽셀과의 조합을 통한 특징을 추출 # feature map을 다양한 convolution을 통해 추출 후 1차원으로 펼친 후 여러 층의 fully connected layer를 통과시켜 분류 # 1차원으로 펼쳐도 이미 주변 정보를 반영한 결괏값으로 존재하기 때문에, 기존의 한계를 해결할 수 있다. # 앞의 conv1,conv2 연산에서 feature map의 크기는 forward부분을 계산한 결과 8*8*16크기의 map이 된다. # 즉 8*8의 2차원 데이터 16개가 겹쳐 있는 형태이다. 이를 1차원 데이터로 펼쳐 이용한다. self.fc2 = nn.Linear(64, 32) self.fc3 = nn.Linear(32, 10) # 원핫인코딩으로 표현된 벡터 값과 loss를 계산해야 하므로 10으로 설정 def forward(self, x): # forward propagation을 정의 x = self.conv1(x) x = F.relu(x) # convolution연산을 통해 생성된 feature map값에 비선형 함수 relu를 적용 x = self.pool(x) # maxpooling을 통해 생성된 feature map에 다운 샘플링을 적용한다. x = self.conv2(x) x = F.relu(x) x = self.pool(x) x = x.view(-1, 8 * 8 * 16) x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) x = F.log_softmax(x) return xReference http://cs231n.stanford.edu/2017/syllabus.html https://velog.io/@guide333/%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-CS231n-5%EA%B0%95-2-Neural-Networks https://dsbook.tistory.com/72?category=780563" }, { "title": "CS231N chapter 4 - Backpropagation and Neural", "url": "/posts/cs231n4/", "categories": "Classlog, CS231N", "tags": "CS231N, backpropagation, neural-network", "date": "2021-09-13 13:00:00 +0900", "snippet": " 3강 리뷰 1) hinge loss 2) loss function에 대해 SVM loss, softmax loss 3) 최적의 loss를 갖게 하는 파라미터 w를 찾기위해 optimization - gradient descent와 Stochastic gradient descent위의 그림은 computational graph라고 부른다. 총 loss를 구하는 linear classifier 과정을 나타낸 것으로, (f= xW)를 통해 score를 구하고, s로부터 hinge loss를 통해 구한 Li 와, W로부터 구한 R(regularization term)을 더해 총 loss를 더한다.Backpropagation(역전파)이 computational graph를 통해 gradient를 구해보자.각 parameter에 대한 편미분 값을 얻어 경사하강법을 진행할 것이다.먼저 computational graph에 순차적으로 값을 집어넣어 함수 값을 구한다.(forward)그 후 역방향으로 차례대로 미분하여 gradient를 구하는데, 이를 backpropagation이라 부른다.(backward)backpropagation : 구하고자 하는 gradient는 x,y,z 각각에 대한 f의 gradient f의 미분값을 global gradient라고 부르고, node에 위치한 것의 미분값은 local gradient라 부른다. 전체 과정을 chain rule을 이용하여 역계산각 local gradient는 편미분을 통해 계산한다. 그 후에 x,y는 f와 바로 연결되어 있지 않고, q와 연결되어 있으므로 chain rule을 이용하여 원하는 값을 구한다.이 경우 w,x에 대한 함수 f는 1/ e^ - (w0x0 + w1x1 + w2) 이다.이 graph는 f를 계산하는데 사용되는 일련의 과정을 node로 그려놓은 것이다.backpropagation을 하기 위해서는 뒤에서부터 시작하게 될 것이다. 최종 변수에 대한 출력의 gradient는 1이다. (df/df =1)한 단계 뒤로 가서 1/x 이전의 input에 대한 gradient를 살펴보기 위해 1/x를 x에 대해 미분한다. 그 후 x 값을 집어넣고 chain rule을 적용하면 -0.53을 구할 수 있다.그 후 input + 1 = output 이므로 output을 fc(x)라 했을 때 df/dx = 1 이다. 따라서 gradient는 -0.53sigmoid처럼 식을 나누지 않고 미분해도 복잡하지 않은 경우 묶어서 생각해볼 수도 있다.pattern in backward flow add gate: 입력되는 기울기를 그대로 출력값으로 전파해준다.(local gradient가 항상 1)(gradient distributor) max gate: z와 x 중 큰 값에만 gradient를 그대로 출력하고, 나머지는 0이 된다.(gradient router) mul gate: 반대편의 값을 들어오는 기울기와 곱해주면 된다.(32=6, -42=8) (gradient switcher) branches: 두 가닥에서 넘어노느 gradient를 받는 경우 그냥 더해준다.gradients for vector변수 x,y,z 에서 숫자대신 벡터를 넣는다고 하자. 모든 흐름은 똑같다. 차이점은 gradient를 구할 때 야코비안 행렬(jacobian matrix)을 사용하는 것이다. 야코비안 행렬의 각 행은 입력값에 대한 출력의 편미분으로 이루어져 있다.입력값과 출력값이 4096차원 벡터면 야코비안 행렬의 크기는 4096x4096이 된다.minibatch를 사용하므로 총 409,600x409,600 의 행렬이 된다. 이는 너무 커서 작업에 실용적이지 않다. 실제로는 이 거대한 jacobian을 계산할 필요가 없다.jacobian 행렬은 대각 행렬의 형태를 띄고 있다. 다만 1만 존재하는 것이 아닌 0과 1이 섞여 있을 뿐이다.위와 같이 forward pass하여 각각의 gate의 값을 구한다. 각 게이트의 값은 행렬곱하면 구할 수 있다.w는 2x2행렬이고 x는 2차원의 벡터이다.q는 w*x인데,f를 q에 대한 표현으로 나타내려고 하는데, 이는 q의 L2 norm과 같이 q1의 제곱과 q2의 제곱을 합친 것과 같다. 이런 방식으로 q를 통해 f를 구한다.출력 node에서의 gradient는 항상 1이다.backpropagation을 하는데, L2 이전의 중간 변수인 q에 대한 gradient를 구하고자 한다.q는 2차원의 벡터이고, 우리가 찾고 싶은 것은 q의 각각의 요소가 f의 최종 값에 어떤 영향을 미치는지다.qi에 대한 f의 gradient는 2qi가 된다. 왜냐하면 f는 q^2이기 떄문이다.이를 벡터의 형태로 ∇qf 로 나타낼 수 있다. 2q를 통해 [0.44 0.52]를 얻을 수 있다.이 후, w의 gradient를 구할 것이다.여기서 chain rule을 사용한다. w에 대한 q의 local gradient를 계산하려면 요소별 연산을 다시 해야 한다. 각각의 q에 대한 영향을 보자.w의 각 요소에 대한 q의 각 요소를 계산하기 위해 jacobian을 사용한다.q = W*x이다. q의 첫번째 요소의 gradient는 wji에서 w11에 대한 q1이다. 이는 x1에 해당한다.w에 대한 f의 gradient를 xj와 wi,j에 대한 qk의 식으로 일반화 할 수 있다.qk에 대한 f의 미분을 합성할 때 df/dqk * dqk/dwij 로 표현가능하다. 이전에 계산한 것과 같이 df/dqk = 2qk 이고, wij에 대한 qk의 gradient는 xj* 1(k=i이면) 가 된다.따라서 2qixj가 된다.유도한 식을 벡터화된 형식으로 작성하면 ∇wf = 2q*x^T 가 된다.이제 xi에 대한 qk를 구할 것이다.dqk / dxi = wki 와 같다. 그 후 chain rule을 사용하여 df/dqk와 dqk/dxi 로 표현하고, df/dqk = 2qk, dqk/dxi=Wk,i 이다.이를 벡터화하면 ∇xf = 2W^T * q, 즉 ∇wf에서 w를 x로 바꾼 것과 같다.아래는 forward pass, backward pass의 API를 이용하여 sigmoid 함수를 포함한 함수의 역전파를 구현한 것이다. 위에서는 forward를 구현하고, 그것을 이용해 backward를 구현했다.forward pass에서는 노드의 출력을 계산하고, backward pass에서는 gradient를 계산한다.class ComputationalGraph(object): def forward(inputs): # 1. [pass inputs to input gates] # 2. forward the computational graph for gate in self.graph.nodes_topologically_sorted(): gate.forward() return loss # the final gate in the graph outputs the loss def backward(): for gate in reversed(Self.graph.nodes_topologically_sorted()): gate.backward() # little piece of backpropagation (chain rull applied) return inputs_gradientsx,y,z가 스칼라일 떄, x,y를 입력으로 받고 z를 리턴하는 computational graph를 구현한 것이다.backward로 진행할 때는 입력으로 dz를 받고 출력으로 입력인 x,y의 gradient인 dx,dy를 출력한다. Summary neural nets will be very large: impractical to write down gradient formula by hand for all parameters backpropagation = recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates implementations maintain a graph structure, where the nodes implement the forward() / backward() API forward: compute result of an operation and save any intermediates needed for gradient computation in memory backward: apply the chain rule to compute the gradient of the loss function with respect to the inputs Neural Networks이제까지는 선형함수 f=W*x만을 다뤘다. 이제는 n-layer Neural Network를 다루고자 한다. n층 신경망은 비선형함수인 활성함수(activation function)을 사용한다.2층 신경망에서는 활성함수인 max함수로 ReLU를 사용한다.2-layer Neural Network로서의 f는 W2max(0,W1x) 로 표현가능하다. 즉, w1 과 x를 행렬 곱한 것에 대한 max를 취하고 W2를 곱해준다.max(w1 * x)를 h라 할 때,h는 w1에서 나오는 score 값인데, 이 score 값을 h에 입력하므로 h의 input data는 w1의 score가 될 것이다. 마찬가지로 w2는 h의 score가 된다.이때, h를 hidden layer이라 한다. 입력층과 출력층 사이의 모든 층을 가르킨다.2층 신경망을 간단히 코드화하면import numpy as npfrom numpy.random import randn# N: Class number, D_in: input행에 들어오는 입력 행렬 size, H:hidden size, D_out: output sizeN, D_in, H, D_out = 64, 1000, 100, 10 x, y = randn(N, D_in), randn(N, D_out) # x,y는 random input data로, randn은 정규 분포에서 matrix array를 생성하는 함수w1, w2 = randn(D_in, H), randn(H, D_out) for t in range(2000): h = 1/(1+np.exp(-x.dot(w1))) y_pred = h.dot(w2) loss = np.square(y_pred - y).sum() print(t, loss) grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h.T.dot(grad_y_pred) grad_h = grad_y_pred.dot(w2.T) grad_w1 = x.T.dot(grad_h * h * (1-h)) w1 -= 1e-4 * grad_w1 w2 -= 1e-4 * grad_w2인간의 뇌에 있는 뉴런을 그림으로 표현하면 아래와 같다.수상돌기에서 신호를 받으면 세포체에서 신호를 종합해서 다른 뉴런으로 신호를 전달시킨다.이는 각 computational node가 하는 일과 비슷하다.입력값(신호)인 x0,x1,x2가 들어오면 세포체(함수)에서 가중치w와 입력값x를 더한 값을 서로 결합하고, 활성함수에 의해 비선형 함수로 만든 후 다음 노드에 전달한다.뉴런의 형태를 코드로 구현하면 아래와 같다.class Neuron: def neuron_tick(inputs): &#39;&#39;&#39;assum inputs and weights are 1-D nunpy arrays and bias is a number&#39;&#39;&#39; cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function return firing_rate사용 가능한 활성함수가 다양하게 존재한다.실제 뉴런과 가장 비슷한 활성함수가 ReLU라고 하기도 한다. 모든 음수 입력값에 대해서는 0, 양수 입력에 대해서는 선형 함수 형태이다.신경망의 architecture를 그림으로 표현했다. 각 노드가 모두 연결된 fully-connected(FC) layer 형태다.위의 뉴런 함수를 본 따 신경망 코드를 구축했다.# forward pass of 3-layer neural networkf = lambda x: 1.0/(1.0+np.exp(-x)) # activation functionx = np.random.randn(3,1) # random input vector (3x1)h1 = f(np.dot(W1,x) + b1) # calculate first hidden layer (4x1)h2 = f(np.dot(W2,h1)+ b2) # calculate second hidden layer (4x1)output = np.dot(W3,h2) + b3 # output (1x1)reference http://cs231n.stanford.edu/2017/syllabus.html https://velog.io/@lilpark/cs231n-Lecture-4.-Introduction-to-Neural-Networks https://velog.io/@twinklesu914/Standford-SC231N-Deep-Learning-4 https://velog.io/@guide333/%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-CS231n-4%EA%B0%95-1-Backpropagation" }, { "title": "CS231N chapter 3 - Loss function and Optimizations", "url": "/posts/cs231n3/", "categories": "Classlog, CS231N", "tags": "CS231N, loss, optimization", "date": "2021-09-12 13:00:00 +0900", "snippet": " 2강 리뷰 1) image deformation 2) 그를 해결하기 위한 data-driven approach 3) Nearest Neighbor method, K-Nearest Neighbor(KNN) 4) cross validation 5) hyperparameter 6) linear classifier linear classifier은 parametric classifier의 한 종류다. parametric classifier란 training data의 정보가 parameter인 행렬 W로 요약된다는 것을 뜻한다.linear classifier은 이미지를 입력받으면 하나의 긴 벡터로 편다. 데이터를 고차원 공간에서의 일종의 decision boundary를 학습시킨다는 것으로 볼 수 있다.Loss Function위의 사진은 linear classifier을 적용한 classification이다. 정답 class가 가장 높은 score를 가져야 좋은 classifier인데, 위의 숫자를 보았을 때, 정답 class가 그렇게 높은 score를 가지지 못한다. 따라서 이는 좋지 못한 classifier이다.score를 높이기 위해서는, 가중치 W를 어떻게 만들고, 가장 좋은 W를 어떻게 구하는지 알아야 한다.w를 입력받아 각 score를 확인하고, 이 w가 얼마나 괜찮은지를 정량화시켜주는 것이 손실함수, loss function이다.이 loss function을 통해 가장 최적의 hyperparameter를 추정하는 것이 바로 최적화 과정, optimization이다.train data에서 이미지에 해당하는 x와 레이블에 해당하는 y가 있고, 보통 x는 알고리즘의 input에 해당된다.이때, test의 x를 통해 y를 예측하고 정답과 비교하게 된다.Loss, L은 : Li : 손실함수 f(xi,W) : 내가 classifier를 사용해서 나온 class의 score yi : 실제 class의 정답 값 ( 고양이의 3.2 ) N : class 의 수최종적인 L은 결국 각 N개 샘플들의 Loss 평균이다.multi-class SVM lossSVM도 마찬가지로 KOOC 강의 - 머신러닝학습개론 을 참고하면 좋다.multi-class SVM(Support Vector Machine)은 여러 class를 다루기 위한 이진 SVM의 일반화 형태다.이진 SVM이란 두 개의 class만 다룬다는 뜻이다. 각 데이터는 positive/negative로 분류된다.SVM Loss Li 는 :Li 공식은 정답 class의 score가 제일 높을 때만 실행한다.Li을 구하기 위해서는 True인 카테고리를 제외한 나머지 카테고리 Y의 합을 구한다. 다시말해, 맞지 않는 카테고리를 전부 합친다는 것이다. 그리고 올바른 카테고리의 score와 올바르지 않은 카테고리의 score을 비교한다.올바른 카테고리의 score가 올바르지 않은 카테고리의 score보다 높고, 그 격차가 일정 마진(safety margin)이상이라면, 전부 0으로 코딩시킨다. 여기서는 1이 마진에 해당한다.Multi-class SVM 은 정답에 대한 score가 높을수록 loss function은 0에 가까워지고, 그 반대일수록 loss function은 계속 커진다. 이를 그래프의 모양을 따서 hinge loss(경첩) 이라고도 불린다.위의 그림에서 Sj는 분류기의 output으로 나온 예측된 score다. 즉, j번째 class의 score인 것이다. yi는 이미지의 실제 정답 번호이다. x축은 Syi 정답 class의 score이고, y축은 Loss이다.정답 카테고리의 점수가 올라갈수록 loss는 선형적으로 줄어들고, 이 로스는 0이 된 후에도 safety margin을 넘어설 때까지 무조건 더 줄어든다.구하고자 하는 것은 정답 스코어가 다른 스코어들보다 높은지를 보는 것이다. 정답 score의 safety margin이 충분히 높지 않으면 Loss는 커지게 된다.아래의 예시를 보자.우선 정답이 아닌 클래스를 본다. cat이 정답 클래스이므로 car과 flog를 계산한다. 이때, cat 보다 car이 더 높으므로 loss는 발생한다.정답 클래스는 cat 이고, 이것을 car과 비교할 경우 =&amp;gt;sj = car score = 5.1, syi =cat score = 3.2max(0,(Car score) 5.1 - (Cat score) 3.2 + 1(magin)) + max(0,(frog score) -1.7 - (Cat score) 3.2 + 1)= max(0,2.9) + max(0, -3.9) = 2.9따라서 SVM Loss = 2.9 가 된다.이 2.9가 얼마나 분류기가 이 이미지를 잘 분류하는지에 대한 척도이다.이 때, frog는 cat score 보다 훨씬 작으므로 loss가 0이라고 빠르게 구할 수도 있다.또한, car로 loss를 구해본다 치면, car의 score가 frog나 cat과의 차가 1보다 크므로 loss는 0 이 된다.frog도 구해보면 6.3+6.6 = 12.9 가 나온다.이제 전체 training dataset에서 구한 loss들의 평균을 구하면 최종적인 loss를 구할 수 있다.Q1. 만약 car score을 조금 변경시키면 loss에는 무슨 일이 일어나는가? loss는 score 간에 상대적인 차이가 중요하기 때문에 큰 영향이 없다.Q2. loss 값의 최솟값과 최댓값을 구하면 어떻게 되나? 모든 정답 클래스의 score가 제일 크면 모든 loss가 0이므로 최솟값은 0, 최댓값은 infiniteQ3. 초기 w가 0에 가까우면 모든 score 값은 0과 비슷해질 것이다. 이때 loss를 구하면? class의 수 - 1 이다. loss를 계산할 때 정답이 아닌 클래스를 순회한다. 그러면 c-1개의 클래스를 순회하게 될텐데 비교하는 두 스코어가 비슷하니 margin 때문에 우리는 1 score을 얻게 된다. 이는 실제로 유용한 디버깅 전략이다. 처음 training을 시작할 때 loss가 (0-0+1)*(class num-1)= class-1이 아니라면 버그가 존재하는 것이다. Q4. SVM Loss는 정답인 class는 빼고 다 더한다. 그렇다면 정답인 class도 더하게 된다면 어떻게 되는가? 일반적으로 우리가 정답만 제외하고 계산하는 이유는 loss가 0이 되야 하는데, 정답 class를 포함하게 되면 SVM Loos의 평균값이 1이 증가해지기 때문에 1이 가장 좋은 값이 된다.Q5. Loss에서 전체합을 쓰는게 아니라 평균을 쓴다면? 어차피 class의 수는 정해져 있기에 scale이 작아지는 것 이외에는 별 차이없다.Q6. 손실함수를 제곱을 취하게 되면 어떻게 되나? 값이 완전히 달라지게 된다.Q7. W는 정답이 1개 뿐인가? W는 test data에 적용할 때 가장 잘 성능이 측정되는 값을 고르는 것이기 때문에, 여러 개가 존재할 수 있다.Regularizationtraining set에 model이 완전하게 fit하지 못하도록 복잡도를 개선하거나 차수를 낮추는 것을 말한다.위의 그림과 같이 파랑 점들이 train data, 초록 점이 test data라 하고파란 선은 train에서 fit한 decision boundary라 하고, 초록 선이 우리가 원하는 decision boundary라 하자.train을 거쳐 나온 db는 원하던 db가 아니다. 따라서 test data를 입력할 시 성능이 좋지 않다. 이것을 overfitting이라고 한다.이때, train dataset에 model이 완전하게 fit하지 못하도록 regularization term을 추가하여 모델이 좀 더 단순한 w를 선택하도록 도와준다.오컴의 면도날과 같이 일반적으로 더 단순한 것을 선호하기 때문이다. 오컴의 면도날 : 단순성의 원리 혹은 절감의 원리로 불린다. 즉, 필요 이상의 고차원을 상정해서는 안된다는 것이다.이렇게 되면 loss는 이제 data loss 와 regularzation loss 두 가지 항을 가지게 된다. 또, hyperparameter인 ⋋가 생긴다.이 두 항은 trade-off 즉, 반비례하는 경향이 있다.regularization의 두 가지 역할이 있는데, 하나는 모델이 더 복잡해지지 않도록 해주고, 두번째는 모델에 soft penalty를 추가하는 것이다.이 soft penalty를 추가하면 계속 복잡한 모델을 사용하고 싶다면 이 penalty를 감수해야 한다. 라는 상황이 된다.regularization에는 여러 종류가 있다. L1 regularization L1 norm으로 w에 패널티를 부과 행렬 w가 희소행렬이 된다. 작은 가중치는 0으로 수렴 =&amp;gt; 중요한 가중치만 남음 L2 regularization 보편적인 방법으로 weight decay라고도 불린다. 가중치 w에 대한 euclidean norm 또는 squared norm 또는 1/2 * squared norm을 사용 w의 euclidean norm에 패널티를 부과 Elastic net regularization L1 과 L2를 섞은 방법 Max norm regularization L1, L2 대신 man norm을 사용 L2 regularization이 모델을 단순화시키는 과정은 아래와 같다. train data인 x와 서로 다른 두 개의 w가 있다. x는 4차원 벡터, w1=[1 0 0 0], w2=[0.25 0.25 0.25 0.25] linear classification을 할 때 x와 w를 내적한다. x와의 내적이 w1과 w2가 같기에 모델은 동일하다고 판단할 수 있지만, 더 단순한 모델을 선호해야 한다. 이때, w1 보다 w2의 norm이 더 작기 때문에 w2를 더 선호하게 된다.반면 L1의 경우 w1을 더 선호한다. L1은 복잡도를 다르게 정의한다. 가중치 w에 0의 갯수에 따라 모델의 복잡도를 판단한다.L2의 경우 W의 요소가 전반적으로 퍼져있을 때 덜 복잡하다고 판단한다.따라서 복잡도를 어떻게 정의하는지, 어떻게 측정하는지에 따라 다른 값이 나온다.SoftmaxMultinomial logistic regression 으로, 딥러닝에서 많이 사용된다.SVM에서는 score 자체에 대한 해석은 고려하지 않았다. 어떤 분류 문제가 있고, 어떤 모델이 10개의 class에 대해 각 class에 해당하는 10개의 숫자를 출력할 때, multi-class SVM의 경우에는 그 스코어 자체는 크게 신경쓰지 않았다.하지만 여기서의 손실함수는 score 자체에 추가적인 의미를 부여한다.score에 지수를 취해서 양수가 되도록 만든다. 그 후 이 지수들의 합으로 다시 정규화시킨다.이를 통해 확률 분포를 얻는다. 이것이 결국 해당 class일 확률을 의미하는 값이 될 것이다.여기에 log를 취하는데, 손실 함수라는 것은 얼마나 잘되는지가 아니라 얼마나 잘 안되는지에 대해 측정하는 것이다. 따라서 (-)를 붙여야 한다.식은 다음과 같다.Li는 Loss, P는 확률 값이자 softmax function 이다. Sj는 예측한 class별 score이고, Syi는 해당 class의 정답 score이다.정리하자면 score이 있으면 softmax를 거치고, 나온 확률 값에 -log를 한다.Q1. Softmax loss의 최솟값과 최댓값은? 0~inf, 정답인 class의 확률은 1, 아닌 class의 확률은 0일 때 최솟값이 되므로, -log(1) = 0 이다. 그리고 만약 정답 class의 확률이 0이라면 -log(0) = inf 이다.Q2. softmax loss에서의 디버깅은, 즉 score가 모두 0 근처에 모여있는 작은 수일 때 loss는? -log(1/C)=log(C)가 된다.SVM의 경우 일정 margin을 넘기기만 하면 더이상 성능 개선에 신경쓰지 않는다. 하지만 softmax는 지속적으로 성능을 높이려 한다.Optimization최적읜 w를 찾는 방법에는 무엇이 있는지 알아보자.가장 간단하게 떠올릴 수 있는 방법은 Random search 이다. 임의로 샘플링한 w들을 엄청 모아 loss를 계산해 찾는다.이방법은 너무 좋지 않은 방법이다.그 다음으로 gradient descent 방식이 있다.함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다.loss의 gradient를 계산할 때, 미분 코드가 맞는지 확인하기 위해서 수치적 gradient를 사용한다.Gradient Descentwhile True: weights_grad = evaluate_gradient(loss_fun,data,weights) weights += step_size * weights_grad # perform parameter update이 간단한 알고리즘은 우선 w를 임의의 값으로 초기화 한 후 loss와 gradient를 계산하고, 가충치를 gradient의 반대 방향으로 업데이트한다.이것을 반복하다보면 결국에는 수렴하게 된다.여기서 step_size는 하이퍼파라미터로 얼마나 나아갈지를 말해주는 것이다. 이는 learning rate라고도 한다. 트레이닝을 시작할 때 learning rate를 가장 먼저 체크하는 것이 좋다.위의 그림은 손실함수에 대한 것으로 빨간 부분으 낮은 loss를 의미한다.처음에는 임의의 w를 설정하고, 이를 -gradient를 계산하여 결국 가장 낮은 지점으로 도달할 것이다.여기서 손실함수의 공식을 볼 때 전체 loss는 전체 training set의 loss의 평균이고, N이 엄청 커질 수도 있다.그래서 Loss를 계산하는 것은 엄청 오래 걸리는 작업이다.실제로는 더 단순하고 좋은 방법을 사용한다.SGD전체 데이터셋의 gradient와 loss를 계산하기보다 mini-batch를 활용하여 작은 training sample 집합으로 나눠서 학습한다.이를 Stochastic Gradient Descent(SGD)라고 한다.이는 SGD에 대한 식이다. minibatch는 일반적으로 32,64,128,256 등을 사용한다.# minibatch gradient descentwhile True: data_batch = sample_training_data(data,256) # 256단위로 minibatch weights_grad = evaluate_gradient(loss_fun,data_batch_weights) weights += step_size * weights_grad # perform parameter update임의의 minibatch를 만들고, minibatch에서 loss와 gradient를 계산한다. 그리고 w를 업데이트한다.추가적으로 이미지의 특징을 분석하는 방법을 조금 설명하고자 한다.HOGHistogram of oriented gradients는 NN이 유명해지기 전 이미지 특징을 분석하는 방법이다.local orientation edges를 측정함으로써 픽셀안에서 가장 많이 존재하는 edge의 방향을 계산하고, edge direction을 양자화해서 양동이에 넣는다. 그 후 다양한 edge orienation에 대한 히스토그램을 계산한다.즉, 각 edge의 방향에 해당하는 양동이가 존재하고, 픽셀마다 가장 많이 존재하는 방향에 해당되는 양동이에 그 픽셀을 집어넣는다.그렇게 전체 특징 벡터는 각각의 모든 8x8 지역들이 가진 edge orientation에 대한 히스토그램이 되는 것이다.visual wordsbag of words라는 자연어처리에서 사용되는 아이디어에서 영감을 받았다.어떤 문장이 있고 BOW에서 이 문장을 표현하는 방법은 문장의 여러 단어의 발생 빈도를 세서 특징 벡터로 사용한다.이를 이미지에 적용하고자 한 것인데, 다양한 이미지를 nxn 픽셀로 나누고 그 조각들을 k-means와 같은 알고리즘으로 군집화한다.군집화를 거치고나면, visual words는 픽셀의 다양한 색을 포착한다.군집화한 것들로 새로운 이미지를 분석한다.Reference http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf https://cs231n.github.io/linear-classify/ https://cs231n.github.io/optimization-1/ https://cs231n.github.io/assignments2017/assignment1/" }, { "title": "Yolov4 + Deep SORT 를 이용한 스포츠 선수 tracking", "url": "/posts/yolo_Deepsort/", "categories": "Projects, Tracking", "tags": "Tracking, Yolov4, Deep-SORT", "date": "2021-09-10 13:00:00 +0900", "snippet": " 논문 읽어보기 Yolo v4 와 Deep Sort 를 통해 visual tracking 하는 블로그 찾아보기 toy project 로 르브론 제임스 농구 영상 tracking 해보기git clone 을 통해 tracking 해보기Reference http://ieeexplore.ieee.org.ssl.access.inu.ac.kr/document/9345010?arnumber=9345010&amp;amp;SID=EBSCO:edseee github https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch code colab https://colab.research.google.com/drive/1ZAy2_AHWF8iW-aVESWViXcboHcRodcSn" }, { "title": "Simple Online and Realtime Tracking With a Deep Association Metric", "url": "/posts/Deepsort/", "categories": "Review, Tracking", "tags": "Tracking, Deep SORT", "date": "2021-09-08 13:00:00 +0900", "snippet": "Deep Sort에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요.AbstractSORT 는 간단하고 효과적인 알고리즘에 초점을 맞춘 Multiple Object Tracking 에 대한 실용적인 접근이다. 하지만 평가 불확실성(estimation uncertainty)이 낮을 때만 association metric이 정확하여 객체가 계속 스위칭 되는 현상이 있었다. 즉, (가려짐)occlusion에 취약했다.본 논문에서는 SORT 의 성능을 향상시키기 위해 appearance information 즉, 외형 정보를 통합한다. 이를 통해, 더 긴 기간동안 가려졌던 물체를 추적하고, ID 스위칭 수를 줄일 수 있다.또한, 복잡한 계산을 offline pretraining 단계에 배치하여, 큰 스케일의 person re-identification dataset 에서 deep association metric을 학습한다.online을 적용하면서, visual appearance 에 대해 nearest neighbor를 사용하여 measurement-to-track association을 만들었다.실험을 통해 ID 스위칭 수가 45%까지 감소할 수 있었고, 높은 프레임률로 좋은 성능을 달성했다. 이때, online 과 offline 학습의 차이는 online: mini batch라 부르는 작은 묶음 단위로 훈련한다. 연속적으로 데이터를 받고 빠른 변화에 스스로 적응해야 하는 시스템에 적합하다. offline : 가용한 데이터를 모두 사용해 훈련시킨다. 시간과 자원을 많이 소모하기 때문에 오프라인에서 가동된다. 시스템을 훈련시키고 적용하면 더 이상 학습 없이 실행 가능하다. 하지만 새로운 데이터에 대해 학습하려면 전체 데이터를 사용하여 시스템의 새로운 버전을 만들어 처음부터 다시 훈련해야 한다. Introduction최근 object detection의 진보로 인해, tracking-by-detection은 Multiple Object Tracking에서 앞서나가는 패러다임이 되었다.이 패러다임에는 객체 궤적은 보통 전체 video batch들을 한 번에 처리하는 전역 최적화 문제(global optimization problem)가 존재했다. 이 batch processing 때문에, 이 방법은 각 시간마다 대상 ID가 반드시 이용가능해야 하는 온라인 방식에는 적용될 수 없었다.전통적인 방법인 MHT(Multiple Hypothesis Tracking) 과 JPDAF(Joint Probabilistic Data Association Filter)은 frame-by-frame 방식을 사용하여 data association을 수행한다.JPDAF에서 단일 state hypothesis 는 data association 에 따라 개별 측정값에 가중치를 부여하여 발생된다. 또, MHT에서는 가능한 모든 hypotheis를 추적하지만 계산 추적성을 위해 pruning schemes를 적용해야 한다. 두 방법은 최근 tracking-by-detection 에서 재검토되어 좋은 결과를 보여줬다.하지만, 두 방법의 성능은 계산 및 구현 복잡성을 증가시키게 된다.SORT는 이미지 공간에서 kalman filtering을 수행하고 bounding box overlap을 측정하는 association metric과 함께 Hungarian method를 사용하여 frame-by-frame data association을 수행하는 간단한 프레임워크다. 이 간단한 방법은 높은 프레임율을 달성했다.MOT challenge dataset에서, 최첨단 detector가 있는 SORT가 표준 detector의 MHT보다 높은 순위를 차지한다.이유는 지난 번 리뷰에도 봤듯이 좋은 detector 가 좋은 tracking 성능을 만들기 때문이다.SORT는 tracking precision 과 accuracy 측면에서 전반적으로 우수한 성능을 보이지만, 상대적으로 높은 ID(identity) 스위칭을 반환한다. SORT에 사용된 association metric이 상태 추정 불확실성(state estimation uncertainty)이 낮을 때만 정확하기 때문이다.또한, SORT는 일반적으로 전면 카메라 장면에서 나타나기 때문에 occlusion을 추적하는 데 결함이 있다.그래서 data association을 motion과 appearance information을 결합하여 더 많은 정보를 가진 metric으로 대체함으로써 이 문제를 해결한다.특히 large-scale의 person re-identification dataset에서, 보행자(pedestrians)를 식별하도록 훈련된 CNN(Convolutional Neural Network)을 적용하고자 한다.CNN을 통해 시스템을 구현하기 쉽고 효율적이며 온라인 방식에 적용할 수 있도록 유지하면서 오류와 occlusion에 대해 견고함을 향상시켰다.SORT with Deep Association Metrics본 논문에서는 반복적인 Kalman filtering과 frame-by-frame data association을 가진 single hypothesis tracking 방법을 선택한다.이 시스템의 핵심 구성 요소에 대해 설명하고자 한다.Track Handling and State EstimationSORT와 동일하게 등속 운동과 선형 관측 모델이 있는 kalman fiter와 Hungarian algorithm을 사용하지만, state space의 형태가 달라졌다. 카메라가 보정되지 않은 상태거나 이용할 수 있는 ego-motion information이 없는 일반적인 tracking 시나리오로 가정한다.이러한 상황은 filter 프레임워크에 문제를 야기하지만, 최근 MOT benchmark에서는 가장 기본적인 설정이다.그래서 우리의 tracking 시나리오는 위와 같이 총 8개 차원의 상태 공간으로 정의된다. (u,v): bounding box 의 중심 위치 r: 가로 세로 비율 h: 높이 (x,˙ y,˙ γ,˙ h˙): 이미지 좌표의 속도이미지의 좌표는 object state의 직접 관측을 통해 얻은 값을 사용한다.각 track K는 measurement association 될 때마다 프레임의 수를 카운트되며, 이 카운트는 kalman filter prediction하는 동안에 증가되고, track이 measurement와 연결되면 0으로 재설정된다.사전정의된 Max age 보다 오래 매칭되지 않은 track은 화면을 떠난 것으로 간주되어 삭제한다.기존 트랙과 연관(association)될 수 없는 각 detection에 대해서는 새로운 track 가설(hypothesis)를 시작한다. 이 새로운 track은 처음 3번의 프레임까지는 잠정적(tentative) 상태로 분류하여 매칭되지 않아도 연관되기 전까지 유지하고, 그래도 안되면 삭제한다.Assignment Problem예측된 kalman state와 새로 받은 측정치(measurement) 사이의 연관성(association)을 해결하는 전통적인 방법은 Hungarian algorithm을 사용하여 해결할 수 있는 할당 문제(assignment problem)를 구축하는 것이다.이 문제 공식에서 두 가지 적절한 metric의 조합을 통해 motion과 appearance information을 통합한다.motion 정보를 통합하기 위해 예측된 kalman state와 새로 받은 측정치 사이의 Mahalanobis distance를 사용한다. Mahalanobis Distance (yi,Si): i번째 track distribution에서 측정 space로의 예상치(projection) (dj): j번째 bounding box detection i번째 track과 j번째 bounding box detection 사이의 거리Mahalanobis distance는 detection이 평균 track location으로부터 표준편차에 대해 얼마나 떨어져 있는지를 측정하여 state estimation uncertainty를 계산하는 것이다.이것은 inverse X² 분포의 95% 신뢰구간만 고려하여 해당되지 않는 association은 배제시키는 방법이다.i번째 track과 j번째 detection 사이의 연관성이 인정되면 1을 출력한다.4차원 measurement space의 경우 Mahalanobis threshold, t(1) = 9.4877 이다.Mahlanobis distance 는 motion uncertainty가 낮을 때는 적절한 association metric 가 된다.Mahalanobis distance는 short-term prediction에 유용한 움직임에 기초하여 가능한 object location에 대한 정보를 제공한다.하지만 kalman filtering 프레임워크에서 얻은 예측 state 분포는 객체 위치의 대략적인 추정치만 제공한다. 특히, 설명되지 않은 카메라 모션으로 인해 이미지 평면에 빠른 변위가 발생할 수 있으므로 이것을 해결하기 위해 Cosine distance를 도입했다. Mahalanobis distance에 대해 블로그에 있는 것을 추가해보았다.Mahalanobis distance: 평균과의 거리가 표준편차의 몇 배인지를 나타내는 값위의 그림을 보면 일반적인 거리 개념으로 볼때, a가 μ에 더 가깝다.하지만, 데이터들의 평균을 중심으로 데이터의 분산이 가장 큰 방향을 기준으로 축을 잡고 표준편차를 이용하여 계산하게 된다. 이 때 축은 점의 약 68%가 한 단위 내, 95%는 두 단위 내에 분포하도록 단위를 설정한다. 이렇게 설정하면 b가 더 가깝게 설정된다. reference : https://kimyo-s.tistory.com/46 Cosine distance각 bounding box detection dj에 대해 절대값이 1인 appearance descriptor, rj 를 계산한다.각 track k에 대해 마지막 100개의 appearance descriptor를 갤러리인 Rk에 보관한다.appearance descriptor는 사전 훈련된 CNN을 통해 구한다. 이것에 대해서는 section 2.4에 더 자세히 설명하겠다.그리고 나서, 이 두번째 측정지표는 appearance space에서 i번째 track과 j번째 detection 사이의 제일 작은 cosine distance를 구한다.또, 측정 지표에 따라 연관성이 허용되는지에 대한 여부를 나타내는 이진 변수를 소개한다.cosine distance는 움직임이 덜 차별적인(discriminative)한 경우 long-term occlusion 후 ID(identity)를 회복하는데에 유용한 appearance 정보를 다룬다.따라서, 위의 두 distance를 결합하면 두 지표가 서로 다른 assignment 문제를 처리함으로써 상호 보완된다.assignment problem을 구축하기 위해 두 metric를 가중치를 주면서 합한다.두 측정지표의 gating 영역 내에 있을 경우 연관성이 있다고 인정된다.실험에서 보면 Camera motino이 클 때 ⋋ = 0 으로 설정하는 것이 효과가 좋았기에, 이때는 Mahalanobis, d⑴은 gating(연관성 cost)에만 사용된다. Matching Cascade전역 할당 문제(global assignment problem)에서 measurement-to-track association을 해결하기보단, 이 문제를 해결하기 위해 cascade를 도입했다.물체가 오랫동안 가려지면 kalman filter는 uncertainty를 증가시켜 확률분포는 퍼지는 모양을 보인다.이 때, Mahalanobis distance는 어떤 detection에 대해서도 표준편차로 계산한 거리가 작아지기 때문에 uncertainty가 높은 것을 선호한다.하지만 이것은 바람직하지 않기 때문에 더 적은 수명을 갖는, 즉 uncertainty가 작은 track에게 우선순위를 부여한다. 부여하는 방법으로 matching cascade를 도입했다. 1.detection과 track 사이의 cost matrix 계산2.detection과 track 사이에서 cost를 기반으로 threshold를 적용하기 위한 마스크 만들기3.mask matrix 초기화4.unmatched detection에 대한 matrix를detection으로 초기화(매칭 시 제거)5.나이가 증가하는 트랙에 대한 선형 할당 문제를 해결하기 위해 모든 age에 대해 for문을 진행 6.마지막 n개 프레임에서 detection이랑 associate 되지 않은 track Tn의 부분집합을 선택 7.Tn안의 track과, 매칭되지 않은 detection U 사이의 최소 cost를 구해서 매칭 8.마스크의 threshold를 적용한 후 match matrix에 넣기 9.조건에 따라 unmatched matrix를 제외시킨다.10.match matrix와 unmatched matrix를 return 한다.위 과정을 거친 후 age==1인 unmatched and unconfirmed tracks와 unmatched detection 사이의 IoU association을 적용한다. 갑작스러운 appearance change를 구하는데 도움을 주기 때문이다. Deep Appearance Descriptor간단한 nearest neighbor 방법을 사용하면서, 위의 방법들을 성공적으로 수행시키기 위해서는 실제 online tracking application을 하기 전에, offline에서 훈련을 통해 특징을 잘 구별할 수 있도록 만들어야 한다.이를 위해 우리는 1261명의 보행자에 대한 11만 개 이상의 이미지를 포함하는 large-scale re-identification dataset(MARS)에 대해 pretraine된 CNN을 사용한다.이것이 곧 deep metric learning 에 해당되는 것이다.CNN Architecture은 아래 표와 같다.표를 보면,두 개의 convolutional layer과 여섯 개의 residual 블록을 가진 큰 residual network를 사용하는 ResNet 구조를 가진다.결과적으로 전체 feature map은 차원 128의 크기를 가지고, dense layer 10에서 계산된다.마지막에 batch와 L2 normalization에서는 정규화를 통해 cosine appearance metric와 호환될 수 있도록 features을 재해석한다.이 네트워크에는 총 280만 개의 매개변수를 가지고 있다.Experiments우리는 MOT16 benchmark에서 움직이는 카메라와 하향식 감시 설정을 포함한 7가지의 까다로운 테스트 시퀀스에서의 tracking 성능을 평가했다.tracker의 입력은 detector에 의존했고, 뛰어난 성능을 보이기 위해 수많은 dataset을 통해 Fast RCNN을 훈련시켰다.공정한 비교를 위해 동일한 detection에서 SORT를 다시 진행했다.평가는 ⋋ = 0 및 Amax = 30 frame을 사용하여 수행했고, detection의 threshold = 0.3 을 사용했다.benchmark에서 제공하는 매개변수로는 : MOTA(↑) : 다중 객체 tracking accuracy MOTP(↑) : 다중 객체 tracking precision FAF(↓) : 프레임 당 잘못 판정된 수 MT(↑) : 주로 추적되는 궤적의 수. 즉, 타겟은 수명의 최소 80%에 대해 동일한 레이블 ML(↓) : 대부분 손실된 궤적의 수. 즉, 타겟의 수명의 최소 20% 동안 추적되지 않은 레이블 ID SW(↓) : ID가 다른 객체로 전환된 횟수 Frag(↓) : miss detection에 의해 추적이 중단된 fragmentation 수(↑) 는 점수가 높을수록 좋은 성능을 나타내고, (↓) 는 반대다.실험 결과 Identity switches를 1423개에서 781개, 약 45%를 성공적으로 줄였다.동시에 occlusion과 miss인 객체도 제거하지 않고 인식함으로써 객체 identity가 유지되면서 track fragmentation도 약간 증가했다.외형 정보 통합(appearance informantion integration)으로 인해, 더 긴 occlusion에도 identities를 성공적으로 유지했다.deep sort는 다른 tracker과 비교하더라도 좋은 성능을 유지하면서 모든 online 방법 중 가장 적은 ID 스위치를 반환한다.또한, 상대적으로 큰 maximum track age를 설정함으로써 더 많은 양의 track들이 제거되지 않고, 물체 궤적에 결합된다.잘못 판단된 track이 감소되었으며, 비교적 안정적인 고정 track들을 생성한다.Conclusionpretrained association metric을 통해 appearance information을 통합하는, SORT의 확장인 deep sort를 제시했다. 그 결과 더 오랜 기간 동안 occlusion 상태를 추적할 수 있었다.Refernece Simple Online and Realtime Tracking With A Deep Association Metric (21 Mar 2017) Nicolai Wojke, Alex Bewley, Dietrich Paulus page : https://arxiv.org/abs/1703.07402v1 https://velog.io/@kimkj38/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Deep-SORTSimple-Online-and-Realtime-Tracking-with-a-Deep-Association-Metric https://kau-deeperent.tistory.com/84 https://kimyo-s.tistory.com/46코드 github https://github.com/ZQPei/deep_sort_pytorch#training-the-re-id-model github https://github.com/nwojke/deep_sort blog https://minding-deep-learning.tistory.com/6 blog https://jjeamin.github.io/posts/deepsort/" }, { "title": "Simple Online and Realtime Tracking", "url": "/posts/SORT/", "categories": "Review, Tracking", "tags": "Tracking, SORT", "date": "2021-09-06 13:00:00 +0900", "snippet": "Simple Online and Realtime Tracking 에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요. 이 논문은 1076회 인용된 논문으로 tracking 에서 유명한 논문입니다.Abstract이 논문은 Realtime application 을 통해 효율적인 Multiple Object Tracking 방법을 설명한다.detection 의 성능이 tracking 성능으로 이어지기 때문에, detector를 변경하면 최대 18.9% 까지 향상시킬 수 있다.Kalman Filter 과 Hungarian algorithm 과 같은 기본적인 조합만 사용하더라도, 엄청난 online 정확도를 보여준다고 한다. 또한, tracking 방법의 단순함을 통해 다른 tracker 보다 20배 이상 빠른 260Hz의 속도를 보여준다.Multiple Object Tracking ( MOT ) MOT 는 다수의 객체들을 추적하기 위해 detection 결과들 간의 연관(association)을 수행하는 것이다. detection 된 객체 정보를 기반으로 각 frame 간의 동일 객체에 대한 detection 을 association 하여 각 객체에 대한 전체 궤도를 생성한다. 그래서 robust 한 MOT를 만들기 위해 데이터 연관(data association) 알고리즘을 개발하였다. MOT의 방식은 객체 detection 결과와의 결합 유무에 따라 크게 “detection-free-tracking” 과 “tracking-by-detection”으로 구분된다. 최근 고성능 detector들의 개발됨에 따라 “tracking-by-detection” 으로 더 발전되고 있다. tracking-by-detection의 과정 tracking-by-detection 방식의 MOT 방법은 연관(association) 방식에 따라 “batch tracking” 과 “online tracking” 으로 구분된다. batch tracking 방식은 전체 tracking 궤도를 형성하기 위해 전체 프레임에서 detection된 반응 연관을 수행한다.전체 프레임에 대한 객체 detection 정보를 사용하여 tracking 궤도를 형성하기 때문에 온라인 트래킹보다 뛰어난 성능을 보인다. 하지만 사전에 전체 프레임에 대한 객체 detection 정보를 가지고 있어야 하기 때문에 실시간 어플리케이션에 적용하기는 어렵다. online tracking 방식은 미래 프레임에 대한 정보 없이 과거와 현재 프레임의 객체 detection 정보만으로 tracking 궤도 형성을 위한 detection 반응 연관을 수행하는 방식이다.그래서, online tracking 방식이 batch tracking 방식에 비해 실시간 어플리케이션에 적합하다. online tracking 방식은 미래 프레임 정보의 부재로 인해 긴 기간의 폐색(occusion)이나 객체 외형(appearance) 변화에 대해 취약하고, tracking 궤도 형성을 위한 detection 반응 연관이 어려워지는 문제가 있다. 이는 트랙 분리(track fragment) 및 IDs 전환(identity switches) 현상을 발생 시켜 tracking 성능을 저하시키는 것이다. ## Introduction본 논문에서는 각 frame에서 감지되고 bounding box로 나타내지는 Multiple Object Tracking(MOT)의 문제에 대한 tracking-by-detection 프레임워크를 보여준다.batch tracking 접근법과는 대조적으로, 이 방법은 전과 현재의 frame에서의 detection 결과만 tracker에 제공되는 online tracking 을 주로 다룬다.realtime tracking 의 효율성에 초점을 두어 자율 주행의 보행자 tracking 등 application 성능을 증진시킨다.MOT 의 문제는 video sequence의 여러 프레임에서 detection을 서로 연관시키는 것이 목적인 데이터 연관(data association) 문제로 볼 수 있다.tracker는 data association을 위해 object의 움직임과 외형을 모델링하기 위해 다양한 방법을 사용한다.본 논문에서는 논문 visual MOT benchmark [ MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking ] 에서 수행한 관찰을 통해 얻은 정보를 통해 만든 방법을 사용한다. 얻은 정보는 다음과 같다. MHT(Multiple Hypothesis Tracking)과 MOT benchmark 에서 가장 높은 랭킹을 차지했던 JPDA(Joint Probabilistic Data Association) 를 포함한 data association 기법 사용 ACF(Aggregate Channel Filter) detector를 사용하지 않는 tracker가 최상의 tracker 인 것을 볼 떄, detection 의 성능이 tracker를 방해할 수 있다.정확한 Tracker은 속도가 매우 느리기 때문에 실시간 application에 적용이 불가능하다. 따라서 정확도와 속도 간의 조율이 필요하다.아래 그림은 여러 tracker의 초당 프레임(FPS)[Hz]단위로 표기된 정확도와 속도 간 그래프이다.그래프를 통해 이 두 관계는 trade-off 즉, 반비례하는 경향이 있다.본 논문은 좋은 online / batch tracker 들 사이에서 전통적인 data association 기법이 부각되면서 MOT를 단순하면서 더 잘 할 수 있는지에 대해 연구한다.Occam`s Razor 에 의해 detection 요소 이외에 외형 요소는 tracking 에서 무시되고, 오직 bounding box의 위치와 사이즈만 motion estimation 과 data association 에 사용된다.매우 드물게 발생하고, tracking에 방해되는 short-term 과 long-term occlusion에 관해서도 무시한다.framework에 상당히 부담을 주고 불필요한 복잡성을 주기 때문이다.객체 재식별(object re-identification) 하는 형태라면, tracking 프레임워크에 상당한 비용이 추가되어 realtime applicaion 사용에 제약이 있다고 판단된다.다양한 dege cases와 detection errors를 처리하기 위해 다양한 구성요소를 통합시키는 visual tracker과는 다르게, 일반적인 frame-to-frame 연관을 효율적으로 사용하게끔 다루는데 초점을 둔다.(=&amp;gt; 단순한 모델을 만들기 위함)detection error에 대한 견고성을 목표로 하기보다는 ACF 또는 CNN 과 같이 발전된 visual object detector를 사용해 정확도를 높인다.또한, 본 논문에서는 고전적이지만 효율적인 Kalman Filter과 Hungarian method 를 적용하여 motion prediction 과 data association 을 처리한다.이는 최소한의 tracking 방식으로 online tracking 의 효율성과 신뢰성을 확보하는 것이다.본 논문은 MOT의 맥락에서 CNN 기반 detector 을 활용 최근 MOT 벤치마크에서 평가되고 있는 Kalman Filter과 Hungarian Algorithm 을 기반으로 한 실용적인 추적 접근법을 제시 (2016 기준)에 기여했다.본 논문은 다음과 같이 구성되어 있다. Section 2 : Multi Object Tracking 영역의 관련 문헌에 대한 간략한 검토 Section 3 : 제안된 lean tracking 프레임워크를 설명 Section 4 : 표준 벤치마크 시퀀스에 대한 제안된 프레임워크의 효과 Section 5 : 학습된 결과에 대한 요약 과 향후 개선Literature Review전통적으로 Multiple Object Tracking 은 MHT(Multiple Hypothesis Tracking) 또는 JPDA(Joint Probabilistic Data Association) 필터를 주로 사용하였다.하지만, 이는 객체 할당에 높은 불확실성이 있는 경우 어려운 결정을 지연시킨다.이러한 접근법의 복잡성은 추적 객체의 수를 기하급수적으로 증가시켜 높은 동적 환경안의 실시간 Application에는 실용적이지 않다.2015년 JPDA revisited 논문과 MHT revisited 논문을 볼 때, JPDA와 MHT 의 더 효율적인 방법을 개발함하였다. 하지만 이 방법들로도 decision making을 지연시키기 때문에online tracking에 부적합하다.많은 online tracking 방법들은 online learning 을 통해 각각의 object[17,18,12] 또는 global model[19,11,4,5]에 대한 appearance model을 만드는 것을 중점으로 한다.appearance model 외에 움직임은 tracklets에 의해 detection을 위해 통합된다.이분할 그래프(bipartite graph)로 모델링되는 관련성을 고려할 때, Hungarian Algorithm 과 같은 전역의 최적 솔루션(globally optical solution) 을 사용할 수 있다.[20]에서 Hungarian algorithm 을 두 단계 과정으로 사용한다. geometry 와 appearance 들을 결합하여 유사도 행렬(affinity matrix)을 만들고 detection과 비슷한 프레임들을 연관지으면서 tracklets를 형성한다 occlusion에 의해 손상된 궤도를 연결하기 위해 다시 geometry와 appearance 단서들을 사용해 tracklet들을 서로 연결시킨다.이 두 단계로 구성된 association 기법은 batch computation으로 제한시킨다.따라서 기본적인 단서들만 이용하여 association하도록 한 단계로 단순화 시키고자 했다.Methodology여기서는 detection의 구성, 미래 frame 으로 object states 전파, 현재 detection 결과와 존재하는 object를 association, tracked object 의 수명 관리 를 설명한다.Detectionbackbone : ZFNet, VGG16parameters : PASCAL VOC를 위해 학습된 기본 parameterCNN 기반의 Faster RCNN detection 프레임워크를 사용한다.Faster RCNN은 end-to-end 프레임워크로 two-stage 로 구성되었다.첫번째 단계에서 특징을 추출하고, 두번째 단계에서 영역을 제안한 것을 바탕으로 각 후보 영역에 대해 CNN을 사용하여 뷴루한다.Faster RCNN의 장점은 다른 Architecture 로 변경이 가능하다는 것이다. 따라서, 실시간 처리에 더 적합한 SSD, YOLO 등의 object detection으로 대체할 수 있다.하지만, RCNN은 정확도가 높지만 속도가 매우 느리다.본 논문에서는 Faster RCNN에 대한 ZF(zeiler and Fergus) 와 VGG16 을 비교한다.보행자를 검출하는 것이 목표이기 때문에, 다른 클래스는 무시하고, 출력 확률이 사람일 확률이 50% 이상일 경우에만 tracking framework로 넘긴다.기존의 online tracker 인 MDP와 논문에서 제안된 tracker 를 사용하여 detector를 비교했다.모델을 비교하는 과정에서 detection quality가 tracking에 큰 영향을 준다는 것을 알 수 있다.Estimation Modelobject tracking 의 task 는 객체의 위치를 예측하는 것이다.본 논문에서는 motion estimation 방법으로 이전 순간의 타겟 state에서 현재 순간의 타겟 state를 예측하는 칼만 필터(Kalman filter) 방법을 사용했지만, 칼만 필터보다 RNN과 LSTM 기반의 motion model을 사용하면 더 좋은 성능을 보인다. kalman filter 는 노이즈가 선형적 움직임을 가지는 타겟의 state를 추적하는 재귀 필터다. 확률 이론에 기반한 예측시스템으로, 노이즈를 포함한 데이터가 입력되었을 때 노이즈를 고려하여 정확한 estimation이 가능하다. 또한, 시간에 따라 진행한 측정을 기반으로 하기 때문에 해당 순간에만 측정한 결과만 사용한 것보다는 좀 더 정확한 estimation이 가능하다. (바로 이전 시간외의 측정값은 사용 x) 각 estimation 계산은 예측(prediction)과 보정(correction) 두 단계로 나눌 수 있다. 예측(prediction)은 이전 시간에 추정된 상태에 대해, 그 상태에서 입력이 들어왔을 때 예상되는 상태를 계산하는 단계이다. 보정(correction)은 앞서 계산된 예측 상태와 실제로 측정된 상태를 토대로 정확한 상태를 계산하는 단계이다. 동영상의 이전 프레임에서 object detector를 통해 얻어진 타겟의 state(bbox 좌표 u,v와 bbox의 scale s, 가로세로 비율 r)를 통해, 이후 프레임의 타겟 state( u˙,v˙,s˙)를 예측한다. (r은 일정하다고 가정) detection이 타겟과 연결되면, detection된 bounding box는 칼만 필터를 통해 속도 성분이 최적으로 해결되는 타겟 state를 update하기 위해 사용된다. 타겟과 detection이 연결되지 않으면(unmatched), 선형 속도 모델을 사용하여 correction 없이 단순히 state가 예측된다. unmatched가 Tlost번 계속되면 대상 ID가 deleted된다.Data Association Data Association 은 MOT 방법을 기반으로 한 tracking-bt-detection의 핵심 단계이다. Hungarian algorithm 은 일반적으로 사용되는 Data Association 최적화 방법이다. kalman filter을 이용해 얻은 예측값은 이후 프레임에서 새롭게 detection한 객체와 association 한다. 그 후, 기존 타겟들의 각 detection과 예측되는 모든 bounding box들 사이의 IoU로 assignment cost matrix 를 계산한다. 그리고 Hungrarian algorithm 을 사용하여 최적의 결과를 얻는다. 최소 IoU가 정의하여 IoUmin 보다 작은 bounding box를 처리한다.(a): 이전 Kalman filter에서 나온 결과(b)의 초록색 박스: 현재 detector 에서 나온 box이 둘을 IoU 하여 Hungarian 에서 짝을 맞춰 (c)처럼 ID를 할당하여 합친다.ExperimentsMetrics표준 MOT 지표와 함께 이미 정의된 평가지표를 사용한다. MOTA(↑) : 다중 객체 tracking accuracy MOTP(↑) : 다중 객체 tracking precision FAF(↓) : 프레임 당 잘못 판정된 수 MT(↑) : 주로 추적되는 궤적의 수. 즉, 타겟은 수명의 최소 80%에 대해 동일한 레이블 ML(↓) : 대부분 손실된 궤적의 수. 즉, 타겟의 수명의 최소 20% 동안 추적되지 않은 레이블 FP(↓) : false detection 수 FN(↓) : missed detection 수 ID SW(↓) : ID가 다른 객체로 전환된 횟수 Frag(↓) : miss detection에 의해 추적이 중단된 fragmentation 수(↑) 는 점수가 높을수록 좋은 성능을 나타내고, (↓) 는 반대다.Performance EvaluationSORT 및 여러 다른 baseline tracker들을 비교했다.SORT는 online tracker에서 가장 높은 MOTA 점수를 달성하고, 훨씬 더 복잡하면서 가까운 미래 프레임을 사용하는 SOTA인 NOMT와 비교할 정도로 좋은 성능을 가진다.Conclusion 본 논문에서는 프레임 간 prediction 과 association에 초점을 맞춘 simple online tracking framwork 이다. tracking 품질이 detection 성능에 의존하고, 최근 detection 기술이 발전함에 따라 기존 tracking 방법만으로 SOTA tracking 품질을 달성했다. 기존의 다른 방법들은 속도와 정확성 측면에서 trade-off 관계를 보여주는 반면, 논문에서 제시된 프레임워크는 속도와 정확성 측면에서 좋은 성능을 달성했다. 제시된 프레임워크의 단순성은 baseline으로 매우 적합하기에 새로운 방법이 객체 재식별에 집중하여 long term occlusion을 처리할 수 있다. 따라서 SORT의 가장 큰 특징은 Faster R-CNN을 기반으로 한 타겟 detection 방법과 SOTA의 정확성을 달성하면서 Multiple Target Tracking 속도를 크게 향상시키는 간단한 조합인 칼만 필터 + Hungarian 알고리즘을 사용했다.Reference Simple Online and Realtime Tracking. (2 Feb 2016). Alex Bewley, ZongYuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroftpage : page https://velog.io/@mink7878/Object-Tracking-Simple-Online-and-Realtime-Tracking-SORT-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 https://blog.naver.com/tlsfkaus4862/222315246114 gitbub https://github.com/abewley/sort blog https://deep-eye.tistory.com/68" }, { "title": "You Only Look Once v5 - Github", "url": "/posts/YoloV5-2/", "categories": "Review, Object Detection", "tags": "Object Detection", "date": "2021-09-06 13:00:00 +0900", "snippet": "Train Custom Datahttps://github.com/ultralytics/yolov5/blob/master/tutorial.ipynbPrepare start시작 전 환경 설정을 위해 repogitory 를 clone 하고, requiements.txt를 설치한다.git clone https://github.com/ultralytics/yolov5cd yolov5pip install -r requirements.txtTrain on Custom Data COCO128 Dataset 을 사용할 것이다. 이는 총 128개의 class를 포함하고 있다.data/coco128.yaml 은 path 를 통해 dataset root 를 설정, train/val/test image directories(txt file about image paths) 설정, number of classes , nc와 class names 설정# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]path: ../datasets/coco128 # dataset root dirtrain: images/train2017 # train images (relative to &#39;path&#39;) 128 imagesval: images/train2017 # val images (relative to &#39;path&#39;) 128 imagestest: # test images (optional)# Classesnc: 80 # number of classesnames: [ &#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39; ] # class names Create LabelsCVAT 나 makesense.ai 에서 툴을 사용하여 coco set 에서 yolo format 으로 변환시켜줘야 한다. Organize Directoriesyolov5/data directory에 image 와 label 폴더를 만들고, 그 안에 각각 train 과 val 폴더를 더 만든다.그리고 다운받은 coco 파일들을 image와 label을 train 과 val에 적절히 분리하여 넣는다. Select a Modelyolov5 의 4가지 종류 중 적절한 모델을 선택한다. s의 경우 가장 작고, 빠르다. traincoco128.yaml, yolov5.pt 파일을 사용하여 yolov5s 모델을 훈련시킨다.coco128.yaml의 경우 이미지와 라벨을 yolo format으로 변환해야 작동된다.yolov5.pt는 미리 훈련된 pretrained weights 이다.# Train YOLOv5s on COCO128 for 3 epochs$ python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache–cache 는 무엇인가Inferenceweights = yolov5s.ptconf = confidence를 나타내는 값이다.source = data/images/!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/Image(filename=&#39;runs/detect/exp/zidane.jpg&#39;, width=600)ValidateCoco val2017 dataset 을 활용하여 테스트를 해본다.data 는 coco.yamlweights 는 yolov5.ptiou 는 0.65# Download COCO val2017torch.hub.download_url_to_file(&#39;https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip&#39;, &#39;tmp.zip&#39;)!unzip -q tmp.zip -d ../datasets &amp;amp;&amp;amp; rm tmp.zip# Run YOLOv5x on COCO val2017!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --halfhalf 는 device의 한 종류로 half가 있다면 cpu 대신 사용할 수 있다.Testcoco test2017 dataset을 다운받아 실행한다.# Download COCO test-dev2017torch.hub.download_url_to_file(&#39;https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip&#39;, &#39;tmp.zip&#39;)!unzip -q tmp.zip -d ../ &amp;amp;&amp;amp; rm tmp.zip # unzip labels!f=&quot;test2017.zip&quot; &amp;amp;&amp;amp; curl http://images.cocodataset.org/zips/$f -o $f &amp;amp;&amp;amp; unzip -q $f &amp;amp;&amp;amp; rm $f # 7GB, 41k images%mv ./test2017 ../coco/images # move to /coco# Run YOLOv5s on COCO test-dev2017 using --task test!python val.py --weights yolov5s.pt --data coco.yaml --task testLoad From Pytorch Hub📚 여기서는 Pytorch Hub(https://pytorch.org/hub/ultralytics_yolov5) 에서 yolov5 🚀 를 불러오는 방법에 대해 설명한다.import cv2import torchfrom PIL import Image# Modelmodel = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)# Imagesfor f in [&#39;zidane.jpg&#39;, &#39;bus.jpg&#39;]: torch.hub.download_url_to_file(&#39;https://ultralytics.com/images/&#39; + f, f) # download 2 imagesimg1 = Image.open(&#39;zidane.jpg&#39;) # PIL imageimg2 = cv2.imread(&#39;bus.jpg&#39;)[..., ::-1] # OpenCV image (BGR to RGB)imgs = [img1, img2] # batch of images# Inferenceresults = model(imgs, size=640) # includes NMS# Resultsresults.print() results.save() # or .show()results.xyxy[0] # img1 predictions (tensor)results.pandas().xyxy[0] # img1 predictions (pandas)# xmin ymin xmax ymax confidence class name# 0 749.50 43.50 1148.0 704.5 0.874023 0 person# 1 433.50 433.50 517.5 714.5 0.687988 27 tie# 2 114.75 195.75 1095.0 708.0 0.624512 0 person# 3 986.00 304.00 1028.0 420.0 0.286865 27 tie## default Model 전체 architectureclass AutoShape(nn.Module): # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS conf = 0.25 # NMS confidence threshold iou = 0.45 # NMS IoU threshold classes = None # (optional list) filter by class max_det = 1000 # maximum number of detections per image def __init__(self, model): super().__init__() self.model = model.eval() def autoshape(self): LOGGER.info(&#39;AutoShape already enabled, skipping... &#39;) # model already converted to model.autoshape() return self @torch.no_grad() def forward(self, imgs, size=640, augment=False, profile=False): # Inference from various sources. For height=640, width=1280, RGB images example inputs are: # file: imgs = &#39;data/images/zidane.jpg&#39; # str or PosixPath # URI: = &#39;https://ultralytics.com/images/zidane.jpg&#39; # OpenCV: = cv2.imread(&#39;image.jpg&#39;)[:,:,::-1] # HWC BGR to RGB x(640,1280,3) # PIL: = Image.open(&#39;image.jpg&#39;) or ImageGrab.grab() # HWC x(640,1280,3) # numpy: = np.zeros((640,1280,3)) # HWC # torch: = torch.zeros(16,3,320,640) # BCHW (scaled to size=640, 0-1 values) # multiple: = [Image.open(&#39;image1.jpg&#39;), Image.open(&#39;image2.jpg&#39;), ...] # list of imagesdefault model을 분석해보자.inference 를 위한 hyperparameters setting 을 한다.model.conf = 0.25 # confidence threshold (0-1)model.iou = 0.45 # NMS IoU threshold (0-1)model.classes = None # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogsresults = model(imgs, size=320) # custom inference sizeinput channelpretrained yolov5 model 의 input channel=4 로 변경하려면model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, channels=4)이 경우, pretrained output layers과 일치하지 않는 output layers 를 제외한 pretrained weights 로 구성되어 있다. output layers는 ramdom weights 로 초기화 된다. number of classes또, nc, number of classes=10으로 설정하려면model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, classes=10)이 경우에도 마찬가지로, pretrained output layers과 일치하지 않는 output layers 를 제외한 pretrained weights 로 구성되어 있다. output layers는 ramdom weights 로 초기화 된다.force reload위의 두 가지 방법에서 문제가 생긴다면 force_reload=True 를 통해 기존 캐시를 삭제하고 pytorch hub에서 최신 yolov5 버전을 새로 다운할 수 있다.model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, force_reload=True) # force reloadscreenshotdesktop screenshot 에서 inference 를 하려면import torchfrom PIL import ImageGrab# Modelmodel = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)# Imageimg = ImageGrab.grab() # take the screenshot# Inferenceresults = model(img)Traininginference 가 아닌 training 에서 yolov5를 불러올 때는, autoshape=False 하는 것이 좋다. 초기 weights를 랜덤하게 설정하고 싶다면 pretrained=False 해야 한다.model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, autoshape=False) # load pretrainedmodel = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, autoshape=False, pretrained=False) # load scratchBase64 ResultsAPI service로 결과를 얻기 위해서는 Flask REST API 또는 #2291 를 참고해라. 아래는 그에 대한 예시로 Bask64를 사용하는 방법을 가져왔다.results = model(imgs) # inferenceresults.imgs # array of original images (as np array) passed to model for inferenceresults.render() # updates results.imgs with boxes and labelsfor img in results.imgs: buffered = BytesIO() img_base64 = Image.fromarray(img) img_base64.save(buffered, format=&quot;JPEG&quot;) print(base64.b64encode(buffered.getvalue()).decode(&#39;utf-8&#39;)) # base64 encoded image with resultsJSON ResultsJSON 형태로 출력하고자 한다면 to_json() 방법을 활용한 .pandas() 를 사용해야 한다. orient 인수를 활용하여 json 형식을 수정할 수 있다.to_json() 참고 블로그results = model(imgs) # inferenceresults.pandas().xyxy[0].to_json(orient=&quot;records&quot;) # JSON img1 predictionsother modelsyolov5s.pt 대신에 custom VOC-trained Yolov5 model 인 best.pt를 불러와 사용할 수도 있다.model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;custom&#39;, path=&#39;path/to/best.pt&#39;) # defaultexport ONNX, TorchScript, CoreML📚 여기서는 Pytorch 에서 ONNX 나 TorchScript 로 YOLOv5 🚀 를 추출하는 방법을 설명한다.export.py를 사용하면 ONNX, TorchScript, CoreML 등으로도 추출할 수 있다.python export.py --weights yolov5s.pt --img 640 --batch 1 # export at 640x640 with batch size 1[Test-Time Augmentation]📚 여기서는 test나 inference 동안 mAP 와 recall(재현율) 을 향상시키는 TTA(test Time Augmentaion) 을 사용하는 방법을 설명한다.val.py 에 명령을 추가하여 TTA를 사용하도록 설정하고, 향상된 결과를 위해 이미지 크기를 약 30% 늘린다.TTA를 활성화된 상태에서 inference 하면 일반적으로 image 가 3개의 다른 해상도로 처리되고, NMS 이전에 출력이 병합되기 때문에 시간이 2~3배정도 더 소요된다.$ python val.py --weights yolov5x.pt --data coco.yaml --img 832 --augment --halfinference with TTAdetect.py 를 활용한 TTA inference 는 val.py TTA 와 동일하게 --augment 를 설정한다.Pytorch Hub TTApytorch Hub 에서 TTA 는 자동으로 통합되어 있다. inference 시에 augment=True 를 입력시키면 된다.import torch# Modelmodel = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;) # or yolov5m, yolov5x, custom# Imagesimg = &#39;https://ultralytics.com/images/zidane.jpg&#39; # or file, PIL, OpenCV, numpy, multiple# Inferenceresults = model(img, augment=True) # &amp;lt;--- TTA inference# Resultsresults.print() # or .show(), .save(), .crop(), .pandas(), etc.Customizemodels/yolo.py 에서 forward_augment()를 수정하여 TTA를 customize 할 수 있다.def forward_augment(self, x): img_size = x.shape[-2:] # height, width s = [1, 0.83, 0.67] # scales f = [None, 3, None] # flips (2-ud, 3-lr) y = [] # outputs for si, fi in zip(s, f): xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max())) yi = self.forward_once(xi)[0] # forward # cv2.imwrite(f&#39;img_{si}.jpg&#39;, 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1]) # save yi = self._descale_pred(yi, fi, si, img_size) y.append(yi) return torch.cat(y, 1), None # augmented inference, train Model Ensembling📚 여기서는 test 나 inference 동안 mAP 와 recall 을 향상시키기 위해서 ensembling을 YOLOv5 🚀 모델에 사용하는 방법을 설명한다.From https://www.sciencedirect.com/topics/computer-science/ensemble-modeling: 앙상블 모델링은 다양한 모델링 알고리즘을 사용하거나 다른 교육 데이터 세트를 사용하여 결과를 예측하기 위해 여러 다양한 모델을 생성하는 프로세스입니다. 그런 다음 앙상블 모델은 각 기본 모델의 예측을 집계하고 보이지 않는 데이터에 대해 최종 예측을 한 번 수행합니다. 앙상블 모델을 사용하는 동기는 예측의 일반화 오류를 줄이기 위한 것입니다. 기본 모형이 다양하고 독립적인 경우 앙상블 접근 방식을 사용할 때 모형의 예측 오차는 감소합니다. 그 접근법은 예측을 할 때 군중의 지혜를 추구한다. 앙상블 모델은 모델 내에 여러 기본 모델이 있지만 단일 모델로 작동 및 수행됩니다.Ensembling Testtest 나 inference 시에 --weights 에 다른 모델을 추가함으로써 ensembling 하여 평가할 수 있다.python val.py --weights yolov5x.pt yolov5l6.pt --data coco.yaml --img 640 --halfEnsembling Inferenceinference 시에도 동일하게 --weights에 두 개를 입력시키면 된다.python detect.py --weights yolov5x.pt yolov5l6.pt --img 640 --source data/imagesPruning / Sparsity📚 여기서는 YOLOv5 🚀 모델에 pruning 을 적용하는 방법을 설명한다. Pruningtorch_utils.prune() 을 이용하여 pruned model 을 test에 적용할 수 있다. val.py 에 아래의 코드를 입력하여 업데이트 하면 된다.# Half #-------- 추가 --------# Prunefrom utils.torch_utils import pruneprune(model, 0.3)#----------------------# configuremodel.eval()30% pruned output:...Fusing layers... Model Summary: 476 layers, 87730285 parameters, 0 gradientsPruning model... 0.3 global sparsity...output 을 통해 pruning 후 모델에서 30%의 sparsity 를 달성했다는 것을 볼 수 있다. 이는 layer이 0인 nn.conv2d 의 weights parameter 가 30% 만 수행했다는 뜻이다.inference 시간은 바뀌지 않으나, model의 AP 와 AP scores 은 감소했다.Hyperparameter Evolution📚 여기서는 YOLOv5 🚀 의 hyperparameter evolution 을 설명한다. hyperparameter evolution 이란 optimization 을 위해 GA(genetic algorithm) 을 사용한 hyperparameter optimization 의 한 종류이다.ML control 의 Hyperparameters 은 최적의 값을 찾는 것이 어렵다. grid search와 같은 전통적인 방법은 1) 높은 차원, 2) 차원 간의 불확실한 상관관계, 3) 각 지점에서 적합성을 평가하는 비용이 너무 많이 발생 함으로써 빠르게 다루기 어렵다. 그래서 GA가 hyperparameter 검색에 적합한 후보가 될 수 있다.Install Hyperparameteryolov5 는 train 에 사용되는 hyperparameter이 30개 정도 사용된다. 이것들은 /data directory 에서 확인할 수 있다. 처음의 좋은 시작은 더 나은 결과를 만든다. 애매하다면, yolov5 coco training 의 parameter을 사용하는 것이 좋다.# yolov5/data/hyps/hyp.scratch.yaml# Hyperparameters for COCO training from scratch # python train.py --batch 40 --cfg yolov5m.yaml --weights &#39;&#39; --data coco.yaml --img 640 --epochs 300 # See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials lr0: 0.01 # initial learning rate (SGD=1E-2, Adam=1E-3) lrf: 0.2 # final OneCycleLR learning rate (lr0 * lrf) momentum: 0.937 # SGD momentum/Adam beta1 weight_decay: 0.0005 # optimizer weight decay 5e-4 warmup_epochs: 3.0 # warmup epochs (fractions ok) warmup_momentum: 0.8 # warmup initial momentum warmup_bias_lr: 0.1 # warmup initial bias lr Define Fitnessfitness는 우리가 최대한 활용하고자 하는 값이다. yolov5에서 우리는 mAP@0.5(weight의 10%) 와 mAP@0.5:0.95(90%를 남김) 과 같은 가중치 결합의 default fitness를 가지고 있다.# yolov5/utils/metrics.pydef fitness(x): # Model fitness as a weighted combination of metrics w = [0.0, 0.0, 0.1, 0.9] # weights for [P, R, mAP@0.5, mAP@0.5:0.95] return (x[:, :4] * w).sum(1) Evolve이 예제에서 base scenario 는 사전 훈련된 yolov5s 를 사용하여 COCO128 을 10 epochs 동안 finetuning 한다.--evolve 를 통해 epoch을 진행하면서 hyperparameter을 최적화한다.# single-GPUpython train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve# Multi-GPUfor i in 0 1 2 3; do nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve --device $i &amp;gt; evolve_gpu_$i.log &amp;amp;done--evolve 는 기본적으로 300번을 반복하면서 hyperparamemter를 최적화하지만, --evolve 1000 과 같이 작성하면 1000번 반복으로 변경할 수 있다. 반복하면서 가장 최적의 parameter를 runs/evolve/hyp_evolved.yaml 에 저장한다.최소한 300번은 해야 좋은 결과를 얻을 수 있다고 한다. 또, evolution 자체도 일반적으로 시간이 오래 걸리고 비싼데, 수천번의 반복을 하게 되면 많은 양의 GPU를 사용하게 될 것이다.Transfer Learning with Frozen Layers📚 여기서는 transfer learning 시에 Yolov5 🚀 layer 를 frozen 시키는 방법을 설명한다. transfer learning 은 새로운 data에 대한 model을 빠르게 구축할 수 있다. 일부 initial weights 를 고정시키고, 나머지 weights 는 loss를 계산하는데 사용되고, optimizer에 의해 업데이트된다. 이는 원래보다 훨씬 적은 비용이 든다. 정확도는 다소 떨어질 수 있으나, training time 을 단축시킬 수 있다.Before startgit clone https://github.com/ultralytics/yolov5cd yolov5pip install -r requirements.txt wandb # add W&amp;amp;B for loggingFrozen Backbonetrain.py 와 동일한 architecture 를 사용하려는 부분은 training 시작 전에 grad=False 로 설정하면 frozen된다.# yolov5/train.py # Freeze freeze = [f&#39;model.{x}.&#39; for x in range(freeze)] # layers to freeze for k, v in model.named_parameters(): v.requires_grad = True # train all layers if any(x in k for x in freeze): print(f&#39;freezing {k}&#39;) v.requires_grad = False #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡfor k, v in model.named_parameters(): print(k)# Outputmodel.0.conv.conv.weightmodel.0.conv.bn.weightmodel.0.conv.bn.biasmodel.1.conv.weightmodel.1.bn.weightmodel.1.bn.biasmodel.2.cv1.conv.weightmodel.2.cv1.bn.weight...model.23.m.0.cv2.bn.weightmodel.23.m.0.cv2.bn.biasmodel.24.m.0.weightmodel.24.m.0.biasmodel.24.m.1.weightmodel.24.m.1.biasmodel.24.m.2.weightmodel.24.m.2.biasyolov5 에서 backbone 은 0~9 layer에 해당하므로 backbone 고정을 위해 9 layer 까지 freeze 한다.python train.py --freeze 10모든 layer을 고정하기 위해서는 아래와 같이 입력하면 된다.python train.py --freeze 24모든 layer freeze 시 정확도가 떨어지고, backbone만 freeze 해도 일반적인 모델보다 떨어지지만, 속도가 빨라지고, 메모리 사용량도 줄어든다. Appendixrepo 기능을 검증하여 테스트한다.###### Reproducefor x in &#39;yolov5s&#39;, &#39;yolov5m&#39;, &#39;yolov5l&#39;, &#39;yolov5x&#39;: !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45 # speed !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65 # mAP###### Unit tests%%shellexport PYTHONPATH=&quot;$PWD&quot; # to run *.py. files in subdirectoriesrm -rf runs # remove runs/for m in yolov5s; do # models python train.py --weights $m.pt --epochs 3 --img 320 --device 0 # train pretrained python train.py --weights &#39;&#39; --cfg $m.yaml --epochs 3 --img 320 --device 0 # train scratch for d in 0 cpu; do # devices python detect.py --weights $m.pt --device $d # detect official python detect.py --weights runs/train/exp/weights/best.pt --device $d # detect custom python val.py --weights $m.pt --device $d # val official python val.py --weights runs/train/exp/weights/best.pt --device $d # val custom done python hubconf.py # hub python models/yolo.py --cfg $m.yaml # inspect python export.py --weights $m.pt --img 640 --batch 1 # exportdone###### Profilefrom utils.torch_utils import profilem1 = lambda x: x * torch.sigmoid(x)m2 = torch.nn.SiLU()results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)###### Evolve!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve!d=runs/train/evolve &amp;amp;&amp;amp; cp evolve.* $d &amp;amp;&amp;amp; zip -r evolve.zip $d &amp;amp;&amp;amp; gsutil mv evolve.zip gs://bucket # upload results (optional)### VOCfor b, m in zip([64, 48, 32, 16], [&#39;yolov5s&#39;, &#39;yolov5m&#39;, &#39;yolov5l&#39;, &#39;yolov5x&#39;]): # zip(batch_size, model) !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}reference “yolov5 github tutorial” - https://colab.research.google.com/drive/1uFK2FT-0c3rmrUoJoKq8QVcKCsbdwxRb “빵형 YOLOV5 train Mask” - https://colab.research.google.com/drive/1E8lRvkLVWs9vijUI1S7febGSQ2eb6hzm#scrollTo=9EflbG16Zt21 “How to train YOLOv5 on a custom dataset code - roboflow” - https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ#scrollTo=GD9gUQpaBxNa “YOLOV5 Mask Wearing Dataset - roboflow” - https://public.roboflow.com/object-detection/mask-wearing “YOLOV5 fake or real - kaggle” - https://www.kaggle.com/orkatz2/yolov5-fake-or-real-single-model-l-b-0-753?scriptVersionId=37672232" }, { "title": "[논문 리뷰] You Only Look Once v5 - blog", "url": "/posts/YoloV5/", "categories": "Review, Object Detection", "tags": "Object Detection", "date": "2021-09-05 13:00:00 +0900", "snippet": "YOLO review입력된 이미지는 low-level feature 에서 high-level feature가 되고, classification을 위해 trainable classifier를 거친다. Yolo는 앞서 정리한 것을 참고하면 좋을 것 같다.간단히 리뷰하자면,bounding box regression 과 classification을 나눠서 진행하는 Two stage 방법과는 다르게 두 개를 합쳐 동시에 진행함으로써 속도를 향상시켰다.이미지를 SxS 그리드로 나누고, 이미지가 있을 법한 곳에 bounding box를 여러 개 예측하고, 각 BB에 대한 confidence를 추출한다. 이 confidence는 box안에 클래스가 존재하는지에 대한 파라미터다.two stage 보다 속도는 뛰어나지만, 정확도 면에서는 떨어진다. What is the YoloV5train YOLOV5 using Custom dataset(open image dataset)&amp;gt; Index1. Preparing Dataset2. Environment Setup3. Configure / Modify files and directory structure.4. Training 5. Inference6. Result preparing datasetopen image dataset를 활용하여 분석할 것이다.다운 받은 데이터셋을 train, validation, test txt files 로 생성하여 각각에 70%, 20%, 10% 로 이동시킨다. 이 때, txt 내용은 사진의 경로를 넣는다. ex) ../data/train/000.jpg environment setupgit clone 을 통해 불러온 가상환경에서 진행한다. 그리고, requirement.txt 파일을 불러와 패키지 설치를 통해 환경을 설정한다.$ git clone https://github.com/ultralytics/yolov5.git$ cd yolov5$ pip install -r requirement.txt# requirement.txtnumpy==1.17scipy==1.4.1cudatoolkit==10.2.89opencv-pythontorch==1.5torchvision==0.6.0matplotlibpycocotoolstqdmpillowtensorboardpyyaml Configure / Modify files and idrectory structureclone을 통해 불러온 것을 위와 같이 경로를 설정한다.데이터셋의 parameter를 설정하기 위해서는 YAML 파일을 설정해야 한다. 필요에 따라 수정할 수 있다.원래의 형태는 아래와 같다.# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]train: ../coco128/images/trainval: ../coco128/images/val# number of classesnc: 80# class namesnames: [&#39;person&#39;, &#39;bicycle&#39;, &#39;car&#39;, &#39;motorcycle&#39;, &#39;airplane&#39;, &#39;bus&#39;, &#39;train&#39;, &#39;truck&#39;, &#39;boat&#39;, &#39;traffic light&#39;, &#39;fire hydrant&#39;, &#39;stop sign&#39;, &#39;parking meter&#39;, &#39;bench&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;sheep&#39;, &#39;cow&#39;, &#39;elephant&#39;, &#39;bear&#39;, &#39;zebra&#39;, &#39;giraffe&#39;, &#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;frisbee&#39;, &#39;skis&#39;, &#39;snowboard&#39;, &#39;sports ball&#39;, &#39;kite&#39;, &#39;baseball bat&#39;, &#39;baseball glove&#39;, &#39;skateboard&#39;, &#39;surfboard&#39;, &#39;tennis racket&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;, &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;, &#39;pizza&#39;, &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;, &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;, &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39;]이를 자신이 cumtomize 하여 사용할 것만 가져오면 된다. 그리고 train, val 부분을 이미지가 있는 경로로 설정한다.# here you need to specify the files train, test and validation txt # files created in step 1.train: ./data/images/trainval: /data/images/validtest: /data/images/test# number of classes in your datasetnc: 1# class namesnames: [&#39;Elephant&#39;]그 후 이미지를 작업 폴더로 옮기고, 각각 train, val, test 폴더로 분리시킨다.from glob import globimport shutilimport osdata =[&#39;train&#39;,&#39;valid&#39;,&#39;test&#39;]for i in data: source = &#39;./data/iamges/&#39; + i + &#39;/&#39; image = &#39;./data/images/&#39; + i labels = &#39;./data/labels/&#39; + i mydict = { image: [&#39;jpg&#39;,&#39;png&#39;,&#39;gif&#39;,&#39;jpeg&#39;,&#39;JPG&#39;], labels: [&#39;txt&#39;,&#39;json&#39;] } for destination, extensions in mydict_items(): for ext in extensions: for file in glob(source +&#39;*&#39; + ext): shutil.move(file, os.path.join(destination, file.split(&#39;/&#39;)[-1])) trainingyolov5는 4가지로 나눌 수 있다. s,m,l,x 는 각각 small,medium, large, xlarge 에 해당한다.이를 나눈 기준은 model depth multiple 과 layer width multiple 의 차이이다.학습할 때 weight는 yolov5s.pt 라는 미리 학습되어 있는 파일을 사용한다. Model size(pixels) mAP-val 0.5:0.95 mAP-test 0.5:0.95 speed V100 (ms) YOLOv5s 640 36.7 36.7 2.0 YOLOv5m 640 44.5 44.5 2.7 YOLOv5l 640 48.2 48.2 3.8 YOLOv5x 640 50.4 50.4 6.1 이제 image를 train 모델에 입력시켜 훈련한다.model을 학습하기 전, 사용할 model 파일을 들어가 nc(number of classes) 파라미터를 앞서 설정했던 것과 동일하게 변경시켜줘야 한다. Inference훈련된 모델에 test image를 넣어 inference를 진행한다.python3 detect.py --source ~/test_img/2011_09_26_drive_0091_sync/image_01/data/ --weights ./runs/exp5_ep50/weights/my_best.ptdetect.py를 통해 test를 하고 –source는 test할 data의 파일 경로 , –weights는 train에서 나온 결과 파일에서 weight 파일을 선택한다. weight file은 best.pt와 last.pt 가 나오는데 나는 best.pt 로 설정했다. Resulttrain YOLOV5 using Custom dataset(COCO dataset)추가적으로 COCO Dataset을 사용할 경우를 보자. 위 사이트를 들어가면 coco dataset을 다운받을 수 있다.여기서 coco dataset을 yolo dataset으로 변경시켜줘야 한다. 변경하는 방법은 직접 변경해도 되지만, 변경해주는 사이트가 따로 있다. 이 링크로 들어가서 jar 파일을 다운 받고 아래와 같이 입력하면 된다.cocotoyolo.jar &quot;json file(annotations) path&quot; &quot;img path&quot; &quot;class&quot; &quot;save path&quot;coco dataset을 다운 받을 때 annotation.json 의 위치, 변환할 이미지의 위치, 그리고 내가 원하는 추출하고자 하는 class 이름, 그리고 저장할 위치이다.coco는 기본적으로 bounding box가 맨 왼쪽의 x,y 좌표로 구성되어 있고, 이에 따른 w,h 로 구성된다.yolo dataset은 x,y 좌표가 bounding box 중앙에 위치하고, 이에 따른 w,h가 존재해야 한다. 따라서 x,y,w,h 좌표를 중앙을 기준으로 하도록 재설정해야 한다.이후에는 open image dataset을 할 때와 같다.&amp;gt; 성능 높이는 방법yolov5 공식 깃허브에 기재되어 있는 성능 높이는 여러 방법이 있다.* background image 넣기 =&amp;gt; bg image를 넣으면 false positive가 줄어든다고 한다. train 이미지 전체의 0~10% 정도 넣어주는 것을 추천한다. * pretrained weight 사용하기 =&amp;gt; 작거나 중간 정도의 사이즈의 데이터셋 사용 시 추천한다. (yolov5.pt 대신 다른 것을 넣는 것이다.)* epoch 을 300부터 시작해서 overfitting 이 발생하면 줄이고 아니면 점차 늘린다.* 기본 파라미터들은 hyp.scatch.yaml에 있다. 먼저 이 parameter들을 학습시켜놓는 것을 추천한다. yolov5에서는 hyperparameter evolution 이라는 기법을 사용한다. 이는 genetic algorithm(GA)를 사용해서 hyperparameter를 최적화한다. 이 기법은 [이 블로그]를 참고하는 것이 좋다. 인자값으로 --evolve를 줘서 이 기법을 사용할 수 있다고 한다. 평가 지표 IoUIoU 는 데이터셋에 대해서 객체 검출하는 모델의 정확도를 측정하는 방법이다. 이는 합성곱 신경망을 사용한 객체 검출 모델(R-CNN, YOLO) 등에 사용된다. IOU를 평가하기 위해서 GT Bounding box와 모델로부터 예측된 bounding box 를 사용한다.classification의 경우 라벨 값만 비교하면 되지만, 객체 검출은 bounding box를 정확하게 예측했는지가 중요하다. 따라서 bounding box의 x,y 좌표를 사용해야 한다.이 때, threshold를 설정하여 0.5라고 가정하면 IOU를 계산했을 때, 0.5보다 큰 것만 가져온다. Precision, RecallPrecision(정밀도)는 모델이 예측한 결과의 positive 결과가 얼마나 정확한지 측정하는 것이고, Recall(재현율)은 모델이 예측한 결과가 얼마나 positive 값들을 잘 찾는지 측정하는 것이다.예를 들어, 암 진단의 경우를 볼 때, precision: 양성으로 예측된 것 중 진짜 양성의 비율 =&amp;gt; 모델이 정답이라고 한 것 중 진짜 정답의 비율recall: 진짜 양성 중 양성으로 예측된 비율 =&amp;gt; 진짜 정답 중에 모델이 정답이라 한 것의 비율precision 과 recall 은 반비례하는 경향이 있다. 따라서 두 값을 종합해서 알고리즘 성능을 평가해야 한다. 그래서 나온 것이 PR곡선(Precision-recall)이다.PR곡선은 confidence 에 따라 값이 변한다. confidence는 알고리즘이 검출한 것에 대해 얼마나 정확하다고 생각하는지 알려주는 값인데, 어떤 물체를 검출했을 때 confidence가 0.99라면 알고리즘은 그 물체가 검출해야 하는 물체와 거의 똑같다고 생각한다. Average Precision(AP)PR 곡선은 알고리즘의 성능을 평가하기에는 좋으나 서로 다른 두 알고리즘을 비교하는데는 좋지 않다.Average precision은 PR그래프에서 그래프 선 아래쪽의 면적에 해당한다. AP가 높을수록 그 알고리즘의 성능이 높다는 뜻이다. 컴퓨터 비전에서는 대부분 AP로 성능을 평가한다. mAP(mean AP)mAP를 구하기 위해서 예측한 모든 bounding box를 가져온다. 이 모든 box를 confidence 기준으로 내림차순한다. 이 모든 box 에 대해 precision, recall 을 계산한다. 계산한 값들로 PR곡선을 그린다. 그 후 곡선 아래의 면적을 계산하면 된다. 객체가 두 개 이상일 경우 AP를 평균 낸다. 그래서 mAP인 것이다. 이때 mAP 0.5:0.05:0.95라고 하는 것은 0.5,0.05,0.95 IOU threshold를 사용하여 계산한 것이다.diffence between YoloV5 and others The First Release of model in pytorch between Yolo famliyyolov5는 처음으로 pytorch에서 구현되었기 때문에, 이 모델을 사용할 때, 보다 더 쉬워졌다. 또한, 모델을 ONNX 나 CoreML로 쉽게 컴파일할 수 있게 되었기 때문에 모바일 기기로의 구현이 간단해졌다. More Faster and More Accurate and Smallyolov5 colab 을 참고할 때, 140 frames per second(FPS)를 달성하였다고 한다. V4와 비교했을 때, 50FPS나 더 빨라진 것이다. 또한, 100 epochs 동안 평균 약 0.895 mAP 를 달성했다. yolov5의 weight 파일은 약 27MB 정도의 크기다. 그에 반해 yolov4는 244MB 이다. Yolov5 ArchitectureBackbone and Head크게 Backbone과 Head가 있다.yolov5가 빨라진 이유는 backbone의 변화의 영향이 가장 크다.backbone의 경우, image로부터 feature map을 추출하는 부분인데, Yolov5는 DarkNet을 사용하고, CSPNet, 그 중에서도 BottleneckCSP를 사용했다.CSPNet는 CNN의 학습 능력을 향상시키는 방법으로 이 블로그에 잘 설명되어 있다.그리고, yolov5는 4가지로 나눌 수 있다. s,m,l,x 는 각각 small,medium, large, xlarge 에 해당한다.Head의 경우, 추출된 feature map을 바탕으로 물체의 위치를 찾는 부분이다. Anchor box를 통해 bounding box를 예측한다. v3와 동일하게 3가지의 scale에서 바운딩 박스를 생성한다. 그 후, 각 scale마다 3개의 anchor box를 사용하기에 총 9개의 anchor box를 만들게 된다. from n params module arguments 0 -1 1 3520 models.common.Focus [3, 32, 3] 1 -1 1 18560 models.common.Conv [32, 64, 3, 2] 2 -1 1 19904 models.common.BottleneckCSP [64, 64, 1] 3 -1 1 73984 models.common.Conv [64, 128, 3, 2] 4 -1 1 161152 models.common.BottleneckCSP [128, 128, 3] 5 -1 1 295424 models.common.Conv [128, 256, 3, 2] 6 -1 1 641792 models.common.BottleneckCSP [256, 256, 3] 7 -1 1 1180672 models.common.Conv [256, 512, 3, 2] 8 -1 1 656896 models.common.SPP [512, 512, [5, 9, 13]] 9 -1 1 1248768 models.common.BottleneckCSP [512, 512, 1, False] 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 12 [-1, 6] 1 0 models.common.Concat [1] 13 -1 1 378624 models.common.BottleneckCSP [512, 256, 1, False] 14 -1 1 33024 models.common.Conv [256, 128, 1, 1] 15 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 16 [-1, 4] 1 0 models.common.Concat [1] 17 -1 1 95104 models.common.BottleneckCSP [256, 128, 1, False] 18 -1 1 147712 models.common.Conv [128, 128, 3, 2] 19 [-1, 14] 1 0 models.common.Concat [1] 20 -1 1 313088 models.common.BottleneckCSP [256, 256, 1, False] 21 -1 1 590336 models.common.Conv [256, 256, 3, 2] 22 [-1, 10] 1 0 models.common.Concat [1] 23 -1 1 1248768 models.common.BottleneckCSP [512, 512, 1, False] 24 [17, 20, 23] 1 229245 Detect [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]] Model Summary: 191 layers, 7.46816e+06 parameters, 7.46816e+06 gradients이는 yolov5의 전체 architeture 을 나타낸 것이다.i가 0~9 까지가 backbone 10~24가 head 이다.Backbonemodel Bottleneckclass BottleneckCSP(nn.Module): # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super(BottleneckCSP, self).__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False) self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False) self.cv4 = Conv(2 * c_, c2, 1, 1) self.bn = nn.BatchNorm2d(2 * c_) # applied to cat(cv2, cv3) self.act = nn.LeakyReLU(0.1, inplace=True) self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)]) def forward(self, x): y1 = self.cv3(self.m(self.cv1(x))) y2 = self.cv2(x) return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))BottleneckCSP는 기본적으로 4개의 Conv layer를 생성한다.이 중, conv2 와 conv3은 convolution + batch normalization을 진행한다.또, CSP 구조이므로 y 값을 2 개 생성한다. y1은 conv1에서 바로 conv3로 갈 때의 연산값이 나오고, y2는 conv2의 값을 받는다. 이를 conv4에 합친 후 반환한다. convconv + batch_norm 레이어다.class Conv(nn.Module): # Standard convolution def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): # ch_in, ch_out, kernel, stride, padding, groups super(Conv, self).__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2) self.act = nn.Hardswish() if act else nn.Identity() def forward(self, x): return self.act(self.bn(self.conv(x))) def fuseforward(self, x): return self.act(self.conv(x))forward 함수를 보면, 이 레이어는 conv 연산을 한 후에 batch normalization 과정을 거친다.활성화 함수로는 Hard swish 함수를 사용한다. Hard swish는 최근에 나온 함수이다.ReLU를 대체하기 위한 구글이 고안한 함수라고 한다. 시그모이드 함수에 x를 곱한 간단한 형태다. 깊은 레이어를 학습시킬 때 뛰어난 성능을 보인다고 한다. SPPSpatial Pyramid Pooling Layer 로, 55, 99, 13*13 feature map 들을 사용한다.class SPP(nn.Module): # Spatial pyramid pooling layer used in YOLOv3-SPP def __init__(self, c1, c2, k=(5, 9, 13)): super(SPP, self).__init__() c_ = c1 // 2 # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1) self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k]) def forward(self, x): x = self.cv1(x) return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1)) Concat단순히 2개의 레이어 연산 값을 합치는 것이다.class Concat(nn.Module): # Concatenate a list of tensors along dimension def __init__(self, dimension=1): super(Concat, self).__init__() self.d = dimension def forward(self, x): return torch.cat(x, self.d)Depth Multipledepth multiple 값이 클수록 BottleneckCSP 모듈(레이어)가 더 많이 반복되어, 더 깊은 모델이 된다.parse modeldef parse_model(d, ch): # model_dict, input_channels(3) logger.info(&#39;\\n%3s%18s%3s%10s %-40s%-30s&#39; % (&#39;&#39;, &#39;from&#39;, &#39;n&#39;, &#39;params&#39;, &#39;module&#39;, &#39;arguments&#39;)) anchors, nc, gd, gw = d[&#39;anchors&#39;], d[&#39;nc&#39;], d[&#39;depth_multiple&#39;], d[&#39;width_multiple&#39;] # print(&quot;depth_multiple : %s&quot; %gd) na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors # number of anchors no = na * (nc + 5) # number of outputs = anchors * (classes + 5) layers, save, c2 = [], [], ch[-1] # layers, savelist, ch out for i, (f, n, m, args) in enumerate(d[&#39;backbone&#39;] + d[&#39;head&#39;]): # from, number, module, args m = eval(m) if isinstance(m, str) else m # eval strings for j, a in enumerate(args): try: args[j] = eval(a) if isinstance(a, str) else a # eval strings except: pass n = max(round(n * gd), 1) if n &amp;gt; 1 else n # depth gain # print(&quot;depth_gain : %s&quot; %n)위의 코드를 보면, depth multiple 값이 gd라는 변수에 저장되는 것을 볼 수 있다. 변수 gd는 depth gain의 변수인 n을 구하는데 사용한다.depth gain의 변수 n을 계산하기 위해서 2개으 변수를 사용한다. 위에서 설명한 gd(=depth_multiple) 값과 yaml 파일에서 number라고 적혀있는 n 값을 사용한다.위의 yolov5 aritecture을 보면 Focus, Conv, SPP 모듈은 number 값이 1이고, bottleneckCSP 모듈만이 number 값을 3,9를 가진다.number의 n과 depth_multiple 의 gd를 곱하고 round로 반올림 한 후, max 를 통해 정수 1보다 작은 것을 제거시켜서 n(=depth gain)을 계산한다.이 n을 활용하여m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n &amp;gt; 1 else m(*args) # modulen 값만큼 반복한다.아래는 n=9인 BottleneckCSP의 구조다.number : 9depth_gain : 3module : BottleneckCSP( (cv1): Conv( (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) (cv2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (cv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (cv4): Conv( (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): LeakyReLU(negative_slope=0.1, inplace=True) (m): Sequential( (0): Bottleneck( (cv1): Conv( (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) (cv2): Conv( (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) ) (1): Bottleneck( (cv1): Conv( (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) (cv2): Conv( (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) ) (2): Bottleneck( (cv1): Conv( (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) (cv2): Conv( (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act): Hardswish() ) ) ))여기서 핵심은 (n):Bottleneck 이다. n=9이고, depth_gain=3 일 경우n * gd = 9 * (1/3) = 2.97 즉 3 정도이기 때문에 0~2 총 3번 반복한다.Width Multipleargs값과 width_multiple 값을 곱한 값이 해당 모듈의 채널 값으로 사용된다. 즉, width_multiple 값이 증가할수록 해당 레이어의 conv필터 수가 증가한다.yolov5s 의 경우 width_multiple 값은 0.5이다. 이 변수는 gw에 저장된다.# yaml file# YOLOv5 backbonebackbone: # [from, number, module, args] [[-1, 1, Focus, [64, 3]], # 0-P1/2 [-1, 1, Conv, [128, 3, 2]], # 1-P2/4 [-1, 3, BottleneckCSP, [128]], [-1, 1, Conv, [256, 3, 2]], # 3-P3/8 [-1, 9, BottleneckCSP, [256]], [-1, 1, Conv, [512, 3, 2]], # 5-P4/16 [-1, 9, BottleneckCSP, [512]], [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32 [-1, 1, SPP, [1024, [5, 9, 13]]], [-1, 3, BottleneckCSP, [1024, False]], # 9 ]yolov5 의 backbone 구조를 나타낸 것이다.def parse_model(d, ch): # model_dict, input_channels(3) logger.info(&#39;\\n%3s%18s%3s%10s %-40s%-30s&#39; % (&#39;&#39;, &#39;from&#39;, &#39;n&#39;, &#39;params&#39;, &#39;module&#39;, &#39;arguments&#39;)) anchors, nc, gd, gw = d[&#39;anchors&#39;], d[&#39;nc&#39;], d[&#39;depth_multiple&#39;], d[&#39;width_multiple&#39;] # print(&quot;depth_multiple : %s&quot; %gd) # print(&quot;width_multiple : %s&quot; %gw) if m in [nn.Conv2d, Conv, Bottleneck, SPP, DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP, C3]: c1, c2 = ch[f], args[0] # print(&quot;args [0] : %s&quot; %c2) c2 = make_divisible(c2 * gw, 8) if c2 != no else c2 # print(&quot;make_divisible_c2 : %s&quot; %c2) ################### utils 폴더 안에 있는 general.py 코드 ####################def make_divisible(x, divisor): # Returns x evenly divisble by divisor return math.ceil(x / divisor) * divisor이때 c2라는 변수도 존재하는데, yaml 파일의 args의 첫번째 변수다. Focus 에서 args로 [64,3]를 가지고 있다. 따라서 64가 c2 값이 된다.gw와 c2는 make_divisible 함수에 의해 두 변수를 곱해서 계산된다.Head# yolov5s.yaml# YOLOv5 headhead: [[-1, 1, Conv, [512, 1, 1]], [-1, 1, nn.Upsample, [None, 2, &#39;nearest&#39;]], [[-1, 6], 1, Concat, [1]], # cat backbone P4 [-1, 3, BottleneckCSP, [512, False]], # 13 [-1, 1, Conv, [256, 1, 1]], [-1, 1, nn.Upsample, [None, 2, &#39;nearest&#39;]], [[-1, 4], 1, Concat, [1]], # cat backbone P3 [-1, 3, BottleneckCSP, [256, False]], # 17 (P3/8-small) [-1, 1, Conv, [256, 3, 2]], [[-1, 14], 1, Concat, [1]], # cat head P4 [-1, 3, BottleneckCSP, [512, False]], # 20 (P4/16-medium) [-1, 1, Conv, [512, 3, 2]], [[-1, 10], 1, Concat, [1]], # cat head P5 [-1, 3, BottleneckCSP, [1024, False]], # 23 (P5/32-large) [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5) ]# yolo.py result (YOLOv5-s&#39;HEAD) i from n params module arguments 10 -1 1 131584 models.common.Conv [512, 256, 1, 1] 11 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 12 [-1, 6] 1 0 models.common.Concat [1] 13 -1 1 378624 models.common.BottleneckCSP [512, 256, 1, False] 14 -1 1 33024 models.common.Conv [256, 128, 1, 1] 15 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 16 [-1, 4] 1 0 models.common.Concat [1] 17 -1 1 95104 models.common.BottleneckCSP [256, 128, 1, False] 18 -1 1 147712 models.common.Conv [128, 128, 3, 2] 19 [-1, 14] 1 0 models.common.Concat [1] 20 -1 1 313088 models.common.BottleneckCSP [256, 256, 1, False] 21 -1 1 590336 models.common.Conv [256, 256, 3, 2] 22 [-1, 10] 1 0 models.common.Concat [1] 23 -1 1 1248768 models.common.BottleneckCSP [512, 512, 1, False] 24 [17, 20, 23] 1 229245 Detect [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]head 또한 from number module arguments 로 구성되어 있다.{conv,upsample,concat,bottleneckCSP}가 한 불록이다. 이 블록이 총 4개 있는 것이고, 마지막 detect 부분으로 연결된다.Concat여기서 concat의 의미는 바로 전 층인 nn.upsample 층과 bottleneckCSP 층과 결합시키는 것이다.첫번째 concat 부분은 backbone의 P4(i=6인 BottleneckCSP)와 결합두번째 concat 부분은 backbone의 P3(i=4인 BottleneckCSP)와 결합 -&amp;gt; 작은 물체 검출세번째 concat 부분은 backbone의 P4(i=14인 conv)와 결합 -&amp;gt; 중간 물체 검출네번째 concat 부분은 backbone의 P5(i=10인 conv)와 결합 -&amp;gt; 큰 물체 검출한다는 것이다. P3가 작은 물체 검출인 이유는 P3의 경우 작은 픽셀을 가지고 있다.elif m is Concat: print(&quot;ch_list : %s&quot; %ch) for x in f: if x == -1: print(&quot;x : %s&quot; %x) print(ch[-1]) else: print(&quot;x+1 : %s&quot; %str(x+1)) print(ch[x+1]) c2 = sum([ch[-1 if x == -1 else x + 1] for x in f]) print(&quot;c2 : %s&quot; %c2)detect말 그대로 i = 17,20,23 layer를 종합하여 detect 한다.LossGIoUGIoU는 bounding box에 관한 loss 함수다. 이는 compute_loss 함수에 있다. 1 - giou 값 = giou loss 이다.giou = bbox_iou(pbox.T, tbox[i], x1y1x2y2=False, CIoU=True) # giou(prediction, target)lbox += (1.0 - giou).mean() # giou lossGIoU 는 general.py 의 bbox_iou 함수에 있다.# Intersection areainter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\ (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)# Union Areaw1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1union = (w1 * h1 + 1e-16) + w2 * h2 - interiou = inter / union # iouif GIoU or DIoU or CIoU: cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1) # convex (smallest enclosing box) width ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1) # convex height if GIoU: # Generalized IoU https://arxiv.org/pdf/1902.09630.pdf c_area = cw * ch + 1e-16 # convex area return iou - (c_area - union) / c_area # GIoU최종적으로 giou loss 는 utils/general.py의 compute_loss 함수에 있다.(1 - giou 값 = giou loss) 이다.obj ( objectness loss), cls(classification loss)objectness loss와 classification loss는 BCEwithLogitsLoss를 사용한다. 이것도 general.py에 있다. BCEwithLogitsLoss는 class가 2개인 경우에 사용하는 loss function인 Binary Cross Entropy 에 sigmoid layer를 추가했다.classcification loss는 객체 탐지가 제대로 탐지되었는지에 대한 loss이다. MSE와 유사하게 (판단 값-실제 값)^2 해서 구한다.cls += BCEcls(ps[:,5:],t) # BCEobjectness loss는 객체 탐지에 대한 loss이다. 객체가 있을 경우의 loss 와 없을 경우의 loss를 따로 구하고, 각 loss에 가중치 값을 곱하여 클래스 불균형 문제를 해결한다.# Lossesnt = 0 # number of targetsnp = len(p) # number of outputsbalance = [4.0, 1.0, 0.4] if np == 3 else [4.0, 1.0, 0.4, 0.1] # P3-5 or P3-6 lobj += BCEobj(pi[..., 4], tobj) * balance[i] # obj lossoptimizerdefault 값으로는 SGD, 추가 설정으로 Adam으로 설정이 가능하다.Reference https://towardsdatascience.com/yolo-v5-is-here-b668ce2a4908 https://blog.roboflow.com/yolov5-is-here/ https://bigdata-analyst.tistory.com/194?category=883085 https://lynnshin.tistory.com/48?category=941325 https://ropiens.tistory.com/44" }, { "title": "CS231N chapter 2 - Image Classification", "url": "/posts/cs231n2/", "categories": "Classlog, CS231N", "tags": "CS231N, KNN", "date": "2021-09-04 13:00:00 +0900", "snippet": "numpy를 이용한 텐서 사용이 익숙하지 않는 사람들을 위한 튜토리얼Image Classification이미지를 볼 때 컴퓨터는 거대한 숫자 그리드로 표현한다. 그것을 통해 컴퓨터는 미리 정의된 class 레이블 중 하나를 지정한다.이 이미지의 경우 800x600 픽셀로 이루어져 있다. 또한, 3채널 즉, RGB로 표현되기 때문에 800x600x3 으로 표현할 수 있다.컴퓨터가 이 숫자 그리드를 보고 고양이라고 추출하는 것은 매우 어려운 일이다. 컴퓨터가 실제로 보고 있는 픽셀 값과 이미지가 가지는 의미론적 값에는 간격이 존재한다. 이를semantic gap이라 한다. image deformation같은 이미지라도 바라보는 시선을 변경할 수 있고, 조명의 변화나, 자세, 생김새, 나이, 색 등의 변화로 인해 형태가 달라질 수도 있다.또한, 고양이의 색깔과 배경의 색이 유사하게 보이는 background clutter 문제가 발생할 수도 있다.따라서 우리의 알고리즘은 이러한 다양성이나 변화에 대해 robust(강인)해야 한다.data-driven approach 방법은 이미지와 레이블을 입력하여 값을 출력한 다음, 출력값을 다시 예측이라는 또 다른 함수 모델에 입력하여 예측한다. Data-Driven Approach 수많은 이미지와 레이블에 대한 데이터셋을 수집한다. 데이터를 이용하여 분류기 classifier을 train시키기 위해 머신러닝을 사용한다. 새로운 이미지를 분류기에 집어넣어 평가를 진행한다. 이를 통해 좀 더 정확한 예측을 진행할 수 있게 되었다.Nearest Neighbor분류기 중 가장 단순한 방식이 Nearest Neighbor 이다. 이는 비어있는 공간이나 예측해야할 픽셀 주변의 값들을 빌려와 채워넣는 것을 말한다.이 방법은 train 단계에서 모든 데이터와 레이블을 학습한다. 그 후 예측 단계에서 test이미지와 가장 유사한 train 이미지를 찾아 그것의 레이블을 출력함으로써 예측한다.작은 크기의 데이터셋으로 학습하고자 할 때는 CIFAR-10이라는 데이터셋을 사용한다. 이는 class( airplane, cat, dog… ) 개수가 10개인 데이터셋을 말한다. 5만개의 train 이미지와 1만개의 test 이미지를 각 class에 동일하게 분할하여 제공한다.test 이미지와 train 이미지가 얼마나 비슷한지 평가를 할 때는 L1 distance 를 사용하는데, 이는 맨허튼 거리라고도 불린다. 이것은 두 이미지를 비교하기 위한 아주 간단하고 쉬운 방법이다.이미지의 개별 픽셀을 비교하는 방식으로, 각 픽셀을 비교한 후 픽셀 값을 다 더해서 출력한다.import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): # Xtr = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072 # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): &quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot; num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in range(num_test): # find the nearest training image to the i&#39;th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred이것은 Nearest Neighbor classifier의 python 코드다.Xtr 은 모든 이미지를 1 dimension 으로 flatten 한 image data 이고, ytr 은 모든 이미지에 대한 labe data 이다.train 함수는 모든 데이터를 기억한다.그 후 L1 distance 함수를 사용하여 test 이미지를 각 train 데이터셋과 비교하고 train 셋에서 test 이미지와 가장 유사한 이미지를 찾는다.이 때 데이터셋의 크기가 증가하더라도 train은 모든 데이터를 복사하면 되기 때문에 걸리는 시간이 일정하다. 하지만 predict 함수에서 test 이미지를 데이터셋의 모든 train 이미지와 각각을 모두 비교해야 하기 때문에 계산 시간이 증가한다.위의 그림에서 점들은 train 데이터를 의미하고, 점의 색상은 class를 나타낸다. train 데이터에서 전체 평명의 각 픽셀에 대해 가장 가까운 점이 무엇인지 계산한 후 클래스에 해당하는 공간 즉,배경을 칠했다.사진을 보면 색깔 중간중간에 다른 색의 점이 존재한다. 실제로는 주변색과 같아야 하지만 잘못 판단된 것이다. 이 점으로 인해 꽤 큰 공간이 오류가 발생한다.K-Nearest Neighbors그래서 K-nearest neighbors 방식으로 업그레이드하여, 가장 가까운 1개의 점이 아닌 점 K개를 찾아 각 이웃 중에서 더 많은 class에 해당하는 것으로 결정된다.이와 같은 방법을 사용할 때 경계를 부드럽게 하고 더 나은 결과를 가져올 수 있게 된다. 흰색은 주변에 이웃이 없음을 나타내기 위해 흰색으로 표시한 것이다. 이는 간단한 예제를 사용했기 때문에 점의 개수가 작아 나타낼수 있지만, 실제 이미지에서는 수많은 점들이 존재하기 때문에 흰색이 표시되지 않을 것이다.k-nearest neighbor을 사용할 때 점의 개수를 조밀하게 하기 위해 더 많은 데이터셋을 사용하면 되지만 성능이 좋아지지는 않는다.k-nearest neighbor classifier은 정확도가 다소 떨어지지만, 그럼에도 정확도를 약간 향상시키는 두 가지 방법이 있다. k 값을 증가시킨다.-&amp;gt; 하지만 K를 아무리 증가시켜도 한계가 존재한다. 정확도를 높이기 위해 픽셀 사이의 절대값의 합을 취하는 L1 방식을 L2로 변경. -&amp;gt; 두 점의 제곱합의 제곱근, L2 방법을 사용하는 것이다. 더 나은 결과를 가져온다고는 할 수 없으나 더 부드러운 값을 가져올 순 있을 것이다.하지만, 매우 느리다는 단점과 L1과 같이 이미지 간의 거리를 측정하는 것이 좋은 방법이 아니기 때문에 k-nearest neighbor 방법은 이미지 분류에 사용되지 않는다 Hyperparameters위와 같이 K값이나 거리 측정법을 사용자가 변화시킬 수 있다. train에서 학습시키는 것이 아니라 사용자가 미리 선택해서 입력시키는 것을 hyperparameters라고 한다.어느 hyperparameters가 가장 완벽한 분류기인지 선택하기 위한 방법으로는 최고의 정확도 또는 최고의 성능 기준이 방법은 사실 좋지 않다. 예를 들어 nearest neighbor의 경우 k=1로 설정하면 항상 train 데이터를 완벽하게 분류한다. 따라서 이 지표를 사용할 경우 항상 K=1이 사용된다. 하지만 앞서 봤듯이 K&amp;gt;1일 경우가 더 나은 성능을 보인다. 전체 데이터셋을 이를 train과 test 데이터로 나눈다. 이 또한 합리적이지 않다. 왜냐하면 우리가 만들고자 하는 알고리즘은 수많은 변화에도 강인해서 모든 곳에 적용할 수 있어야 한다. 하지만 train에 있던 데이터셋을 test로 사용하게 되면 새로운 이미지에 대한 수행을 잘 하지 못한다. 전체 데이터셋을 train, 검증셋(validation), test와 같이 3개의 다른 셋으로 나눈다.위 두가지보다는 좋은 방법이다. train에서 알고리즘을 훈련시키고, 검증 셋에서 알고리즘을 평가한 후, 검증셋에서 가장 잘 수행되는 hyperparameters를 선택한다. 그것을 통해 test 셋에 실행시킨다.Cross-Validation교차 검증이라는 hyperparameters를 설정하는 데 사용되는 방법이 있다. 먼저 데이터셋을 train과 test로 나눈다. 그 후 train 데이터셋을 k개로 분할한다. 분할된 k를 1개씩 검증셋으로 사용하여 train을 k번 훈련한다.이 방법을 통해 얻은 최적의 hyperparameters를 test셋에 적용한다.대규모 모델이 아니거나 계산 비용이 매우 많지 않은 경우에 자주 사용된다.이때 k가 너무 커도 과적합 경향을 보이기 때문에, 어느 k가 가장 정확도가 높은지 판단하는 것도 중요하다. 그래프를 볼 때, k가 7일 때 가장 좋은 성능을 보인다.Summary image classification에서는 이미지와 레이블의 training set과 예측 label의 test set을 가진다. k-nearest neighbor classifier은 가까운 train image에 대한 레이블을 가져와 예측한다. validation set을 이용하여 hyperparameters를 선정할 때, test set은 1번만 실행해야 한다.Linear Classificationlinear classifier에는 x라는 input 데이터와 W라는 매개변수 또는 가중치 weight가 존재하고, 이 두 개를 받는 함수가 존재한다.linear classifier 에서의 함수 f(x,w) 는 W와 x를 곱하여 Wx를 만든다.f(x,W) = W*x이 때, x는 32x32x3의 image이다. 이를 reshape해서 3072x1로 만든다. 그리고, output은 10가지 class에 대한 score를 출력해야 되기 때문에 10x1 이다. 그렇기에 W는 10x3072가 되어야 한다.이때, train 데이터와 상호 작용하지 않는 편향 벡터 b를 추가할 수 있다.좀 더 세세하게 구조를 살펴보자.이와 같이 2x2 image가 있고, 3개의 class가 존재한다고 가정해보자. linear classification 이 작동하기 위해서는 2x2를 열 행렬인 4x4로 만들어야 한다. 그렇다면 w는 3x4이 된다. 그리고, 편향 벡터 b를 더한다.linear classifier 의 경우 각 이미지를 고차원에서의 한 점으로 판단할 수 있다.linear classifier 는 linear decision boundary 를 통해 class를 구분한다.처음에는 decision boundary 를 무작위로 설정하고 train을 통해 적절한 경계를 설정할 수 있다.하지만 적절한 decision boundary 가 비선형인 경우나, 점들이 비규칙적일 경우 올바른 선을 결정할 수 없기 때문에 문제가 발생한다.decision boundary는 kooc 강의 - 머신러닝 학습 개론을 통해 배운 것을 참고하는 것을 추천한다.Refernece https://cs231n.github.io/classification/ https://cs231n.github.io/linear-classify/ https://velog.io/@leejaejun/AIFFEL-DeepML-CS231n-2%EA%B0%95-Image-Classification-1" }, { "title": "CS231N chapter 1 - Course Introduction", "url": "/posts/cs231n1/", "categories": "Classlog, CS231N", "tags": "CS231N", "date": "2021-09-04 13:00:00 +0900", "snippet": "A brief history of Computer Vision Biological Vision아주 먼 옛날, 눈을 통해 동물들은 진화했다. Human Vision카메라와 같이 사람의 눈과 유사하게 사진을 담을 수 있도록 만들어졌다. Computer Vision** David Mars**이미지를 인식하고 최종 단계는 3D 표현이다. 이에 도달하기 위해 여러 프로세스를 거친다. 이미지가 들어온다. primal sketch라고 하여 대부분의 가장자리나 막대 끝, 가상선, 곡선, 경계가 표현되는 곳을 정의한다. 그 다음 프로세스를 2.5D 스케치라고 한다. 여기에서는 표면, 깊이 정보, 레이어 또는 시각적 장면의 불연속성을 결합하기 시작한다. 모든 것을 결합되면 표면 및 원시적 관점에서 계층적으로 구성된 3D 모델을 갖게 된다. 70년대에 접어들면서 우리가 물체를 인식하고 표현할 수 있는 방법에 대해 생각하기 시작했다. 그래서 크게 두 가지 이론을 제시했다.하나는 Generalized Cylinder라고 하고, 다른 하나는 Pictorial Structure라고 한다.두 아이디어는 모든 객체가 기본적인 기하학적 요소로 구성될 수 있다고 가정한다. 예를 들어, 사람은 일반화된 원통형 모양으로 만들수도 있고, 점으로 찍어 그 사이를 연결하는 모양으로 만들어 질수도 있다. 따라서 두 아이디어는 복잡한 구조를 더 간단한 모양으로 축소할 수 있다.이 물체 인식 기술은 SVM, Boosting, graphical model과 같은 기술로 인해 발전하기 시작했다. AdaBoost 알고리즘을 사용하여 Paul Viola와 Michael Jones의 실시간 얼굴 감지를 수행했다.그 후, 전체론적 장면을 인식하기 시작했다. spatial pyramid matching이 이에 해당한다. 풍경, 부엌, 고속도로 등 어떤 장면인지에 대한 단서를 제공할 수 있는 이미지의 특징이 있다. 이것을 다른 해상도로 만들어 feature descriptor에 넣어서 특징을 가져온다. 이러한 기능을 잘 결합하여 사실적인 이미지를 인식할 수 있는 방법을 살펴보는 여러 작업이 있다.하나는 histogram of gradients(HOG)다른 하나는 deformable part model(DPM) 이라고 한다.PASCAL Visual Object Challenge2000년대 초반 객체 인식에 필요한 데이터셋을 구성했다. PASCAL 데이터셋이라 하여 20개의 객체 클래스로 구성되었다. 카테고리당 약 1만 개의 이미지로 구성되어있다. ImageNet위 대부분의 방법은 과적합될 가능성이 매우 높은 기계 학습 모델이다. 우리는 매우 높은 차원을 입력하고, 적합한 매개변수가 많이 있어야 하며 훈련 데이터가 충분하지 않을 때 과적합이 잘 발생하기 때문이다.이 과적합을 해결하고자 ImageNet이라는 것을 만들었다. 우리가 찾을 수 있는 모든 사진들을 가능한 가장 큰 데이터셋으로 모아 훈련시킨다. 거의 1500만의 이미지와 22000개의 카테고리를 구성했다.Convolutional Neural Networksobject detection은 전체 이미지를 한 클래스로 분류하는 것이 아니라 bounding box를 그려 어디에 개가 있고, 어디에 고양이가 있는지를 표현한다.Convolutional Neural Networks(CNN)이 등장하면서 매우 복작합 모델도 가능하게 되었다. 이는 멀티 레이어를 가지는 복합적인 모델이다. 또한 부가적으로 pooling 등을 수행할 수도 있다. 이 결과를 Classifier에 입력하여 물체를 분류하게 된다.Reference http://cs231n.stanford.edu/2017/syllabus.html https://jeongukjae.github.io/posts/CS231n-Lecture-1.-Introduction-to-Convolution-Neural-Networks-for-Visual-Recognition/" } ]