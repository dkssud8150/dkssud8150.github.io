---
title:    "[데브코스] 7주차 - DeepLearning Ping-Pong Ball Detection"
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-05-16 20:20:00 +0800
categories: [Classlog, devcourse]
tags: [devcourse, deeplearning]
toc: true
comments: true
math: true
---

<br>

# 프로젝트 개요

## 목표

1. RGB 카메라로부터 입력된 이미지에 존재하는 탁구공을 검출
- 데이터 수집, 학습 데이터 생성
- 학습 (+ augmentation)
- deep learning inference

2. 탁구공의 실제 위치를 추정
- camera calibration
- camera + lidar sensor fusion
- 위치 추정 (bbox의 위치와 크기를 통해 or 바닥과 평면의 관계를 사용)

3. 2D 지도에 탁구공의 위치를 표시
- occupancy grid map

<br>

## 파이프라인

1. 실시간으로 카메라로부터 이미지를 입력받음 (opencv , gstreamer)
2. 딥러닝 데이터 준비 / 처리
- 데이터 왜곡 : 모델에 넣는 이미지는 **왜곡을 삭제**하는 것이 좋다. 그 이유는 탁구공을 검출할 때 원이 아닌 타원으로 입력되면 검출이 이상해질 수 있다. 또는 왜곡이 있어도 잘 되었다고 하더라도, 실제 위치를 추정하기 위해서는 왜곡을 보정해야 한다.
    - 왜곡을 수정할 때는 레이블링 이전에 해도 되고, 모델 학습 이후 bbox에 대해서만 왜곡을 보정해줄 수도 있다. 분명 전체 이미지에 대해 보정할 필요가 없을 수 있지만, bbox에 대해서만 왜곡을 보정하면 디버깅이 다소 난해할 수 있다.
- 데이터 레이블링
- 데이터 증강
- 모델 학습
    - yolov3
- 인식 결과
    - bbox (2D)
- 탁구공 2D 위치 추정
    - **카메라의 기하학적 투영 방법 활용**
        - 사전 조건이 필요
        - extrinsic 필요
    - 탁구공의 평면과 지면 사이의 평면 변환식을 사용
        - 데이터를 변환하는 식에 대한 추가 노력이 필요
    - -\> **두 방법 모두 bbox 하단부분을 통해** 탁구공의 종/횡방향 위치를 찾을 수 있다.
    - sensor fusion
        - object data가 image data로 어떻게 투영되어 있는지에 대한 `RT`를 계산하는 것이 중요
- 3D 위치 추정 (OGM)
    - 실제 차량의 물리적 셀을 지정하고, 위치에 대한 grid 포인트도 계산이 가능

<img src="/assets/img/estim/tesla.png">

이 사진은 테슬라 AI day에서 보여준 실제 위치를 추정한 결과를 보여준 것이다.

<br>

## 프로젝트 가이드

### data collection
1. 차량에 장착된 카메라를 사용하여 탁구공 이미지 또는 비디오를 촬영
    - 이미지 : 일정 시간 간격을 가지고 데이터를 촬영해야 함
    - 비디오 : 이미지 학습 데이터를 만들 때 원하는 부분만 잘라서 사용할 수 있음
2. 일정 시간 간격에 대한 이미지를 추출
3. 데이터 레이블링을 위해 별도 디렉토리에 저장

### data labeling
4. 적절한 레이블링 툴 선택 : **CVAT**, LabelImg, Labelme 
5. [Data collection]에서 저장한 데이터에 대해 레이블링 수행
6. 레이블링 데이터의 형식을 지정 : 어떤 모델을 사용할지, 어떤 데이터셋을 기준으로 할지 지정
    - kitti2yolo, bdd2yolo

### model training
7. 적절한 OD(Object detection) 모델을 선택
8. 학습 데이터 형식 지정
9. 레이블링 데이터를 모델 학습 데이터 형식에 맞게 변환
10. 모델 학습
11. 파라미터 튜닝
    - hyperparameter 조정
    - dataset size up or data augmentation

### model inference
12. 학습 그래프의 추이를 확인, 적절한 모델 파일을 선택 (best epoch model pth)
13. 실제 차량에서 동작 가능한 inference 코드를 작성
    - 더 작은 모델을 사용
        - pruning , quantization
        - **GPU 가속기** : tensorRT - nvidia에서 제공하는 모델 가속기 방법
            - pytorch -\> onnx -\> tensorRT
    - 성능 지표
        - model inference FPS (한 장의 이미지를 처리하는데 걸리는 시간) 
            - 15 fps 정도면 좋은 성능이다. 일반적인 8 fps를 목표로 하기
        - model prediction Accuracy (실제 환경에서 검출이 잘 되는지)
            - F1 score, mAP
        
### object position estimation
14. 사전 지식
    - 탁구공의 실제 지름을 측정
    - 카메라 높이, 카메라의 extrinsic 정보
15. 모델의 예측 결과(bbox의 크기와 위치, 클래스) 중에서 활용 가능한 정보를 선택
16. object 거리 정확도 (종/횡 방향 정확도) 비교
    - 카메라와 탁구공 사이의 실제 거리를 측정해서 비교

<br>

## 프로젝트 결과

프로젝트를 통해 어떤 것들을 얻어가야 할지와 파이프라인에서 기본적인 방법들을 소개했는데, 추후 정보들을 찾아보기 위한 키워드를 소개한다.

<br>

1. 모델 학습 개선 방향
- fine tunning for hyper parameter
- more training dataset
- another OD model

2. 차량에서의 모델 결과
- model quantization
- model acceleration

3. 위치 추정 정확도 또는 장애물의 속도와 가속도 추정
- vision geometry
- LiDAR, RADAR
- sensor fusion

<br>

이때까지는 간단한 파이프라인을 소개했지만, top down 방식으로 더 많은 분야를 연구해볼 수 있을 것이다.

1. 데이터 레이블링
    - data labeling/processing
    - exploratory Data analysis
    - data augmentation
2. Object detection
    - 2d/3d object detection
    - segmentation
3. model release
    - model release / deploy
    - model quantization
4. position estimation
    - distance/position estimation
    - modo-depth estimation
    - sensor fusion ( multi-model sensor )
5. real world position estimation
    - occupancy grid map
    - semantic map
    - local dynamic map


> devops에서 사용되는 자동화 방법을 차용하여 Mechine learning 에서의 자동화 방법을 `MLOps`라 한다. 이는 `ML-flow`라는 패키지가 있으므로 참고하면 좋다.

<br>

<br>

# 프로젝트 진행

