---
title:    "You Only Look Once v5 - Github"
author:
  name: JaeHo-YooN
  link: https://github.com/dkssud8150
date: 2021-09-16 12:00:00 +0800
categories: [Review, Object Detection]
tags: [Object Detection]
toc: True
comments: True
math: true
mermaid: true
---

## [Train Custom Data]

https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb

### Prepare start

ì‹œì‘ ì „ í™˜ê²½ ì„¤ì •ì„ ìœ„í•´ repogitory ë¥¼ clone í•˜ê³ , requiements.txtë¥¼ ì„¤ì¹˜í•œë‹¤.

```git
git clone https://github.com/ultralytics/yolov5
cd yolov5
pip install -r requirements.txt
```


### Train on Custom Data

1. COCO128 Dataset ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì´ëŠ” ì´ 128ê°œì˜ classë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. 

[data/coco128.yaml] ì€ `path` ë¥¼ í†µí•´ dataset root ë¥¼ ì„¤ì •, `train`/`val`/`test` image directories(txt file about image paths) ì„¤ì •, number of classes , `nc`ì™€ `class names` ì„¤ì •

```yaml
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/coco128  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/train2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
nc: 80  # number of classes
names: [ 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
         'hair drier', 'toothbrush' ]  # class names
```

2. Create Labels

[CVAT] ë‚˜ [makesense.ai] ì—ì„œ íˆ´ì„ ì‚¬ìš©í•˜ì—¬ coco set ì—ì„œ yolo format ìœ¼ë¡œ ë³€í™˜ì‹œì¼œì¤˜ì•¼ í•œë‹¤. 


3. Organize Directories

![Full-width image](_site/assets/img/2021-09-05/dir.png)

yolov5/data directoryì— `image` ì™€ `label` í´ë”ë¥¼ ë§Œë“¤ê³ , ê·¸ ì•ˆì— ê°ê° `train` ê³¼ `val` í´ë”ë¥¼ ë” ë§Œë“ ë‹¤.

ê·¸ë¦¬ê³  ë‹¤ìš´ë°›ì€ coco íŒŒì¼ë“¤ì„ imageì™€ labelì„ train ê³¼ valì— ì ì ˆíˆ ë¶„ë¦¬í•˜ì—¬ ë„£ëŠ”ë‹¤. 


4. Select a Model

yolov5 ì˜ 4ê°€ì§€ ì¢…ë¥˜ ì¤‘ ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•œë‹¤. sì˜ ê²½ìš° ê°€ì¥ ì‘ê³ , ë¹ ë¥´ë‹¤.


5. train

coco128.yaml, yolov5.pt íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ yolov5s ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤. 

coco128.yamlì˜ ê²½ìš° ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ yolo formatìœ¼ë¡œ ë³€í™˜í•´ì•¼ ì‘ë™ëœë‹¤.

 yolov5.ptëŠ” ë¯¸ë¦¬ í›ˆë ¨ëœ pretrained weights ì´ë‹¤. 

```shell
# Train YOLOv5s on COCO128 for 3 epochs
$ python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache
```

--cache ëŠ” ë¬´ì—‡ì¸ê°€


### Inference

`weights` = yolov5s.py

`conf` = confidenceë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤.

`source` = data/images/

```shell
!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/Image(filename='runs/detect/exp/zidane.jpg', width=600)
```


### Validate

Coco val2017 dataset ì„ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¸ë‹¤.

`data` ëŠ” coco.yaml

`weights` ëŠ” yolov5.pt

`iou` ëŠ” 0.65

```shell
# Download COCO val2017
torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')

!unzip -q tmp.zip -d ../datasets && rm tmp.zip


# Run YOLOv5x on COCO val2017
!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half
```

`half` ëŠ” deviceì˜ í•œ ì¢…ë¥˜ë¡œ halfê°€ ìˆë‹¤ë©´ cpu ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

### Test

coco test2017 datasetì„ ë‹¤ìš´ë°›ì•„ ì‹¤í–‰í•œë‹¤. 

```shell
# Download COCO test-dev2017
torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')
!unzip -q tmp.zip -d ../ && rm tmp.zip # unzip labels
!f="test2017.zip" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images
%mv ./test2017 ../coco/images  # move to /coco

# Run YOLOv5s on COCO test-dev2017 using --task test
!python val.py --weights yolov5s.pt --data coco.yaml --task test
```

ì™œ testì—ì„œë§Œ f=ì´ ìˆëŠ”ê°€?



2. this ordered seed list will be replaced by the toc
{:toc}

## [Load From Pytorch Hub]

ğŸ“š ì—¬ê¸°ì„œëŠ” Pytorch Hub(https://pytorch.org/hub/ultralytics_yolov5) ì—ì„œ yolov5 ğŸš€ ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤.


```python
import cv2
import torch
from PIL import Image

# Model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Images
for f in ['zidane.jpg', 'bus.jpg']:
    torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images
img1 = Image.open('zidane.jpg')  # PIL image
img2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)
imgs = [img1, img2]  # batch of images

# Inference
results = model(imgs, size=640)  # includes NMS

# Results
results.print()  
results.save()  # or .show()

results.xyxy[0]  # img1 predictions (tensor)
results.pandas().xyxy[0]  # img1 predictions (pandas)
#      xmin    ymin    xmax   ymax  confidence  class    name
# 0  749.50   43.50  1148.0  704.5    0.874023      0  person
# 1  433.50  433.50   517.5  714.5    0.687988     27     tie
# 2  114.75  195.75  1095.0  708.0    0.624512      0  person
# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie
```

```python
## default Model ì „ì²´ architecture
class AutoShape(nn.Module):
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    classes = None  # (optional list) filter by class
    max_det = 1000  # maximum number of detections per image

    def __init__(self, model):
        super().__init__()
        self.model = model.eval()

    def autoshape(self):
        LOGGER.info('AutoShape already enabled, skipping... ')  # model already converted to model.autoshape()
        return self

    @torch.no_grad()
    def forward(self, imgs, size=640, augment=False, profile=False):
        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:
        #   file:       imgs = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images
```

default modelì„ ë¶„ì„í•´ë³´ì.

inference ë¥¼ ìœ„í•œ hyperparameters setting ì„ í•œë‹¤.

```python
model.conf = 0.25  # confidence threshold (0-1)
model.iou = 0.45  # NMS IoU threshold (0-1)
model.classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs

results = model(imgs, size=320)  # custom inference size
```

### input channel

pretrained yolov5 model ì˜ `input channel`=4 ë¡œ ë³€ê²½í•˜ë ¤ë©´

```python
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', channels=4)
```

ì´ ê²½ìš°, pretrained output layersê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” output layers ë¥¼ ì œì™¸í•œ pretrained weights ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. output layersëŠ” ramdom weights ë¡œ ì´ˆê¸°í™” ëœë‹¤. <br>

### number of classes

ë˜, nc, `number of classes`=10ìœ¼ë¡œ ì„¤ì •í•˜ë ¤ë©´

```python
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', classes=4)
```

ì´ ê²½ìš°ì—ë„ ë§ˆì°¬ê°€ì§€ë¡œ, pretrained output layersê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” output layers ë¥¼ ì œì™¸í•œ pretrained weights ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. output layersëŠ” ramdom weights ë¡œ ì´ˆê¸°í™” ëœë‹¤. <br>

### force reload

ìœ„ì˜ ë‘ ê°€ì§€ ë°©ë²•ì—ì„œ ë¬¸ì œê°€ ìƒê¸´ë‹¤ë©´ `force_reload=True` ë¥¼ í†µí•´ ê¸°ì¡´ ìºì‹œë¥¼ ì‚­ì œí•˜ê³  pytorch hubì—ì„œ ìµœì‹  yolov5 ë²„ì „ì„ ìƒˆë¡œ ë‹¤ìš´í•  ìˆ˜ ìˆë‹¤.

```python
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)  # force reload
```

### screenshot

desktop screenshot ì—ì„œ inference ë¥¼ í•˜ë ¤ë©´

```python
import torch
from PIL import ImageGrab

# Model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Image
img = ImageGrab.grab()  # take the screenshot

# Inference
results = model(img)
```


### Training

inference ê°€ ì•„ë‹Œ training ì—ì„œ yolov5ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•ŒëŠ”, `autoshape=False` í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì´ˆê¸° weightsë¥¼ ëœë¤í•˜ê²Œ ì„¤ì •í•˜ê³  ì‹¶ë‹¤ë©´ `pretrained=False` í•´ì•¼ í•œë‹¤.

```python
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)  # load pretrained
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=False)  # load scratch
```


### Base64 Results

API serviceë¡œ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” [Flask REST API] ë˜ëŠ” [#2291] ë¥¼ ì°¸ê³ í•´ë¼. ì•„ë˜ëŠ” ê·¸ì— ëŒ€í•œ ì˜ˆì‹œë¡œ Bask64ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ê°€ì ¸ì™”ë‹¤.

```python
results = model(imgs)  # inference

results.imgs # array of original images (as np array) passed to model for inference
results.render()  # updates results.imgs with boxes and labels
for img in results.imgs:
    buffered = BytesIO()
    img_base64 = Image.fromarray(img)
    img_base64.save(buffered, format="JPEG")
    print(base64.b64encode(buffered.getvalue()).decode('utf-8'))  # base64 encoded image with results
```

### JSON Results

JSON í˜•íƒœë¡œ ì¶œë ¥í•˜ê³ ì í•œë‹¤ë©´ `to_json()` ë°©ë²•ì„ í™œìš©í•œ `.pandas()` ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. `orient` ì¸ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ json í˜•ì‹ì„ ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤. 


[to_json() ì°¸ê³  ë¸”ë¡œê·¸](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html)



```python
results = model(imgs)  # inference

results.pandas().xyxy[0].to_json(orient="records")  # JSON img1 predictions
```


### other models

yolov5s.pt ëŒ€ì‹ ì— 
custom VOC-trained Yolov5 model ì¸ `best.pt`ë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.

```python
model = torch.hub.load('ultralytics/yolov5', 'custom', path='path/to/best.pt')  # default
```



### export ONNX, TorchScript, CoreML

ğŸ“š ì—¬ê¸°ì„œëŠ” Pytorch ì—ì„œ ONNX ë‚˜ TorchScript ë¡œ YOLOv5 ğŸš€ ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.

`export.py`ë¥¼ ì‚¬ìš©í•˜ë©´ ONNX, TorchScript, CoreML ë“±ìœ¼ë¡œë„ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤. 

```shell
python export.py --weights yolov5s.pt --img 640 --batch 1  # export at 640x640 with batch size 1
```


## [Test-Time Augmentation]

ğŸ“š ì—¬ê¸°ì„œëŠ” testë‚˜ inference ë™ì•ˆ mAP ì™€ recall(ì¬í˜„ìœ¨) ì„ í–¥ìƒì‹œí‚¤ëŠ” TTA(test Time Augmentaion) ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.

`val.py` ì— ëª…ë ¹ì„ ì¶”ê°€í•˜ì—¬ TTAë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•˜ê³ , í–¥ìƒëœ ê²°ê³¼ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì•½ 30% ëŠ˜ë¦°ë‹¤. 

TTAë¥¼ í™œì„±í™”ëœ ìƒíƒœì—ì„œ inference í•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ image ê°€ 3ê°œì˜ ë‹¤ë¥¸ í•´ìƒë„ë¡œ ì²˜ë¦¬ë˜ê³ , NMS ì´ì „ì— ì¶œë ¥ì´ ë³‘í•©ë˜ê¸° ë•Œë¬¸ì— ì‹œê°„ì´ 2~3ë°°ì •ë„ ë” ì†Œìš”ëœë‹¤.

```shell
$ python val.py --weights yolov5x.pt --data coco.yaml --img 832 --augment --half
```


### inference with TTA

`detect.py` ë¥¼ í™œìš©í•œ TTA inference ëŠ” `val.py` TTA ì™€ ë™ì¼í•˜ê²Œ `--augment` ë¥¼ ì„¤ì •í•œë‹¤.


### Pytorch Hub TTA

pytorch Hub ì—ì„œ TTA ëŠ” ìë™ìœ¼ë¡œ í†µí•©ë˜ì–´ ìˆë‹¤. inference ì‹œì— `augment=True` ë¥¼ ì…ë ¥ì‹œí‚¤ë©´ ëœë‹¤.

```python
import torch

# Model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5x, custom

# Images
img = 'https://ultralytics.com/images/zidane.jpg'  # or file, PIL, OpenCV, numpy, multiple

# Inference
results = model(img, augment=True)  # <--- TTA inference

# Results
results.print()  # or .show(), .save(), .crop(), .pandas(), etc.
```

### Customize

`models/yolo.py` ì—ì„œ `forward_augment()`ë¥¼ ìˆ˜ì •í•˜ì—¬ TTAë¥¼ customize í•  ìˆ˜ ìˆë‹¤. 

```python
def forward_augment(self, x): 
    img_size = x.shape[-2:]  # height, width 
    s = [1, 0.83, 0.67]  # scales 
    f = [None, 3, None]  # flips (2-ud, 3-lr) 
    y = []  # outputs 
    for si, fi in zip(s, f): 
        xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max())) 
        yi = self.forward_once(xi)[0]  # forward 
        # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save 
        yi = self._descale_pred(yi, fi, si, img_size) 
        y.append(yi) 
    return torch.cat(y, 1), None  # augmented inference, train 
```



## [Model Ensembling](https://github.com/ultralytics/yolov5/issues/318)

ğŸ“š ì—¬ê¸°ì„œëŠ” test ë‚˜ inference ë™ì•ˆ mAP ì™€ recall ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ì„œ ensemblingì„ YOLOv5  ğŸš€ ëª¨ë¸ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.

From https://www.sciencedirect.com/topics/computer-science/ensemble-modeling:

>ì•™ìƒë¸” ëª¨ë¸ë§ì€ ë‹¤ì–‘í•œ ëª¨ë¸ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ë‹¤ë¥¸ êµìœ¡ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì•™ìƒë¸” ëª¨ë¸ì€ ê° ê¸°ë³¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì§‘ê³„í•˜ê³  ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì— ëŒ€í•´ ìµœì¢… ì˜ˆì¸¡ì„ í•œ ë²ˆ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì•™ìƒë¸” ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë™ê¸°ëŠ” ì˜ˆì¸¡ì˜ ì¼ë°˜í™” ì˜¤ë¥˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. ê¸°ë³¸ ëª¨í˜•ì´ ë‹¤ì–‘í•˜ê³  ë…ë¦½ì ì¸ ê²½ìš° ì•™ìƒë¸” ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•  ë•Œ ëª¨í˜•ì˜ ì˜ˆì¸¡ ì˜¤ì°¨ëŠ” ê°ì†Œí•©ë‹ˆë‹¤. ê·¸ ì ‘ê·¼ë²•ì€ ì˜ˆì¸¡ì„ í•  ë•Œ êµ°ì¤‘ì˜ ì§€í˜œë¥¼ ì¶”êµ¬í•œë‹¤. ì•™ìƒë¸” ëª¨ë¸ì€ ëª¨ë¸ ë‚´ì— ì—¬ëŸ¬ ê¸°ë³¸ ëª¨ë¸ì´ ìˆì§€ë§Œ ë‹¨ì¼ ëª¨ë¸ë¡œ ì‘ë™ ë° ìˆ˜í–‰ë©ë‹ˆë‹¤.
Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.

### Ensembling Test

test ë‚˜ inference ì‹œì— `--weights` ì— ë‹¤ë¥¸ ëª¨ë¸ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ensembling í•˜ì—¬ í‰ê°€í•  ìˆ˜ ìˆë‹¤. 

```shell
python val.py --weights yolov5x.pt yolov5l6.pt --data coco.yaml --img 640 --half
```


### Ensembling Inference

inference ì‹œì—ë„ ë™ì¼í•˜ê²Œ `--weights`ì— ë‘ ê°œë¥¼ ì…ë ¥ì‹œí‚¤ë©´ ëœë‹¤.

```shell
python detect.py --weights yolov5x.pt yolov5l6.pt --img 640 --source data/images
```


## [Pruning / Sparsity](https://github.com/ultralytics/yolov5/issues/304)
ğŸ“š ì—¬ê¸°ì„œëŠ” YOLOv5 ğŸš€ ëª¨ë¸ì— pruning ì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.

* Pruning

`torch_utils.prune()` ì„ ì´ìš©í•˜ì—¬ pruned model ì„ testì— ì ìš©í•  ìˆ˜ ìˆë‹¤. `val.py` ì— ì•„ë˜ì˜ ì½”ë“œë¥¼ ì…ë ¥í•˜ì—¬ ì—…ë°ì´íŠ¸ í•˜ë©´ ëœë‹¤.

```python

# Half 

#-------- ì¶”ê°€ --------
# Prune
from utils.torch_utils import prune
prune(model, 0.3)

#----------------------

# configure
model.eval()
```

30% pruned output:

```
...
Fusing layers... 
Model Summary: 476 layers, 87730285 parameters, 0 gradients
Pruning model...  0.3 global sparsity
...
```

output ì„ í†µí•´ pruning í›„ ëª¨ë¸ì—ì„œ 30%ì˜ sparsity ë¥¼ ë‹¬ì„±í–ˆë‹¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” layerì´ 0ì¸ nn.conv2d ì˜ weights parameter ê°€ 30% ë§Œ ìˆ˜í–‰í–ˆë‹¤ëŠ” ëœ»ì´ë‹¤.

inference ì‹œê°„ì€ ë°”ë€Œì§€ ì•Šìœ¼ë‚˜, modelì˜ AP ì™€ AP scores ì€ ê°ì†Œí–ˆë‹¤.


## [Hyperparameter Evolution](https://github.com/ultralytics/yolov5/issues/607)
ğŸ“š ì—¬ê¸°ì„œëŠ” YOLOv5 ğŸš€ ì˜ hyperparameter evolution ì„ ì„¤ëª…í•œë‹¤. hyperparameter evolution ì´ë€ optimization ì„ ìœ„í•´ GA(genetic algorithm) ì„ ì‚¬ìš©í•œ hyperparameter optimization ì˜ í•œ ì¢…ë¥˜ì´ë‹¤.

ML control ì˜ Hyperparameters ì€ ìµœì ì˜ ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì–´ë µë‹¤. grid searchì™€ ê°™ì€ ì „í†µì ì¸ ë°©ë²•ì€ 1) ë†’ì€ ì°¨ì›, 2) ì°¨ì› ê°„ì˜ ë¶ˆí™•ì‹¤í•œ ìƒê´€ê´€ê³„, 3) ê° ì§€ì ì—ì„œ ì í•©ì„±ì„ í‰ê°€í•˜ëŠ” ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ë°œìƒ í•¨ìœ¼ë¡œì¨ ë¹ ë¥´ê²Œ ë‹¤ë£¨ê¸° ì–´ë µë‹¤. ê·¸ë˜ì„œ GAê°€ hyperparameter ê²€ìƒ‰ì— ì í•©í•œ í›„ë³´ê°€ ë  ìˆ˜ ìˆë‹¤.



### Install Hyperparameter

yolov5 ëŠ” train ì— ì‚¬ìš©ë˜ëŠ” hyperparameterì´ 30ê°œ ì •ë„ ì‚¬ìš©ëœë‹¤. ì´ê²ƒë“¤ì€ `/data` directory ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì²˜ìŒì˜ ì¢‹ì€ ì‹œì‘ì€ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë§Œë“ ë‹¤. ì• ë§¤í•˜ë‹¤ë©´, yolov5 coco training ì˜ parameterì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

```python
# yolov5/data/hyps/hyp.scratch.yaml
# Hyperparameters for COCO training from scratch 
# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300 
# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials 
  
  
lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3) 
lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf) 
momentum: 0.937  # SGD momentum/Adam beta1 
weight_decay: 0.0005  # optimizer weight decay 5e-4 
warmup_epochs: 3.0  # warmup epochs (fractions ok) 
warmup_momentum: 0.8  # warmup initial momentum 
warmup_bias_lr: 0.1  # warmup initial bias lr 
```

### Define Fitness

fitnessëŠ” ìš°ë¦¬ê°€ ìµœëŒ€í•œ í™œìš©í•˜ê³ ì í•˜ëŠ” ê°’ì´ë‹¤. yolov5ì—ì„œ ìš°ë¦¬ëŠ” mAP@0.5(weightì˜ 10%) ì™€ mAP@0.5:0.95(90%ë¥¼ ë‚¨ê¹€) ê³¼ ê°™ì€ ê°€ì¤‘ì¹˜ ê²°í•©ì˜ default fitnessë¥¼ ê°€ì§€ê³  ìˆë‹¤. 

```python
# yolov5/utils/metrics.py
def fitness(x): 
    # Model fitness as a weighted combination of metrics 
    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95] 
    return (x[:, :4] * w).sum(1) 
```

### Evolve

ì´ ì˜ˆì œì—ì„œ base scenario ëŠ” ì‚¬ì „ í›ˆë ¨ëœ yolov5s ë¥¼ ì‚¬ìš©í•˜ì—¬ COCO128 ì„ 10 epochs ë™ì•ˆ finetuning í•œë‹¤.

`--evolve` ë¥¼ í†µí•´ epochì„ ì§„í–‰í•˜ë©´ì„œ hyperparameterì„ ìµœì í™”í•œë‹¤. 

```shell
# single-GPU
python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve

# Multi-GPU
for i in 0 1 2 3; do
  nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve --device $i > evolve_gpu_$i.log &
done
```

`--evolve` ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 300ë²ˆì„ ë°˜ë³µí•˜ë©´ì„œ hyperparamemterë¥¼ ìµœì í™”í•˜ì§€ë§Œ, `--evolve 1000` ê³¼ ê°™ì´ ì‘ì„±í•˜ë©´ 1000ë²ˆ ë°˜ë³µìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. ë°˜ë³µí•˜ë©´ì„œ ê°€ì¥ ìµœì ì˜ parameterë¥¼ `runs/evolve/hyp_evolved.yaml` ì— ì €ì¥í•œë‹¤.

ìµœì†Œí•œ 300ë²ˆì€ í•´ì•¼ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ë˜, evolution ìì²´ë„ ì¼ë°˜ì ìœ¼ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  ë¹„ì‹¼ë°, ìˆ˜ì²œë²ˆì˜ ë°˜ë³µì„ í•˜ê²Œ ë˜ë©´ ë§ì€ ì–‘ì˜ GPUë¥¼ ì‚¬ìš©í•˜ê²Œ ë  ê²ƒì´ë‹¤.


## [Transfer Learning with Frozen Layers](https://github.com/ultralytics/yolov5/issues/1314)

ğŸ“š ì—¬ê¸°ì„œëŠ” transfer learning ì‹œì— Yolov5 ğŸš€ layer ë¥¼ frozen ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤. transfer learning ì€ ìƒˆë¡œìš´ dataì— ëŒ€í•œ modelì„ ë¹ ë¥´ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤. ì¼ë¶€ initial weights ë¥¼ ê³ ì •ì‹œí‚¤ê³ , ë‚˜ë¨¸ì§€ weights ëŠ” lossë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë˜ê³ , optimizerì— ì˜í•´ ì—…ë°ì´íŠ¸ëœë‹¤. ì´ëŠ” ì›ë˜ë³´ë‹¤ í›¨ì”¬ ì ì€ ë¹„ìš©ì´ ë“ ë‹¤. ì •í™•ë„ëŠ” ë‹¤ì†Œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜, training time ì„ ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

### Before start

```shell
git clone https://github.com/ultralytics/yolov5
cd yolov5
pip install -r requirements.txt wandb # add W&B for logging
```


### Frozen Backbone

`train.py` ì™€ ë™ì¼í•œ architecture ë¥¼ ì‚¬ìš©í•˜ë ¤ëŠ” ë¶€ë¶„ì€ training ì‹œì‘ ì „ì— `grad=False` ë¡œ ì„¤ì •í•˜ë©´ `frozen`ëœë‹¤.

```python
# yolov5/train.py
 # Freeze 
 freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze 
 for k, v in model.named_parameters(): 
     v.requires_grad = True  # train all layers 
     if any(x in k for x in freeze): 
         print(f'freezing {k}') 
         v.requires_grad = False 


#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡


for k, v in model.named_parameters():
    print(k)

# Output
model.0.conv.conv.weight
model.0.conv.bn.weight
model.0.conv.bn.bias
model.1.conv.weight
model.1.bn.weight
model.1.bn.bias
model.2.cv1.conv.weight
model.2.cv1.bn.weight
...
model.23.m.0.cv2.bn.weight
model.23.m.0.cv2.bn.bias
model.24.m.0.weight
model.24.m.0.bias
model.24.m.1.weight
model.24.m.1.bias
model.24.m.2.weight
model.24.m.2.bias
```

yolov5 ì—ì„œ backbone ì€ 0~9 layerì— í•´ë‹¹í•˜ë¯€ë¡œ backbone ê³ ì •ì„ ìœ„í•´ 9 layer ê¹Œì§€ freeze í•œë‹¤.

```shell
python train.py --freeze 10
```

ëª¨ë“  layerì„ ê³ ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì…ë ¥í•˜ë©´ ëœë‹¤.
```shell
python train.py --freeze 24
```



![image](https://user-images.githubusercontent.com/26833433/98394485-22081580-205b-11eb-9e37-1f9869fe91d8.png)

ëª¨ë“  layer freeze ì‹œ ì •í™•ë„ê°€ ë–¨ì–´ì§€ê³ , 
backboneë§Œ freeze í•´ë„ ì¼ë°˜ì ì¸ ëª¨ë¸ë³´ë‹¤ ë–¨ì–´ì§€ì§€ë§Œ, 
ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ ì¤„ì–´ë“ ë‹¤.




* Appendix

repo ê¸°ëŠ¥ì„ ê²€ì¦í•˜ì—¬ í…ŒìŠ¤íŠ¸í•œë‹¤.

```python
###### Reproduce
for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':
  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed
  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP

###### Unit tests
%%shell
export PYTHONPATH="$PWD"  # to run *.py. files in subdirectories

rm -rf runs  # remove runs/
for m in yolov5s; do  # models
  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained
  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch
  for d in 0 cpu; do  # devices
    python detect.py --weights $m.pt --device $d  # detect official
    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom
    python val.py --weights $m.pt --device $d # val official
    python val.py --weights runs/train/exp/weights/best.pt --device $d # val custom
  done
  python hubconf.py  # hub
  python models/yolo.py --cfg $m.yaml  # inspect
  python export.py --weights $m.pt --img 640 --batch 1  # export
done

###### Profile
from utils.torch_utils import profile

m1 = lambda x: x * torch.sigmoid(x)
m2 = torch.nn.SiLU()
results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)


###### Evolve
!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve
!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)

### VOC
for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)
  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}
```





```
>ì°¸ê³  ì½”ë“œ
- "yolov5 github tutorial" -  https://colab.research.google.com/drive/1uFK2FT-0c3rmrUoJoKq8QVcKCsbdwxRb
- "ë¹µí˜• YOLOV5 train Mask" - https://colab.research.google.com/drive/1E8lRvkLVWs9vijUI1S7febGSQ2eb6hzm#scrollTo=9EflbG16Zt21
- "How to train YOLOv5 on a custom dataset code - roboflow" - https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ#scrollTo=GD9gUQpaBxNa
- "YOLOV5 Mask Wearing Dataset - roboflow" - https://public.roboflow.com/object-detection/mask-wearing
- "YOLOV5 fake or real - kaggle" - https://www.kaggle.com/orkatz2/yolov5-fake-or-real-single-model-l-b-0-753?scriptVersionId=37672232

```



<!--link-->
[Train Custom Data]: https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data

[data/coco128.yaml]: https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml

[CVAT]: https://github.com/openvinotoolkit/cvat

[makesense.ai]: https://www.makesense.ai/

[Load From Pytorch Hub]: https://github.com/ultralytics/yolov5/issues/36

[#2291]: https://github.com/ultralytics/yolov5/pull/2291

[Flask REST API]: https://github.com/ultralytics/yolov5/tree/master/utils/flask_rest_api

[Test-Time Augmentaion]: https://github.com/ultralytics/yolov5/issues/303