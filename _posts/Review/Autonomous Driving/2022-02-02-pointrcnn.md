---
title:    "[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud"
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-02-02 00:17:00 +0800
categories: [Review, Autonomous Driving]
tags: [Autonomous Driving, PointRCNN]
toc: True
comments: True
math: true
mermaid: true
image:
  src: /assets/img/autodriving/pointrcnn/fig1.png
  width: 500
  height: 800
---

`PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud` 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.

<br>

# Abstract

본 논문에서는 포인트 클라우드로부터 3D 객체 검출에 대한 PointRCNN을 제안한다. 전체 프레임워크는 2단계로 구성되어 있는데, 1단계는 상향식 3D 제안 발생을 진행하고, 2단계에서는 표준 좌표안에서 제안을 정제하여 최종적인 객체 결과를 얻는다. RGB 이미지로부터 제안을 발생하거나 이전 방식대로 조감도나 복셀들로 포인트 클라우드를 투영하는 것 대신, 이 1단계 서브 네트워크는 전체 포인트 클라우드를 전경/배경으로 분할을 통해 상향식으로 포인트 클라우드에서 작은 수의 하이 퀄리티의 3D 제안을 직접 생성한다. 2단계 서브 네트워크는 각 제안의 풀링된 포인트를 표준 좌표로 변환하여 더 나은 로컬 공간적 특징을 학습하고, 이는 1단계에서 학습된 각 포인트의 전역 의미론적 특징과 결합되어 정확한 박스 세분화 및 신뢰도 예측을 제공한다. KITTI 데이터셋의 3D 객체 벤치마크에서의 실험은 제안된 아키텍쳐가 입력으로서 포인트 클라우드만을 사용하여 최첨단 방법들을 엄청난 차이를 보이면서 능가한다는 것을 보여준다. 코드는 [이 주소](https://github.com/sshaoshuai/PointRCNN)를 참고하면 된다.

<br>

# 1. Introduction

딥러닝은 객체 검출과 인스턴스 세그멘테이션을 포함한 2D 컴퓨터 비전에서 엄청난 진보를 이루고 있다. 2D 장면을 이해하는 것 이상으로 3D 객체 탐지는 중요하고 로봇이나 자율주행과 같은 현실에 적용하기에 필요 불가결하다. 최근 개발된 2D 탐지 알고리즘은 이미지에서 큰 범위의 뷰 포인트 및 배경 클러터를 처리할 수 있지만, **포인트 클라우드가 있는 3D 개체를 감지**하는 것은 여전히 3D 개체의 6DoF(Degrees-of-Freedom)의 불규칙한 데이터 형식과 대규모 검색 공간에서 큰 어려움에 직면해 있다. 

<img src='/assets/img/autodriving/pointrcnn/fig1.png'>

자율주행에서 보통 3D 센서로 장면들의 3D 구조를 포착하여 3D 포인트 클라우드를 발생시키는 LIDAR 센서를 사용한다. 포인트 클라우드 기반의 3D 객체 탐지의 어려움은 주로 포인트 클라우드의 불규칙성에 있다. 최첨단 3D 객체 탐지 방법들은 위의 그림a처럼 조감도 뷰나 전방뷰 또는 규칙적인 3D 복셀로 포인트 클라우드를 투영하는 좋은 2D 탐지 프레임워크를 활용하는데, 이는 최적이 아니며 정보 손실을 겪을 수 있다. 특징 학습을 위해 포인트 클라우드를 복셀이나 다른 규칙적인 데이터 구조로 변환하는 대신, 포인트 클라우드 분류와 분할에 대한 포인트 클라우드 데이터로부터 직접적으로 3D 표현을 학습하기 위해 PointNet을 제안한다. 

위의 그림b와 같이, 이 아키텍처는 pointNet을 적용하여 2D RGB 탐지 결과로부터 자른 frustum 포인트 클라우드를 기반으로 한 3D 바운딩 박스를 측정했다. 그러나 이 방법의 성능은 2D 탐지에 크게 의존하고, 강인한 바운딩 박스 제안을 생성하기 위한 3D 정보의 장점을 활용할 수 없다.

2D 이미지로부터 객체 탐지와는 달리 자율주행에서 3D 객체는 주석이 달린 3D 바운딩 박스에 의해 자연스럽고 잘 분리된다. 즉, 3D 객체 탐지에 대한 훈련 데이터는 직접적으로 3D 객체 분할에 대한 의미론적 마스크를 제공한다. 이는 2D 탐지와 3D 탐지 훈련 데이터 사이의 중요한 차이가 된다. 2D 객체 탐지에서 바운딩 박스는 오직 시멘틱 분할에 대한 약한 지도(supervision)를 제공할 수 있다.

이 관찰을 기반으로 정확하고 강인한 3D 탐지 성능을 달성하고 3D 포인트 클라우드에서 직접적으로 동작하는 `PointRCNN`이라는 새로운 2단계 3D 객체 탐지 프레임워크를 제시한다. 이에 대한 설명은 위의 그림c를 통해 볼 수 있다. 이 제시된 프레임워크는 2단계로 구성되어 있는데, 1단계는 상향식으로 3D 바운딩 박스 제안 생서을 목표로 한다. GT 분할 마스크를 생성하기 위한 3D 바운딩 박스를 활용하여 1단계는 배경 점을 분할하고, 분할된 점들로부터 소수의 바운딩 박스 제안을 생성한다. 이러한 전략은 전체 3D 공간 안에서 많은 3D 앵커 박스 사용을 피하고, 계산을 줄인다. 

두번쨰 단계에서는 표준 3D 박스 교정을 수행한다. 3D 제안이 발생된 후에, 포인트 클라우드 영역 풀링 작업을 적용하여 1단계로부터 학습된 점 표현을 풀링한다. 전역 박스 좌표를 직접적으로 측정하는 3D 방법들과 달리 풀링된 3D 점들은 표준 좌표로 변형되고, 상대 좌표 교정을 학습하기 위해 1단계의 분할 마스크뿐만 아니라 풀링 포인트 특징과 결합된다. 이 전략은 1단계의 분할과 제안으로부터 제공된 모든 정보를 활용한다. 더 효과적인 좌표 정제를 위해, 제안 생성 및 개선을 위한 전체 **bin 기반의 3D 상자의 회귀 손실**도 제안하며, 절제 실험 결과 다른 3D 상자의 회귀 손실보다 수렴 속도가 빠르고 회수율이 높은 것으로 나타났다. 

이 논문을 통해 3가지를 기여할 수 있다. (1)첫번째는 포인트 클라우드를 전경 객체 및 배경으로 분할하여 소수의 고품질 3D 제안을 생성하는 새로운 상향식 포인트 클라우드 기반의 3D 바운딩 박스 제안 알고리즘을 제안한다. 분할로부터 학습된 점 표현은 제안 발생에 좋을 뿐만 아니라 추후의 박스 정제에도 도움을 준다. (2)제안된 표준 3D 바운딩 좌표 정제는 1단계로부터 생성된 높은 리콜 박스 제안의 이점을 활용하고, 강력한 빈 기반의 손실로 표준 좌표의 박스 좌표 정제를 예측하는 방법을 학습한다. (3)제안된 3D 탐지 프레임워크인 pointRCNN은 엄청난 차이로 최첨단 기술들을 능가하고, 입력으로 포인트 클라우드만을 사용함으로써 KITTI 3D 탐지 테스트에서 공개된 모든 모델 중 1등을 차지했다.

<br>

# 2. Related Work

* 3D object detection from 2D images

이미지로부터 3D 바운딩 박스를 추정하는 방법들이 있다. 3D와 2D 바운딩 박스 사이의 공간적 제약 조건을 활용하여 3D 객체 자세를 복구하기도 하며, 이미 정의된 3D 박스들을 평가하기 위한 에너지 함수로서 객체의 3D 구조적 정보를 형성하기도 한다. 이러한 작업들은 깊이 정보가 부족하여 대략적인 3D 감지 결과만 생성할 수 있으며, 외관 변화에 상당한 영향을 받을 수 있다.

* 3D object detection from point clouds

최첨단 3D 객체 탐지 방법들은 희박한 3D 포인트 클라우드로부터 뚜렷한 특징을 학습하기 위한 다양한 방법들을 제안했다. 포인트 클라우드를 조감도로 투영하고, 2D CNN을 활용하여 3D 박스 발생을 위한 포인트 클라우드 특징을 학습하기도 한다. 점들을 복셀로 그룹화하고, 3D CNN을 활용하여 3D 박스들을 발생시키기 위한 복셀의 특징들을 학습하기도 한다. 그러나 조감도 투영과 복셀화는 데이터 정량화하는 동안에 정보 손실을 겪고, 3D CNN은 메모리와 계산이 너무 많다. 그래서 정확한 2D 탐지기를 활용하여 이미지로부터 2D 제안을 생성하고, 각 잘린 이미지 영역안에서 3D 점의 크기를 줄이기도 했다. 그런 다음 PointNet은 3D 박스 추정에 대한 포인트 클라우드의 특징들을 학습하는데 사용되기도 했다. 그러나 2D 이미지 기반의 제안 생성은 3D 공간에서만 잘 관찰될 수 있는 일부 까다로운 사례에 대한 것은 잘 적용되지 않는다. 그러한 실패는 3D 박스 추정 단계로는 복구할 수 없었다. 

대조적으로 우리의 상향식 3D 제안 생성 방법은 포인트 클라우드에서 직접적으로 강력한 3D 제안을 생성하는데, 이는 효율적이다.

* Learning point cloud representations

복셀이나 멀티뷰 포맷으로서 포인트 클라우드를 표현하는 대신, 포인트 클라우드 분류 및 분할의 속도와 정확도를 크게 높이는 포인트 클라우드에서 점들의 특징을 직접 학습할 수 있는 PointNet 아키텍처를 제시한다. 후속 작업에서는 포인트 클라우드의 로컬 구조를 고려하여 추출된 특징 품질을 더욱 개선한다. 

우리의 작업은 점 기반의 특징 추출기를 3D 포인트 클라우드 기반의 객체 탐지로 확장하여 원시 포인트 클라우드에서 3D 박스 제안과 감지 결과를 직접 생성하는 새로운 2단계 3D 탐지 프레임워크를 제시한다.

<br>

# 3. PointRCNN for Point Cloud 3D Detection

이번 섹션에서는 불규칙적인 포인트 클라우드로부터 3D 객체를 검출하기 위한 2단계 검출 프레임워크인 PointRCNN을 제안한다. 전체 구조는 아래 그림과 같다. 1단계는 상향식 제안 발생 단계와 표준 바운딩 박스 정제 단계로 구성되어 있다.

<img src='/assets/img/autodriving/pointrcnn/fig2.png'>

## 3.1 Bottom-up 3D proposal generation via point cloud segmentation

현존하는 2D 객체 탐지 방법들은 1단계와 2단계 방법들로 분류할 수 있는데, 1단계 방법들은 일반적으로 빠르지만 정제없이 객체 바운딩 박스를 직접 추정하며, 2단계 방법들은 제안을 먼저 발생시키고 나서 두번째 단계에서 제안과 신뢰도를 정제한다. 그러나, 2D에서 3D로의 2단계의 직접적인 확장은 포인트 클라우드의 불규칙한 포맷과 매우 큰 3D 탐색 공간때문에 필요하다. AVOD는 제안을 생성하기 위해 각 앵커마다 80~100K개의 앵커 박스를 3D 공간에 배치하고 멀티뷰에 각 앵커에 풀링을 배치한다. F-PointNet은 2D 이미지로부터 2D 제안들을 생성하고, 2D 영역으로부터 잘라낸 3D 점들을 기반으로 3D 상자를 추정한다. 이는 3D 공간에서만 명확하게 관찰할 수 있는 까다로운 객체를 놓칠 수 있다. 그래서 전체 포인트 클라우드 분할을 기반으로한 1단계 서브 네트워크로서 정확하고 강인한 3D 제안 발생 알고리즘을 제시한다. 그리고 각기 다른 중첩없이 자연스럽게 분리된 3D 장면들안에서 객체를 관찰한다. 모든 3D 객체 분할 마스크는 그들의 3D 바운딩 박스 주석을 통해 직접적으로 얻어진다. 즉, 3D 상자 내부의 3D 점이 전경 점들로 간주한다.

그러므로 상향식에서 3D 제안을 생성할 것을 제안한다. 특히, 포인트별 특징을 학습하여 원시의 포인트 클라우드를 분할하는 동시에 분할된 전경 포인트들로부터 3D 제안을 생성한다. 상향식 전량을 기반으로 3D 공간에서 미리 정의된 큰 용량의 3D 박스들을 사용하지 않고, 3D 제안 발생을 위한 탐색 공간을 많이 제한한다. 실험을 통해 제안된 3D 박스 제안 방법이 3D 앵커 기반의 제안 발생 방법들보다 엄청 높은 리콜을 달성한다는 것을 볼 수 있다. 

* Learning point cloud representations

뚜렷한 포인트별 특징을 학습하여 원시의 포인트 클라우드를 설명하기 위해, 백본 네트워크로서 멀티 스케일 그룹화와 함께 PointNet++을 활용한다. 몇몇의 다른 포인트 클라우드 네트워크 구조에 대한 모델이나 희박한 컨볼루션을 하는 VoxelNet이 있는데, 이 또한 백본 네트워크로서 적용할 수 있다. 

* Foreground point Segmentation

전경 점들은 객체의 위치와 방향을 예측하는데 많은 정보를 제공한다. 전경 점들을 분할하는 법을 학습하고 나면, 포인트 클라우드 네트워크는 정확한 포인트별 예측을 만들기 위해 맥락적 정보를 포착해야만 하는데, 이는 3D 박스를 발생하는데 효과적이다. 상향식 3D 제안 발생 방법을 설계하여 전경 점들로부터 직접적으로 3D 박스 제안을 생성한다. 즉, 전경 분할과 3D 박스 제안 발생은 동시에 진행된다. 

포인트별 특징을 고려할 떄, 백본 포인트 클라우드 네트워크를 살짝 수정하여 사용하는데, 전경 마스크를 추정하기 위한 1개의 분할 헤드와 3D 제안을 발생시키기 위한 1개의 박스 회귀 헤드를 삽입한다. 점 분할동안 GT 분할 마스크는 3D GT 박스에 의해 자연스럽게 제공된다. 전경 점들의 갯수는 큰 크케일의 바깥 배경보다 훨씬 더 작다. 그래서 우리는 *focal loss*를 사용하여 불균형 문제를 다룬다. 식은 다음과 같다.

<img src='/assets/img/autodriving/pointrcnn/focal.png'>

포인트 클라우드 분할을 훈련하는 동안, 기본값으로 αt = 0.25, γ = 2 로 정한다.

* Bin-based 3D bounding box generation

위에서 언급한 것과 같이, 박스 회귀 헤드는 전경 점 분할과 동시에 상향식 3D 제안을 생성하기 위해 추가된다. 훈련동안 오직 전경 점들로부터 3D 바운딩 박스 위치를 회귀하기 위한 박스 회귀 헤드만을 필요로 한다. 이 떄, 배경 점들에 대한 회귀는 하지 않지만, 포인트 클라우드 네트워크의 수용 필드때문에 이러한 점들은 박스 생성을 위한 추가적인 정보도 제공한다. 

3D 바운딩 박스는 LIDAR 좌표계에서 (x,y,z,h,w,l,θ)로서 표기된다. 이 떄, (x,y,z)는 객체의 중심 위치, (h,w,l)은 객체 크기, θ는 조감도에서의 객체 방향이다. 생성된 3D 박스 제안을 제한하기 위해 bin-based 회귀 손실을 제안하여 객체의 3D 바운딩 박스를 추정한다.

<img src='/assets/img/autodriving/pointrcnn/fig3.png'>

객체의 중심 위치를 추정하기 위해 위 그림과 같이, 각 전경 점들의 주변 영역을 X축과 Z축을 따라 일련의 연속되지 않는 빈(점)으로 분할한다. 특히, 현재 전경 점의 각 X,Y축에 대해 탐색 범위인 S를 설정하고, 각 1D 탐색 범위는 X-Z평면에서 또 다른 객체 중심(X,Z)를 나타내기 위해 균일한 길이 δ의 빈으로 나뉜다. smooth L1 Loss로의 직접적인 회귀대신에 X,Z축에 대한 cross-entropy loss를 통한 빈 기반의 분류를 사용하게 되면 더 정확하고 강인한 중심 위치를 불러올 수 있다. X,Z축에 대한 위치화 loss는 두 개의 항으로 구성되는데, 1개는 각 X,Z축에 따른 빈 분류에 대한 항이고, 다른 하나는 분류된 빈 내의 남은 회귀 분석에 대한 항이다. 수직의 Y축에 따른 중심 좌표 y에 대해서는 대부분의 객체의 y값이 매우 작은 범위 안에 있기 때문에 smooth L1 loss를 통해 회귀한다. L1 loss를 사용하는 것은 정확한 y값을 얻기에 충분하다. 

그렇게 되면 타겟의 위치는 다음과 같이 형성된다.

<img src='/assets/img/autodriving/pointrcnn/local.png'>

- (x^(p),y^(p),z^(p)): 전경 관심 지점의 좌표
- (x^p,y^p,z^p): 해당 객체의 중심 좌표
- binx^(p), binz^(p): X,Z축을 따라 배치되는 GT bin
- resx^(p), resz^(p): 할당된 bin안에 추가적인 위치 정제를 위한 잔류 GT 
- C: 정규화를 위한 빈 길이

타겟의 방향 θ와 크기 (h,w,l) 추정을 위해 2π를 n개의 빈으로 나누고, x와 z의 예측과 같은 방법으로 빈 분류 타겟인 binθ^(p)와 잔류 회귀 타겟인 resθ^(p)를 계산한다. 객체 크기 (h,w,l)은 전체 훈련 셋에서 각 클래스의 평균 객체 크기에 대한 잔류(resh^(p),resw^(p),resl^(p))를 계산하여 직접적으로 회귀된다.

추론 단계에서 빈 기반으로 예측된 파라미터 x,z,θ에 대해 먼저 예측된 신뢰도가 가장 높은 빈 중심을 선택하고, 예측된 잔류를 추가하여 정제된 파라미터를 얻는다. y,h,w,l과 같이 직접적으로 회귀된 파라미터들은 그들의 초기값에 예측된 잔류를 더한다.

전체 3D 바운딩 박스 회귀 손실인 *L*reg는 다음과 같이 공식화된다.

<img src='/assets/img/autodriving/pointrcnn/loss.png'>


