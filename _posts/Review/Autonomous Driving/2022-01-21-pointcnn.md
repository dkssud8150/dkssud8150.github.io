---
title:    "PointCNN: Convolution on X-transformed points"
author:
  name: JaeHo-YooN
  link: https://github.com/dkssud8150
date: 2022-01-21 12:00:00 +0800
categories: [Review, Autonomous Driving]
tags: [Autonomous Driving, PointCNN]
toc: True
comments: True
math: true
mermaid: true
image:
  src: _site/assets/img/autodriving/pointcnn/fig4.png
  width: 800
  height: 500
---

1. this ordered seed list will be replaced by the toc
{:toc}

`pointCNN: Convolution on X-transformed points`논문에 대한 리뷰입니다. 이 논문은 2018에 투고된 논문이며 약 1100회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.

<br>

# Abstract

저자는 point cloud로부터 피쳐 학습에 대한 간단하고 일반적인 프레임워크를 소개하고자 한다. CNN의 성공의 열쇠는 grid에서 조밀하게 표현된 데이터의 공간적-지역적(local) 상관관계를 활용할 수 있는 convolution 연산이다. 그러나 point cloud는 불균일하고 무질서하기 때문에, 점과 관련된 피쳐에 대해 커널을 직접 연산을 하면 형상 정보가 사라지고 점의 정렬에 대한 변형이 발생한다. 이 문제를 해결하기 위해 저자는 두 가지 원인: (1) 점과 관련된 입력 피쳐의 가중치와 (2) 잠재적이고 표준적인 순서에 대한 점들의 순열, 을 동시에 촉진하기 위해 입력 점들로부터 x-transform을 배울 것을 제안한다. 일반적인 convolution 연산자의 요소별 곱과 합 연산은 x-transformation기능 이후에 적용된다. 이 방법은 point cloud로부터 피쳐 학습에 대한 전형적인 CNN의 표본이므로, 저자는 이를 PointCNN이라 부른다. 

<br>

# 1. Introduction

공간적-지역적(spatially-local) 상관관계는 데이터 표현과 독립적인 다양한 유형의 데이터의 아주 흔한 특성이다. 이미지와 같이 일반 도메인에 표시되는 데이터의 경우, convolution 연산은 다양한 작업에서 CNN에서의 주된 성공 요인으로서, 저 상관관계에 활용하면 효과적인 결과를 보여준다. 그러나 무질서하고 불균일한 point cloud 형태의 데이터에 대해서는 convolution 연산이 데이터에서 공간적-지역적 상관관계를 활용하는 것이 적합하지 않다. 

<img src="/_site/assets/img/autodriving/pointcnn/fig1.png">

위의 그림은 point cloud에 convolution을 적용할 때의 문제를 설명하고 있다. (i)~(iv)에 대해 C차원의 입력 피쳐을 가진 무질서한 데이터를 F = {fa, fb, fc, fd} 라고 하고, 4xC 형태를 가진 1개의 커널을 K = [kα, kβ, kγ, kδ]^T 를 가진다고 가정해보자.(i)의 경우, 규칙적인 그리드 구조에 의해 규칙적인 정렬을 보여주고 있으며, 이는 2 x 2 patch 지역안의 `fi = Conv(K,[fa,fb,fc,fd]^T)`의 형태를 가진 K로부터 convolution되어 4xC 형상의 [fa,fb,fc,fd]^T가 될 수 있다. 이 때, Conv(,)는 요소별 곱을 뜻한다. 그리고, (ii)~(iv)의 경우 점들은 주변 점들에 의해 샘플링되므로 점의 순서가 임시적일 수 있다. 위의 그림에 적힌 순서를 따른다고 생각했을 때, 입력 피쳐 셋 F는 (ii)와 (iii)에서는 [fa,fb,fc,fd]^T이 될 것이고, (iv)에서는 [fc,fa,fb,fd]^T가 될 것이다. 만약 이 상태로 convolution 연산을 직접적으로 적용했을 때, 세 케이스의 출력 피쳐들은 위 그림의 (1a)처럼 계산된다. 모든 케이스에 대해 fii = fiii로 하고, 대부분의 케이스에 대해 fiii != fiv로 고정시킨다. 그렇게 되면, 결과적으로 직접적인 연산이 fii = fiii와 같은 형상 정보는 버리고, fiii != fiv와 같은 순서에 대한 변형은 남겨두는 결과를 초래한다,

본 논문에서는 X = MLP(p1,p2,...,pk)와 같은 다층 퍼셉트론인 입력 점들(p1,p2,...,pk)인 K의 좌표에 대한 K x K의 X-transformation을 제안한다. 목표는 가중치 부여와 입력 피쳐 변형을 동시에 수행한 다음, 그 뒤에 변형된 피쳐들에게 전형적인 convolution을 적용하는 것이다. 이 과정을 `X-conv`라 하고, 이것이 PointCNN에 기본적인 블록이 된다. 이를 통해 fig1의 (ii)~(iv)에 대한 X가 4x4 행렬이고, K = 4인 X-conv를 적용하면 위 그림의 (1b) 방정식처럼 된다. 이 때, xii와 xiii는 각기 다른 점들로부터 학습된 것이기 때문에, 입력 피쳐의 가중치가 달라져 fii != fiii를 만들 수 있다. xiii와 xiv에 대해 ⫪가 (c,a,b,d)를 (a,b,c,d)로 순열하기 위한 순열 행렬일 때, Xiii = xiv x ⫪를 만족한다면, fiii = fiv를 만들 수 있다.

fig1의 분석을 통해, 이상적인 X-transformation에서 X-conv는 순서를 바꾸지 않고, 점의 형상 정보를 얻을 수 있는 것을 확인했다. 실제로는, 순열 등식 측면에서 x-transformation 학습은 이상과 차이가 있다. 그럼에도 불구하고, X-conv를 통해 만들어진 PointCNN은 point cloud로 일반적인 convolution을 직접 적용하는 것보다 훨씬 좋고, PointNet++와 같이 point cloud 입력 데이터에 대해 설계된 최첨단 신경망보다도 더 좋다.

<br>

# 2. Related Work

* Feature Learning from Regular Domains

CNN은 이미지(2D 규칙적인 그리드안의 픽셀들)안의 공간적-지역적 상관관계를 잘 활용해왔다. 3D voxels와 같이 고차원의 규칙적인 도메인에 대한 응용CNN도 잘 나와있다. 그러나 입력과 convolution 커널들은 모두 고차원이고, 그래서 매우 많은 계산 비용과 메모리를 필요로 한다. Octree, Kd-tree, Hash 기반의 접근 방식은 빈 공간에 대해서는 convolution 연산을 하지 않으며 계산을 절약했다. point cloud을 그리드로 분할하고, 3D 커널과 conv연산을 위해 그리드 평균 점과 벡터로 각 그리드를 나타낸다. 이러한 접근 방식에서 커널 그 자체는 밀도있고, 높은 차원이다. 그러나 이 접근 방식은 계측적 특징을 학습하는 데 반복적으로 사용할 수 없다. 이러한 방법들과 비교하여 PointCNN은 입력 표현과 conv커널이 밀도가 희박하다.

<br>

* Feature Learning from Irregular Domains

3D 센싱의 급격한 발전에 따라 3D point cloud로부터 특징 학습의 발전이 더뎌졌다. PointNet과 Deep Sets는 입력에 대한 대칭 함수를 사용하면서 입력 순서 불변을 성공시켰다. 커널 상관성과 그래프 풀링은 PointNet과 같은 방법들을 개선시키기 위해 제안되기도 했다. RNN은 정렬된 point cloud 조각들로부터 풀링함으로써 특징들을 프로세싱하는데 사용되었다. 이 많은 대칭 풀링 기반의 방법들이 순서 불변성을 달성하는데 큰 도움을 주었지만 정보를 버리는 대가를 치뤄야만 했다. CNN 커널은 point cloud에 대한 CNN을 일반화하기 위해 이웃 점 위치에 대한 파라미터를 활용한 함수로 표현된다. 그 커널은 독립적으로 파라미터화된 각 점들과 연관이 있지만, 이 논문에서의 방법 안에 x-transformation는 각 이웃으로부터 학습되고, 그래서 지역 구조에 더 적응을 잘 할 수 있다. point cloud이외에 비규칙적인 도메인안의 희박한 데이터는 그래프나 그물망으로 표현되고, 몇몇 작업들은 이러한 표현들을 통해 피쳐 학습을 제안하기도 한다. 

<br>

* Invariance Vs Equivariance

불변성을 달성하는 데 있어 풀링의 정보 손실 문제를 해결하기 위해 등분할을 목표로 하는 선구적인 방법들이 제안되기도 했다. 저자의 x-transformation은 이상적으로 등가성 실현이 가능하게 했고, 실제로도 효과를 볼 수 있다. 저자는 PointCNN과 SPN(Spatial Transformer Networks)와 유사함을 발견했다. 두 가지 모두 정규화를 진행할 때 명시적인 손실이나 제약없이 입력을 더 많이 처리하기 위해 잠재된 표준 형태로 "변환"하는 메커니즘을 제공한다는 점에서 유사하다. 실제로 이 방법은 네트워크가 학습 메커니즘을 더 잘 활용할 수 있는 방법이라는 것을 밝혀냈다. pointCNN에서 x-transformation은 가중치 부여와 순열을 제공하며, 이는 일반적인 행렬로서 모델화된다. 그래서 순열 행렬이 바람직한 출력인 모델과는 다르게 **이중 확률 행렬**에 의해 근사될 것이다.

<br>

# 3. PointCNN

convolution의 계층적 적용은 CNN을 통한 계층적 표현을 학습하는데 필수적이다. PointCNN은 같은 디자인으로 설계되어 있고, 이를 point cloud로 일반화한다. 먼저, PointCNN에서의 계층적 convolution을 소개한 후, X-conv 연산자에 대해 자세하게 설명하고, 다양한 태스크에 적합한 PointCNN 아키텍쳐를 제시할 것이다.

## 3.1 Hierarchical Convolution

<img src="/_site/assets/img/autodriving/pointcnn/fig2.png">

PointCNN의 계층적 convolution을 설명하기 전에, 위 그림과 같이 일반 그리드에 대한 설명을 먼저 보고자 한다. 그리드 기반의 CNN의 입력은 R1xR1xC1의 형태를 가진 F1 특징맵이다. 이 때, R1은 해상도, C1은 피쳐의 채널 깊이이다. 로컬 패치의 형태인 KxKxC1의 커널을 가진 F1과는 달리, KxKxC1xC2의 형태를 가진 convolution의 커널 K은 R2xR2xC2의 형태를 가진 F2의 특징 맵을 만든다. fig2에서 파라미터, R1 = 4, K = 2, R2 = 3이라 가정해보자. F1과 F2를 비교했을 때, 보통 R2 보다 R1의 해상도가 높고, 깊이는 C1보다 C2가 더 깊으며 F2가 F1보다 더 높은 레벨의 정보를 가지고 있다. 위의 그림처럼 4x4 -> 3x3 -> 2x2 의 방식으로 재귀적으로 해상도가 감소하고, 깊이는 더욱 깊어지면서 특징맵을 만들게 된다. 

PointCNN의 입력은 다음과 같다.

<img src="/_site/assets/img/autodriving/pointcnn/input.png">

이 때, 점은 {p1,i : p1, i ∈ R^dim}, 특징은 {f1,i : f1,i ∈ R^C1} 에 해당되고, 그리드 기반의 CNN의 계층적 구조에 따라 X-conv를 F1에 적용하여 높은 레벨의 표현 F2를 얻고자 한다. 이 때, F2는 다음과 같다.

<img src="/_site/assets/img/autodriving/pointcnn/input2.png">

이 때, {p2,i}는 {p1,i}의 대표 점이고, F2는 F1보다 더 작은 해상도와, 깊은 특징 채널을 가진다. F1에서 F2로 변환되는 X-conv 프로세스를 재귀적으로 적용시키면 fig2 하단에서처럼, 입력 점들의 특징은 더 작은 수의 점들로(9 -> 5 -> 2) 투영되나, 특징들은 더 풍부해질 것이다.

저자는 분류 태스크에서 {p1,i}의 무작위 다운 샘플링을 적용하고, 분할 작업은 균일한 점 분포를 더 요구하기 때문에 분할 작업에서 가장 먼 점과 샘플링한다. 추가적으로 본 모델에서도 적용한 Deep Points와 같이 기하학적 처리에서 좋은 성능을 보여준 더 좋은 점 선택을 의심한다. 그래서 더 나은 대표 점 생성 방법에 대한 탐구를 추후에 진행할 예정이다.

<br>

## 3.2 X-conv Operator

X-conv는 F1에서 F2로 변환하는 핵심적인 연산이다. 여기서는, 입력과 출력, 연산의 절차를 먼저 설명하고, 연산의 근거를 설명하겠다.

<img src="/_site/assets/img/autodriving/pointcnn/xconvop.png">

그리드 기반 CNN의 convolution과 유사하게 공간적-지역적 상관관계를 활용하기 위해 X-conv는 local 지역에서 작동한다. 출력 특징이 대표점{p2,i}와 연관되어야 하므로 X-conv는 {p1,i}의 인접한 점과 관련 특징을 입력으로 한다. 간편함을 위해 {p2,i}를 p로 두고, p에 대한 특징을 f, {p1,i}의 K neighbor을 N으로 두어, p에 대한 x-conv의 입력은 다음과 같이 정의된다.

<img src="/_site/assets/img/autodriving/pointcnn/xconvinput.png">

이 때, S는 정렬되지 않은 상태다. S는 훈련 가능한 convolution 커널인 K에 대해 K x Dim의 행렬, P = (p1,p2,...,pk)^T 와 K x C1 행렬, F = (f1,f2,...,fk)^T로 표현될 수 있다. 이 입력을 통해 입력 특징들을 대표점 p로 투영한 특징 Fp를 계산할 수 있다. 

위의 과정, 즉 algorithm 1을 요약하면 다음과 같다.

<img src="/_site/assets/img/autodriving/pointcnn/algorithmsum.png">

PointNet에서처럼, MLPδ()는 각 점이 독립적으로 적용된 다층 퍼셉트론이다. x-conv에 포함된 모든 연산들, 예를 들어 `Conv(.,.)(= 행렬 곱 (.) x (.))` 와 `MLP(.)(= MLPδ())`은 미분이 가능하다. 따라서 x-conv도 미분이 가능하게 되고, 역전파를 통한 훈련을 위해 심층 네트워크에 연결할 수도 있게 된다.

위의 알고리즘 안에 4-6번째 줄은 섹션 1에서 설명했던 1b방정식에 해당되는 x-transformation의 핵심 코드이다. 그리고 이제, 알고리즘1 안에 1-3번째 줄에 대한 근거를 자세히 설명할 것이다. x-conv는 local점에서 작동하도록 설계되어 있고, 출력은 p와 p의 인접 점들의 절대 좌표와는 독립적이어야 하지만, 상대 좌표에는 의존적이어야 한다. 그러기 위해 fig3-b와 알고리즘1의 1번 줄은 대표 점에 지역 좌표계를 위치시킨다는 뜻이다. 공통된 특징들에 대한 인접 점의 지역 좌표계이고, 이것이 출력 피쳐들을 정의한다. 

하지만, 지역 좌표계는 연관된 특징들과는 다른 차원과 표현이다. 이 문제를 해결하기 위해 알고리즘1의 2번줄에서 좌표를 더 높은 차원과 더 추상적인 표현에 대한 좌표로 만든다. 그리고 나서, 3번줄에서 더 나은 프로세싱을 위해 연관된 특징들과 이 좌표와 결합한다. 이에 대한 그림은 fig3-c에서 볼 수 있다.

MLP(.)를 통해 지역 좌표계를 형상들과 동일하게 만들 수 있다. 

<br>

## 3.3 PointCNN Architectures

fig2를 통해 그리드 기반의 CNN에서의 Conv layer과 pointCNN에서의 X-conv의 차이점을 볼 수 있다. 그것은 2가지로 (1) 로컬 지역을 추출하는 방법 (KxK patches Vs 대표 점들 주변의 K neighboring points), (2) 그 로컬 지역으로부터 정보를 배우는 방법 (Conv Vs X-conv) 이다. 그러나 x-conv layer를 통해 심층 네트워크를 합치는 과정은 그리드 기반의 CNN과 유사하다.

<img src="/_site/assets/img/autodriving/pointcnn/fig4.png">

위 그림은 2개의 X-conv layer를 가진 간단한 pointcnn의 구조를 나타낸 것이다. 여기서 N은 출력 대표점 개수, C는 특징 차원, K는 각 대표점마다의 인접 점 개수, D는 x-conv의 팽창률을 의미한다. 그리고, a와 b는 분류를 위한 아키텍처, c는 분할을 위한 아키텍처 구조이다. 

K는 이웃 점 수를, N는 이전 layer에서의 점 개수에 대해 `K/N`비로서 각 대표 점의 수용 필드(receptive field)를 정의할 수 있다. 이를 전체적으로 적용하면 최종 점은 1.0의 수용 필드를 가지게 되는데, 이를 통해 전체 모양을 볼 수 있을 것이고, 형상에 대한 의미론적 학습에 도움이 된다. 그 후 마지막 x-conv뒤에 FC layer를 붙이고, 네트워크 훈련을 위한 loss를 구한다.

fig4-a를 보면, 차원과 점들의 갯수가 점차 줄어듬에 따라 상위 x-conv layer은 훈련 샘플이 급격히 줄어들게 되어 과적합 우려가 발생한다. 이를 해결하기 위해 fig4-b처럼 x-conv layer안에 더 많은 점이 남아 있을 수 있도록 빽빽한 연결이 있는 pointCNN을 제시한다. 하지만 네트워크의 깊이와 수용 필드의 성장률도 유지되어야 하기 때문에, 상위로 올라갈수록 대표 점들이 전체 형상에서의 점점 더 큰 비율을 차지하도록 한다. 그리드 기반의 CNN에서의 확대된 convolution을 사용하여 이를 제작했다. 입력으로 항상 K neighboring 점들을 받는 대신, 팽창률 D에 대한 K x D를 균일하게 샘플링한다. 이 경우 이웃 점의 갯수나 커널 사이즈를 키우지 않고도, 수용 필드가 K/N이 아닌 (K x D)/N로 증가시킬 수 있다.

fig4-b에서 두번째 x-conv layer을 보면 확장률 D = 2를 사용하여 1개가 아닌 4개의 대표점을 남길 수 있고, 이것들 모두를 예측하는데 사용할 수 있다. 이를 통해 상위의 x-conv layer를 더 철저하게 훈련시킬 수 있으며 더 많은 연결이 네트워크를 더 개선시킬 수 있다. 테스트 시에 다중의 대표 점 출력은 예측을 안정화시키기 위해 softmax바로 직전에 평균화한다. 

분할 태스크에서는 더 높은 해상도의 점 단위 출력이 필요하므로 fig4-c에서처럼, 글로벌 정보, 즉 Conv에 사용된 점들을 고해상도 예측으로 추가로 전달하는 역할을 하는 DeConv가 있는 Conv-DeConv 구조를 사용했다. Conv와 Deconv는 같은 x-conv 연산을 사용한다. 

과적합을 방지하기 위해 FC layer 이전에 dropout을 적용했다. 또한, "subvolume supervision"도 사용하여 과적합을 방지했다. 마지막 x-conv layer에서 수용 필드가 1보다 작도록 세팅되며, 대표 점에 의해 부분적인 정보만 볼 수 있도록 한다. 더 높은 성능을 위해 훈련시에는 부분 정보만 사용하는 방법을 사용하기도 한다.

* Data Augmentation

x-conv의 파라미터를 훈련시키기 위해 특정 대표점에 대해 동일한 인접 점 집합을 동일한 순서로 계속 사용하는 것은 좋지 않다. 따라서 무작위 샘플링과 입력 점을 셔플하여 인접 점의 데이터와 순서가 매 반복마다 달라지도록 한다. 입력으로 N개의 점들을 받는 모델을 훈련시키고자 할 때, 가우시안 분포에 따른 *N*=(N,(N/8)^2)개의 점들이 훈련에 사용된다.

<br>

# 4. Experiments

분류 작업에서는 총 6개의 데이터셋(ModelNet40, ScanNets,TC-Berlin, Quick Draw, MNIST, CIFAR 10)을 적용하여 pointcnn을 평가했고, 분할 작업에서는 총 3개의 데이터셋(ShapeNet Parts, S3DIS, ScanNet)을 적용하여 pointcnn을 평가했다. 

## 4.1 Classification and Segmentation Results

<img src="/_site/assets/img/autodriving/pointcnn/table1.png">

위의 그림은 ModelNet40과 ScanNet의 3D point cloud 분류 결과와 point cloud에 대한 몇몇의 신경망에 대한 비교를 요약한 것이다. ModelNet40의 3D 구조의 많은 부분이 수직 방향과 수평 방향에 미리 맞춰져 있다. 만약 무작위 수평 회전이 훈련이나 테스트에 적용되어 있지 않았다면 상대적으로 일관된 수평 방향만 활용되고, 이에 기반한 방법들은 수평 회전하는 모델과 직접적으로 비교할 수 없다. 그래서 ModelNet40과 ScanNet 둘다에 적용했는데, 두 데이터셋에서 모두 높은 성능을 보였다.

이제 분할 태스크에 대해 비교를 해볼 것이다.

<img src="/_site/assets/img/autodriving/pointcnn/table2.png">

분할은 ShapeNet Parts, S3DIS, ScanNet 데이터셋에서 비교했고 그 결과는 위의 테이블에서 볼 수 있다. PointCNN은 분할에 특화되어 있는 최첨단 기술인 SSCN, SP-Graph, SGPN등을 포함하여 비교된 모든 방법들보다 뛰어났다.

<img src="/_site/assets/img/autodriving/pointcnn/table3.png">

스케치는 2D 공간에서의 1D 곡선이므로 2D 이미지들보다 point cloud으로 더 효과적으로 표현될 수 있다. pointCNN을 TU-Berlin과 Quick Draw sketches에서 평가했고, 그 결과를 image CNN 기반의 방법들을 포함해 PointNet++와 성능을 비교하여 위의 테이블에 나타냈다. pointCNN은 두 데이터셋에서 PointNET++을 능가했다. TU-Berline 데이터셋에서, pointCNN은 일반적인 image CNN인 AlexNet보다 약간 더 좋은 성능을 보였지만, Sketch-a-Net과는 조금 떨어지는 모습을 보였다. 이는 Sketch-a-Net에서의 구조적 요소들이 현재의 pointCNN보다 뛰어남을 보여준다.

x-conv가 이상적으로 conv의 일반화 버전이라서 만약 기본 데이터가 같고 단지 다르게 표현된 것이라면, CNN과 성능이 비슷해야 한다. 이를 확인하기 위해 MNIST와 CIFAR10에서 point cloud의 표현에서의 pointCNN의 성능을 평가했다.

<img src="/_site/assets/img/autodriving/pointcnn/table4.png">

MNIST 데이터에서 PointCNN은 다른 방법들과 비슷한 성능을 달성하여 숫자의 모양 정보에 대한 효과적인 학습을 보여줬다. 형상 정보가 대부분 없는 CIFAR10에서는 pointCNN은 RGB 특징에서의 공간적-지역적 상관관계에서 대부분 학습해야 했고, pointCNN과 유명한 image CNN과는 차이가 있긴 하나 꽤나 잘 수행했다. 이를 통해 일반 이미지에 대해서는 원래의 CNN들이 더 나은 선택이라는 결론이 도출된다.

<br>

## 4.2 Ablation Experiments and Visualizations

* Ablatino test of the core X-conv operator

x-transformation의 효과를 확인하기 위해 알고리즘1에서 `Fp <- Conv(K,F*)`인 4-6번 줄을 제거한 baseline을 제시하고자 한다. 이를 pointCNN과 비교하면 훈련 가능한 파라미터가 더 적고, 알고리즘1의 4번줄인 MLP(.)을 제거했기 때문에 더 얕다. 공정한 비교를 위해 wider/deeper인 W/D 그리고, X-conv 존재 여부로 나누어 비교하고자 한다. 저 둘은 대량 같은 양의 파라미터를 가진다. pointCNN w/o X (deeper)의 모델의 깊이는 pointCNN에서 MLP(.)의 제거에 대해서 영향을 받는다. 결과는 다음 테이블에서 볼 수 있다. 

<img src="/_site/assets/img/autodriving/pointcnn/table5.png">

PointCNN은 다른 변형들보다 훨씬 뛰어났고, PointCNN과 PointCNN w/o X 의 차이는 모델의 파라미터 수나 모델의 깊이 때문이 아니다. 

<br>

* Visualization of X-Conv features

각 대표 점은 이웃한 점들을 특정한 순서로 나타내며, 그에 해당하는 `F*`과 `C = Cδ + C1`에 해당하는 R^(KxC)안의 Fx를 가진다. 같은 대표 점에서 네트워크로 근처 점들을 다른 순서로 넣게되면, *`F*`*과 *Fx*를 갖게 된다. 유사하게 `F*`을 PointCNN w/o X에서는 *F0*로 정의한다. 분명한 것은, 입력 점들의 순서의 차이는 다른 `F*`을 나타내기 때문에, *`F*`*은 R^(KxC)공간 안에서 분산될 수 있다. 다른 한편, 학습된 X가 `F*`를 완벽하게 표현할 수 있다면 *Fx*는 공간에서 표준 점에 있어야 한다.

이를 확인하기 위해 ModelNet40 데이터셋에서의 15개의 무작위로 선택된 대표 점의 *`F0`*와 *`F*`*, *`Fx`*에 대한 T-SNE 시각화할 것이고, 이 결과는 아래 그림에서 볼 수 있다.

<img src="/_site/assets/img/autodriving/pointcnn/fig5.png">

fig5-a에서처럼 *`F0`*는 서로 다른 대표 점의 특징이 서로 차별적이지 않음을 나타내는 상당한 "불규칙"을 보인다. fig5-b에서처럼 *`F*`*은 *`F0`*보다 더 나은 살짝 모여있지만, fig5-c에서처럼 *`Fx`*는 X에 의해 집중되어 있음을 볼 수 있고, 따라서 각 대표 점들은 매우 큰 차별성을 보인다. 집중에 대한 정량적 기준을 고려할 때, 먼저 다른 대표점들의 중심 피쳐를 계산하고, 그 후 중심에서 가까운 정도를 기반으로 모든 특징 점을 그들이 속한 대표 점으로 분류한다. 분류 결과는 *`F0`*, *`F0`*, *`Fx`* 각각 76.83%, 89.29%, 94.72%의 분류 정확도를 가진다. 이 결과 한 점에 집중되지는 않지만, 피쳐 학습에서 pointCNN의 성능이 좋다는 것을 볼 수 있다.

<br>

* Optimizer, model size, memory usage and timing

<img src="/_site/assets/img/autodriving/pointcnn/table6.png">

저자는 tensorflow, lr = 0.01의 ADAM optimizer을 사용했다. 위의 테이블에서 볼 수 있듯이 배치 사이즈 16, 1024개의 입력 점을 nVidia Tesla P100 GPU에서 다른 방법들과 분류 태스크에 대해 작업 시간을 비교했다. PointCNN은 이 셋팅에서 훈련/추론에 대해 각 배치마다 0.031/0.012sec의 시간을 달성했다. 게다가 4.4M개의 파라미터를 가진 2048개의 입력 점을 분할하는 모델에서 배치 사이즈 12, 훈련/추론에 대해 각 배치마다 0.61/0.25sec를 달성했다.

<br>

# 5. Conclusion

저자는 point cloud로 표현된 데이터에서 공간적-지역적 상관관계를 활용하도록 CNN을 일반화한 PointCNN을 제안했다. PointCNN의 핵심은 전형적인 convolution으로 작업되기 전에 입력 점과 특징들을 행렬 곱하고 가중치를 부여하는 X-conv 연산이다. 

오픈 소스는 [이 페이지](https://github.com/yangyanli/PointCNN)에 올려두었다.

코드에 대한 리뷰는 [다음 페이지](https://dkssud8150.github.io/classlog/pointcnn2.html)에서 진행할 것이다.


<br>

# Reference

* [PointCNN: Convolution On X -Transformed Points](https://proceedings.neurips.cc/paper/2018/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf)