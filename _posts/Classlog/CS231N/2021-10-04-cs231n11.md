---
title:    " CS231N chapter 11 - Detection and Segmentation "
author:
  name: JaeHo-YooN
  link: https://github.com/dkssud8150
date: 2021-10-03 12:00:00 +0800
categories: [Classlog]
tags: [CS231N,classification,detection,segmentation]
toc: True
comments: True
math: true
mermaid: true
#image:
#  src: /commons/devices-mockup.png
#  width: 800
#  height: 500
---

>10강 리뷰
>
>1) Recurrent Neural Network
>
> * language model
>
>2) CNN + RNN = image captioning
>
>3) LSTM

<br>

<br>

* Computer Vision Tasks

![image](/assets/img/2021-10-06/0017.jpg)

다양한 computer vision task가 있다. semantic segmentation / classification+localization / object detection / instance segmentation 등이 있다.

<br>

# Semantic Segmentation

![image](/assets/img/2021-10-06/0019.jpg)

semantic segmentation은 입력은 이미지, 출력은 이미지의 모든 픽셀에 카테고리를 정하는 것이다. 그 픽셀이 어떤 물체에 해당하는지 결정한다. 

<br>

![image](/assets/img/2021-10-06/0021.jpg)

semantic segmentation은 개별 객체를 구분하지 않는다. 픽셀의 카테고리만 구분할 뿐이다.

그에 반해 instance segmentation은 개별 객체까지 구분한다.

segmentation초기 모델에서는 sliding window를 적용했다. 입력 이미지를 아주 작은 단위로 쪼개어 classification을 하는 것이다.

이 방법은 비용이 엄청나게 크다. 모든 픽셀에 대해 작은 영역으로 쪼개고, 이 모든 영역을 forward/backward pass 하는 일은 상당히 비효율적이다.

<br>

![image](/assets/img/2021-10-06/0022.jpg)

그래서 fully convolutional network를 사용해보았다. FC Layer을 없애고 convolution layer로 구성된 네트워크이다. 

이 네트워크의 출력 tensor는 C x H x W이다. c는 카테고리 수에 해당한다. 이 출력값은 입력 이미지의 모든 픽셀 값에 대해 classification score을 매긴 값이다. 

이 네트워크를 학습시키려면 모든 픽셀의 classification loss를 계산하고 평균 값을 취한다. 그리고 backpropagation을 수행한다.

<br>

training data를 만드는 방법은 입력 이미지에 모든 픽셀에 대해 라벨링하면 된다. 사람이 툴을 만들기도 한다. 객체의 외관선만 그려주면 그 안을 채워넣는 식이다. 

또한, 손실 함수는 출력의 모든 픽셀에 corss entropy를 적용한다. 출력의 모든 픽셀과 실제 값(ground truth)와의 cross entropy를 계산한다. 이 값들을 모두 더하거나 평균화시켜 loss를 구한다.

<br>

문제는 이 네트워크는 입력 이미지의 spatial size(공간적 사이즈)를 계속 유지시켜야 한다. 그래서 비용이 아주 크다. 

<br>

![image](/assets/img/2021-10-06/0024.jpg)

실제 네트워크의 형태는 대부분 이렇다. 특징맵을 downsampling & upsampling 한다. 

이미지 전체를 계속 convolution 시키기보다 max pooling/ stride convolution을 통해 특징맵을 downsampling 한다. 그 후 다시 입력 이미지의 해상도와 같도록 upsampling 한다. 이 방법을 사용하면 계산 효율이 더 좋아진다.

<br>

![image](/assets/img/2021-10-06/0026.jpg)

upsampling의 방법으로 nearest neighbor 이나 bed of nails 가 있다. 

nearest neighbor upsampling의 경우는 해당하는 receptive field로 값을 그대로 복사한다. 주변의 값들을 복사하는 것이다.

bed of nails upsampling 의 경우는 원래 값을 가져오고 나머지 공간에는 0을 집어넣는다.

<br>

![image](/assets/img/2021-10-06/0027.jpg)

Max unpooling 이라는 방법도 있다. pooling과 연관을 지어 downsampling 시에 maxpooling에 사용했던 요소들을 기억하고 그 자리에 값들을 집어넣는 것이다. 

maxpooling을 하면서 특징맵의 공간 정보를 잃게 된다. 이 방법을 사용하면 공간정보 손실을 줄일 수 있기 때문에 더 좋은 결과를 얻을 수 있다.

<br>

![image](/assets/img/2021-10-06/0028.jpg)

transpose convolution이라는 것도 있다. 앞서 배운 것(upsampling , max unpooling)들은 고정함수이기 때문에 학습하지는 않는다. 

하지만, transpose convolution은 어떤 방식으로 upsampling할지 학습을 할 수 있다. 

일반적인 3x3 *convolution filter*(stride=1,padding=1)은 입력도 출력도 4x4이다.

<br>

![image](/assets/img/2021-10-06/0032.jpg)

하지만, **strided convolution** 의 경우 입력이 4x4이고, 출력이 2x2이다. 

이유는 3x3 필터에 대한 strided convolution은 한 픽셀씩 이동하면서 계산하지 않고, 두 픽셀씩 움직여야 한다. 따라서 stride=2인 strided convolution은 학습 가능한 방법으로 2배 downsampling한다.

<br>

![image](/assets/img/2021-10-06/0035.jpg)

transpose convolution은 반대로 입력이 2x2 이고, 출력이 4x4다. 

여기서는 내적을 수행하지 않고, 우선 input 특징맵에서 값(빨간색)을 하나 선택한다. 이 하나의 값(스칼라)과 필터를 곱한 값을 3x3 영역에 넣는다. 

transpose convolution은 필터와 입력의 내적의 계산이 아니라 입력 값이 필터에 곱해지는 가중치 역할을 한다. 따라서 출력은 **필터 * 입력(가중치)** 가 된다.

입력이 한칸씩 움직이면 출력은 2칸씩 움직인다. 이 때, 서로 겹치는 값이 존재할 경우 두 값을 더한다. 

<br>

![image](/assets/img/2021-10-06/0039.jpg)

transpose convolution의 구체적인 예시이다. 입력은 a,b 필터는 x,y,z 이다. 출력값을 계산해보면 오른쪽과 같은 형태가 된다.

<br>

![image](/assets/img/2021-10-06/0041.jpg)

이름이 transpose인 이유를 설명하자면, 

왼쪽은 일반적인 convolution 이고, 오른쪽은 transpose convolution이다. 같은 행렬이지만, 오른쪽의 행렬을 transpose(전치)를 취한 후 곱 연산한다. 왼쪽은 stride 1 convolution이고, 오른쪽은 stride 1 transpose convolution인 것이다.

왼쪽의 필터의 양 끝단의 0 은 padding으로 인한 값이다.

<br>

![image](/assets/img/2021-10-06/0043.jpg)

하나 더 보자면, stride 1 transpose convolution을 보면 비슷하게 생겼다. padding 등을 고려하면 좀 달라지지만 기본적으로 비슷하다. 하지만 여기서 stride = 2로 설정되어 있다. 다른 점은 xyz가 2칸씩 움직인다. 

이 때, stride가 1을 초과하면 transpose convolution과 convolution이 같지 않게 된다. 아마 a가 a,b,c,d가 아닌 a,b만 나오기 때문이 아닐까 싶다.

<br>

<br>

# Classification + Localization

![image](/assets/img/2021-10-06/0044.jpg)

이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체가 어디에 있는지 알기 위해 **bounding box**를 친다. 

*object detection*은 다중 객체를 인식할 때 사용되고, *localization*은 단일 객체를 분류할 때 자주 사용된다. 

<br>

![image](/assets/img/2021-10-06/0046.jpg)

localization의 기본 구조를 나타낸 것이다. 입력 이미지를 받아 네트워크에 넣고 출력 레이어 직전의 FC layer는 class score로 연결되어 카테고리를 정한다. 그리고 4개의 원소를 가진 vector와 연결된 FC layer가 하나 더 있다. 이 4개의 원소는 width/height/x/y로 bounding box의 위치를 나타내는 것이다.

따라서 localization을 할 때는 2개의 출력 값을 가진다. 하나는 클래스 스코어, 하나는 bounding box 좌표이다.

> 이 때, 학습 이미지에는 카테고리 레이블과 해당 객체의 bounding box Ground Truth를 동시에 가지고 있어야 한다.

<br>

![image](/assets/img/2021-10-06/0048.jpg)

그래서 loss도 두 개 존재한다. 스코어에 대한 loss는 softmax, bounding box에 대한 loss는 L2 loss로 구한 후 두개를 더해 최종적인 loss를 구한다. L2 대신 smooth L1을 더 많이 사용되긴 한다. 

이처럼 두 개의 loss를 합친 loss를 Multi-task loss라고 한다. gradient를 구하려면 네트워크 가중치들의 각각의 미분 값을 계산해야 한다. loss가 두 개이므로 미분 값도 두 개다. 

loss의 수식을 보면 두 loss의 가중치를 조절하는 하이퍼파라미터가 존재한다. 이 두 값을 조절하는 것은 매우 까다롭다. 그래서 loss 값이 아닌 다른 지표(MAP(mean average precision), model size, speed, num of parameters)를 통해 성능을 비교하는 것이 좋다.

<br>

![image](/assets/img/2021-10-06/0050.jpg)

classification + localizatino은 human pose estimation에도 적용할 수 있다. 사람의 관절이 어디에 위치하는지 예측한다. 관절의 위치로 사람의 포즈를 정의한다. 정의한 관절의 위치를 GT와 비교하여 regression Loss를 구한다. 

<br>

<br>

# Object Detection

object detection은 엄청 많은 곳에 적용해볼 수 있는 방법이다. visual tracking, pose estimation, 등등 object detection 모델에 segmentation이나 tracking 모델을 더해서 사용할 수 있기 때문이다. 실제로 이런 방식을 사용하면 더 성능이 좋게 나온다.

<br>

![image](/assets/img/2021-10-06/0057.jpg)

object detection을 할 때도 초기 모델에서는 sliding window를 많이 사용했다. 입력 이미지를 매우 작은 픽셀로 나누고 그 위를 window가 모든 픽셀을 읽어들이도록 한다.

![gif](https://miro.medium.com/max/478/1*wbuckxPMCs4BEemAuqd94g.gif)

silding window의 문제는 어떻게 영역을 추출할지와 이미지에 object가 몇 개 존재하는지, 어디에 존재하는지 파악하지 못한다. object의 크기도 알수가 없다. 또, 작은 영역 하나마다 거대한 CNN을 통과시키면 계산량이 엄청나다.

<br>

![image](/assets/img/2021-10-06/0062.jpg)

그래서 **Region Proposals**를 개발했다. 이 방법은 현재 딥러닝에 사용되지 않지만 전통적인 방식이다.

object가 있을법한 2000개의 bounding box 후보군을 찾아내는데, 찾는 방법으로 selective search가 있다. selective search를 돌려 객체가 있을만한 2000개의 region proposal을 만들어내는 것이다. 찾아낸 region proposal을 CNN의 입력으로 한다. 

이 방법을 사용하면 silding window방식보다 계산량이 줄어든다. 하지만, 이 방법은 노이즈가 심하다. 대부분은 실제 객체가 아닐지라도 recall이 매우 높다. 

region proposal을 region of interest(ROI)라고도 한다. 실제로 ROI라는 말을 더 많이 사용한다. 

여기서 ROI의 사이즈가 각양각색이다. 추출된 ROI로 CNN Classification을 수행하기 위해서는 FC-layer의 특성에 의해 사이즈가 다 동일해야 한다. 따라서 고정된 사이즈로 크기를 맞춰준다. 이를 **warp**시킨다고 한다.

<br>

## R-CNN

![image](/assets/img/2021-10-06/0068.jpg)

R-CNN의 경우 ROI들의 최종 classification에 SVM classifier을 사용하여 score를 분류한다. 

R-CNN은 Bounding Box를 보정해주기 위한 offset 값 4개도 예측한다.

고정된 알고리즘인 selective search를 사용하기 때문에 region proposal은 학습되지 않는다.

<br>

![image](/assets/img/2021-10-06/0069.jpg)

R-CNN의 문제점으로는 계산비용이 크고 느리다. 2000개의 ROI를 독립적으로 CNN에 넣기 때문이다. 그리고 CNN에서 나온 feature 들을 디스크에 저장하므로 용량이 엄청나다. training time/test time 둘다 매우 느리다.

<br>

## Fast R-CNN

![image](/assets/img/2021-10-06/0077.jpg)

위의 문제들을 해결하고자 Fast R-CNN을 개발했다. 

* 전체 알고리즘
1. 입력 이미지를 미리 학습된 CNN을 통과시켜 feature map을 추출한다.
2. selective search를 통해서 찾은 각각의 ROI에 대해 ROI Pooling을 진행하여 고정된 크기의 feature vector를 얻는다.
3. feature vector는 fully connected layer들을 통과한 뒤, 두 개의 브랜치로 나뉘게 된다.
4.  
    1) 하나의 브랜치는 softmax를 통과하여 해당 ROI가 어떤 물체인지 classification한다. SVM은 사용하지 않는다.

    2) 나머지 브랜치는 bounding box regression을 통해서 selective search로 찾은 박스의 위치를 조정한다.


R-CNN과 다른 점은 각 ROI마다 각각을 CNN에 넣지 않고, 입력 이미지에 CNN을 수행하여 고해상도 feature map을 얻고, 거기에 ROI를 적용시킨다. 이에 따라 여러 ROI가 서로 feature을 공유할 수 있게 되었다. 

<br>

feature map에서 가져온 ROI는 FC layer의 입력에 알맞게 크기를 조정해야 한다. 이를 위해 **ROI pooling layer**을 적용한다. 

![image](/assets/img/2021-10-06/0078.jpg)

feature map에서 가져온 ROI의 크기를 조정(wraped)한 후, FC layer의 입력으로 넣어 classification score과 regression box를 얻을 수 있다.

<br>

<br>

![image](/assets/img/2021-10-06/0079.jpg)

R-CNN과의 차이는 CNN을 한 번만 통과시키기 때문에 계산이 줄어들고, SVM을 사용하지 않는다는 것이다.

CNN을 한번만 통과시킨 뒤 그 feature map을 공유하는 것은 SPPNet에서 고안한 방법이다.

fast R-CNN에서는 feature들을 서로 공유하기 때문에 계산이 엄청 빠르다. region proposal을 계산하는 시간이 대부분이다. faster R-CNN에서는 계산 시간을 줄이고자 했다.

<br>

## Faster R-CNN

![image](/assets/img/2021-10-06/0081.jpg)


지금까지 사용되왔던 selective search대신 RPN(region proposal network)을 사용한다. 

입력 이미지 전체를 CNN에 넣어 feature map을 얻는다. 여기서 RPN을 적용시켜 region proposal을 예측한다. 

>* RPN 알고리즘
>1. feature map을 input으로 받는다.
>2. feature map에 3x3 convolution을 256 또는 512 채널(depth)만큼 수행한다.
>3. convolution된 map을 입력으로 받아 classification과 bounding box regression 예측 값을 계산한다.
>4. 앞서 얻은 값들로 ROI를 구한다.

학습시에 RPN도 함께 학습시키는데, ground truth object(실제 객체)와 일정 threshold(한계 = 정도)이상 겹치는 proposal(제안)을 positive라 하고, 그 이하는 negative라고 한다.

Faster R-CNN에서는 최종 classification loss/bounding box regression loss도 구하고, proposal에 대한 classification loss와 bounding box regression loss까지 총 4개를 구하고 이들에 대한 loss를 합쳐 최종 loss를 계산한다.

<br>

<br>

<br>

위의 R-CNN 시리즈는 2-stage model들이다. 즉, classification과 bounding box regression을 각각 수행한다. 하지만 YOLO/SSD의 경우 1-stage model로 이 둘을 함께 수행한다. 따라서 2stage는 정확도는 좋지만, 속도가 느리고, 1stage의 경우는 속도는 빠르나 정확도가 다소 떨어진다.

<br>

## You Only Look Once / Single Shot Detection

![image](/assets/img/2021-10-06/0084.jpg)

YOLO(you only look once) 모델은 요즘 매우 핫한 모델로 2021년도에 개발된 yolov5 까지 있다. 이 두 모델은 거대한 CNN을 통과하면 모든 것을 담은 예측값이 한번에 나온다.

<img src="_site/assets/img/2021-10-04/yolo.png">

region proposal 단계를 제거하고, bbox regression과 classification을 한 번에 수행하는 구조다.

먼저 입력 이미지를 S X S 그리드 영역으로 나눈다. 각 그리드 영역에서 물체가 있을만한 영역에 해당하는 B개의 bounding box를 예측한다. 그 다음 해당 박스의 신뢰도를 나타내는 confidence를 계산한다. 또한, bounding box 안에 객체가 존재할 가능성을 의미하는 classification score도 계산한다. 

출력은 S x S x (5*B + C) [B: bounding box의 offset, C: 클래스 개수]의 3-dimension를 가지는 tensor이다. 이 값을 CNN으로 학습시킨다.

<br>

<br>

![image](/assets/img/2021-10-06/0085.jpg)

base network(backbone network)에는 VGG, ResNet, MobileNet 등이 있다. 아키텍처에는 2stage인 R-CNN 시리즈와 1stage인 YOLO 등이 있다. 2stage와 1stage의 중간인 R-FCN도 있다.

<br>

<br>

* Dense Captioning

![image](/assets/img/2021-10-06/0086.jpg)

object detection + captioning 을 dense captioning으로 명명했다. 각 region에 대해 카테고리 대신 문장(caption)을 예측하는 것이다.

데이터셋으로는 각region에 caption이 있는 데이터셋이 필요하다. 

이 모델을 end-to-end로 학습시켜 모든 것을 동시에 예측할 수 있도록 만들었다.

![image](/assets/img/2021-10-06/0087.jpg)

네트워크에는 Region proposal을 사용했고, caption을 예측해야 하기에 softmax loss 대신 RNN Language model을 도입했다.

<br>

<br>

# Instance Segmentation

![image](/assets/img/2021-10-06/0089.jpg)

semantic segmentation과 object detection을 섞은 것이다. 이미지 내의 두 마리 개가 있을 때 2마리를 구분해야 한다. 또한 BBox 가 아닌 각 객체애 해당하는 segmentation mask를 예측해야 한다.

<br>

Mask R-CNN 아키텍처가 instance segmentation에 속한다.

![image](/assets/img/2021-10-06/0090.jpg)

입력 이미지는 CNN과 RPN을 거친다. 그 후 classification과 regression이 아닌 bbox마다의 segmentation mask를 예측한다. RPN으로 뽑은 ROI 영역 내에서 각각 semantic segmentation을 수행한다. feature map으로부터 ROI Pooling을 수행하면 두 갈래로 나뉜다. 첫번째는 각 region propasal이 어떤 카테고리에 속하는지 계산하고, region proposal의 좌표를 보정해주는 bbox regression도 예측한다. 다른 갈래는 각 픽셀마다 객체인지 아닌지 분류한다.

<br>

![image](/assets/img/2021-10-06/0092.jpg)

Mask R-CNN에 pose estimation도 융합할 수 있다. 관절의 좌표를 예측하는 부분을 추가하면 된다. region proposal에 한 갈래를 추가해 region proposal 안의 객체의 관절 좌표를 예측하면 된다.

<br>

<br>

* Mask R-CNN 코드
[https://colab.research.google.com/drive/1dWg0nx7KEYGSH05heY2_z5hosHBK3EbP#scrollTo=iTL-OFlWNSWq](https://colab.research.google.com/drive/1dWg0nx7KEYGSH05heY2_z5hosHBK3EbP#scrollTo=iTL-OFlWNSWq)

<br>

<br>

<br>

# Reference
* [http://cs231n.stanford.edu/2017/syllabus.html](http://cs231n.stanford.edu/2017/syllabus.html)
* [https://lsjsj92.tistory.com/416](https://lsjsj92.tistory.com/416)

