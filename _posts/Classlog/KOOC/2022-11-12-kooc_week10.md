---
title:    "[KOOC] 인공지능 및 기계학습 개론 10주차 - Sampling Based Inference "
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-11-09 21:10:00 +0800
categories: [Classlog, kooc]
tags: [kooc]
toc: true
comments: true
math: true
---

# Chapter 10. Sampling Based Inference

- 목차

1. basic sampling method
  - Markov chain Monte Carlo
  - apply MCMC to the parameter inference of Bayesian Networks
  - mechanism of rejection sampling
  - mechanism of importance sampling
2. sampling based inference
  - concept of Metropolis-Hastings algorithm
  - mechanism of Gibbs sampling
3. case study of sampling based inference
  - Latent Dirichlet Allocation model
  - collapsed Gibbs Sampling
  - derive Gibbs sampling formula for LDA

&nbsp;

&nbsp;

## 10-1. Basic Sampling methods

&nbsp;

### Forward Sampling

<img src="/assets/img/kooc/week910/forward_sampling.png">

간단한 Baysian network를 살펴보기 위해 이전에 봤던, Alarm/Call 문제를 살펴보자. 이 때, $ P(E = T\|MC = T) $ 의 확률을 구하기 위해 E=T와 MC=T의 경우를 카운트하고, MC=T인 경우를 카운트한다. 반복을 엄청 많이 하여 오차를 줄인다.

Gaussian Mixture Model에 대해 forward sampling, 즉 직접 테스트를 통해 카운트하여 확률을 계산하는 방법을 사용하여 확률 그래프를 그려볼 수 있다. GMM에서는 latent factor인 z가 존재하고, 이 z는 π에 의해 mixture distribution을 따르는 지표로서 샘플링된다.

따라서 샘플 x는 $ N(\mu_z,\sum_z) $ 의 정규분포를 따른다. (=$ P(x\|z) = N(x\|\mu_z, \sum_k) $)

&nbsp;

&nbsp;

### Rejection Sampling

<img src="/assets/img/kooc/week910/rejection_sampling.png">

Rejection sampling이란 많은 반복을 통해 P(E=T\|MC=T,A=F) 를 구할 때, 이에 해당하지 않는 샘플들 즉, MC=T이지만, A=F인 경우는 버리고(reject), 다시 반복 샘플링하여 MC=T,A=F인 경우는 카운트한다.

그 후, 최종적인 확률은 카운트한 값에서 전체 샘플의 개수를 나눠주면 된다.

&nbsp;

<img src="/assets/img/kooc/week910/rejection_sampling_numericalview.png">

rejection sampling을 수치적으로 살펴보자. 먼저 x_i 를 샘플링한다. q(x)는 $ N(\mu_z,\sum_z) $의 정규분포에서 x_i가 샘플링될 확률을 나타낸다. 즉, target distribution이라 할 수 있다.

그 다음, 이 q(x)를 둘러쌀 수 있는 p(x)를 생성하고, p(x)와 q(x)에 상수 m을 곱한 것을 비교하여 상수 u보다 크다면 카운트하고, 그렇지 않다면 카운트하지 않고, 다시 샘플링한다.

&nbsp;

<img src="/assets/img/kooc/week910/rejection_sampling_gmm.png">

Q Mixture 부분에서 1/3, 3 부분이 `M`에 해당하고, 그래프에서 빨간색 선 아래 부분이 rejection region을 나타낸다.

첫번째 그래프의 경우는 p(x) 또한 Mixture distribution으로 모델링하여 target distribution과 거의 유사하게 그려진다. 그러나 두번째 Q mixture 그래프의 경우 단순한 p(x)를 normal distribution으로 생성했기 때문에 target distribution과 다소 다른 모습을 볼 수 있다.

&nbsp;

&nbsp;

### Importance Sampling

<img src="/assets/img/kooc/week910/importance_sampling.png">

random variable에 의해 생성될 함수에 대한 기대값을 게산해보자. 

$ E(f) = \int f(z)p(z) dz = \int f(z)\frac{p(z)}{q(z)}q(z) dz = \frac{1}{L}\sum_{l=1}^L \frac{p(z^l)}{q(z^l)} f(z^l) $

f함수의 기대값 E(f)는 z가 샘플링될 확률 분포 p(z)와 true function인 f(z)를 z에 대해 적분한 것과 같다. 기대값을 정확하게 계산하기 위해서는 true function을 알고 있어야 하므로, f(z)를 알고 있다는 가정이 필요하다.

여기에서 q(z) 라는 계산의 편의를 위해 생성한 정규분포를 따르는 q(z) 를 분모, 분자에 각각 곱해준다. 샘플 z에 대해 무한대가 아닌 1부터 샘플의 개수인 L까지로 범위를 정해서 적분을 하게 되면 

$ \sum_{l=1}^L \frac{p(z^l)}{q(z^l)} f(z^l) q(z^l) $

인데, $ \sum_{l=1}^L q(z^l) $ 을 상수 1/L로 근사시키면, 정규화 상수가 된다.

적분이 시그마가 되는 이유는 간단하다. 예를 들어, f(x)=a 이라는 함수에 대해 1부터 L까지 적분을 하면, `L x f(x)` 가 된다. 이는 풀어쓰면 f(x=1) + f(x=2) + ... + f(x=L) 이다.

&nbsp;

이 때, p(z)/q(z)를 빼고 생각을 해보면, 모든 z값들에 대한 f(z)의 평균을 구하는 식이 되므로, p(z)/q(z)는 중요도 가중치 역할을 수행하게 된다.

&nbsp;

만약 p와 q가 normalize하지 않다면, 우리가 직접 정규 분포로 만들어주면 된다. 그 방법으로 p(z)에서 z에 대해 1/z_p 를 하고, q에 대해서도 마찬가지로 1/z_q 해주면 된다.

&nbsp;

만약, Z가 1보다 클 때의 확률을 구하고자 한다면, f(z)를 1보다 클 때는 1, 1보다 작을 때를 0으로 하는 indentity function으로 만들어주면 된다.

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week910/likelihood_weight_algorithm.png">

likelihood weight를 적용한 알고리즘의 동작방식은 다음과 같다. 전체 샘플은 오른쪽의 Baysian network를 따르고 있다고 가정한다.

- 목적 : P(E=T\|MC=T,A=F) 를 구하고자 함.
- 초기 세팅
  - SumSW = NormSW = 0
- 반복 과정
  1. SW = SampleWeight = 1
  2. sample 생성
    1. if, sample == (A=F\|B=F,E=F), JC\|A=T, MC=T\|A=F :
      - P(A=F\|B=F,E=F) = 0.999
      - SW = 1 * 0.999
      - P(MC=T\|A=F) = 0.01
      - SW = 1 * 0.999 * 0.01
    2. if, sample in E=T : 
      - SumSW += SW
  3. NormSW += SW
- Return SumSW/NormSW

&nbsp;

&nbsp;

## 10-2. Sampling Based Inference

Sampling method에서 가장 많이 사용되는 기법이 **Gibbs sampling**이다. 이 Gibbs sampling은 Metropolis-hastings 알고리즘의 특별한 케이스이다.

이전에 배웠던 EM 알고리즘에서 Expectation step에서는 optimization을 통해 Z를 할당했다. 이제는 sampling 기반으로 Z를 할당해도 잘 수행되는지 살펴보고자 한다.

&nbsp;

### Markov Chain

<img src="/assets/img/kooc/week910/markov_chain.png">

그리고, 이전에 배웠던 Forward, rejection, importance sampling에서는 현재의 z와 다음 시간에서의 z는 연관이 없었다. metropolis hastings 알고리즘에서는 버련던 instance(sample)들을 활용하여 다음 z를 샘플링하고자 한다. 이 알고리즘을 위해서는 `markov chain` 개념을 알아야 한다.

&nbsp;

markov chain에서 각 노드는 state의 확률 분포를 가진다. z_t가 어느 정도의 확률로 할당이 될 것인지에 대한 확률이다.

한 시스템이 3개의 state를 가진다고 가정할 때, 특정 시간 t에서의 확률 P(z_t)는 [a b c] (a,b,c는 확률) 을 가진다.

이 a,b,c는 정확한 관찰이라 생각하면 True=1 또는 False=0으로 할당될텐데, 확률론적으로 생각한다면 각각 0~1의 값을 가진다.

예를 들어, 앞서 살펴봤던 알람과 콜에 대한 baysian network에서 발생할 수 있는 P(z)는 \[P(A=T,JC=T) P(A=T,JC=F) P(A=F,JC=T) P(A=F, JC=F)\] 라는 4개의 state를 가진다. 각 state는 확률로서 표현된다.

&nbsp;

현재 시간 t에서의의 P(z_t)와 다음 시간 t+1에서의 P(z_t+1) 을 연결하는 matrix인 transition matrix, $ T_{i,j} $ 가 존재하여, P(z_t) 와 transition matrix를 가지고 있다면, 이후 시간에서의 P를 계산할 수 있다.

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week910/markov_chain_properties.png">

Markov chain의 특성은 접근 가능성(Accessible), 축소/환원성(Reducibility), 주기성(Periodicity), 일시성(Transience), Ergodicity이 있다.

- Accessible
    - i ⇢ j : 샘플링할 때, i state에서 j state로 전의(이동)가 될 확률이 0보다 크다.
    - i ⇹ j : i에서 j로도, j에서 i로도 이동이 가능하다면 i와 j는 양방향(communicate)성을 가진다.

&nbsp;

- Reducibility
    - 만약, i와 j가 양방향이고, i와 j가 모두 전체 state인 S에 포함된다면, markov chain은 더이상 줄일 수 없는 상태, 즉 irreducible하다.

&nbsp;

- Periodicity
    - 특정 state i에 방문하는 주기(period)가 4,8,12... 라고 한다면, period d는 전체 period의 최대공약수인 4로 간주한다. 
    - 만약 주기가 1,3,4,11,13 이라고 한다면 d=1이 되므로, 이 경우를 aperiodic, 즉 주기가 없다고 정의한다.

&nbsp;

- Transience
    - X_0 일 때, j state가 할당되었는데, 이후 특정시간에서 다시 j state를 방문한다면 이를 재방문(recurrent)이라 정의한다.
    - 만약 이후 다시는 방문하지 않는 경우를 반대로 *transient*라 한다.

&nbsp;

- Ergodicity
    - 한 state가 재방문을 하나, 그게 언제인지는 모를 때, 즉 aeriodic할 때, *ergodic*이라 한다.
    - 모든 state가 ergodic하다면 markov chain은 ergodic하다.

&nbsp;

&nbsp;

### Stationary Distribution

<img src="/assets/img/kooc/week910/stationary_distribution.png">

markov chain의 특수 케이스인 **Stationary Distribution**이 metropolis-hastings를 동작시키는 핵심 개념이라 할 수 있다.

$ RT_i $ 는 state i에 방문한 이후 다시 state i에 방문하는 시간 t1,t2,t3 중 가장 작은 값을 의미한다.

&nbsp;

만약 markov chain이 irreducible 하고 ergodic하다면 즉, 특정 state에 대해 재방문을 하고 state i와 j가 양방향인 상황이라 할 수 있다. 이런 상황에서 **stationary distribution**, π_i를 정의한다.

$ \pi_i = lim_{n⇢\inf} T_{i,j}^{(n)} = \frac{1}{E[RT_i]}$

이 때, π_i에 대한 조건은 다음과 같다.

$ \pi_i \geq 0,\: \sum_{i \in S} π_i = 1, \pi_j = \sum_{i \ in S} \pi_iT_{i,j} $

이 때, S는 state의 개수를 의미하고, 특정 시간 t에서의 π_t 는 latent variable인 z_t와 동일한 역할을 하고, S가 3일 때 π_t = \[a, b, c] 의 형태를 가진다.

&nbsp;

만약 T를 가지고 있는 상황에서 π를 계산할 수 있다. π와 T를 matrix로 생각을 해보면 다음과 같이 구성할 수 있다.

$ \pi(I_{\|S\|,\|S\|} - T + 1_{\|S\|,\|S\|}) = 1_{1, \|S\|} $

이 때, I는 identity matrix, 1은 1로 구성되어 있는 matrix를 의미한다. 이는 위의 조건 두,세번째를 활용하여 구성한 것이다.

&nbsp;

**Reversible Markov chain** 또는 **balance Equation** 이라는 개념이 있다. 오른쪽 위의 코드와 같이 transition matrix(T)가 있을 때 stationary distribution의 특성을 활용하여 π(pi)를 쉽게 구할 수 있다. stationary distribution의 특성 상 πT = π 이므로, 성립되는 것을 볼 수 있다.

state i와 j가 있을 때, i에서 j로 transition 하는 확률과 j에서 i로 transition 하는 확률이 같으면 reversible하다 할 수 있고, 같지 않으면 irreversible하다고 할 수 있다.

$ \pi_i T_{i,j} = \pi_j T_{j,i} $

&nbsp;

&nbsp;

이 이론을 활용하여 MCMC(Markov Chain Monte Carlo)를 정의한다. 즉, 원래의 Markov Chain은 stationary distribution, π를 찾는 것이었으나, MCMC에서는 π가 주어진 상태에서 transition matrix를 찾는다.

state i에서의 z_i에서 z_j로 이동하기 위한 transition matrix를 잘 정의하여 stationary distribution을 잘 생성할 수 있게 샘플링해야 한다. 이 때의 z는 state마다 이어져 있는 형태이다.

$ z^{(1)} -> z^{(2)} -> \dots -> z^{(m)} -> \dots -> z^{(m+n)} $

&nbsp;

<img src="/assets/img/kooc/week910/markov_chain_z.png">

예를 들어, 이전의 baysian network Alarm과 JC,MC에 대해 살펴보자. Alarm과 MC가 Evidence, 관측된 값이라 하고, 나머지 3개를 latent variable, Z라 하자. 이 Z를 모두 반복적으로 샘플링하여 True/False를 정한다.

&nbsp;

&nbsp;

### Metropolis-Hastings Algorithm

<img src="/assets/img/kooc/week910/metropolis-hastings.png">

stationary distribution을 알고 있는 상황에서 어떻게 transition matrix를 잘 만들어 낼 수 있는지에 대한 알고리즘이 Metropolis-Hastings 알고리즘이다.

MCMC(Markov Chain Mento Carlo)의 일반적인 알고리즘은 현재의 latent variable, z^t가 있고, 현재의 z^t에서 z\*로 이동하고자 하며 이 때 z\*에 대한 후보로서 q(z\*\|z^t)를 생성하고, 이 q_t를 **proposal distribution** 이라 한다. 

또한, acceptance probability, α를 정의한다. α에 따라 샘플링한 z*을 받아들일지 버릴지를 결정한다. 받아들인다면 다음 z^t+1은 z\*이 되고 버린다면 다시 z^t가 된다.

&nbsp;

여기서 중요한 것은, 우리가 얻고자 하는 것은 stationary distribution이므로, 우리가 정의한 `q`를 잘 설정해주어야 한다. 따라서 Metropolis-Hastings 알고리즘에서는 **ratio**, r(z*\|z^t)를 정의하여 `q`를 잘 설정하고자 했다. 

$ r(z*\|z^t) = \cfrac{q(z^t\|z*)P(z*)}{q(z*\|z^t)P(z^t)} $

이 때, stationary distribution의 reversible markov chain의 특성을 통해 ratio는 1이 되어야 한다. 만약 ratio가 1보다 작으면 분모가 큰 것이고 이를 잘 생각해보면 z* -> z^t로의 확률은 낮고, z^t -> z*로의 확률이 크다는 것을 알 수 있다. 따라서 ratio를 1로 만들어주기 위해 acceptance probability를 잘 조정해야 한다. 지금은 z^t -> z\*에 대한 샘플을 줄이거나 반대를 키워야 하므로 *->t는 1로, t->\*는 ratio로 지정한다.

반대로 1보다 크다면 t->\*은 1로, *->t는 ratio로 설정한다.

&nbsp;

결론적으로 acceptance probability는 다음과 같이 정의된다.

$ \alpha(z*\|z^t) = min{1, r(z*\|z^t)} $

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week910/random_walk_MH.png">

t에서 \*로의 Transition probability는 $ T_{t,*}^{MH} = q(z*\|z^t) \alpha(z*\|z^t) $ 를 만족한다.

우리는 이미 P(z)를 알고 있다고 가정했기에 q(z)에 대해서만 정의해주면 된다. q(z)를 샘플링하는 방법으로 **Random walk M-H algorithm** 이 있다. 그에 대한 과정은 다음과 같다.

z*을 normal distribution, N(z^t, σ^2)에 대해 랜덤 위치로 샘플링한다. 그러면 q(z\*\|z^t)는 normal distribution식을 따르게 되고, z\*을 받아들일지 말지를 결정한 후, 받아들이면 z^t+1 의 위치는 z\*가 되고, 그 후 다시 z^t=z\*에 대해 무작위로 z\*을 샘플링한다.

&nbsp;

<img src="/assets/img/kooc/week910/result_random_walk_mh.png">

z*의 normal distribution을 생각해봤을 때, σ가 클수록 큰 폭으로 이동되고, σ가 작을수록 이동하는 폭이 작아진다는 것을 알 수 있다. 그 결과가 위의 그래프이다. Gaussian Mixture model에 대해 샘플링하고 있으며, 맨 위가 true distribution이고, 중간이 latent, z^t를 나타내고 있다. 그래프에서 볼 수 있듯이 σ가 작으면 샘플링되는 폭도 작아서 변화하는 폭이 작다.

&nbsp;

&nbsp;

### Gibbs Sampling

<img src="/assets/img/kooc/week910/gibbs_sampling.png">

Gibbs sampling이란 Metropolis Hastings의 특별 케이스라 할 수 있다. z^t는 state의 개수만큼 vector형태로 존재하는데, 이에 대해 각 샘플링마다 1개의 state만을 업데이트하고자 했다. 업데이트하는 것을 $ z_k^t $라 했을 때, 나머지 업데이트되지 않는 것은 $ z_{-k}^t $ 로 표현한다. 그렇다면 z\*으로 업데이트를 하고 난 후에는 z*과 변화하지 않은 z_{-k}^t가 있을 것이다.

이런 과정을 MH 알고리즘에 적용하면

$ q(z*\|z^t) = P(z_k*,z_{-k}^t\|z_{-k}^t) = P(z_k*\|z_{-k}^t) $

reversible markov chain을 만족시켜야 하므로 $ P(z^t)q(z*\|z^t) = P(z*)q(z^t\|z*) $ 가 되어야 한다. 따라서

$ P(z^t)q(z*\|z^t) = P(z_k^t,z_{-k}^t)P(z_k*\|z_{-k}^t) = P(z_k^t\|z_{-k}^t)P(z_{-k}^t)P(z_k*\|z_{-k}^t) $

joint probability를 condition probability형태로 변환한 후 이를 다시 joint probability로 변환한다. P(z_k*, z_{-k}^t) 는 어차피 z^t만 업데이트했으므로 P(z*)와 같으므로 변환하고, *에서 t로 오는 것은 P(z_k^t\|z_{-k}^t) 와 같으므로 변환하면 원래의 balance equation을 만족하게 된다.

따라서 항상 balance equation을 만족하므로, acceptance probability를 사용하지 않아도 된다. gibbs sampling에서는 acceptance probability를 사용하지 않으므로 받아들일지 말지를 판단하지 않고 무조건 수용한다.

&nbsp;

Alarm/Call에 대한 baysian network에 Gibbs sampling을 적용해보자. Alarm, Marycall이 관찰되 상황에서 buglary, johncall, earthquake를 latent variable이라 생각했을 때 먼저 Buglary만을 업데이트하고자 한다면 Markov blanket을 통해 alarm과 earthquake만 알고 있다면 나머지는 연관이 없다.

또한, johncall을 업데이트할 때, Alarm만 알고 있다면 나머지의 variable에 대해서는 연관성이 없다. Alarm은 이미 알고 있는 값이므로 JC만 true/false를 선택하여 적용하면 업데이트할 수 있다.

&nbsp;

&nbsp;


<img src="/assets/img/kooc/week910/concept_gibbs_sampling.png">

gibbs sampling의 개념을 좀 더 자세히 살펴보자. 1번의 step마다 1개의 variable만을 변경시키는 기법으로서 최종적인 과정과 그림은 위 사진의 하단과 같다.

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week910/gibbs_sampling_algorithm.png">

Gibbs sampling의 과정을 자세히 살펴보자.

1. i=1에서의 z_i를 초기화한다.
2. step을 T번 반복한다.
3. 각 step마다 1개의 state만 변경시킨다.

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week910/gibbs_sampling_GMM.png">

이러한 gibbs sampling의 과정을 그래프화하면 위와 같다. Gaussian Mixture Model에 gibbs sampling을 적용하게 되면, EM(expectation, maximiation step)보다 수렴이 다소 느리다. 그러나 EM은 local optima에 빠지기 쉽지만, Gibbs sampling은 local에 빠질 확률이 EM보다 적다.

&nbsp;

&nbsp;

## 10-3. Latent Dirichlet Allocation

### Topic Modeling

<img src="/assets/img/kooc/week910/topic_modeling.png">

자연어처리에서 가장 많이 사용되는 기법이다. 여러 개의 단어들을 주고, 이에 대한 latent variable을 추론하여 각 단어의 의미를 분석한다.

&nbsp;

&nbsp;

### Latent Dirichlet Allocation

<img src="/assets/img/kooc/week910/lda.png">

Latent Dirichlet Allocation은 텍스트 데이터에 대해 soft clustering하고 baysian model을 따른다.

오른쪽 baysian network에서 α는 prior에 해당한다. 이 α는 dirichlet distribution을 따르는 형태를 지니고 있다. 이 distribution에서 문저에 대한 assignment인 θ가 존재한다. z는 latent variable을 의미하여 단어별 topic assignment, 즉 어떤 의미를 지니는지에 대한 숨은 의미에 대한 할당이다. β도 prior 정보를 의미하고, φ는 topic별 등장할 단어의 확률로 gaussian mixture model에서 개별 cluster에 대한 gaussian distribution으로 모델링하는 부분이다. K는 topic의 개수를 의미한다. M은 문서의 개수, N는 한 문서에서의 단어의 개수이다. w는 evidence 즉 문서에서의 실제 단어를 의미한다.

&nbsp;

<img src="/assets/img/kooc/week910/find_topic_assignment.png">

특정 문장을 생성하는 프로세스는 다음과 같다.

- *Dir* : dirichlet distribution
- *Multi* : multinomial distribution
- α : dirichlet distribution prior, 전체 corpus의 topic distribution에 대한 값이다.
- θ : 문서 레벨의 topic assignment, 특정 문서에 대한 topic assignment들의 비율을 나타낸다.
- β : dirichlet distribution prior, 개별 단어들마다 어떤 토픽에 어떤 단어가 얼마나 사용될지에 대한 확률의 사전 지식
- φ : 어떤 토픽이 어느 정도의 확률로 등장할지에 대한 값
- z : 특정 cluster에 대해 선택되는 latent variable, topic별 assignment를 나타낸다.
- w : 어떤 주제에서 어떤 단어를 선택할지에 대한 pi를 인자로 받아 하나의 단어를 선정한다. 

&nbsp;

만약 Z를 안다면, α prior을 활용하여 θ를 구할 수 있고, β prior,z,w들을 통해 pi도 구할 수 있다.

따라서 Z를 구하는 것이 topic modeling에 가장 중요한 부분을 차지한다.

&nbsp;

<img src="/assets/img/kooc/week910/gibbs_sampling_on_z1.png">

확률을 구하기 위해 가장 먼저해야 할 부분은 *factorization*이다. 모든 변수와 prior를 표현하여 확률을 나타낸다. 이 때, `;`는 뒤의 내용이 사전정보라는 것을 나타낸다. 앞서 정의했던 texture modeling에 대한 baysian network을 활용하여 factorization한다.

$ P(W,Z,\theta, \phi; \alpha, \beta) = \prod_{i=1}^K P(\phi_i;\beta) \prod_{j=1}^M P(\theta_j;\alpha) \prod_{l=1}^N P(Z_{j,i}\|\theta_j) P(W_{j,l}\|\phi_{Z_{j,l}}$




&nbsp;

&nbsp;

<img src="/assets/img/kooc/week910/gibbs_sampling_on_z2.png">

<img src="/assets/img/kooc/week910/gibbs_sampling_on_z3.png">

<img src="/assets/img/kooc/week910/collapse_from_conjugacy.png">

<img src="/assets/img/kooc/week910/gibbs_sampling_formula1.png">

<img src="/assets/img/kooc/week910/gibbs_sampling_formula2.png">

<img src="/assets/img/kooc/week910/gibbs_sampling_formula3.png">

<img src="/assets/img/kooc/week910/gibbs_sampling_formula4.png">

<img src="/assets/img/kooc/week910/lda_model_params_infer_with_gibbs.png">