---
title:    "[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm "
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-10-15 18:10:00 +0800
categories: [Classlog, kooc]
tags: [kooc]
toc: true
comments: true
math: true
---

# Chapter 8. K-Means Clustering and Gausssian Mixture Model

- 목차

1. Clustering task and K-Means algorithm
  - unsuperveised learning
  - K-Means iterative process
  - limitation of K-Means algorithm
2. Gaussian mixture model
  - multinomial distribution and multivariate Gaussian distribution
  - why mixture model
  - parameter updates are derived from the Gaussian mixture model
3. EM algorithm
  - fundamentals of the EM algorithm
  - how to derive the EM updates of a model

&nbsp;

&nbsp;

## 8-1. K-Means algorithm

<img src="/assets/img/kooc/wee78/machine_learning.png">

1~6주차에서는 supervised learning에 대해 배웠다. 이제는 k-means라는 unsupervised learning을 배워보고자 한다.

unsupervised learning이란 label이 존재하지 않은 데이터셋을 활용하여 task를 수행하는 것이다. true value를 모르는 상황에서 패턴을 찾아야 한다.

&nbsp;

<img src="/assets/img/kooc/wee78/clustering.png">

만약, 우리가 이 데이터셋에 대해 색상 데이터를 가지고 있다면, supervised task가 될 것이다. 그러나 오른쪽과 같이 어떤 군집이 만들어지는지에 대해 모르고, 군집을 찾아야 한다면 이는 unsupervised learning이 될 것이다.

&nbsp;

<img src="/assets/img/kooc/wee78/k_means.png">

k-means algorithm이란, 잠재적으로 생각하기에 내부적으로 k개의 동력이 있어서 n개의 데이터 포인트들이 clustering될 것이라 생각하는 것이다. 즉 위의 데이터들에 대해 k=3개의 동력원이 존재하여 n개의 데이터들이 3개의 동력원들에 의해 군집화된 것이다.

&nbsp;

여기서 중요한 것은 K-Means와 K-Nearest Neightbor algorithm은 동작방식이 비슷하나, 다른 기법이다. K-Means는 군집을 찾기 위한 unsupervised task를 위한 기법이고, KNN은 supervised task를 위한 기법이다. KNN는 한 데이터의 주변에 가장 가까운 점 k개를 찾아 그에 대해 분류하고, 한 데이터를 주변 점들이 많이 분포되어 있는 값으로 지정해주는 것이다. 그러나 K-Means는 실제 보는 것은 색상이 없는 검은색 점들을 볼 것이다. 그리고 중심점들도 주어지지 않는다. 이러한 상황에서 중심점들을 찾고, 그에 관련된 데이터들을 분류해야 한다.

따라서 K-Means는 크게 두 단계로 구성되어 있다. 첫번째로는 중심점을 찾는 것이고, 그 후에 중심점들과의 거리를 통해 데이터들을 분류한다.

&nbsp;

clustering에 대한 식은 다음과 같다.

<img src="/assets/img/kooc/wee78/clustering_function.png">

- $ r_{nk} $ : assignment of data point to cluster
- $ \mu_k $ : location of centroids

$ r_{nk} $ 는 cluster에 대한 파라미터로서, 각 데이터 포인트는 1개의 centroid(중심점)에 대해 할당된다. 그러므로, 할당된 centroid에 대해서는 1로, 나머지 centroid에 대한 것은 0으로 할당하여 연산하고자 하는 값이다. $ \mu_k $ 는 중심점의 위치이다.

최종적으로 **J** 를 최소화하는 것이 최적화의 목적이다. 그러나 이 때는 파라미터가 2개이므로, 어떻게 최적화를 할 수 있을까? 이러한 경우는 반복적인 최적화를 통해 1번은 $ r_{nk} $를 최적화하고, 최적화된 $ r_{nk} $ 를 통해 $ \mu_k $를 최적화하고, 다시 최적화된 $ \mu_k $를 통해 $ r_{nk} $를 최적화하는 방식을 통해 J를 최소화한다.

&nbsp;

&nbsp;

<img src="/assets/img/kooc/wee78/EM.png">

- Expectation
    - 파라미터($ r_{nk} $)에 의해 구해지는 확률을 통해 centroid를 중심으로 한 데이터 포인트들을 할당해준다.
- Maximization
    - likelihood에 대한 파라미터($ \mu_k $)를 최대화시킴으로써 centroid 위치를 업데이트한다.

$ \mu_k $ 에 대해 optimization하는 과정은 다음과 같다. 최적화하는 방법은 동일하게 미분하여 =0인 값을 찾는 것이다.

<img src="/assets/img/kooc/wee78/mu_optimization.png">

결론적으로 구해보면, mu_k는 할당된 데이터 포인트들의 평균값이 된다.

&nbsp;

k-means algorithm의 과정은 다음과 같다.

1. 특정 반복 횟수만큼 반복한다.
    1. mu_k를 데이터들 중 random 하게 n개를 골라 centroid로 지정한다.
    2. r__nk를 가까운 centroid(mu_k)에  대해 assign한다.
    3. assign된 데이터들의 평균값을 구해 mu_k를 업데이트한다.
    4. 다시 r_nk를 업데이트한다.

이러한 과정을 반복하면, 점차 최적의 centroid를 찾을 수 있다.

<img src="/assets/img/kooc/wee78/example_kmean.png">

&nbsp;

<img src="/assets/img/kooc/wee78/limitation_kmeans.png">

이 간단한 k-means algorithm에는 한계가 존재한다.

1. centroid의 개수가 명확하지 않다.
2. centroid의 초기 위치가 잘못 지정되면 잘못된 optimization이 수행될 수도 있다.
3. limitation of distance metrics
    - euclidean distance만으로는 데이터의 특정을 정확하게 파악하기 힘들다.
4. hard clustering
    - r_nk가 0 또는 1이므로 다소 discrete 하다는 단점이 있다.

&nbsp;

&nbsp;

## 8-2. Gaussian Mixture Model

gaussian mixture model을 배우기에 앞서, Multinomial distribution에 대해 다시 한 번 짚고 넘어가자.

&nbsp;

<img src="/assets/img/kooc/wee78/multinomial_distribution.png">

먼저 binomial distribution이란, 0 또는 1에 대해서만 나타내어지는 distribution이다. binomial distributino에 있어서 X=(0,0,1,0,0,0) 에 대해 3개를 선택하는 것에 대한 식은 다음과 같다.

$$ \sum_k x_k = 1, P(X|\mu) = \prod_{k=1}^K \mu_k^{x_k} $$

이 때의 제약조건(Subject)는 

- $ \mu_k >= 0 $
- $ \sum_k \mu_k = 1 $

&nbsp;

이 0 또는 1의 값을, 0,1,2,3,... 에 대한 값으로 늘린 것이 **multinomial distribution** 이다.

&nbsp;

N개의 선택지($x_1$,...,$x_n$)를 가진 데이터셋 D가 있다고 할 때의 확률은 다음과 같다.

$$ P(X|\mu) = \prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{\sum_{n=1}^N x_{nk}} = \prod_{k=1}^K \mu_k^{m_k} $$

이 때, $ m_k = \sum_{n=1}^N x_{nk} $ 이다. 

MLP, 즉 μ가 최대가 되는 값을 구하기 위한 식은 다음과 같다.

- Maximize $ P(X\|\mu) = \prod_{k=1}^K \mu_k^{m_k} $
- 제약조건 $ \mu_k >= 0, \sum_k \mu_k = 1 $

&nbsp;

MLP task에서 제약조건이 존재할 때에는 **lagrange function**을 활용하여 MLP를 수행할 수 있다.

$ L(μ,m,𝜆) = \sum_{k=1}^K m_k ln\mu_k + \lambda(\sum_{k=1}^K \mu_k - 1) $

lagrange function을 μ_k 에 대해 미분하면

<img src="/assets/img/kooc/wee78/lagrange_function_deriative.png">

&nbsp;

이 때, 제약조건인 $ \sum_k \mu_k = 1 $를 활용하면

<img src="/assets/img/kooc/wee78/constraint_function.png">

이 때, $ \sum_k \sum_{n=1}^N x_{nk} = N $ 인 이유는 모든 선택지에 대한 summation인 k와 개별 선택지마다 모든 데이터 포인트가 선택되는지에 대한 summation인 N에 대해 x_nk를 summation하는 것이므로, N이 된다. 이 때 x는 각 instance k에 대해 n개의 선택지가 선택될 확률이다.

&nbsp;

결론적으로 MLE에 대한 μ_k는 m_k/N이 된다. 이는 특정 선택지가 선택된 개수 / 관측한 선택지 전체를 의미한다.

&nbsp;

&nbsp;


