---
title:    "[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network "
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-10-15 18:10:00 +0800
categories: [Classlog, kooc]
tags: [kooc]
toc: true
comments: true
math: true
---

# Chapter 7. Bayesian Network

- 목차

1. Probability
  - recover the probability concepts
  - recover the probability theorems
  - recover the concepts of the marginal and the conditional independencies
2. Bayesian networks
  - syntax and semantics of Bayesian Networks
  - how to factorize Bayesian networks
  - calculate a probability with given conditions
3. inference of Bayesian networks
  - calculate parameters of Bayesian networks
  - list the exact inference of Bayesian networks

&nbsp;

&nbsp;

## 7-1. theorems of Probability

bayesian network에 들어가기 전에, probability에 대한 복습을 하고자 한다. 

&nbsp;

- Conditional Probability

conditional probability란 특정 상황이 주어졌을 때의 확률을 구하는 것이다.

<img src="/assets/img/kooc/week78/conditional_probability.png">

예를 들어, A와 B에 대한 상황이 존재한다고 생각해보자. B가 주어졌을 때의 A의 확률을 구한다는 것은 P(F=True)인 영역 속에서 P(H=True)인 영역에 해당한다.

이는 P(A = true \| B = true) 와 같이 표현한다.

$$ P(A = true | B = true) = \cfrac{P(A,B)}{P(B)} $$

&nbsp;

- Joint Probability

비슷한 개념으로 Joint Probability라는 것이 있다. P(A = true, B = true) 와 같이 표현할 수 있고, 이는 A = true 와 B = true 인 확률을 나타낸다.

$$ P(A = true, B = true) = P(A|B)P(B) $$

<img src="/assets/img/kooc/week78/joint_probability.png">

&nbsp;

이 두 개념을 활용하여 개별 확률인 marginal probability를 구할 수 있다.

$$ P(a) = \sum_b P(a,b) = \sum_b P(a|b)P(b) $$

&nbsp;

만약, joint distribution인 P(a,b,c,d) 가 주어졌을 때, 우리는 P(b)에 대해 식을 세울 수 있다.

$$ P(b) = \sum_a \sum_c \sum_d P(a,b,c,d) $$

또는, joint distribution을 통해 conditional probability를 구할 수 있다.

$$ P(c|b) = \sum_a \sum_d P(a,c,d | b) = 1/P(b) \sum_a \sum_d P(a,c,d,b) $$

이 때, 1/P(b) 는 normalization constant(정규화 상수) 이다.

&nbsp;

이렇게만 보면 joint probablity가 매우 강력한 개념이라 생각이 되지만, joint probability를 구하기 위해서는 개별 확률들을 알아야 하므로, feature의 개수와 개별 경우의 수에 따라 기하급수적으로 파라미터가 증가한다.

만약 feature 개수가 4개이고, 개별 feature이 true/false로 구성된다면 총 16-1 개의 파라미터를 구해야 한다.

&nbsp;

joint probability의 또다른 특징으로는 **chain rule** 이다.

$$ P(a,b,c,...z) = P(a|b,c,...,z)P(b,c,...,z) = P(a|b,c,...,z)P(b|c,...,z)P(c|...,z)...P(z) $$

&nbsp;

&nbsp;

- Independence

variable A와 B가 독립적일 때는 다음과 같은 관계가 성립된다.

- P(A\|B) = P(A)
- P(A,B) = P(A)P(B)
- P(B|A) = P(B)

&nbsp;

따라서 P(C1,...,Cn) 의 joint distribution을 계산하는 것은 간단하다.

$$ P(C_1,...,C_n) = \prod_{i=1}^n P(C_i) $$

&nbsp;

- Conditional Independence & Marginal Independence
    - Marignal independence
        - P(A=true\|B=true) > P(A=true) 라면 이 경우는 marginally independent가 아니다.
        - 즉, A와 B는 P(A) = P(A\|B) 일때만 독립적이어야 한다.
    - Conditional Independence
        - P(A=true\|B=true, C=true) = P(A=true\|C=true) 이면, conditional independent하다.

&nbsp;

&nbsp;

## 7-2. Bayesian Network

이전에 Naive Bayes Classifier을 배웠고, 이에 대한 function을 정의했다.

$$ f_{NB}(x) = argmax_{Y=y}P(Y=y) \prod_{1 \leq i \leq d} P(X_i = x_i | Y = y) $$

<img src="/assets/img/kooc/week78/bayesian_network.png">

bayesian network는 graphical notation 한 특성을 가지고 있다. 즉

- random variable
- conditional independence
- obtain a compact representation of the full joint distribution

에 대한 정보가 들어있다.

&nbsp;

- Syntax(문법)

bayesian network는

- acyclic(사이클이 존재x), directed graph
- nodes
    - random variable
    - P(X_i\| Parents(X_i)) (e.g. P(X1\|Y))
- link
    - Direct influence from the parent to the child

네트워크의 구조는 conditional independence를 가정하고 있다. 

<img src="/assets/img/kooc/week78/conditional_independence.png">

즉, toothache(치통)과 stench(악취)는 충치(cavity)와는 관계가 있지만, weather과는 관계가 없다. 따라서 weather은 variable과는 독립적이고, toothache와 stench는 cavity와 conditionally independent하다.

&nbsp;

유명한 예시를 하나 들어보자.

<img src="/assets/img/kooc/week78/bayesian_network_example.png">

도둑(buglary)이 들거나, 지진(earthquake)이 발생했을 때, 알람(Alarm)이 울리는데, 알람이 울리면, 이웃인 John과 Mary가 전화를 하는 상황이다.

알람에 대한 확률은 P(A\|B,E) 로 나타낼 수 있고, John이 전화하는 것에 대한 확률은 P(J\|A), Mary가 전화할 확률은 P(M\|A) 이다.

이 network에서의 Variable은 Burglary, Earthquake, Alarm, JohnCalls, MaryCalls, 모두이다.

&nbsp;

정성적(Qualitative)인 요소로는 

- 인과관계에 대한 사전정보
- 데이터로부터의 학습
- 네트워크의 구조

&nbsp;

정량적(Quantitative)인 요소로는

- 조건부 확률 테이블(이미지에서의 우하단 표)

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week78/local_Structure.png">

위의 구조는 간단했지만, 이러한 network를 복잡하게 구성할 수도 있다. 이럴 때, 지역적인 variable들에 대한 관계를 정의할 수 있다. 

- Common parent
  - 동일한 parent variable을 가진 다른 variable에 대해서는 독립적이다. 즉, John이 전화할 확률과 Mary가 전화할 확률은 서로 독립적이다.
  - 𝐽 ⊥ 𝑀\|𝐴
    - P(J,M\|A) = P(J\|A)P(M\|A)

- Cascading
  - 연쇄작용으로서, 연결되어 있는 variable일 때, A가 주어졌다면, B와 M은 독립이다. mary가 전화할 확률은 A를 알고 있다면 B는 필요가 없다. 즉 direct influence에 대한 확률을 안다면 indirect influence variable과는 독립적이다.
  - B ⊥ M\|A
    - P(M\|B,A) = P(M\|A) 

- V-structure
  - B와 E는 공통의 child를 가지는 상황을 V-structure이라 하는데, A가 주어지지 않았다면, B와 M은 관계가 없으므로, 독립적이나, A가 주어진다면, B와 E는 관계가 생기는 것이므로 독립이 되지 않는다.
  - ~ (B ⊥ E\|A)
    - P(B,E,A) = P(B)P(E)P(A\|B,E)

&nbsp;

<img src="/assets/img/kooc/week78/bayes_ball_algorithm.png">

이러한 복잡한 관계를 쉽게 이해하기 위해 **Bayes Ball Algorithm** 이라는 개념을 도입한다. 만약 $ X_A ⊥ X_B \| X_C $ 를 확인하고자 할 때, 즉 X_C가 주어졌을 때, X_A와 X_B가 독립인지에 대해 판단하고자 할 때 사용할 수 있다.

공을 굴린다고 생각해서, 만약 방향이 -\> 일 때, 오른쪽 variable이 주어진다면(회색) 공이 굴러가지지 않고, variable이 주어지지 않는다면(무색) 굴러갈 수 있다. 반대로 방향이 \<- 이라면, 오른쪽 variable이 주어졌을 때는 굴러가지고, variable이 주어지지 않았을 때는 굴러갈 수 없다.

&nbsp;

위의 local structure들에 적용해보자.

- Common parent

두 개의 variable이 공통의 한 개 parent를 가지는데, parent가 주어진다면, 공이 굴러갈 수 없으므로 이 두 variable은 독립적이고, 주어지지 않는다면, 독립이 아니다.

&nbsp;

- Cascading

세 개의 variable이 연속적으로 존재하는데, 중간의 variable이 주어진다면, 나머지 두 개의 variable은 공이 굴러갈 수 없어서 독립이고, 주어지지 않는다면, 공이 굴러갈 수 있으므로 독립이 아니다.

&nbsp;

- V-structure

두 개의 variable이 공통의 한 개 child를 가지는데, child가 주어진다면, V-structure은 common parent와 반대로, 공이 굴러 갈 수 있어서 두 variable은 독립이 아니다. 그러나 주어지지 않는다면 공이 굴러갈 수 없어서 독립이다.

&nbsp;

간단한 예시를 들어보자.

<img src="/assets/img/kooc/week78/example_of_bayesball.png">

x1~x6의 variable이 존재하고, 위와 같은 관계로 형성되어 있다고 해보자.

1. x2가 주어졌을 때, x1과 x4의 관계 (x1 ⊥ x4 \| x2)

x2가 주어졌을 때, x1과 x4는 x2 방향으로는 cascading구조이므로 지나갈 수 없다. x3방향으로 지나간다 하더라도, x1과 x6는 v-structure구조이므로 지나갈 수 없다.

따라서 x1과 x4는 독립이다.

&nbsp;

2. x1이 주어졌을 때, x2와 x5의 관계 (x2 ⊥ x5 \| x1)

x1이 주어졌을 때, x2와 x5는 v-structure 구조이므로, x6가 주어졌을 때만 지나갈 수 있다. x1방향으로 지나간다 하더라도, common parent 구조이므로 x1이 알려져 있지 않아야 지나갈 수 있다.

따라서 x1가 주어졌을 때는 x2와 x5는 독립이다.

&nbsp;

3. x2,x3가 주어졌을 때, x1과 x6의 관계 (x1 ⊥ x6 \| {x2,x3})

x2와 x3가 주어졌을 때, x1과 x6는 x2방향으로는 cascading 구조이므로 x2가 주어졌을 때는 지나갈 수 없다. x3방향으로 지나간다 하더라도, cascading 구조이므로 x3가 주어졌을 때는 지나갈 수 없다.

따라서 x2와 x3가 주어졌을 때는 x1와 x6는 독립이다.


&nbsp;

4. x1,x6가 주어졌을 때, x2와 x3의 관계 (x2 ⊥ x3 \| {x1,x6})

x1과 x6가 주어졌을 때, x2와 x3는 x1방향으로는 common parent 구조이므로 x1이 주어졌을 때는 지나갈 수 없다. x6방향으로 지나간다 하면, x2와 x5가 v-structure 구조이고, x6가 주어졌으므로 지나갈 수 있고, x6,x5,x3가 cascading 구조인 상황에서 x5가 주어지지 않았으므로 x3로 도착할 수 있다. 

따라서 x1와 x6가 주어졌을 때는 x2와 x3는 독립이 아니다.

&nbsp;

&nbsp;

이러한 bayes ball algorithm을 활용하여 다양한 기법을 정의할 수 있다.

1. Markov Blanket

<img src="/assets/img/kooc/week78/markov_blanket.png">

A라는 특정 variable이 bayesian network안에 존재한다고 할 때, 주변 특정 관계에 있는 variable만 알면, 나머지의 variable에 대해서는 conditional independent하다.

특정 관계에는 parents, children, children's other parents 이다.

- parents -\> cascading 관계에 있는 variable과 관련
- children -\> cascading 관계에 있는 varable과 관련
- children's other parents -\> v-structure 관계에 있는 variable과 관련

&nbsp;

&nbsp;

2. D-Seperation(directly-seperated)

- Y가 주어졌을 때, Z와 X는 d-seperated 이다. (X ⊥ Z \| Y)
