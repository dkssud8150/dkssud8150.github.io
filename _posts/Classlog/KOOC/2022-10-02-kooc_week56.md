---
title:    "[KOOC] 5,6주차 - Support Vector Machine, Training, Regularization "
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-10-04 23:01:00 +0800
categories: [Classlog, kooc]
tags: [kooc]
toc: true
comments: true
math: true
---

# Chapter 5. Support Vector Machine

- 목차

1. Support Vector Machine Classifier
  - maximum margin idea of the SVM
  - formulation of the optimization problem
2. soft-margin and penalization
  - penalization term
  - difference between the log-loss and the hinge-loss
3. kernel trick
  - primal problem and the dual problem of SVM
  - types of kernels
  - apply the kernel trick to SVM and logistic regression

&nbsp;

&nbsp;

## 5-1. Support Vector Machine

<img src="/assets/img/kooc/week56/dataset.png">

decision boundary은 모델의 성능에 영향을 많이 끼친다. 따라서 decision boundary를 잘 지정하는 것이 중요하다. 위의 그래프처럼 빨간색 점과 파란색 점으로 이루어진 데이터가 있고, 그에 대한 decision boundary를 구하고자 한다면, 어떻게 그어야 잘 그어질 수 있을까? 대충 빨간색과 파란색이 잘 구별되도록 그을 수 있겠지만, 그렇게 하면 새로운 데이터에 대해 분별력이 떨어질 수도 있다. 따라서 이를 잘 설정할 수 있도록 `margin`을 설정하여 그려본다.

&nbsp;

<img src="/assets/img/kooc/week56/decision_boundary.png">

예를 들어, 가장 하단의 빨간색 점 2개를 지나는 빨간색 직선을 긋고, 그 직선과 동일한 기울기를 가진 가장 상단의 파란색 점을 지나는 파란색 직선을 또 그려본다. 그리고는, 이 두 직선과 같은 거리를 가진 직선인 초록색 선을 그린다. 그러면 초록색 선 기준으로 현재 데이터에 대해 알맞는 직선 중 가장 먼 거리에 위치한 직선은 각각 이 빨간색 직선과 파란색 직선이 될 것이다. 

수식적으로 바라본다면, decision boundary 직선의 방정식은 아래 방향을 향한 법선 벡터 w, bias 텀 b를 가지고, 직선 위의 점을 fitting했을 때 `wx + b = 0`로 나타낼 수 있고, 파란색 점들을 positive case, 빨간색 점들을 negative case라 할 때, positive point들은 wx + b > 0, negative point들은 wx + b < 0 이 된다. 이 때, positive case를 +1, negative case를 -1로 가정하고, confidence level을 $ (wx_i + b)y_i $ 라고 정의를 하면, confidence level은 항상 양수가 될 것이고, 이 confidence를 높이는 것을 목표로 할 수 있다.

decision boundary와 가장 근접해 있는 점과의 수직 거리를 margin이라 나타낼 수 있다. 빨간색 직선과 파란색 직선이 maximum margin을 가진 decision boundary라 할 수 있다.

&nbsp;

<img src="/assets/img/kooc/week56/margin_distance.png">

`wx + b = f(x)`로 가정했을 때, 직선 위의 점 x_p에 대해서는 `wx_p + b = 0`, 또, positive point인 $ x_pos $ 들에 대해서는 `wx_pos + b > a , a > 0` 이 성립된다. 

임의의 점 X에 대해 decision boundary와의 margin(distance)를 구해보자. 직선 위의 점 x_p 와 X의 거리를 r이라 할 때, decision boundary의 법선 벡터인 w를 통해 X를 구할 수 있다. 그 후, margin인 r를 구하기 위해 직선의 방정식에 대입한다.

$$ x = x_p + r \cfrac{w}{||w||}$$

$$ f(x) = w \cdot x + b = w(x_p + r \cfrac{w}{||w||} + b = w x_p + b + r \cfrac{w \cdot w }{||w||} = r||w|| $$

&nbsp;

margin $ r = \frac{a}{\|\|w\|\|} $ 로 나타낼 수 있고,  margin이 최대가 되는 w와 b를 값을 구하기 위해 optimization을 수행하면 $ argmax_{w,b} 2r = \cfrac{a}{||w||} $ 로 정의할 수 있다. 이 때, 2r이 이유는 margin은 위 아래 양쪽으로 거리가 있는데 r은 한 방향에 대한 거리만 정의했으므로 2를 곱한다. 사실 2는 상수이므로 중요한 값은 아니다. 아까 confidence level에 대한 식을 $ (wx_i + b)y_i >= a $ 라 정의했는데, a는 임의의 수이므로 1로 지정해볼 수 있다. a를 1로 하게 되면 $ argmax_{w,b} 2r = \cfrac{1}{||w||} $ 이 될 것이고, 분모, 분자를 바꾸어서 $ argmin_{w,b} 2r = ||w|| $ 로 할 수 있다. 이 때, w는 개별 요소를 살펴보면 w1, w2, w3... 이므로 $ ||w|| = \sqrt{w_1^2 + w_2^2} $ 와 같은 quadratic problem으로 정의된다. 이 w에 대해 최적화해야 하는데, matlab에서는 linear programming, quadratic programming과 같은 optimization 기법이 존재하므로 이를 사용하여 최적화를 수행하면 된다.

&nbsp;

<img src="/assets/img/kooc/week56/hard_margin.png">

그러나, 우리가 가진 데이터는 완벽하게 나뉘어질 수 있는 상황이지만, 만약에 빨간색 점이 파란색 점들 사이에 존재하게 되면, 완벽한 decision boundary를 만들 수 없다. 

이러한 완벽한 decision boundary에 대한 margin을 **hard margin**이라 해보자. hard margin인 경우에는 어떠한 에러도 존재해서는 안된다.

&nbsp;

실제 현실은 error가 항상 존재하므로, 에러를 일부 인정하는 margin을 **soft margin**이라 한다. soft margin을 하게 되면, decision boundary를 선형으로 유지하면서 decision boundary를 찾을 수 있다. 또는 soft margin을 가정하지 않고, decision boundary를 비선형으로 그어주면 hard margin을 유지하면서 완벽한 decision boundary를 만들 수 있다. 

그래서 soft margin과 비선형 decision boundary를 만들 수 있게 해주는 kernel trick에 대해 배워보고자 한다.

<img src="/assets/img/kooc/week56/other_decision_boundary.png">

&nbsp;

&nbsp;

## 5-2. Soft Margin

<img src="/assets/img/kooc/week56/soft_margin1.png">

error case를 허용하는 decision boundary를 구성하고자 한다.

&nbsp;

아까 전, 구했던 optimization 식 $ min_{w,b}||w|| $ 에 error 개수와 constant 항을 추가하여 최적화한다. 

$$ min_{w,b}||w|| + C \times N_{error}$$

C는 패널티와 같은 항으로, loss function에 해당한다.

loss function에는 zero-one loss function이 있다. 이는 상단에서 아래로 이동한다고 생각할 때, loss를 0으로 하다가 decision boundary를 지나가는 순간인 wx + b가 0 이하로 되는 순간 loss를 1로 하는 것이다.

그러나 이는 너무 단순하므로, Hinge loss라고 하는 function을 사용할 수도 있다. 파란색 직선과 decision boundary의 margin은 항상 1이다. 동일하게 상단에서 아래로 이동한다고 생각할 때, 파란색 선을 지날 때부터 loss가 error case와의 거리에에 따라 점차 증가하는 선을 따라간다.

이를 수식적으로 표현하기 위해 `slack variable`이라는 개념을 도입한다. 즉 오분류되었을 때, 오류의 정도를 slack으로 관리한다는 것이다. C는 slack parameter를 얼마 정도의 강도로 적용할 것인지에 대한 값이다. slack($\xi_j$) 를 다 합한 것을 최소화하는데, 원래 $ (wx_i + b)y_i >= 1 $ 이었으나 여기에 slack 을 추가하여 $ (wx_i + b)y_i >= 1 - \xi_i \:,\: \xi_i >= 0 $ 로 만들어준다. 이렇게하면 quadratic programming을 수행하는데에는 적합하나, C라는 파라미터가 추가되었기에 더 복잡해질 수가 있다.

&nbsp;

<img src="/assets/img/kooc/week56/soft_margin2.png">

이렇게 slack variable을 추가하여 decision boundary를 결정짓는 모델을 **soft-margin SVM**이라 한다. 그래프와 같이 파란색 점이 어디에 위치하냐에 따라 slack variable 값이 달라지는데, 파란색 선보다 위에서는 0, desicion boundary와 파란색 선 사이에서는 0~1 사이의 값, decision boundary보다 아래로 내려가게 되면 1보다 큰 slack variable을 가지게 된다. 이러한 slack variable을 추가함으로써 다소 soft한 모델을 만들 수 있게 되었다. 최종적인 패널티값은 C x $ \sum_j \xi_j $ 이다.

&nbsp;

<img src="/assets/img/kooc/week56/compareison_logistic_function.png">

logistic function에 대한 loss function은 **log loss**가 있다. 예전에 optimization을 위해 log를 씌워 연산을 수행했다. argmax 안에 log부분이 logistic function인데, 이 부분을 풀어보면, $ Y_iX_i\theta - log(1 + e^{X_i\theta})$ 가 되는데, 여기의 log부분이 아까 배웠던 slack variable과 비슷한 역할을 한다. 이 log부분은 그래프에서의 파란색 형태의 loss function을 가지고 있다.

| logistic function과 SVM을 비교하기는 힘드나, SVM이 더 최근에 나왔다.

&nbsp;

<img src="/assets/img/kooc/week56/strength_loss_function.png">

이 때, C값을 어떻게 결정짓냐에 따라 모델이 큰 차이를 보인다. C가 작으면, 패널티가 작아져서 decision boundary를 넘어가더라도, 영향이 크지 않게 된다. 그래서 C를 키웠더니 C가 특정값 이상이 되면 decision boundary가 고정이 된다. 그렇다면, C를 아주 크게 잡아놓고 진행을 하는 것이 무조건 좋을 수 있으나, 새로운 데이터에 대해 decision boundary가 얼마나 신뢰도가 큰지에 대해 다시 생각을 해봐야 하므로 많은 시행착오가 필요하다.

&nbsp;

&nbsp;

## 5-3. Kernel Trick

<img src="/assets/img/kooc/week56/non_linear_db.png">

이때까지는 linear한 decision boundary에 대해 공부했다. soft-margin decision boundary는 decision boundary를 넘어가는 점에 대해 error term으로 처리하고 선형으로 decision boundary를 설정했다. 그러나 만약 한 개의 점만이 아닌, 연속적으로 불규칙한, 즉 선형으로 설명할 수 없는 데이터가 있다고 하면, error term으로 설명하기 어렵다.

일전에 linear regression에서 단순히 linear regression만이 아닌, 임의로 차수를 높여 non-linear regression을 수행해보았다.이러한 테크닉을 동일하게 적용하여 non-linear한 decision boundary를 구할 수도 있다.

&nbsp;

<img src="/assets/img/kooc/week56/nonlinear_regression.png">

간단하게 얘기해서, decision boundary를 구할 때는 (x1, x2)에 대해 식을 세웠다. 이 x1, x2를 임의로 제곱, 세제곱하거나 두 값을 곱하는 등의 테크닉을 통해 차수를 높여준다. 그렇게 하여 오른쪽 하단에 위치한 그래프처럼 non linear 한 decision boundary를 구할 수 있다.

&nbsp;

위에서는 3차까지 늘렸지만, 이를 무한대까지 늘리는 방법이 있을까? 이를 위해 사용하는 기법이 **kernel trick**이다. 이 kernel trick을 사용하기 위해서는 optimization을 정확하게 이해하고 있어야 한다. 간략하게 얘기하면 prime problem과 dual problem이 있을 때, 이 두개가 같은 성질을 가진 문제라고 정의하고 optimization을 할 수 있다.

&nbsp;

<img src="/assets/img/kooc/week56/dual_problem.png">

SVM의 경우 primal problem에 해당하고, kernel을 도입하기 위해서는 이 prime을 dual problem로 바꿔줘야 한다. SVM은 quadratic programming을 통해 optimization을 수행했다. 이를 dual problem으로 바꾸기 위해서는 `Lagrange method` 라는 것을 잘 이해해야 한다.

SVM에서 optimization을 수행하는 식을 아래와 같이 정의할 수 있다.

- $ min_x f(x) $
- s.t. $ g(x) \leq 0,\: h(x) = 0 $

이 때, Lagrange method에서의 Prime Function L에 대해 $ L(x, \alpha, \beta) = f(x) + \alpha g(x) + \beta h(x) $ 로 정의하여 g(x)와 h(x)를 f(x)로 포함시킬 수 있다. 이 때 단순하게 포함시키는 것이 아닌 alpha와 beta라는 `Lagrange Multiplier` 라는 상수를 정의하여 포함시키는 것이다. 이 때의 alpha는 0보다 크거나 같아야 한다.

이를 통해 Lagrange Dual Function `d`를 Lagrange Prime Function을 활용하여 정의할 수 있다.

$$ d(\alpha, \beta) = inf_{x \in X} L(x, \alpha, \beta) = min_x L(x, \alpha, \beta) $$

이 식을 통해, optimization ($ max_{\alpha \geq 0, \beta} L(x, \alpha, \beta) $)을 풀 때, 특정 범위 내에서는 f(x)와 동일하게 동작하는 성질이 있다. 따라서 optimization이 되어 있는 Lagrange Prime Function을 f(x)와 동일하게 사용하겠다, 즉 f(x) 대신 optimization 식을 쓰겠다는 것이 primal problem에서 dual problem으로 넘어가는 과정이다.

&nbsp;

<img src="/assets/img/kooc/week56/primal_dual_problem.png">

정리하면, Primal problem에서의 optimization 식은 $ g(x) \leq 0, h(x) = 0 $ 일 때, $ min_x f(x) $ 이고, 이를 Lagrange Prime Function으로 변환하면 $ min_x max_{\alpha \geq 0, \beta} L(x, \alpha, \beta) $ 가 된다.

Dual Problem 관점에서 바라보면, dual function을 정의하여 optimization을 수행한다. optimization 식은 $ \alpha > 0 $ 일 때, $ max_{\alpha > 0, \beta} d(\alpha, \beta) $ 이고, 이를 앞서 정의했던 식을 통해 `d`를 치환하면 $ max_{\alpha \geq 0, \beta} min_x L(x, \alpha, \beta) $ 이 된다. 

&nbsp;

여기서 가장 중요한 것은 **Strong duality** 라는 개념이 만족되어야 한다. dual function에서 나온 solution과 원래 가지고 있었던 Prime function이 같게 되어야 한다는 것이다.

$$ d* = max_{\alpha \geq 0, \beta} min_x L(x, \alpha, \beta) = min_x max_{\alpha \geq 0, \beta} L(x, \alpha, \beta) = p* $$

&nbsp;

<img src="/assets/img/kooc/week56/kkt_condition.png">

이 strong duality를 보장해주는 조건이 바로 **KKT Condition** 이다. 즉 지금까지는 Prime Function으로 문제를 풀어왔는데, 이를 Dual Function으로 풀고자 하고, 이 dual function으로 만들고자 할 때 만족해야 하는 조건이 KKT Condition이다.

KKT condition
- $ \triangledown L(x^*, \alpha^*, \beta^*) = 0 $
- $ \alpha^* \geq 0 $
- $ g(x^*) \leq 0 $
- $ h(x^*) = 0 $
- $ \alpha* g(x^*) = 0 $

&nbsp;

| 이 strong duality와 kkt condition은 어려운 개념이므로 따로 공부를 해야 할 것이다. 이 강좌에서는 중요한 부분이 아니므로 빠르게 넘어간다.

&nbsp;

<img src="/assets/img/kooc/week56/dual_problem_svm.png">

SVM에 Lagrange dual problem을 적용해보고자 한다. 먼저 SVM에서의 optimization은 $ (wx_i + b)y_i \geq 1 $ 에 대해 $ min_{w,b} \|\|w\|\| $ 이다. 이 때, 1을 좌항으로 넘겨 g(x)를 정의하고, \|\|w\|\| 를 f(x)로 정의한다. 이 때, Prime Problem에서의 optimization은 $\alpha_j \geq 0 $ 에 대해 $ min_{w,b}max_{\alpha \geq 0, \beta} \frac{1}{2}w \cdot w - \sum_j \alpha_j [(wx_j + b)y_i - 1] $ 이다. 이 min과 max의 위치를 바꿔주어 $ max_{\alpha \geq 0, \beta}min_{w,b} \frac{1}{2}w \cdot w - \sum_j \alpha_j [(wx_j + b)y_i - 1] $ 로 하고, KKT Condition을 가져와서 optimization을 단순화할 것이다. KKT에서 사용할 조건들은 다음과 같다.

- $ \triangledown L(x^*, \alpha^*, \beta^*) = 0\:=\>\: \cfrac{\partial L(w,b,a)}{\partial w} = 0, \cfrac{\partial L(w,b,\alpha )}{\partial b} = 0\:=\>\: w = \sum_j \alpha_j x_jy_j,\:\sum_j \alpha_j y_j = 0 $
- $ \alpha_i ((wx_i + b)y_i - 1) = 0 $

&nbsp;

그 후, Lagrange Prime Function 을 풀어보자.

<img src="/assets/img/kooc/week56/lagrange_prime_function.png">

푸는 과정에서 위에서 정의했던 KKT의 조건들을 사용했다. 이렇게 최종적인 식을 구했는데, 첫번째 항은 linear한 항이고, 두번째 항은 square한 항이므로 quadratic programming을 적용하여 alpha를 구할 수 있다.

만약, $ \alpha_j $를 알고 있다면, w와 b를 쉽게 구할 수 있다. 그러나 이렇게 구했지만 아직 linear한 decision boundary에 머물러 있다.

&nbsp;

&nbsp;

다음으로 넘어가기 전에 **kernel** 이라는 것을 먼저 살펴보자. kernel은 두 벡터의 내적(inner product)로 계산하는데, 이 두 벡터는 다른 space 상의 vector를 의미한다.

$ K(x_i, x_j) = \varphi(x_i) \cdot \varphi(x_j) $

x_i와 x_j를 다른 차원으로 보낸 후의 내적을 한 것이 kernel이다. 이런 식으로 정의할 수 있는 kerenl의 종류는 다양하다.

- Polynomial(homogeneous)
  - $ k(x_i, x_j) = (x_i \cdot x_j)^d\:,\:\: (d : degree) $
- Polynomial(inhomogeneous)
  - $ k(x_i, x_j) = (x_i \cdot x_j + 1)^d $
- Gaussian kernel function (Radial Basis Function)
  - $ k(x_i, x_j) = exp(- \gamma \|\|x_i - x_j\|\|^2) $
- Hyperbolic tangent (Sigmoid Function)
  - $ k(x_i, x_j) = tanh(kx_i \cdot x_j + c) $

이러한 kernel들이 처음 정의했던 $ \varphi $ 에 대한 식이 되는지 살펴보자.

degree가 1일 때의 polynomial을 구한다면,

$$ K(<x_1, x_2>, <z_1, z_2>) = <x_1^2, \sqrt{2}x_1x_2, x_2^2> \cdot <z_1^2, \sqrt{2}z_1z_2, z_2^2> = (x_1z_1 + x_2z_2) = (x \cdot z)^2 $$

&nbsp;

degree가 2일 때의 polynomial을 구하면,

$$ K(<x_1, x_2>, <z_1, z_2>) = <x_1^2, \sqrt{2}x_1x_2, x_2^2> \cdot <z_1^2, \sqrt{2}z_1z_2, z_2^2> = x_1^2z_1^2 + 2x_1x_2z_1z_2x_2^2z_2^2 = (x_1z_1 + x_2z_2)^2 = (x \cdot z)^2 $$

&nbsp;

따라서 degree가 n일 때의 polynomial kernel function은 $ K(<x_1,x_2>, <z_1, z_2>) = (x \cdot z)^n $ 이다. x와 z를 먼저 내적해서 다른 차원으로 보내는 것이나, 다른 차원으로 먼저 보낸 후 내적을 한 것이나 결과가 같게 된다. 그러면 10차원 100차원 무한대 차원까지 증가시켰을 때의 계산도 간편하게 할 수 있다.

&nbsp;

&nbsp;

이제 다시 SVM으로 돌아가서 optimization식에 x_i, x_j를 Kernel을 적용하여 nonlinear하게 진행해보자.

$ max_{a \geq 0} \sum_j \alpha_j - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j \varphi(x_i), \varphi(x_j) =  max_{a \geq 0} \sum_j \alpha_j - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j K(x_i, x_j) $

또한, w,b에 대해서도 정의할 수 있다.

$ w = \sum_{i=1}^N \alpha_i y_i \varphi(x_i) $

$ b = y_i - \sum_{i=1}^N \alpha_i y_i \varphi(x_i) \varphi(x_j) $

&nbsp;

이 때, w에 대해서는 kernel trick을 사용할 수가 없다. 그러나 전체적으로 다시 생각해보면, 우리의 최종 목표는 w와 b를 구하는 것이 아니라, 이 w와 b를 통해 만들어진 decision boundary를 통해 분류를 잘 하는 것이다. 즉, w와 x,b가 있는 식에 x를 집어넣어서 +가 나오면 positive case, -가 나오면 negative case로 분류하기 위함이다. 

그래서 다시 처음으로 돌아가서 wx + b라는 식에서 x를 다른 차원으로 보내어 식을 세워보면 $ w \varphi(x) + b $ 이 되는데, 이는 kernel trick을 사용할 수 없다. 그러나 이 식을 풀어보면

$$ sign(w \varphi(x) + b) = sign(sum_{i=1}^N \alpha_i y_i \varphi(x_i) \cdot \varphi(x_i) + y_j - \sum_{i=1}^N \alpha_i y_i \varphi(x_i) \varphi(x_j)) = sign(\sum_{i=1}^N \alpha_i y_i K(x_i, x) + y_j - \sum_{i=1}^N \alpha_i y_i K(x_i, x_j)) $$

&nbsp;

KKT condition에 의해 생성된 식에 의해 결국 kernel trick을 사용할 수 있게 된다. 예전처럼 w,b를 직접적으로 구할 수는 없지만 위의 식을 통해 classifier를 할 수 있다.

&nbsp;

<img src="/assets/img/kooc/week56/svm_with_various_kernel.png">

4차원에 대한 kernel을 적용한 예와 RBF라는 알고리즘을 적용한 예시이다.

&nbsp;

&nbsp;

<img src="/assets/img/kooc/week56/logistic_regression_kernel.png">

logistic regression에 kernel을 적용한 예이다.

&nbsp;