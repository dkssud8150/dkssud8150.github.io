---
title:    "[KOOC] 5,6주차 - Support Vector Machine, Training, Regularization "
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-10-04 23:01:00 +0800
categories: [Classlog, kooc]
tags: [kooc]
toc: true
comments: true
math: true
---

# Chapter 5. Support Vector Machine

- 목차

1. Support Vector Machine Classifier
  - maximum margin idea of the SVM
  - formulation of the optimization problem
2. soft-margin and penalization
  - penalization term
  - difference between the log-loss and the hinge-loss
3. kernel trick
  - primal problem and the dual problem of SVM
  - types of kernels
  - apply the kernel trick to SVM and logistic regression

&nbsp;

&nbsp;

## 5-1. Support Vector Machine

<img src="/assets/img/kooc/week56/dataset.png">

decision boundary은 모델의 성능에 영향을 많이 끼친다. 따라서 decision boundary를 잘 지정하는 것이 중요하다. 위의 그래프처럼 빨간색 점과 파란색 점으로 이루어진 데이터가 있고, 그에 대한 decision boundary를 구하고자 한다면, 어떻게 그어야 잘 그어질 수 있을까? 대충 빨간색과 파란색이 잘 구별되도록 그을 수 있겠지만, 그렇게 하면 새로운 데이터에 대해 분별력이 떨어질 수도 있다. 따라서 이를 잘 설정할 수 있도록 `margin`을 설정하여 그려본다.

&nbsp;

<img src="/assets/img/kooc/week56/decision_boundary.png">

예를 들어, 가장 하단의 빨간색 점 2개를 지나는 빨간색 직선을 긋고, 그 직선과 동일한 기울기를 가진 가장 상단의 파란색 점을 지나는 파란색 직선을 또 그려본다. 그리고는, 이 두 직선과 같은 거리를 가진 직선인 초록색 선을 그린다. 그러면 초록색 선 기준으로 현재 데이터에 대해 알맞는 직선 중 가장 먼 거리에 위치한 직선은 각각 이 빨간색 직선과 파란색 직선이 될 것이다. 

수식적으로 바라본다면, decision boundary 직선의 방정식은 아래 방향을 향한 법선 벡터 w, bias 텀 b를 가지고, 직선 위의 점을 fitting했을 때 `wx + b = 0`로 나타낼 수 있고, 파란색 점들을 positive case, 빨간색 점들을 negative case라 할 때, positive point들은 wx + b > 0, negative point들은 wx + b < 0 이 된다. 이 때, positive case를 +1, negative case를 -1로 가정하고, confidence level을 $ (wx_i + b)y_i $ 라고 정의를 하면, confidence level은 항상 양수가 될 것이고, 이 confidence를 높이는 것을 목표로 할 수 있다.

decision boundary와 가장 근접해 있는 점과의 수직 거리를 margin이라 나타낼 수 있다. 빨간색 직선과 파란색 직선이 maximum margin을 가진 decision boundary라 할 수 있다.

&nbsp;

<img src="/assets/img/kooc/week56/margin_distance.png">

`wx + b = f(x)`로 가정했을 때, 직선 위의 점 x_p에 대해서는 `wx_p + b = 0`, 또, positive point인 $ x_pos $ 들에 대해서는 `wx_pos + b > a , a > 0` 이 성립된다. 

임의의 점 X에 대해 decision boundary와의 margin(distance)를 구해보자. 직선 위의 점 x_p 와 X의 거리를 r이라 할 때, decision boundary의 법선 벡터인 w를 통해 X를 구할 수 있다. 그 후, margin인 r를 구하기 위해 직선의 방정식에 대입한다.

$$ x = x_p + r \cfrac{w}{||w||}$$

$$ f(x) = w \cdot x + b = w(x_p + r \cfrac{w}{||w||} + b = w x_p + b + r \cfrac{w \cdot w }{||w||} = r||w|| $$

&nbsp;

margin $ r = \frac{a}{||w||} $ 로 나타낼 수 있고,  margin이 최대가 되는 w와 b를 값을 구하기 위해 optimization을 수행하면 $ argmax_{w,b} 2r = \cfrac{a}{||w||} $ 로 정의할 수 있다. 이 때, 2r이 이유는 margin은 위 아래 양쪽으로 거리가 있는데 r은 한 방향에 대한 거리만 정의했으므로 2를 곱한다. 사실 2는 상수이므로 중요한 값은 아니다. 아까 confidence level에 대한 식을 $ (wx_i + b)y_i >= a $ 라 정의했는데, a는 임의의 수이므로 1로 지정해볼 수 있다. a를 1로 하게 되면 $ argmax_{w,b} 2r = \cfrac{1}{||w||} $ 이 될 것이고, 분모, 분자를 바꾸어서 $ argmin_{w,b} 2r = ||w|| $ 로 할 수 있다. 이 때, w는 개별 요소를 살펴보면 w1, w2, w3... 이므로 $ ||w|| = \sqrt{w_1^2 + w_2^2} $ 와 같은 quadratic problem으로 정의된다. 이 w에 대해 최적화해야 하는데, matlab에서는 linear programming, quadratic programming과 같은 optimization 기법이 존재하므로 이를 사용하여 최적화를 수행하면 된다.

&nbsp;

<img src="/assets/img/kooc/week56/hard_margin.png">

그러나, 우리가 가진 데이터는 완벽하게 나뉘어질 수 있는 상황이지만, 만약에 빨간색 점이 파란색 점들 사이에 존재하게 되면, 완벽한 decision boundary를 만들 수 없다. 

이러한 완벽한 decision boundary에 대한 margin을 **hard margin**이라 해보자. hard margin인 경우에는 어떠한 에러도 존재해서는 안된다.

&nbsp;

실제 현실은 error가 항상 존재하므로, 에러를 일부 인정하는 margin을 **soft margin**이라 한다. soft margin을 하게 되면, decision boundary를 선형으로 유지하면서 decision boundary를 찾을 수 있다. 또는 soft margin을 가정하지 않고, decision boundary를 비선형으로 그어주면 hard margin을 유지하면서 완벽한 decision boundary를 만들 수 있다. 

그래서 soft margin과 비선형 decision boundary를 만들 수 있게 해주는 kernel trick에 대해 배워보고자 한다.

<img src="/assets/img/kooc/week56/other_decision_boundary.png">

&nbsp;

&nbsp;

## 5-2. Soft Margin

<img src="/assets/img/kooc/week56/soft_margin1.png">

error case를 허용하는 decision boundary를 구성하고자 한다.

&nbsp;

아까 전, 구했던 optimization 식 $ min_{w,b}||w|| $ 에 error 개수와 constant 항을 추가하여 최적화한다. 

$$ min_{w,b}||w|| + C \times N_{error}$$

C는 패널티와 같은 항으로, loss function에 해당한다.

loss function에는 zero-one loss function이 있다. 이는 상단에서 아래로 이동한다고 생각할 때, loss를 0으로 하다가 decision boundary를 지나가는 순간인 wx + b가 0 이하로 되는 순간 loss를 1로 하는 것이다.

그러나 이는 너무 단순하므로, Hinge loss라고 하는 function을 사용할 수도 있다. 파란색 직선과 decision boundary의 margin은 항상 1이다. 동일하게 상단에서 아래로 이동한다고 생각할 때, 파란색 선을 지날 때부터 loss가 error case와의 거리에에 따라 점차 증가하는 선을 따라간다.

이를 수식적으로 표현하기 위해 `slack variable`이라는 개념을 도입한다. 즉 오분류되었을 때, 오류의 정도를 slack으로 관리한다는 것이다. C는 slack parameter를 얼마 정도의 강도로 적용할 것인지에 대한 값이다. slack($\xi_j$) 를 다 합한 것을 최소화하는데, 원래 $ (wx_i + b)y_i >= 1 $ 이었으나 여기에 slack 을 추가하여 $ (wx_i + b)y_i >= 1 - \xi_i \:,\: \xi_i >= 0 $ 로 만들어준다. 이렇게하면 quadratic programming을 수행하는데에는 적합하나, C라는 파라미터가 추가되었기에 더 복잡해질 수가 있다.

&nbsp;

<img src="/assets/img/kooc/week56/soft_margin2.png">

이렇게 slack variable을 추가하여 decision boundary를 결정짓는 모델을 **soft-margin SVM**이라 한다. 그래프와 같이 파란색 점이 어디에 위치하냐에 따라 slack variable 값이 달라지는데, 파란색 선보다 위에서는 0, desicion boundary와 파란색 선 사이에서는 0~1 사이의 값, decision boundary보다 아래로 내려가게 되면 1보다 큰 slack variable을 가지게 된다. 이러한 slack variable을 추가함으로써 다소 soft한 모델을 만들 수 있게 되었다. 최종적인 패널티값은 C x $ \sum_j \xi_j $ 이다.

&nbsp;

<img src="/assets/img/kooc/week56/compareison_logistic_function.png">

logistic function에 대한 loss function은 **log loss**가 있다. 예전에 optimization을 위해 log를 씌워 연산을 수행했다. argmax 안에 log부분이 logistic function인데, 이 부분을 풀어보면, $ Y_iX_i\theta - log(1 + e^{X_i\theta})$ 가 되는데, 여기의 log부분이 아까 배웠던 slack variable과 비슷한 역할을 한다. 이 log부분은 그래프에서의 파란색 형태의 loss function을 가지고 있다.

| logistic function과 SVM을 비교하기는 힘드나, SVM이 더 최근에 나왔다.

&nbsp;

<img src="/assets/img/kooc/week56/strength_loss_function.png">

이 때, C값을 어떻게 결정짓냐에 따라 모델이 큰 차이를 보인다. C가 작으면, 패널티가 작아져서 decision boundary를 넘어가더라도, 영향이 크지 않게 된다. 그래서 C를 키웠더니 C가 특정값 이상이 되면 decision boundary가 고정이 된다. 그렇다면, C를 아주 크게 잡아놓고 진행을 하는 것이 무조건 좋을 수 있으나, 새로운 데이터에 대해 decision boundary가 얼마나 신뢰도가 큰지에 대해 다시 생각을 해봐야 하므로 많은 시행착오가 필요하다.

&nbsp;

&nbsp;

## 5-3. Kernel Trick

