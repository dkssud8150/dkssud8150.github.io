---
title:    "[데브코스] 7주차 - DeepLearning CNN"
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-04-19 01:40:00 +0800
categories: [Classlog, devcourse]
tags: [devcourse, deeplearning]
toc: true
comments: true
math: true
---

<br>

# Convolutional Neural Network

## Convolution

- padding : add zero in boundary of input image
- stride : elements of sliding window of convolution kernel

output shape
- output height : (input height - kernel height + padding size * 2) // stride + 1
- output width : (input width - kernel width + padding size * 2) // stride + 1

`A * w = B`의 연산의 경우
- A shape : [batch, input channel, input height, input width]
- w shape : [output channel ,input channel, kernel height, kernel width]
- B shape : [batch, output channel, output height ,output width]

<br>

연산량을 따질 때는 MAC(Multiply Accumulation Operation)단위를 사용한다.

convolution MAC : kw\*kh\*kc\*oc\*ow\*oh\*b(k:kernel, o:output, b:batch)

이 convolution은 sliding window 방식을 통해 연산을 수행하기 되고, 이를 for문으로 나타내면 다음과 같다.

```python
for b in batch:
    for oh in output_height:
        for ow in output_width:
            for oc in output_channel:
                for kc in kernel_channel:
                    for kh in kernel_height:
                        for kw in kernel_width:
```

총 7번의 루프로 동작한다.

<br>

이는 너무 비효율적이기 때문에 `IM2COL & GEMM` 방식을 통해 더 간편한 연산 방식을 사용할 수 있다.

- IM2COL

n-dimension의 data를 2D matrix data로 변환시켜 더 효율적으로 연산한다.

<img src="/assets/img/dev/week10/day2/im2col.png">
<img src="/assets/img/dev/week10/day2/im2col2.png">

data와 kernel을 2차원으로 변환하여 연산하면 2차원의 값이 출력될 것이다. 이를 다시 원래의 차원으로 변환하면 연산이 효율적으로 진행된다.

- kernel : [oc, kh\*kw\*ic]
- input : [kh\*kw\*ic, oh*ow]
- output : [oc, oh*ow]

이렇게 변환된 matrix를 연산하는 과정 자체를 **GEMM(General Matrix to Matrix Multiplication)**이라고 한다.

[참고자료](https://welcome-to-dewy-world.tistory.com/94)

<br>

<br>

# CNN (numpy)

이 CNN의 연산을 pytorch나 tensorflow가 아닌 numpy만을 사용하여 구현해보고자 한다.

과정
1. sliding window convolution
2. IM2COL GEMM convolution

<br>

## sliding window 방식

- function/convolution.py

```python
import numpy as np

class Conv:
    # dilation : kernel이 얼마나 간격을 띄우고 연산할지에 대한 값
    def __init__(self, batch, in_c, out_c, in_h, in_w, k_h, k_w, dilation, stride, pad):
        self.batch = batch
        self.in_c = in_c
        self.out_c = out_c
        self.in_h = in_h
        self.in_w = in_w
        self.k_h = k_h
        self.k_w = k_w
        self.dilation = dilation
        self.stride = stride
        self.pad = pad

        self.out_h = (in_h - k_h + 2 * pad) // stride + 1
        self.out_w = (in_w - k_w + 2 * pad) // stride + 1

    def check_out(self, a, b):
        return a > -1 and a < b

    # naive convolution, sliding window matric
    def conv(self, A, B):
        # A * B = C
        # defice C size
        C = np.zeros((self.batch, self.out_c, self.out_h, self.out_w), dtype=np.float32)

        # 7 loop
        for b in range(self.batch):
            for oc in range(self.out_c):
                # each channel of output
                for oh in range(self.out_h):
                    for ow in range(self.out_w):
                        # each pixel of output shape
                        a_j = oh * self.stride - self.pad # a's y value == input's y value
                        for kh in range(self.k_h):
                            if self.check_out(a_j, self.in_h) == False: # a_j 가 in_h보다 크다면 연산 x
                                C[b, oc, oh, ow] += 0
                            else:
                                a_i = ow * self.stride - self.pad # a's x value == input's x value
                                for kw in range(self.k_w):
                                    if self.check_out(a_i, self.in_w) == False:
                                        C[b, oc, oh, ow] += 0
                                    else:
                                        C[b, oc, oh, ow] += np.dot(A[b, :, a_j, a_i], B[oc, :, kh, kw])
                                    a_i += self.stride # add x direction moving unit for kernel 
                            a_j += self.stride # add y direction moving unit for kernel
        return C
```

<br>

- main.py

```python
import numpy as np

from function.convolution import Conv

def convolution():
    print("convolution")

    # define the shape of input & weight
    in_w = 3
    in_h = 3
    in_c = 1
    out_c = 16
    batch = 1
    k_w = 3
    k_h = 3

    # define matrix
    x = np.arange(9, dtype=np.float32).reshape([batch, in_c, in_h, in_w])
    w = np.array(np.random.standard_normal([out_c, in_c, k_h, k_w]), dtype=np.float32)
    #print(x,"\n\n", w)

    Convolution = Conv(batch = batch,
                        in_c = in_c,
                        out_c = out_c,
                        in_h = in_h,
                        in_w = in_w,
                        k_h = k_h,
                        k_w = k_w,
                        dilation = 1,
                        stride = 1,
                        pad = 0)

    print("x shape : ", x.shape)
    print("w shape : ", w.shape)
    L1 = Convolution.conv(x,w)
    #print(L1)
    print("C shape : ", L1.shape) # batch, out_c, out_h, out_w

if __name__ == "__main__":
    convolution()

# ------------ # 

x shape :  (1, 1, 3, 3)  # batch, in_c, in_h, in_w
w shape :  (16, 1, 3, 3) # out_c, in_c, k_h, k_w
C shape :  (1, 16, 1, 1) # batch, out_c, out_h, out_w
```

<br>

## im2col 방식

- function/convolution.py

```python
    # IM2COL, change n-dim input to 2-dim matrix
    def im2col(self, A):

        # define output 
        mat = np.zeros((self.in_c * self.k_h * self.k_w, self.out_w * self.out_h), dtype=np.float32) 

        # matrix index
        mat_i = 0
        mat_j = 0

        # transform from A to mat
        for c in range(self.in_c):
            for kh in range(self.k_h):
                for kw in range(self.k_w):
                    in_j = kh * self.dilation - self.pad
                    for oh in range(self.out_h):
                        if not self.check_out(in_j, self.in_h):
                            for ow in range(self.out_w):
                                mat[mat_j, mat_i] = 0
                                mat_i += 1
                        else:
                            in_i = kw * self.dilation - self.pad
                            for ow in range(self.out_w):
                                if not self.check_out(in_i, self.in_w):
                                    mat[mat_j, mat_i] = 0
                                    mat_i += 1
                                else:
                                    mat[mat_j, mat_i] = A[0, c, in_j, in_i] # [batch, ic, ih, iw],   batch = 1이므로 0index
                                    mat_i += 1 # 1 x move
                                in_i += self.stride # move the stride unit as x axis
                        in_j += self.stride # move the stride unit as y axis
                    mat_i = 0 # initialization
                    mat_j += 1 # move next row at input

        return mat
    
    # gemm, 2D matrix multiplication
    def gemm(self, A, B):
        a_mat = self.im2col(A)
        b_mat = B.reshape(B.shape[0],-1) # kernel 4차원 텐서 차원을 reshape로 바꿀 수 있음, kernel은 [output channel ,input channel, kernel height, kernel width] 로 되어 있는데, 이를 [oc, kh*kw*ic]로 변환하기에
        c_mat = np.matmul(b_mat, a_mat)

        c = c_mat.reshape([self.batch, self.out_c, self.out_h, self.out_w])
        return c
```

<br>

- main.py

```python
    L2 = Convolution.gemm(x,w)

    print(L2)
    print("L2 shape : ", L2.shape) # batch, out_c, out_h, out_w
```

<br>

## pytorch와 위의 2방식 시간 비교

```python
# main.py
import time

    l1_time = time.time()
    for i in range(100):
        L1 = Convolution.conv(x,w)
    print("L1 time : ", time.time() - l1_time)


    l2_time = time.time()
    for i in range(100):
        L2 = Convolution.gemm(x,w)
    print("L2 time : ", time.time() - l2_time)


    # pytorch
    torch_conv = nn.Conv2d(in_c,
                            out_c,
                            kernel_size = k_h,
                            stride = 1,
                            padding = 0,
                            bias = False,
                            dtype = torch.float32)
    torch_conv.weight = torch.nn.Parameter(torch.tensor(w)) # 우리가 직접 생성한 weight를 집어넣음

    l3_time = time.time()
    for i in range(100):
        L3 = torch_conv(torch.tensor(x, requires_grad=False, dtype=torch.float32)) # x가 numpy로 생성되었기 때문에 tensor로 변환하여 실행
    print("L3 time : ", time.time() - l3_time)
    print(L3)

# ------------------- #

L1 time :  0.40502333641052246
L2 time :  0.017976760864257812
L3 time :  0.00850367546081543
```

L1 >> L2 >> L3 순으로 시간이 단축되는 것을 볼 수 있다.