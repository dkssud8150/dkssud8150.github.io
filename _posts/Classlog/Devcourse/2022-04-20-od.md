---
title:    "[데브코스] 7주차 - DeepLearning Object Detection"
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-04-20 00:10:00 +0800
categories: [Classlog, devcourse]
tags: [devcourse, deeplearning]
toc: true
comments: true
math: true
---

<br>

# Object detection

<img src="/assets/img/cs231n/2021-10-06/0017.jpg">

classification은 단일 객체에 대해 분류를 하는 것이다. 이 단일 객체의 위치를 찾아주는 것을 localization이라 하는데, 멀티 객체애 대해 탐색을 하고자 했고, 이 방법이 object detection이다.

<br>

localization에서의 출력은 classifer 및 객체 위치에 대한 x,y,w,h 이다.

<br>

예전에는 sliding window와 같은 방법으로 객체를 검출했다. 그러나 이는 너무 연산량도 많고, 속도도 느리다는 단점이 존재했다. 그래서 최근에는 classifier와 regressor, 두 개를 출력하는 two stage 모델과 이 두개를 한꺼번에 하는 one stage 모델이 개발되었다. 

two stage에는 RCNN시리즈가 있고, one stage에는 SSD, Yolo 등이 있다.

<br>

이전의 classfication의 마지막 단은 fc layer였다. 그러나 object detection에서의 마지막 단은 conv layer로 구성되어 있다. 이 둘의 차이는 flatten를 하냐 마냐의 차이다. flatten을 하게 되면 위치 정도가 사라지게 되고, conv layer로 출력이 되면 위치 정보가 그대로 유지할 수 있다.

one stage의 object detection의 출력은 [H,W class num + box_offset_confidence]

<br>

## One stage detection

SSD, YOLO 등이 이에 해당하는데, 속도는 빠르지만 정확도가 다소 낮은 편이다.

<br>

### 모델 구조

<img src="/assets/img/dev/week10/day3/onestage.png">

- backbone

classification 모델에서 봤듯이 input가 입력되면 feature map이 점점 작아지면서 더 함축적인 의미의 feature map을 만드는 역할을 하는 것을 **backbone** 또는 **feature extractor**이라 한다.

layer가 깊어질수록 추상화된다.

- neck

이렇게 추출된 각기 다른 해상도의 feature map들은 neck 단계에서 합쳐진다. 두개 의 feature map을 결합을 할 때는 동일한 크기의 feature map으로 만들어 준 후 elementwise concat 또는 add을 통해 결합하게 된다.

<img src="/assets/img/dev/week10/day3/neckconcat.png">

A matrix [C, H, W]와 B matrix [C, H, W]를 결합하면 concat matrix는 [2C, H, W]의 shape을 가지게 된다. add matrix는 [C, H, W]의 shape을 가진다.

<br>

- dense prediction

합쳐진 feature map들을 통해 prediction을 한다. 중요한 것은 하나의 feature map에서 결과가 나오는 것이 아니라 여러 개의 feature map에서 나온 결과를 합치는 것이다.

class와 bounding box를 출력하는 부분이다. 이를 header 또는 prediction layer이라 부른다.

<br>

## Two stage detection

RCNN 시리즈가 two stage detection 모델에 해당하는데, 속도가 조금 느리나 정확도가 높은 편이다.

가장 큰 특징은 region proposal을 추출하고, CNN을 연산한 후 이 rego=ion proposal과 결합하여 예측한다.

<img src="/assets/img/dev/week10/day3/fasterrcnn.png">

dense prediction까지는 동일하나 뒤에 prediction 단이 하나 더 존재한다. dense prediction에서는 후보군을 출력하고, 이에 대해 NMS를 통해 박스의 개수를 줄여 출력한다. 

<br>

<br>

# 용어

object detection을 하다보면 다양한 용어들이 나온다. 그 용어들을 차근차근 살펴보고자 한다.

## Grid

header layer의 마지막 feature map에서의 픽셀 수를 grid라 한다. 즉, network를 통해 출력된 feature map이 13x13을 가진다면 grid도 13x13의 사이즈를 가지고 있다고 할 수 있다.

마지막 feature map은 이미지의 정보를 담고 있는 것이므로 이미지에 삽입해보면 전체 이미지를 grid로 나눈 그림을 볼 수 있다.

<img src="/assets/img/dev/week10/day3/yolo.jpg">

한 grid마다 box prediction, objectness score, class score의 정보를 anchor Box 개수만큼 가지고 있다. 즉, 각 BOX마다의 box prediction[tx,ty,tw,th], objectness score[confidence], class score[p1,p2,...,pc] 의 정보를 가지고, 이것이 box개수만큼 존재한다.

<br>

## Anchor

feature map에서 bounding box를 예측할 떄 사용되는 detector이다. anchor는 각 grid안에 특정 개수로 존재하고, 1개의 anchor 당 1개의 object를 예측한다. 이 anchor의 크기는 미리 정의해줘야 한다.

이 때, 크기는 다양하게 정의해야 한다. 그 이유는 object의 모양이 어떻게 되어 있는지 확인하기 어려우므로 여러 개의 anchor size를 선언해줘야 잘 예측할 수 있다.

anchor가 5개라고 하면, 각 anchor마다 box offset, objectness confidence, class confidence가 5개씩 존재한다. box offset(4) + objectness confidence(1) + class confidence(n_classes)개수만큼의 element를 가지므로, class의 개수가 20개라 하면 25개의 element를 가진다. 1개의 grid안에 5개의 anchor가 있다고 하면 feature map의 채널은 (25*5=125)채널을 가진다. 

<br>

## objectness, class score

objectness score란 원하는 object에 대해서만 gt를 1로 두는 것이다. class score는 class 별로 gt를 구분해놓고 class별로 객체에 대한 class를 예측하는 부분에 대한 score이다.

<br>

## IOU(Intersection over union)

두 bounding box를 얼마나 잘 예측했는지에 대해 비교하기 위해 IOU개념을 사용한다. 용어 그대로 영역의 결합 정도를 추론한다. 이 IOU가 threshold보다 크면 positive box, 작으면 negative box라 할 수 있다.

$$ IOU = \frac{A \cap B}{A \cup B} $$

<br>

## NMS(Non-Maximum Suppression)

1개의 grid에 대해 n개의 anchor가 있고, 1개의 anchor마다 1개의 객체를 예측하면 n개의 bounding box가 나오게 된다. 같은 객체를 검출하는 bounding box들에 대해 anchor들을 filtering하는데, 가장 큰 score을 가진 것만 남긴다.

<br>

<br>

추가적으로 많이 사용되는 함수들에 대해 자세히 설명하려고 한다.

# softmax

우리가 예측한 class score는 0.9,0.4,0.2 등의 값으로 존재한다. 이를 softmax를 적용하면 total sum을 1이 되도록 만들어진다. 즉 [0.9,0.4,0.2]에 softmax를 적용하면 [0.4755, 0.2844, 0.2361]이 된다.

<br>


# loss function

## Cross entropy loss

<img src="/assets/img/dev/week10/day3/crossenloss.png">

cross entropy loss에는 positive loss와 negative loss 텀이 존재하는데, class에 대해 잘 예측했다면 이는 positive loss 텀이 작아지고, 잘못 검출했다면 negative loss 텀이 커지는 방식이다.

이에 내가 원하는 label이었는지 아닌지에 대해 판단하기 위해 one hot encoding을 통해 내가 원했던 label에 대해서만 1로 두고, 아닌 label에는 0으로 두는 방식을 사용한다.

식을 보면 positive loss가 1, 즉 원했던 label이고, nevative loss가 내가 원하지 않았던 label에 대한 loss이다. 아까 얻었던 softmax의 출력값 [0.4755, 0.2844, 0.2361]과 one hot label [1, 0, 0]을 통해 cross entropy를 적용하면 

0index에 대해서는 1 * log(0.4755), 1index에 대해서는 (1-0) * log(1-0.2884), 2index는 (1-0) * log(1-0.2361)이 되고, 이를 다 더하여 loss를 구한다.

<br>

## MSE loss, MAE loss

<img src="/assets/img/dev/week10/day3/msemaeloss.png">

이전에 얘기했던 n1 norm, n2 norm과 비슷하게 MSE는 제곱을 통해 Loss를 구하는 방식이고, MAE는 절대값을 사용해서 loss를 구하는 방식이다.

이 loss는 box offset에 적용을 한다.

<br>

<br>

# Prepare Data

object detection에서의 data를 사용하려면 data annotation이 필요하다. 우리의 데이터를 사용하려면 데이터 전처리를 통해 각 bounding box와 label에 대한 파일을 만들어야 한다. `labelimg`라는 툴을 통해 많이 사용한다.

실제로 딥러닝 개발자가 직접 이 label을 만들지 않더라도, 이 annotation들이 적절하게 잘 적용되어 있는지 확인하지 않으면 모델 학습 자체가 잘못될 수 있다. 그래서 직접 data annotation을 수행해보면서 생성해보는 것이 중요하다.

<br>

gt 파일에는 txt나 yaml파일로 구성되어 있고, coco dataset에서의 gt파일을 살펴보게 되면 순서는 다음과 같다.

```txt
45 0.479492 0.688771 0.955609 0.5955
45 0.736516 0.247188 0.498875 0.476417
50 0.637063 0.732938 0.494125 0.510583
45 0.339438 0.418896 0.678875 0.7815
49 0.646836 0.132552 0.118047 0.096937
49 0.773148 0.129802 0.090734 0.097229
49 0.668297 0.226906 0.131281 0.146896
49 0.642859 0.079219 0.148063 0.148062
```

첫번째는 class index일 것이고, 2:5번째는 bounding box의 좌표로 구성되어 있다. 어떤 coco dataset의 경우 조금 다른 포맷을 가지기도 한다.

```txt
car 0.80 0 -2.09 1013.39 182.46 1241.00 374.00 1.57 1.65 1.35 4.43 1.65 5.20 -1.42
```

첫번째는 class name, 두번째는 truncation, 즉 가려진 정도나 이미지에서 벗어난 정도를 나타내준다. 세번째는 occulation, 가려짐에 대한 레벨을 나타낸다. 그 다음은 [x min, y min, x max, y max]인 bounding box에 대한 정보를 나타낸다.

<br>

여기서 coco dataset의 경우 bounding box가 [x,y,w,h]로 되어 있지만, yolo의 경우 [x center, y center, w, h]로 구성되어 있다. 따라서 자신이 사용하는 dataset과 모델을 적절하게 조정하는 것이 중요하다.

아래는 kitti dataset의 gt파일이다.

```txt
Pedestrian 0.00 0 -0.20 712.40 143.00 810.73 307.92 1.89 0.48 1.20 1.84 1.47 8.41 0.01
```

첫번째는 object의 정보, 두번째는 truncated level, 세번째는 occluded level, 네번째는 object의 각도가 정의되어 있다. 마지막으로 bbox에 대한 4개의 value가 있다.

<br>

<br>

데이터를 불러온 후 dataset을 설정할 때는 train:valid:test = 8:1:1 정도로 맞춘다.

<br>

<br>


# Evaluation metric

- precision
내가 예측한 바운딩 박스들 중 참인 것에 대한 비율


- recall
gt박스들 중 내가 예측한 참인 바운딩 박스의 비율

<img src="/assets/img/dev/week10/day4/ap.png">

<br>

precision과 recall은 trade-off 관계를 가지고 있다. 따라서 이 두 값을 종합해서 성능을 평가한다.

## F1 score

$ F_1 = 2 \frac{precision x recall}{precision + recall} $

F1 score은 precision과 recall의 조화평균을 사용한 방법이다. 

<br>

## PR curve

precision과 recall에 대한 곡선을 그리는 방법으로, confidence가 높은 것을 맨 앞으로 두고, 차례대로 그림을 그린다. 누적 TP/FP를 통해 각각의 예측값마다의 precision과 recall을 구한 후 곡선을 그려준다. 그러나 이 PR 곡선은 서로 다른 두 알고리즘을 비교하는 데에는 적합하지 않다. 그래서 나온 것이 mAP이다.

- mAP

이는 Mean average precision을 말하는데, PR 곡선에서 아래 영역으로 계산할 수 있다. 모든 box를 confidence기준으로 내림차순하고, 이 모든 box에 대해 precision과 recall을 계산한다. 곡선 아래 면적을 계산해주면 되는데, 객체가 두 개 이상 검출되었을 경우 AP를 평균하여 계산한다.

<br>

## Confusion matrix

각 클래스간 예측한 값들을 카운팅해서 matrix로 보여주는 방식이다. 파란색이 정답인 label이고, 나머지는 틀리게 예측한 label이다. 이를 통해 특정 label에 대해 어떤 class를 모델이 헷갈려하는지를 확인할 수 있다.

<img src="/assets/img/dev/week10/day4/confusion.png">

