---
title:    "[데브코스] 15주차 - Visual-SLAM Introduction "
author:
  name: JaeHo YooN
  link: https://github.com/dkssud8150
date: 2022-05-16 14:20:00 +0800
categories: [Classlog, devcourse]
tags: [devcourse, visual-slam]
toc: true
comments: true
math: true
---

<br>

# Introduction

SLAM을 수행하게 되면 지금까지 스캔했었던 공간을 기억할 수 있어서 공간에 대한 지도를 생성할 수 있게 된다. 지도를 가지게 된다는 것은 기존의 딥러닝을 통해 즉각적인 추론만으로 판단하는 것보다 훨씬 더 높은 수준의 제어가 가능해진다. 또한 현재의 위치를 파악할 수 있게 되고, 그렇게 되면 주변의 벽이나 객체의 위치도 파악이 가능해진다. 하지만 현재의 위치가 틀리게 추론된다면 주변의 벽이나 객체의 위치도 틀리게 된다는 단점이 존재한다. 

Visual SLAM을 사용하게 되면 주변 정보와 위치정보를 동시에 추론하게 되면서 서로 상호 보완이 가능해진다. 그로 인해 더 정확한 추론이 가능하다.

<br>

<br>

# SLAM 이란?

SLAM이란 Simultaneous Localization and Mapping 을 줄인 말로 동시적(simultaneous) 위치 추정(localization) 및 지도 작성(mapping)이다. SLAM은 원래는 로보틱스에서 출발했는데, 로봇 제어를 하기 위해서는 위치를 추정하고 지도를 작성할 줄 알아야 한다.

지도 작성을 할 때는 센서를 통해 들어온 데이터를 perception(딥러닝)을 통해 주변 환경을 파악한다.

센서는 로봇의 다양한 감각을 담당한다. 카메라, 라이다, 레이다, 초음파, IMU 등등이 존재하고, 이를 통해 다양한 환경을 파악한다.

사전 정보가 존재한다면 사전 정보를 통해 현재의 위치를 추정하고, 그를 통해 더 정확한 추정이 가능할 것이다. 그러나 SLAM은 사전 정보가 없이 위치를 추정을 하고, 지도를 제작하는 것이 특징이다. SLAM을 수행한다는 것 자체가 사전 정보를 제작한다고 할 수 있다.

<br>

위치 추정과 지도 작성 두 개를 동시에 수행하는 것과 SLAM의 차이점, localization과 mapping을 융합한 이유는, slam과 지도 작성의 차이점, 

<br>

<br>

## 로보틱스 기술의 진화

SLAM의 뿌리는 로보틱스에 있다. 특히 고정되어 있지 않고 움직일 수 있는 moblie robotics에서부터 시작되었는데, 이는 위험하거나 사람이 가기 어려운 곳과 같이 사람이 가지 못하는 곳을 대신 탐색하거나 사람 대신 기계가 대신하도록 만들기 위한 기술이다. 가장 기초에는 고정되어 있는 로봇을 바퀴나 레일을 달아 움직일 수 있도록 했는데, 움직이는 로봇으로 만들기 위해서, 즉 자율이동체로 만들기 위한 조건으로는 스스로 **인지**하고 **결정**하고 **행동**해야 한다. 그러나 단순하게 바퀴나 레일을 통해 한정적인 움직임으로는 mobile robotics라 할 수 없다. 

스스로 공간을 인지하는 것을 연구한 것이 SLAM이다. 공간을 인지한다는 것은 이동을 할 수 있는 공간과 없는 공간을 인지할 줄 알아야 하고, 벽이나 장애물도 인지해야 한다. 벽과 같이 외부 환경을 인지하기 위해 사용되는 센서를 exteroceptive sensing(외부 감각)이라 한다.

센서는 항상 노이즈를 가지고 있는데, 센서를 통해 정확하게 추정하려면 노이즈를 제거해주어야 한다. 수많은 센서로부터 들어오는 데이터의 노이즈를 잘 처리하는 과정이 복잡하고, 어렵다. 노이즈로 인해 아주 정확한 SLAM은 불가능하기에 SLAM은 **확률적인 프로세스**라고 할 수 있다.

<br>

mobile robotics에서 주변 환경을 파악하는 것도 중요하지만, 자신의 움직임도 잘 인식해야 한다. 자신의 움직임을 인식하는 센서를 Proprioceptive sensing이라 하고, IMU, GPS 등이 있다.

<br>

움직이면서 주변을 파악하기 위해서는 이 두 sensing을 반복하면 된다. 주변 환경을 탐지하고 벽이 없는 곳으로 이동하고, 제대로 이동했는지 위치를 추정하고, 다시 주변 환경을 탐지하고 , 이동을 반복한다. 이를 `perception & control feedback loop`이라 하는데, 극 초기 단계의 mobile robotics는 이러한 형태를 띄었다. 현재는 사용하지 않는 이유는 여러 가지가 있다.

1. proprioceptive sensing은 확률적인 분포를 가지기에 안정성 확보가 어렵다.
2. exteroceptive sensing과 proprioceptive sensing은 항상 노이즈를 가지므로 신뢰도가 떨어질 수 있다.
3. 노이즈 분석을 하는 동안에는 이동이 불가능하다.

이들을 보완하기 위해서는 여러 개의 센서값들을 확보해서 중간값을 추정하면 되지만, 이는 여러 개의 센서를 확보해야 하므로 가격이 올라가고, 각각의 센서마다 정확도가 다를 수 있어서 어떤 것이 맞고, 어떤것이 틀린지 알수가 없다. 

또 다른 방법으로는 exteroceptive sensing을 통해 proprioceptive sensing이 맞는지 틀린지 확인해볼 수 있다. 로봇이 1m 전진했다면 벽과의 간격이 1m가 줄어야 할 것이다. 그러나 평소에는 오차가 0.2m의 오차를 가지지만, 갑자기 특정 환경에 의해 값이 뻥튀기 된다면 어떤 센서를 믿어야 할지 알 수 없을 것이다. exteroceptive sensing을 하기 위해서는 움직이지 않고 멈춰야 한다. 그러면 움직이다가 샘플링을 위해 멈추고, 다시 움직이고, 샘플링을 위해 다시 멈추고를 반복한다면 평균 이동속도가 엄청나게 감소될 것이다.

<br>

<br>

# Localization, Mapping,, and SLAM

우리가 생각하는 완벽한 mobile robotics는 지속적으로 움직이면서 모션과 주변 환경을 인지할 줄 알아야 하고, 두 sensing이 안정적으로 취득되어야 한다. 이 완벽한 시스템을 만들기 위해 proprioceptive sensing을 집중적으로 보는 localization과 exteroceptive sensing을 집중적으로 보는 mapping, 두 가지 기술이 개발되었다. 이 두 기술은 각각 개발되다가 추후에 혼합되면서 SLAM으로 개발되었다.

<br>

모든 센서는 확률 분포를 가지고 있다. 그렇다면 exteroceptive sensing과 proprioceptive sensing을 조합할 수는 없을까?

두 확률 분포를 조합할 때는 조심해야 하는 부분이 있다. proprioceptive sensing이 상대적으로 부정확하므로 exteroceptive sensing을 통해 약간의 보정이 가능했다. 그러나 반대로 exteroceptive sensing도 부정확하다면 proprioceptive sensing을 보정해줄 수 없다. exteroceptive을 보정하기 위해 proprioceptive을 활용해줄 수도 있다.

이처럼 두 확률 분포 중 하나만 정확하다면 다른 하나를 보정해줄 수 있다. 높은 정확도를 가진 proprioceptive sensing을 통해 exteroceptive sensing을 보정해주는 과정을 `mapping`, 반대로 높은 정확도의 exteroceptive sensing을 통해 proprioceptive sensing을 보정해주는 과정을 `localization`이라 한다.

<br>

## Mapping

<img src="/assets/img/dev/week15/map.png">

대동여지도는 사람이 직접 움직이면서 주변을 살펴보고 작은 지도를 계속 그려나가 전체 지도를 만들었을 것이다. 이처럼 주변 환경을 다양한 시점에서 바라보면서 나 자신의 위치를 정확하게 알고 있다는 가정하에(정확한 proprioceptive sensing 값을 알고 있을 때) 불안정한 주변 환경을 보정해 나가며 주변 환경을 그려나가는 것을 `Mapping`이라 할 수 있다.

<br>

<br>

## Localization

<img src="/assets/img/dev/week15/lotteworld.png">

롯데월드를 갔다고 생각했을 때, 정확한 지도를 보며 자신의 위치를 파악할 수 있을 것이다. 즉, 정확한 exteroceptive sensing의 결과인 지도를 통해 불안정한 proprioceptive sensing의 결과인 나의 위치와 움직임을 보정하여 나의 위치와 움직임을 추론하는 것을 `localization`이라 할 수 있다.

<br>

<br>

이 두 기술을 자율주행에 빗대어 말하면, 지도를 정확하게 그려내야 자동차가 어떤 차선에서 달리고, 적절한 시기에 차선을 바꿀 수 있고, 경로를 미리 계획할 수 있다. 또한 지도를 통해 내가 갈 수 있는 곳과 가면 안되는 곳을 파악할 수도 있다. 주행에 필요한 신호등이나 표지판도 잘 파악할 수 있다.

지도를 정확하게 그리는 것 뿐만 아니라 위치 정보를 정확하게 파악하는 것도 중요하다. 인도와의 경계, 신호등 위치 등을 정확하게 알아도 내 위치가 어디에 있는지 모르면 사고가 난다.

<br>

<br>

그런데 만약 prior 정보가 주어지지 않거나 이 사전 정보가 정확하지 않다면 localization과 mapping이 불가능할까?

### Monte Carlo Localization

localization에서 가장 유명한 기법인 monte carlo localization의 작동 방법은 지도가 사전적으로 주어졌을 때, particle filter를 통해 위치를 추정한다. particle filter는 exteroceptive sensing의 확률 분포와 proprioceptive sensing의 확률 분포를 융합하여 최적의 값을 찾는 기법 중 하나이다.

<img src="/assets/img/dev/week15/monte_carlo_localization.png">

monte carlo localization의 단계는 다음과 같다.
1. initialization : 파티클(사전 정보)를 분포시킨다. 사전 정보란 로봇이 존재할 수 잇는 모든 위치를 의미한다.
2. motion update : 뿌려진 파티클마다 사전 센서로부터 들어온 모션 정보를 추가해서 위치 정보를 업데이트시킨다. 이 때 파티클이 벽으로 들어간다던가, 존재할 수 없는 위치에 있는 파티클은 전부 삭제한다.
3. measurement : 뿌려진 파티클마다 exteroceptive 센서로부터 들어온 정보를 덧씌운다.
4. weight update : 해당 위치에 실제로 이 관측값, exteroceptive센서로부터 들어온 정보가 나올 수 있는지, 즉 현재 위치와 주변 환경의 정보가 맞아 떨어지는지 계산한다.
5. resampling : 잘 맞아 떨어지는 파티클만 남기고, 그 주변에서 다시 파티클을 새로 뿌려서 resampling을 수행한다.

조금 복잡하지만, 로봇이 어디에 존재해야 proprioceptive센서의 값과 exteroceptive센서의 값이 어디에 위치해야 잘 맞아떨어지는지를 찾는 것이 이 알고리즘의 핵심이다.

로봇이 존재할 수 있는 위치는 단 한 곳 뿐일 것이고, 이 알고리즘을 통해 지도 정보와 여러 센서를 조합하면서 점점 더 정답에 가까워질 것이다.

그러나 위의 그림에서 resampling을 보면 여러 곳에 점이 찍혀있다. 이는 측정값들이 나올 수 잇을 만한 위치를 모두 찍은 것으로 꽤 다양한 위치로 예측이 되는 것을 볼 수 있다. 이렇게 되면 위치를 헷갈려할 수 있게 된다.

<br>

이 알고리즘의 문제점은 지도를 전적으로 믿고 파악을 하기 때문에 지도가 틀리면 monte carlo localization은 다른 위치의 파티클을 믿게 될 수도 있고, 이곳저곳 우왕좌왕 할수도 있을 것이다. 

또는 지도가 주어지지 않으면 이 알고리즘을 수행할 수 없다. 정확한 위치를 알기 위해 알고리즘을 수행하는데, 이 알고리즘에서 정확한 지도가 필요하다는 역설적인 상황이 발생된다. 예전에는 매우 비싼 센서를 통해 이를 해결했다. 좋은 센서를 통해 정확한 위치 정보를 얻고, 이 정보를 기반으로 정확한 매핑을 해서 지도를 만든다. 그를 통해 정확한 localization을 수행했다.

이는 매우매우 비싼 센서를 요하기도 하고, 실내에서만 사용이 가능하다던지, 실외에서만 사용 가능하다고 하는 여러 제약 조건이 존재했다. 또는 엄청 비싼 센서로 작업을 했지만, 주변 환경이 달라지면 다시 매핑을 해야 했다.

<br>

주변 환경이 급격하게 자주 바뀌는 상황 속에서는 이 방법을 사용할 수 없었다. 그래서 SLAM이라는 기술이 개발되게 된 것이고, SLAM은 사전 정보없이 최적의 지도와 최적의 위치 정보를 동시에 추정한다.

<br>

<br>

SLAM을 통해 사전 정보없이 exteroceptive센서와 proprioceptive센서만으로 최적의 지도외 위치 정보를 추론할 수 있다. 고품질의 사전 정보를 가지고 있다면, SLAM을 굳이 사용하지 않고 Localization을 하거나 Mapping을 하면 되고, 사전 정보가 없을 경우에만 SLAM을 사용한다고 볼 수 있다.

<br>

<br>

# SLAM에서 사용할 수 있는 센서

## proprioceptive Sensors

자율주행에 사용되는 대표적인 센서로는 IMU, Wheel encoder가 있다.

### Wheel encoder

<img src="/assets/img/dev/week15/wheel_encoder.jpg">

wheel encoder는 자동차 바퀴에 탑재되어 바퀴의 회전량을 측정하는 센서이다. 회전량을 측정하는 방법에는 brush encoder, 위 그림과 같이 빛을 이용하는 optical sensor, 자기장이나 전기장을 사용하는 magnetic sensor 등이 있다. 

측정량에 바퀴의 둘레를 곱하면 이동량도 계산할 수 있다. wheel encoder를 통해 거리를 계산하는 알고리즘으로는 dead recoding 기법이 있는데, 모션값을 누적해 나가서 차체의 위치를 추정하는 방법이다. 그러나 이 방법은 시간이 오래 지나면 지날수록 에러가 점차 누적된다. GPS 등을 통해 이 에러값을 보정해줄 수 있지만, 루프를 빠르게 돌아야 하는 제어 시스템에서는 dead recoding 기법은 위험할 수 있다. 

wheel encoder는 다양한 이유로 에러가 생길 수 있다. 기본적으로 나타나는 센서의 노이즈도 있고, 비나 눈이 오는 날에 바퀴와 바닥의 접지가 좋지 않아 바퀴가 헛돌아서 오차가 발생할 수 있다. 또는 타이어가 눌려서 둘레가 바뀌는 경우에도 바뀔 수 있고, 코너를 돌 때 왼쪽과 오른쪽의 타이어 둘레가 달라질수도 있고, 고속도로 주행중 타이어 마찰에 의해 팽창되어 오차가 생길수도 있다. 최근에는 CAN 통신을 통해 타이어의 변화를 보정해주기도 한다.

<br>

### IMU

선형 가속도를 측정하는 Linear accelerator와 각속도를 측정하는 Angular gyroscope 센서를 혼합한 센서다. 간단하게 IMU 센서는 관성을 측정하는 센서로, 관성을 측정하는 기술로 기계공학에서는 Spring-damper system이라고 부른다. 이 시스템을 칩으로 만든 것을 IMU라고 할 수 있다. 미세한 진동에도 변화를 감지할 수 있고, 자동차에 장착되는 IMU는 빛을 이용하는 optical system을 사용하여 온도나 자기장 변화에 더 강인하다.

<br>

최근 slam에서는 카메라와 IMU를 결합하는 visual inertial geometry 기법이 유행하고 있고, 라이다와 IMU를 결합하는 Lidar inertial geometry 기법도 유행하고 있다. 이와 같이 IMU를 여러 센서를 결합하는 이유는 imu로 얻은 prior 정보를 카메라나 라이다에 적용해서 정확한 계산 결과를 더 빠르게 얻기 위함이다.

IMU 센서는 자율주행 이외 제품에서는 저렴한 편이고, 높은 민감도를 가지고 있으며 높은 FPS(100Hz ~ 4000Hz)를 가진다. 자동차 안에서의 IMU센서는 안전 규제를 위한 수많은 검증 프로세스를 거치게 되면서 가격이 올라가게 되었다.

단점으로는 에러에 대한 오차가 굉장히 커서 보정을 꼭 거쳐야 한다는 것이다.

<br>

<br>

## exteroceptive Sensors

자율주행에서 사용할 수 있는 exteroceptive sensor로는 `GNSS`, `LiDAR`, `Camera`, `Ultrasound`, `RADAR`, `Microphone` 등이 있다.

<br>

### GNSS(GPS)

GPS는 사실 미국에서 사용하는 GNSS 시스템을 지칭하는 말이다. GNSS(Global Navigation Satellite System)은 인공위성과의 통신을 통해 삼각측량을 하여 localization을 수행한다. 

GNSS는 싸고 사용하기 쉽다는 편이지만, 부정확하고(10~20m 오차),  인공위성과 일직선으로 신호가 도달해야 하는데, 고층빌딩 사이에서는 빌딩 벽에 반사되어 위치를 잘못 추정할 수 있는데 이를 multi-path라 하고, 실내나 지하에서는 사용할 수 없다.

<br>

### LiDAR

LiDAR(Light detection and ranging sensors)는 적외선 레이저를 쏘고 반사 시간을 측정하여 거리를 추정하는 센서다. 주변 환경을 3D point cloud 형태로 바로 알 수가 있다.

장점으로는 exteroceptive센서중에서는 가장 정확한 편이고, 자율주행용 라이다는 ~100m 의 유효거리를 가진다. 빛의 파장이 이렁나지 않기 때문에 낮/밤 둘다 사용이 가능하다.

단점으로는 비싸고, 카메라에 비해 해상도가 낮다. 그리고 눈이나 비/안개에 영향을 받기도 한다. 주변을 전부 파악하는 lidar도 있지만, 한 방향만 바라보는 solid-state LiDAR도 있는데, 이를 사용하기 위해서는 모든 방향을 보기 위해 여러 방향으로 여러 개를 탑재해야 한다. 또한, 레이저를 쏘기 때문에 여러 대를 사용할 경우 서로 간섭이 일어날 수도 있다. 또는 반사가 잘 이루어지는 물질에 의해서 물체의 위치를 잘못 추정할수도 있다.

<br>

### Camera

카메라는 광센서를 이용해서 빛 신호를 받고, 아날로그 신호를 0~255값의 디지털 신호로 바꾸어 RGB 색으로 재구성시킨다. 3채널의 값을 가지게 하기 위해 debayering 프로세스를 사용한다.

장점으로는 저렴하고, 모든 픽셀들이 값을 가지기 때문에 밀도있는 데이터를 생성하고 컬러를 가지며 높은 FPS를 가지기 때문에 좋은 성능을 가진다고 할 수 있다. 렌즈를 교환함으로써 시야각 변경이 가능하다. 사람이 보는 것과 거의 유사하기 때문에 시각화하기 가장 좋다.

단점으로는 Depth 정보가 손실된다. 깊이 정보를 추정하기 위해 depth camera나 rgb-d 카메라가 개발되기도 했고, 최근에는 카메라와 라이다를 퓨전하기도 하고, monocular depth estimation을 통해 픽셀이 얼마나 거리값을 가지고 있는 지 추정하는 방법도 개발되고 있다. 또는 조명에 영향을 많이 받기 때문에 조명이 변함에 따라 다른 값들이 들어오기 된다.

<br>

### Ultrasound

초음파(Ultrasound) 센서는 레이다와 작동 방식이 동일하다. 장점으로는 저렴하고, 가까운 범위에서 잘 동작한다.

단점으로는 물체의 형태를 잘 추정하지 못하고, 그래서 거리 센서로만 사용한다. 그리고 노이즈가 많다.

<br>

### RADAR

전파를 쏘고 반사되어 돌아오는 전파를 통해 거리를 재는 센서이다. doppler 효과를 이용해서 이동중인 물체의 속도를 추정 가능하다. 테슬라에서도 예전에는 레이다를 사용하기도 했다. 테슬라에서 레이다를 사용하지 않는 이유는 테슬라 보드를 통해 딥러닝 네트워크의 성능이 엄청 올라가서 레이다를 사용하는 것보다 비전을 사용하는 것이 훨씬 더 높은 정확도를 가지기 때문이다.

장점으로는 빛을 사용하지 않기 때문에 날씨의 영향을 받지 않고, 타 센서에서는 얻지 못하는 속도 값을 추정할 수 있다. 속도를 추정할 수 있다는 것은 카메라나 라이다로는 추정이 불가능하기 때문에, 차량이 주차되어 있는 것인지 움직이지만 같은 속도로 달려서 정지되어 보이는 것인지를 확인할 수 없다. 그래서 레이다를 통해 이를 추정할 수 있다.

단점으로는 작은 물체들은 검출이 불가능하고, lidar보다 더 낮은 해상도를 가지고 있다.

<br>

### Microphone

마이크는 유일하게 소리를 측정할 수 잇는 센서로, 공기의 진동을 transducer 센서를 통해 전기 신호로 변환하는 센서다. 마이크를 하나만 사용하는 것이 아닌 여러 개의 마이ㅡ를 통해 소리의 근원에 대한 위치를 계산할 수 있다.

장점으로는 저렴하다는 것이고, 단점으로는 노이즈가 너무 심하다.

<br>

<br>

# SLAM의 종류

사용하는 센서에 따라 종류가 달라진다.

1. Visual SLAM/VSLAM

카메라를 사용하는 SLAM 방법이다.

2. LiDAR SLAM
3. RADAR SLAM

추가적으로 마이크를 사용하면 microphone slam 이라고 할 수 있을 것이다. 또, 여러 개의 센서를 사용할수도 있는데, 이 때는 여러 개의 센서 타입을 나열해서 이름을 붙여준다.

- e.g. Visual-LiDAR SLAM(camera / LiDAR), Visual-inertial odometry(camera / IMU)...

<br>

| 여러 가지가 있지만, 나의 경우는 Visual SLAM을 주로 다룰 예정이다.

<br>

## Visual SLAM

visual slam은 카메라를 사용하는 slam이다.

장점
- 저렴한 센서
- 센서의 성능을 조절하기 쉽다. (e.g. 렌즈 교체 -\> 시야각, 초점, 노출 시간)
- 빠른 FPS
- 이미지 기반 딥러닝 적용 가능 ( object detection / segmentation )
- 이미지로 사람이 이해하기 쉬운 시각화 가능

단점
- 갑작스러운 빛 변화에 대응 불가능
- 시야가 가려지거나 어두운 곳에서 사용 불가능

카메라는 3D 공간을 2D로 담는 것인데, 이를 통해 깊이 정보가 소실된다. VSLAM은 3D 공간을 재구축하기 위해 깊이 정보를 추정한다.

<br>

SLAM을 잘 하기 위해서는 알고리즘 뿐만 아니라 센서에 대해서도 이해도가 높아야 한다. 모든 센서는 노이즈를 가지고 있고, SLAM은 확률적인 프로세스이므로 센서가 어떤 노이즈를 가지고 있는지 파악해서 노이즈를 제거해야 한다.

<br>

아까 말했듯이 카메라는 3D 공간을 2D로 담아내는 것이므로, 카메라의 노이즈는 이 과정에서 생기는 한계점에 의해 나타나는 노이즈일 것이다. 이는 카메라의 칩에서 발생하는 노이즈일수도 있고, 렌즈에서 나타나는 노이즈일수도 있다. 칩에서 발생하는 노이즈는 아날로그를 디지털로 변환하는 과정에서 발생하는 것과 같이 전자기학적인 문제가 발생하는 것이다. 렌즈에서 발생하는 노이즈는 3D에서 2D로 차원 변화를 하면서 나타나는 노이즈다. 차원을 축소할 때 렌즈에 대한 수학적인 모델링이 완벽해야 하지만, 그렇지 않은 경우 노이즈가 발생한다.

이를 파악하기 위해서는 칩의 종류도 파악해야 하고, 렌즈의 종류도 파악해야 한다. 이 종류에 따라 기하학적 모델링이 다 달라지기 때문이다.

칩의 종류에 따른 카메라의 종류로는
- RGB Camera
- Grayscale Camera
- Multi-spectral Camera
- Polarized camera (편광 카메라)
- Event Camera

<br>

렌즈의 종류에 따른 카메라 종류로는
- perspective Camera
- Wide FOV camera (광각 렌즈)
- Telecentric Camera (망원 렌즈)
- Fisheye Camera (어안 렌즈)
- 360 degree Camera

<br>

<br>

Visual SLAM에서도 카메라 1대만 사용할수도 있지만, 다양한 카메라 종류를 사용할 수 있다.

- 카메라 1대 : Monocular visual SLAM
- 카메라 2대 / 카메라 여러 대 : Stereo visual SLAM / Multi visual SLAM
- depth 카메라 : Depth visual SLAM

카메라 2대를 사용하는 것과 여러 대를 사용하는 것은 거의 동일하므로 저 두개를 묶어서 말하기도 한다.

<br>

<br>

### Monocular VSLAM

1대의 카메라에서만 이미지를 받는 것이 특징이다.

- 장점

다른 VSLAM에 비해 가격, 전력 소비량, 이미지 데이터 송수신 대역폭 등의 측면에서 저렴하다.

<br>

- 단점

1. 1장의 이미지로는 depth를 추정할 수가 없다. 그래서 monocular vslam의 경우는 depth가 없는 2D 이미지로부터 실제 세상에 대한 스케일정보를 받을 수 없기 때문에, 재구축되는 3D 환경은 임의의 스케일로 생성되게 된다. 그렇게 되면 구축한 환경 내에서는 동일한 스케일이 유지되지만 실제 세상되는 얼마나 다른지는 확인할 수 없다. 이를 `Scale ambiguity problem`라 한다. 실제 세상에 대한 스케일을 `Metric scale`이라 하는데, 이는 실제 미터 단위를 의미한다. 이처럼 실제 스케일이 아닌 임의의 스케일을 up-to-scale이라 부르기도 한다.

<br>

이 문제를 해결하기 위해 카메라와 IMU 센서를 융합해서 3d 세상을 구축할 수 있도록 해주기도 한다. 또한 융합을 통해 모션 사전 정보를 통해 더 정확한 정보를 더 빠르게 사용할 수 있다. 그러나 IMU에 사용되는 wheel odometry는 가벼운 알고리즘을 사용하는데 이 경우 모션 정보가 앞/뒤/좌/우, yaw인 3축만 나오게 된다. 6축을 요구하는 VSLAM에 대해서는 충분한 정보를 제공하지 못한다는 단점이 존재한다.

또는 최근 딥러닝 기반에 monocular depth estimation을 통해 이 문제를 해결하려는 시도도 있다.

<br>

2. 카메라만 사용하는 VSLAM은 구현하기도 어렵고, 시스템적으로 더 어려운 시스템을 구축해야 한다.

<br>

이와 같이 많은 단점이 존재하기 때문에 아직은 잘 사용하지 않지만, 많은 상용화된 시스템들이 stereo 시스템(카메라 2대)를 사용하고 있고, 1대만을 사용하는 monocular VSLAM으로 가려는 움직임이 많다.

<br>

<br>

<img src="/assets/img/dev/week15/dso.jpg">

Monocular VSLAM의 가장 유명한 논문으로는 `DSO`가 있다. 이 논문에서는 2016년에 나온 논문이고, direct tracking 기법을 통해 모션을 추정하고 있다. 그리고 픽셀값을 통해 밝기값이 많이 차이나는 edge들만 tracking하고 있고, 과거의 값들과 현재의 값들을 조합해서 3D 공간을 추론한다. 실제의 색상을 볼 수는 없지만, 대략적인 지도를 표현할 수 있다. 

<img src="/assets/img/dev/week15/sparse_slam.jpg">

이 3d 공간 특징들을 point cloud 형태로 표현을 하고 있으며, 3D point cloud를 띄엄띄엄 표현을 한 방식을 sparse SLAM이라 한다. monocular vslam은 대부분 sparse slam을 한다. dense slam을 못하는 것은 아니지만, monocular vslam의 특징을 봤을 때, 카메라가 싸고, 이미지가 단 1개를 통해 연산을 수행하므로 데이터가 적고, **컴퓨팅 보드가 가벼워도 충분히 돌 수 있다는 장점**을 활용하기 위해 sparse slam을 수행한다.

<br>

<br>

### Stereo / Multi camera VSLAM

<img src="/assets/img/dev/week15/stereo.jpg">

2대~ 여러 대를 사용하는 VSLAM 방법을 말하는데, 인접한 카메라들간의 거리 값(baseline 거리)을 metric scale로 알고 있어야 거리와 깊이 추정이 가능하다. 이 baseline거리가 정확하게 구해져야 한다. 조금이라도 오차가 있다면 slam의 모든 과정에 에러가 포함된다. 

문제는 완벽한 baseline은 구할수가 없다. 그럼에도 정확한 측량을 위해 섬세한 calibration 과정을 자동차 하나하나마다 모든 카메라에 수행해야 한다. 그래서 자율주행 회사들은 3D 룸을 만들어서 여러 개의 사진을 찍어서 calibration을 수행한다.

그렇다고 해서 calibration을 대충하게 되면 모든 slam 정보가 잘못 추정될 수 있기에 정확하게 수행되어야 한다. 

<br>

- 장점

두 이미지간의 disparity 정보를 이용해서 픽셀마다 depth를 추정할 수 있다. 픽셀마다 depth를 추정하는 것은 상당히 많은 연산을 필요로 하는데, 예를 들어 1280 x 720 의 해상도를 가지는 이미지라고 하면 2장이므로 최소 1280 x 720 x 2 의 연산을 필요로 하게 된다. cpu에서 이를 수행할 수가 없으므로 gpu에서 이를 수행한다.

<br>

<img src="/assets/img/dev/week15/vio.png">
<img src="/assets/img/dev/week15/vio2.png">

Stereo VSLAM 의 예시로는 `stereo VIO` 라고 하는 알고리즘이 있다. 이 알고리즘은 모션 추정에 특화되어 있는 가벼운 알고리즘이다.

<br>

<img src="/assets/img/dev/week15/omni_slam.jpg">

Multi VSLAM 의 예시로는 `OmniSLAM`이 있다. 이는 dense mapping에 초점을 둔 vslam 알고리즘이다. 주변 환경을 완벽하게 매핑하기 위해서 초광각 카메라 4개를 사용했고, 생성되는 맵이 매우 dense한 특징이 있다.

<br>

이처럼 목적에 따라 파이프라인이 다 다르므로 알고리즘을 살펴볼 때는 어떤 목적의 알고리즘인지 파악하는 것이 중요하다.

<br>

<br>

### RGB-D VSLAM

<img src="/assets/img/dev/week15/rgb-d.gif">

구조광(structured light) 또는 ToF(time-of-Flight) 센서를 이용한 카메라를 사용한다. 센서가 depth를 직접 얻어주기 때문에 계산이 필요없다. 두 센서 모두 대략 10m의 유효거리를 가진다.

depth 데이터를 통해 3D 공간을 metric scale로 실시간으로 복원이 가능하다. 

단점으로는 10m안에서만 dpeth 데이터가 정확하고, FOV가 작다. 그리고 적외선 파장이 햇빛과 간섭이 발생하므로 실외에서 사용이 불가능하다.

<br>

RGB-D 카메라에서의 이미지와 depth 정보는 동일한 스케일이 아니라서 동일하게 맞춰주는 과정이 필요하다.

<br>

<br>

# SLAM 기술의 적용

## 자율주행 로보틱스

- 로봇 청소기

<img src="/assets/img/dev/week15/cleaner.png">

가장 상용화가 잘 된 제품 중에 하나가 로봇 청소기다. 로봇은 지도를 가지고 있지 않기 때문에 집을 직접 돌아다니면서 탐색작업을 해야 한다. 이 과정에서도 청소가 가능할 것이고, 처음 보는 맵에 대해 최소 거리를 움직여서 맵을 완성해야 하는 최적화 문제를 수행해야 한다. 이를 active SLAM이라 하고, 이 SLAM은 SLAM과 path planning이 융합되어 있다고 볼 수 있다.

예전에는 카메라, 2D 라이다 등을 사용했는데, 최근에는 전방 카메라와 2D 라이다, 천장을 바라보는 카메라 등이 존재한다.

<br>

- 산업 현장 측량 로봇

<img src="/assets/img/dev/week15/measurement.jpg">

로봇이 돌아다니면서 매핑을 하고 측량을 수행한다. 이를 통해 바닥이 기울어져 있는지, 벽이 잘 세워졌는지를 측량하기도 한다. 여기에는 라이다가 탑재되어 있고, RGB-D 카메라가 탑재되어 있으나 내비게이션을 위해 탑재되어 있다.

<br>

- 배달 / 물류 로봇

<img src="/assets/img/dev/week15/deliver.jpg">

병원에서 약품을 배달하기도 하고, 실내 뿐만 아니라 실외를 배달하기도 한다.

<br>

<br>

## 자율주행 자동차





<br>

<br>

## 자율비행 드론






<br>

<br>

## 메타버스 - VR/AR






<br>

<br>

각각의 분야에서 SLAM의 특성이 존재한다. 

1. 자율주행 로보틱스
  - 제품화를 위한 규제
    - e.g. 배터리 폭발로 인한 위험성으로 인도에 다닐 수 없다
  - 자유로운 알고리즘 개발
    - 개발 언어, 개발 프로세스, 개발 알고리즘 등 다양하개 개발이 가능하다.
  - 전세계적으로 경쟁

2. 자율주행 자동차
  - 높은 수준의 딥러닝 솔루션 존재
  - 딥러닝 + SLAM
  - 안전한 소프트웨어 요구
    - 오픈소스 알고리즘의 신뢰도
    - 믿을 수 없다면 알고리즘을 직접 짜야함
    - 다양한 환경에 의해 센서가 고장났을 때의 유연성

3. 자율비행 드론
  - 제품화를 위한 규제
  - 가장 빠른 SLAM 요구 -\> 가장 어려움
  - 전력이 많이 소요됨 -\> 컴퓨팅 전력이 작아짐 -\> 가벼운 알고리즘 요구
  - 배터리 폭발과 같은 위험성에 대해 안정성

4. 메타버스 - VR/AR
  - 가장 자유로운 개발 규제
    - 익명 사고가 존재하지 않기 때문에 매우 자유로움
  - 디바이스가 제한될 수 있음
  - 빠르고, 정확하고, 가벼운 SLAM -\> GPU를 사용해야 빠른 속도를 이룰 수 있지만, VR/VR과 같이 그래픽 렌더링을 위해 CPU를 사용해야 함
