<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Lane Detection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요." /><meta property="og:description" content="Lane Detection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요." /><link rel="canonical" href="https://dkssud8150.github.io/posts/tusimple/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/tusimple/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-29T03:20:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-04-29T19:13:04+09:00","datePublished":"2022-04-29T03:20:00+09:00","description":"Lane Detection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요.","headline":"[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/tusimple/"},"url":"https://dkssud8150.github.io/posts/tusimple/"}</script><title>[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 650'%3E%3C/svg%3E" data-src="/assets/img/autodriving/row_based/fig8.png" class="preview-img bg" alt="Preview Image" width="500" height="650" data-proofer-ignore><h1 data-toc-skip>[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-04-29 03:20:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Fri, Apr 29, 2022, 3:20 AM +0900" >Apr 29, 2022</em> </span> <span> Updated <em class="timeago" date="2022-04-29 19:13:04 +0900 " data-toggle="tooltip" data-placement="bottom" title="Fri, Apr 29, 2022, 7:13 PM +0900" >Apr 29, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4691 words"> <em>26 min</em> read</span></div></div></div><div class="post-content"><p>Lane Detection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요.</p><h1 id="abstract">Abstract</h1><p>최근 Lane detection 방법들은 픽셀 단위의 segmentation 기반의 방법으로 다양한 환경과 처리 속도 문제를 해결하기 위해 노력해왔다. 사람의 인지 시스템을 참고하여, 심각한 가려짐이나 매우 힘든 밝기 조건을 전 후 상황이나 전체적인 정보(global information)를 활용하여 차선을 인식한다. 본 논문에서는 전체적인 정보를 활용하기 위한 <strong>row-based selecting</strong>을 통해 차선을 인식하여 계산 비용을 줄이고, 처리 속도를 올렸다. 확장된 receptive field는 더 많은 global feature를 받아옴으로써 모델을 robust하게 만들었다. 또한, 차선의 구조를 명시적으로 모델링하기 위한 구조적 손실(structural loss)도 제안했다.</p><p>코드는 다음 github를 참고하면 된다. https://github.com/cfzd/Ultra-Fast-Lane-Detection</p><p><br /></p><h1 id="1-introduction">1. Introduction</h1><p>lane detection은 광범위하게 적용할 수 있는 메커니즘이다. lane detection은 크게 2가지의 방법으로 분류되는데, openCV를 통한 차선 인식, deep segmentation을 활용한 차선 인식이 있다. 최근에는 딥러닝의 성능이 향상됨에 따라 deep segmentation 방법이 성능이 좋아졌지만, 아직 풀지 못한 문제가 존재한다. 자율 주행에 탑재하기 위한 차선 인식 알고리즘은 매우 낮은 계산 비용을 필요로 한다. 게다가 여러 개의 카메라를 사용하기 때문에 낮은 연산량은 필수적이다.</p><p>self-distilling(모델 압축)을 통해 이를 해결하려 했지만, segmentation의 특성상 픽셀 단위의 예측이 이루어지기 때문에 여전히 문제가 발생했다. 또 다른 문제점으로, <code class="language-plaintext highlighter-rouge">no-visual-clue</code>, 즉 다양한 환경에서 가려짐이나 빛 조건에으로 인한 차선 탐지의 문제가 발생한다.</p><p><img data-src="/assets/img/autodriving/row_based/fig1.png" data-proofer-ignore></p><p>이러한 경우 차선에 대한 높은 수준의 의미론적 정보(semantic analysis)가 필요하다. 그래서 인접한 픽셀을 활용한 메커니즘이 제안되기도 했지만, 픽셀 단위의 연산으로 인해 더 큰 계산 비용이 요구되었다.</p><p>또, 차선이 선이나 곡선이 아닌 이진화된 특징으로 표현되고 있어서 차선에 대한 매끄러움이나 강직도 등의 차선의 특성을 잘 활용하지 못하고 있다.</p><p><br /></p><p>그래서 차선 탐지의 속도를 높이고, no-visual-clue문제를 해결하기 위해, 본 논문에서는 차선의 사전 정보(매그러움이나 강직도)를 잘 활용할 수 있도록 <strong>구조적 손실</strong>을 제안한다. 특히 우리는 부분적 receptive field를 기반한 픽셀 단위의 차선 탐지 대신, <strong>global feature을 활용한 미리 정의된 행에 존재하는 차선의 위치</strong>를 선택하는 방법을 사용한다.</p><p><img data-src="/assets/img/autodriving/row_based/fig2.png" data-proofer-ignore></p><p>전체적인 특징을 기반으로 한 행-선택 메커니즘을 통해 no-visual-clue의 문제를 해결할 수 있다. global features를 통해 전체 이미지에 대한 receptive field를 가지기 때문에, 제한된 receptive field에 비해 가려지는 정보나 다른 위치에서의 추가적인 정보를 학습하고 활용할 수 있다. 위의 그림은 row anchor(row based selecting)에 의해 차선을 인식하는 모습을 보여주고 있다.</p><p>이러한 메커니즘은 segmentation map 대신, 각기 다른 row에서의 차선의 위치를 나타낸다. 이를 통해 선택된 위치간의 관계, 즉 구조적 손실(structural loss)을 최적화함으로써 prior 정보(강직도(rigidity)나 매끄러움(smoothness))를 직접적으로 활용할 수 있다.</p><p><br /></p><h1 id="2-related-work">2. Related Work</h1><h2 id="21-traditional-method">2.1 Traditional method <a href="#21-traditional-method" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>전통적인 방법들은 시각적 정보를 통해 차선 인식을 했다. 시각적 정보가 부족할 경우 tracking을 통해 정보를 얻는다.</p><h2 id="22-depp-learning-models">2.2 Depp learning models <a href="#22-depp-learning-models" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>딥러닝을 사용한 모델들은 semantic segmentation task로서 문제를 해결했다. 예를 틀어, VPGNet은 차선이나 road marking 인식에 대해 사라지는 점들을 탐지할 수 있는 multi-task network를 제안했고, SCNN은 특별한 convolution 연산을 활용했다. 이는 찾아낸 얇은 feature들을 하나씩 덧대어 다른 차원의 정보를 얻는데, 이는 반복적인 신경망과 비슷한 메커니즘이다. SAD의 경우 real time 적용을 위해 가벼운 가중치들을 사용하려 했다. 이는 모델 압축 메커니즘을 적용한 것인데, 이는 high layer와 low layer가 선생과 제자의 관계처럼 연결되어 있다.</p><p>LSTM과 같은 연속적인 예측과 클러스터링을 하는 방법도 있다.</p><p><br /></p><p><br /></p><h1 id="3-method">3. Method</h1><p>새로운 알고리즘과 <strong>차선 구조적 손실(lane structural loss)</strong>에 대해 설명하고자 한다. 또한, high level의 sementics와 low level의 시각적 정보에 대한 특징 추출도 설명한다.</p><p><br /></p><h2 id="31-new-formulation-for-lane-detection">3.1 New formulation for lane detection <a href="#31-new-formulation-for-lane-detection" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>이번 장에서는 전체 이미지의 특징을 기반으로 한 row-based selecting 방법을 설명한다. 즉, 각각의 미리 정의된 row들에 대해 global feature을 활용한 차선의 위치를 선택한다.</p><p><img data-src="/assets/img/autodriving/row_based/fig3.png" data-proofer-ignore></p><p>그림을 조금 더 자세히 설명하자면, 원래의 segmentation방법은 특정 pixel_w, pixel_h을 가진 픽셀에 대해 [pixel_w, pixel_h]의 픽셀 값에서 연산을 했다. 한 픽셀 안에서도 각각의 채널 수만큼 각각을 연산해야 했다. 이렇게 하면 가려짐이나 시각적 정보가 없는 픽셀에 대해서는 예측이 불가능하다.</p><p>그에 반해 본 논문에서의 방법은 한 픽셀에 대해서가 아닌 grid로 잘린 셀들마다의 특정 grid_w, grid_h를 가진 cell에 대해 [grid_w, grid_h]각각을 연산하는 것이 아닌 특정 grid_h를 가진 w 전체를 한번에 연산하여 각각의 채널마다 차선 위치를 예측하게 되는 것이고, grid_h마다의 출력되는 개수는 C, 채널의 수와 같다.</p><p><img data-src="/assets/img/autodriving/row_based/table1.png" data-proofer-ignore></p><p>a에서의 차선의 수의 최대값은 C, row anchor의 수는 h, gridding cells의 수는 w이다. 그리고 global image feature을 나타내는 X와 j번째 row anchor의 i번쨰 lane에 대한 분류기 $ f^{i,j} $ 라는 변수도 선언했다. 따라서 차선의 예측값은 다음과 같이 정의된다.</p>\[P_{i,j} = f^{i,j}(X), i \in [1,C], j \in [1,h] (1)\]<p>이 때, $ P_{i,j} $ 는 j번째 row anchor의 i번째 lane의 (w+1)개의 gridding cell의 확률값에 대한 (w+1)차원의 벡터이다. 차원이 (w+1)인 이유는 차선이 없는 경우가 포함되어야 하기 때문이다. 그리고, $ T_{i,j} $ 는 gt값에 대한 one-hot label일 때, 최적화 수식은 다음과 같다.</p>\[L_{cls} = \sum_{i=1}^C \sum_{j=1}^h L_{CE}(P_{i,j,:},T_{i,j,:}) (2)\]<p>L(CE)는 교차 엔트로피 손실을 나타내고, 식 (1)을 통해 global feature를 기반으로 한 각 row anchor에 대한 모든 위치의 확률 분포를 예측한 후, 식 (2)를 통해 확률 분포를 기반으로 차선에 대한 올바른 위치를 선택한다.</p><blockquote><p>이 때, cls(classification)이라 표현하는 이유는 차선이 해당 grid에 있는지 없는지를 분류한다는 의미인 듯하다.</p></blockquote><p><br /></p><p>본 논문의 알고리즘이 빠른 속도를 가질 수 있는 이유는 원래의 segmentation 방법들보다 계산이 간단하기 때문이다. 이미지 크기를 H x W 라고 가정을 해보자. 그러면 gridding cells의 수는 w &lt;&lt; W, row anchor의 수도 h &lt;&lt; H 로서 생성될 것이다. 원래의 segmentation은 H x W x (C+1)개의 예측을 만들어야 한다. 이 때, c+1인 이유도 차선이 없는 경우를 포함시키기 때문이다. 반면 본 논문의 방법의 경우 C x h x (w + 1)개의 예측을 만든다. 그러면 당연히 H x W x (C + 1)보다 C x h x (w + 1)이 계산 비용이 적다.</p><p>예를 들어, CULane dataset에서의 기존의 segmentation 방법의 계산 비용은 1.15 x 10^6 이지만, 본 논문의 방법으로는 1.7 x 10^4 로 계산이 된다.</p><p><br /></p><p>no-visual-clue 문제를 해결하기 위해 다른 위치에서의 정보를 활용한다. 예를 들어 차선이 차에 의해 가려진다해도 본 논문의 방법은 다른 차선, 도로 형태, 차량 방향으로부터 정보를 얻어 차선 예측이 가능하다.</p><p>receptive field를 전체 이미지로 확장시킴으로써, 맥락적 정보와 이미지 내 다른 위치에서의 정보를 통해 no-visual-clue를 해결할 수 있다.</p><p>학습의 관점에서 봤을 때도, 도로 형태나 방향과 같은 사전 정보를 structural loss를 활용해서 학습함으로서 no-visual-clue를 해결한다.</p><p>또한, row-based selecting을 통해 얻은 차선 위치는 다른 행의 위치와의 관계를 만들 수 있다.</p><p><br /></p><h2 id="32-lane-structural-loss">3.2 Lane structural loss <a href="#32-lane-structural-loss" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>차선 분류 loss외에도 차선 위치들의 관계를 모델링하는 것 또한 목표로 하기 때문에, 2가지의 loss를 추가로 제안한다.</p><p>첫번째는 차선의 연속성에 대한 loss이다. 인접한 row anchor에 대해서는 차선의 위치가 가까워야 한다. 차선 분류에 대한 값은 벡터로 만들어져 있으므로, 연속성은 인접한 row anchor에 대한 분류 벡터의 분포를 제한하여 만들어진다. 즉, 얼마까지 가까워야 인접하다고 판단할 수 있는지에 대한 제한값이 필요하다는 말인데, 이를 위해 유사성에 대한 손실 함수는 다음과 같이 정의될 수 있다.</p>\[Lsim = \sum_{ㅑ=1}^C \sum_{j=1}^{h-1} || P_{i,j,:} - P_{i,j+1,:} ||_1\]<p>이 때, $ P_{i,j} $는 j번째 row anchor, i번째 lane에 대한 예측값을 의미한다. 그리고 $ || . ||_1 $ 는 L1 norm을 나타낸다.</p><p>두번째는 차선의 모양에 대한 loss이다. 차선의 모양은 대부분 직선이고, 곡선이라 해도 미소 단위로 보면 직선이라 할 수 있다. 그래서 직선에서는 0을 만들기 위해 <strong>2차 미분 방정식</strong>을 사용하여 차선의 모양을 추정한다.</p><p>j번째 row anchor, i번째 lane index에 대해 차선의 위치에 대한 수식은 다음과 같이 표현된다.</p>\[Loc_{i,j} = argmax_k P_{i,j,k} , k \in [1,w]\]<p>이 때, k는 차선의 위치 index에 대한 상수이다. 즉, fig3 (a)에서 lane #1, lane #2, 에 대해 각각의 차선 위치를 나타내고 있다. 이 나타내는 위치가 어디인지에 대한 index가 k이다. k는 배경 gridding cell을 카운트하지 않기 때문에 1~w의 값을 가진다.</p><p>여기서 문제는 2차 미분 방정식을 사용해야 하는데, argmax는 미분이 불가능한 함수이므로 다른 row anchor들과의 관계를 설명하기 어렵다. 이 문제를 해결하기 위해 softmax를 사용한다.</p>\[Prob_{i,j,:} = softmax(P_P{i,j,1:w})\] \[Loc_{i,j} = \sum_{k=1}^w k \cdot Prob_{i,j,k}\]<p>이 때도 마찬가지로 배경 gridding cell을 카운트하지 않기 때문에 1~w값을 가진다. 그리고, $ P_{i,j,1:w} $는 w 차원의 벡터를 가지며, $ Prob_{i,j,:} $ 는 각 위치에서의 확률값을 나타낸다.</p><p>그리고 $ Prob_{i,j,k} $ 는 j번째 row anchor, i번째 lane에서의 k번째 위치에 대한 확률값이다.</p><p><br /></p><p>softmax를 통해 미분이 가능해졌으므로 2차 미분을 적용한 차선의 모양(shp == shape)에 대한 loss는 다음과 같이 정의할 수 있다.</p>\[L_{shp} = \sum_{i=1}^C \sum_{j=1}^{h-2} ||(Loc_{i,j} - Loc_{i,j+1}) - (Loc_{i,j+1} - Loc_{i,j+2}) ||_1\]<p>$ Loc_{i,j} $ 는 j번째 row anchor, i번째 lane, 에 대한 위치값이고, 1차 미분은 거의 대부분의 경우가 0이 되지 않으므로, 1차 미분 대신 2차 미분을 사용했다. 그래서 차선 위치에 대한 1차 미분의 분포를 학습하기 위해 추가적인 파라미터가 필요하다. 또한, 2차 미분의 제약은 1차 미분보다 상대적으로 약하기에 차선이 직선이 아닐 때 영향이 줄어든다.</p><p>최종적으로 structural loss, Lstr는 다음과 같다.</p>\[L_{str} = L_{sim} + \lambda L_{shp}\]<p>이 때, $ \lambda $ 는 상수이다. 유사성에 대한 loss와 shape에 대한 loss를 더해서 최종적인 구조적 손실을 구한다.</p><p><br /></p><blockquote><p>2차 미분 방정식을 사용한다는 의미는 차선이 있을 때, 위치를 x,y로 나타내면, 1차 미분 방정식을 사용하여 x,y의 변화율을 판단하면 $ \frac{dy}{dx} $ 이므로 직선일 때조차 0이 아니다. 그러나 2차 미분 방정식을 사용한다면 x,y의 변화율이 아닌 x,y의 변화율의 변화율 즉, dx,dy의 변화율을 판단하게 된다. 그러면 $ dx_1 = x_{1,2} - x_{1,1}, dy_1 = y_{1,2} - y_{1,1} $ 이라 하고, $ dx_2 = x_{2,2} - x_{2,1}, dy_2 = y_{2,2} - y_{2,1} $ 이라 할 때 2차 미분 방정식의 수식은 $ \frac{d^2 y}{dx^2} = \frac{dy_2 - dy_1}{dx_2 - dx_1} = \frac{(y_{2,2} - y_{2,1}) - (y_{1,2} - y_{1,1})}{(x_{2,2} - x_{2,1}) - (x_{1,2} - x_{1,1})} $ 가 된다. 이 때, 직선이면 2차 미분 방정식은 0이 된다.</p></blockquote><p><br /></p><h2 id="33-feature-aggregation">3.3 Feature aggregation <a href="#33-feature-aggregation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>3.2에서는 global feature에 대한 loss를 설명했다. 이번 세션에서는 auxiliary feature 추정 방법에 대해 설명하고자 한다.</p><p>multi scale feature들을 활용하는 auxiliary segmentation task는 local feature을 모델링한다. auxiliary segmentation loss에 대해 cross entropy를 사용했고, 전체 total loss는 다음과 같다. segmentation task란 원래 객체마다의 이진 분류가 아닌 색상별로 분류하는 task이다.</p>\[L_{total} = L_{cls} + \alpha L_{str} + \beta L_{seg}\]<p>Lseg는 segmentation loss이고, Lstr은 방금 전 설명했던 structural loss, cls는 classification loss이다. 이 때, $ \alpha, \beta $ 는 손실 계수이다.</p><p>auxiliary sengmentation task는 훈련 시에만 사용할 수 있고, test 시에는 삭제된다.</p><p><br /></p><p>전체적인 구조는 아래 그림과 같다.</p><p><img data-src="/assets/img/autodriving/row_based/fig4.png" data-proofer-ignore></p><p>res block(backbone) 을 통해 여러 개의 feature map을 만들고, classification, 차선에 대한 이진 분류만 할지, instance segmentation을 수행하여 차선끼리도 색상을 분리할지를 결정할 수 있다.</p><p>원래의 모델은 classification만 수행하지만, segmenation을 수행하려면 auxiliary기능을 따로 생성해줘야 한다.</p><p><br /></p><h1 id="4-experiments">4. Experiments</h1><h2 id="41-experimental-setting">4.1 Experimental setting <a href="#41-experimental-setting" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="dataset">Dataset <a href="#dataset" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/autodriving/row_based/table2.png" data-proofer-ignore></p><p>TuSimple과 CULane, 2가지의 데이터셋을 사용했다. TuSimple dataset은 고속도로 위의 안정적인 빛 조건에서 수집된 데이터셋이고, CULane dataset은 도시 지역에서의 normal, crowd, curve, dazzle, light, night, no line, shadow, arrow, 9가지의 다른 시나리오로 구성되어 있다.</p><p><br /></p><h3 id="evaluation-metrics">Evaluation metrics <a href="#evaluation-metrics" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>두 데이터셋의 평가 방법은 다르다. TuSimple의 경우 정확도를 기준으로 평가하고, CULane은 각 차선을 30픽셀 너비에 대해 IoU를 계산한다.</p>\[TuSimple's accuracy = \frac{\sum_{clip} C_{clip}}{\sum_{clip} S_{clip}}\] \[CULane's F1-measure = \frac{2 * Precision * Recall}{Precision + Recall}\]<p>Cclip은 올바르게 예측된 차선의 점들의 개수이고, Sclip은 각 이미지마다 ground truth의 총 개수이다.</p><p>CULane에서는 IoU가 0.5보다 큰 값을 positive로 예측하고, 그에 대한 F1-measure에서 precision과 recall은 다음과 같다.</p>\[Precision = \frac{TP}{TP + FP}, Recall = \frac{TP}{TP + FN}\]<p>TP는 true positive, FP는 false positive, FN은 false negative 이다.</p><p><img data-src="/assets/img/autodriving/row_based/tpfpfntn.png" data-proofer-ignore></p><p><br /></p><h3 id="implementation-details">Implementation details <a href="#implementation-details" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>TuSimple</p><ul><li>image height : 720<li>row anchor : range(160,710,10)<li>number of gridding cell : 100</ul><p>CULane</p><ul><li>image height : 540<li>row anchor : range(260,530,10)<li>number of gridding cell : 150</ul><p><br /></p><p>optimizing</p><ul><li>resize : 288 x 800<li>optimizer : Adam<li>learning rate : 4e-4, cosine decay learning rate strategy<li>loss coefficients $ \lambda, \alpha, \beta $ : 1<li>batch size : 32<li>epoch<ul><li>TuSimple : 100<li>CULane : 50</ul></ul><p><br /></p><h3 id="data-augmentation">Data Augmentation <a href="#data-augmentation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/autodriving/row_based/fig5.png" data-proofer-ignore></p><p>overfitting을 방지하기 위해 rotation, vertical/horizontal shift augmentation을 수행했고, 차선 구조를 보존하기 위해 이미지가 변형되어도 이미지 경계 끝까지 선을 그려주었다.</p><p><br /></p><h2 id="42-ablation-study">4.2 Ablation study <a href="#42-ablation-study" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="effects-of-number-of-gridding-cells">Effects of number of gridding cells <a href="#effects-of-number-of-gridding-cells" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>Tusimple dataset에서 gridding cell의 개수에 따라 성능이 달라지는지 판단하기 위해 gridding cells의 개수를 25,50,100,200을 각각 적용해보았다. 그 결과는 다음과 같다.</p><p><img data-src="/assets/img/autodriving/row_based/fig6.png" data-proofer-ignore></p><p>gridding cells이 증가함에 따라 성능이 떨어진다. 이는 gridding cell이 많아질수록 더 세분화되고 분류가 어려워지기 때문이다. 그러나 evaluation에서는 다른 그래프를 볼 수 있다. 100일 때 evaluation 정확도가 가장 높았다.</p><p><br /></p><h3 id="effectiveness-of-localization-methods">Effectiveness of localization methods <a href="#effectiveness-of-localization-methods" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>본 논문에서는 각각의 픽셀을 판단하는 것이 아닌 group으로 묶어서 분류를 하기 때문에 분류와 비슷한 메커니즘일 수 있다. 그래서 비슷한 본 논문의 구조에서 classification head를 비슷한 구조의 regression head로 바꿔보았다. REG, REG norm, CLS, CLS exp 총 4가지의 경우에 대해 실험했는데, CLS는 classification based 방법이고, REG는 regression based 방법을 의미한다. Norm은 normalization을 의미한다.</p><p><img data-src="/assets/img/autodriving/row_based/table3.png" data-proofer-ignore></p><p>classlfication head가 앞도적으로 높은 성능을 보였다. 그 이유는 regression은 argmax를 사용하는데 반해 이 논문에서는 softmax를 사용했기 때문이라고 한다.</p><p><br /></p><p><br /></p><h2 id="results">Results <a href="#results" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>backbone을 Resnet18, Resnet34사용했고, Resnet18-reg, Resnet34-reg을 포함한 LaneNet, EL-GAN, SCANN, SAD 에 대해 본 논문의 모델과 비교했다.</p><p><img data-src="/assets/img/autodriving/row_based/table5.png" data-proofer-ignore></p><p>정확도 측면에서는 거의 비슷하나 RunTime의 관점에서 엄청난 차이를 볼 수 있었다.</p><p><img data-src="/assets/img/autodriving/row_based/fig8.png" data-proofer-ignore></p><p><br /></p><p><br /></p><h3 id="reference">reference <a href="#reference" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><a href="https://arxiv.org/abs/1802.05591">Ultra Fast Structure-aware Deep Lane Detection</a><li>https://go-hard.tistory.com/57</ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/tusimple/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/tusimple.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/review/'>Review</a>, <a href='/categories/object-detection/'>Object Detection</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/object-detection/" class="post-tag no-text-decoration" >Object Detection</a> <a href="/tags/tusimple/" class="post-tag no-text-decoration" >tusimple</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/tusimple/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/tusimple/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/tusimple/&amp;text=[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/tusimple/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/YoloV5/"><div class="card-body"> <em class="timeago small" date="2021-09-05 13:00:00 +0900" >Sep 5, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] You Only Look Once v5 - blog</h3><div class="text-muted small"><p> YOLO review 입력된 이미지는 low-level feature 에서 high-level feature가 되고, classification을 위해 trainable classifier를 거친다. Yolo는 앞서 정리한 것을 참고하면 좋을 것 같다. 간단히 리뷰하자면, bounding box regression 과 classific...</p></div></div></a></div><div class="card"> <a href="/posts/YoloV5-2/"><div class="card-body"> <em class="timeago small" date="2021-09-06 13:00:00 +0900" >Sep 6, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>You Only Look Once v5 - Github</h3><div class="text-muted small"><p> Train Custom Data https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb Prepare start 시작 전 환경 설정을 위해 repogitory 를 clone 하고, requiements.txt를 설치한다. git clone https://github.com/ultral...</p></div></div></a></div><div class="card"> <a href="/posts/MMdetection/"><div class="card-body"> <em class="timeago small" date="2021-12-22 13:00:00 +0900" >Dec 22, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] MMDetection: Open MMLab Detection Toolbox and Benchmark</h3><div class="text-muted small"><p> MMDetection에 대한 논문을 리뷰한 내용입니다. 혼자 공부하기 위해 정리한 내용으로 이해가 안되는 부분들이 많을 거라 생각됩니다. 참고용으로만 봐주세요. Abstract mmdetection은 object detection과 instance segmentation을 다루는 유명하고 다양한 모델을 하나의 toolbox로 구현한 일종의 플랫폼이다...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/models/" class="btn btn-outline-primary" prompt="Older"><p>[데브코스] 11주차 - DeepLearning Many CNN models structure</p></a> <a href="/posts/dataset/" class="btn btn-outline-primary" prompt="Newer"><p>[데브코스] 12주차 - DeepLearning Open Dataset and Dataset label format Convert to Yolo format</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>