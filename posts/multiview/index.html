<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Multi-View 3D Object Detection Network For Autonomous Driving 논문에 대한 리뷰한 글입니다. 이 논문은 2017 CVPR에 투고된 논문이며 약 1594회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><meta property="og:description" content="Multi-View 3D Object Detection Network For Autonomous Driving 논문에 대한 리뷰한 글입니다. 이 논문은 2017 CVPR에 투고된 논문이며 약 1594회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><link rel="canonical" href="https://dkssud8150.github.io/posts/multiview/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/multiview/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-17T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-02-08T13:53:55+09:00","datePublished":"2022-01-17T13:00:00+09:00","description":"Multi-View 3D Object Detection Network For Autonomous Driving 논문에 대한 리뷰한 글입니다. 이 논문은 2017 CVPR에 투고된 논문이며 약 1594회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.","headline":"[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/multiview/"},"url":"https://dkssud8150.github.io/posts/multiview/"}</script><title>[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-src="/assets/img/autodriving/MV3D/pointcloud.png" class="preview-img bg" alt="Preview Image" width="800" height="500" data-proofer-ignore><h1 data-toc-skip>[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-01-17 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 17, 2022, 1:00 PM +0900" >Jan 17, 2022</em> </span> <span> Updated <em class="timeago" date="2022-02-08 13:53:55 +0900 " data-toggle="tooltip" data-placement="bottom" title="Tue, Feb 8, 2022, 1:53 PM +0900" >Feb 8, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="8237 words"> <em>45 min</em> read</span></div></div></div><div class="post-content"><p><code class="language-plaintext highlighter-rouge">Multi-View 3D Object Detection Network For Autonomous Driving</code> 논문에 대한 리뷰한 글입니다. 이 논문은 2017 CVPR에 투고된 논문이며 약 1594회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.</p><p><br /></p><h1 id="abstract">Abstract</h1><p>이 논문은 자율주행 시나리오에서 3D Object Detection의 높은 정확도를 목표로 하고 있다. 저자는 LIDAR point cloud와 RGB 이미지를 입력으로 하여 3D object 경계 상자를 예측하는 센서 퓨전 프레임워크인 MV3D(Multi-View 3D networks)를 제안했다. 또, 소량의 다중 뷰(multi-view)만으로 3D point cloud를 인코딩했다. 네트워크는 <code class="language-plaintext highlighter-rouge">3D object 제안 생성</code>을 위한 네트워크와, <code class="language-plaintext highlighter-rouge">multi-view 특징 퓨전</code>을 위한 네트워크로 구성되어 있다. 제안 네트워크(proposal network)는 3D point cloud를 조감도(bird`s eye view)적 표현을 통해 3D 후보 box(3D candidate box)를 생성한다. 저자는 여러 관점(view)에서 지역별 특징을 결합하고, 서로 다른 경로의 중간 레이어들 간의 상호 작용을 가능하게 하기 위해 깊은 결합 체계를 설계했다. KITTI benchmark에서의 실험은 저자의 접근 방식이 3D localization과 3D Detection 태스크에서 25~30% AP정도 능가했다고 한다. 게다가, 2D detection에서는 LIDAR 기반의 최신 기술보다 14.9% AP가 더 높았다.</p><p><br /></p><h1 id="1-introduction">1. Introduction</h1><p>3D object detection는 자율주행차에 시각적 인지 시스템에서 중요한 역할을 한다. 최근 자율주행 차는 보통 LIDAR과 카메라와 같은 다중 센서를 탑재하고 있다. 레이저 스캐너는 정확한 깊이 정보를 제공하는 반면 카메라는 훨씬 더 디테일한 의미 정보를 보존한다. LIDAR point cloud와 RGB 이미지의 융합은 자율주행 차에서 더 높은 수행과 정확도를 성취하게 될 것이다.</p><p>이 논문의 중점은 LIDAR와 image data 모두 활용한 3D Object Detection이다. 그래서 저자는 도로 상에서 3D localization과 물체 인지의 높은 정확도를 목표로 한다. 최근 LIDAR 기반의 방법론들은 3D voxel grid에 3D window를 배치하여 point cloud에 점수를 매기거나, dense box prediction 체계에서 전면 뷰 point 맵에 convolution network를 적용한다. image-based 방법론들은 대체로 먼저 3D box proposal들을 생성한 후 Fast R-CNN 파이프라인을 사용한 region-based 인지를 수행한다. LIDAR point cloud 기반의 방법론들은 보통 더 정확한 3D 위치를 성취하는 반면 이미지 기반의 방법들은 2D box 평가에 관한 더 높은 정확도를 가진다. 초기 또는 후기 융합 방식을 채택함으로써 2D dection을 위해 LIDAR과 이미지를 융합한다. 그러나 3D object detection 태스크에서는 여러 양식의 강점을 갖도록 더 잘 설계된 모델이 필요하다.</p><p>이 논문에서, 다방면의 데이터를 입력으로 받고 3D 공간에서 3D 물체를 예측하는 MV3D(Multi-View 3D object detection network)을 제안한다. 다방면의 정보를 활용하는 중심 아이디어는 지역기반의 특징 융합을 수행하는 것이다. 먼저 3D point cloud의 작고 효과적인 표현을 얻기 위한 체계를 인코딩하는 multi-view를 제안한다.</p><p><img data-src="/assets/img/autodriving/MV3D/fig1.png" data-proofer-ignore></p><p>이 그림에서 볼 수 있듯이 multi-view 3D dectection 네트워크는 <code class="language-plaintext highlighter-rouge">3D 제안 네트워크</code>, <code class="language-plaintext highlighter-rouge">Region-based 융합 네트워크</code> 2개로 구성되어 있다. 3D 제안 네트워크는 더 정확한 3D 후보 상자들을 발생시키기 위한 새 관점에서의 표현을 활용한다. 3D object 제안의 이점은 3D 공간에서 모든 뷰들을 투영할 수 있다는 것이다. multi-view 융합 네트워크는 다중 뷰들로부터 특징맵에 대한 3D 제안들을 투영함으로써 지역별 특징을 추출한다. 다른 뷰들로부터 중간 레이어들 간의 상호 작용을 가능하게 하기 위해 깊은 융합 방식을 설계했다. drop-path training과 보조 손실이 결합된 방식이 초기/후기 융합 체계에 비해 우수한 성능을 보여준다. 다중 뷰 특징 표현을 고려할 때, 네트워크는 3D 공간에서 물체의 정확한 3D 위치, 크기 및 방향을 예측하는 oriented 3D box regression을 수행한다.</p><p>저자는 까다로운 KITTI object detection benchmark에서 3D 제안 생성, 3D localization, 3D detection, 3D detection 수행에 대한 접근 방식을 평가했다. 실험은 우리의 3D 제안이 최근 3D 제안 방법론인 3DOP과 Mono3D를 엄청 능가했다는 것을 보여준다. 특히, 300개의 제안에서는 IoU threshold가 0.25일때는 99.1%, 0.5일 때는 91% 3d recall을 얻는다. 이 논문의 LIDAR 기반의 방식은 3D localization task에서는 25% 더 높은 정확도를, 3D object detection task에서의 3D AP(average precision)에서는 30% 더 높은 정확도를 성취했다. 이것은 까다로운 KITTI test 데이터셋에서의 2D detection에 대한 14.9% AP를 가지는 다른 LIDAR 기반의 방법론를을 능가했다. 이미지와 융합한다면, LIDAR 기반의 결과에 대한 추가적인 개선이 가능하다.</p><p><br /></p><h1 id="2-related-work">2. Related Work</h1><p>point cloud나 이미지에서 3D object detection, 다양한 융합 방법 및 3D object proposals에 대한 기존 연구를 간략히 검토하고자 한다.</p><h2 id="3d-object-detection-in-point-cloud">3D object detection in point cloud <a href="#3d-object-detection-in-point-cloud" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>대부분의 존재하는 방법들은 voxel grid 표현에서의 3D point cloud를 인코딩한다. sliding Shapes와 Vote3D는 SVM 분류기를 사용하여 지역 특징을 인코딩한 3D grid들을 계산한다. 최근 제안된 방법들은 3D convolution을 통한 특징 표현을 개선한다. 하지만 비싼 계산을 요구한다. 3D voxel 표현 외에도, VeloFCN은 FV(Front view)에 대한 point cloud를 투영하여 2D point map을 얻는다. 여기서는 2D point map에 FCN(fully convlutional network)를 적용하고, convolutional feature map으로부터 3D box들을 예측한다. 3D 객체 분류를 위한 point cloud의 크기와 다중 뷰 표현을 조사한다. 본 연구에서는 멀티 뷰 feature map으로 3D point cloud를 인코딩하여 다양한 융합을 위한 지역 기반 표현을 가능하게 한다.</p><p><br /></p><h2 id="3d-object-detection-in-images">3D Object Detection in Images <a href="#3d-object-detection-in-images" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>3DVP는 3D voxel 패턴을 소개하고, 2D detection과 3D pose estimation을 할 수 있는 ACF detector를 생산한다. 3DOP는 스테레오 이미지들로부터 Depth를 재구성하고, 에너지 최소화 접근 방식을 사용하여 3D box proposal을 생성하고, 이는 객체 인식을 위해 R-CNN 파이프라인에 공급된다. Mono3D는 3DOP와 같은 파이프라인을 구성하고 있고, 이것은 monocular images(단안상)으로부터 3D 제안들을 발생시킨다. 저자의 실험을 통해 3D localization을 개선하기 위한 LIDAR point cloud를 통합시키는 방법을 볼 수 있을 것이다.</p><p><br /></p><h2 id="multimodal-fusion">Multimodal Fusion <a href="#multimodal-fusion" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>자율주행에서 오직 소수의 모델들만이 다양한 양식의 데이터를 사용한다. 저자는 FractalNet과 Deeply-Fused Net을 기반으로 한 깊은 융합 방식을 설계한다. FractalNet에서, 기본 방식은 기하급수적으로 증가하는 경로를 통해 네트워크를 구성하기 위해 반복한다. 간단하게, 얕은 서브 네트워크들과 깊은 서브 네트워크들을 융합함으로써 deeply-fused network를 구성하는 것이다. 이 논문의 네트워크는 각 열마다 동일한 베이스 네트워크를 사용하지만 정규화를 위한 보조 경로와 손실을 추가한다는 점에서 두 네트워크와 다르다.</p><p><br /></p><h2 id="3d-object-proposals">3D Object proposals <a href="#3d-object-proposals" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>2D object proposals와 유사하게, 3D object proposal 방법들은 3D 공간에서 객체 대부분을 복사하기 위해 작은 3D 후보 상자들을 생성한다. 이것을 위해 3DOP는 스테레오 point cloud의 일부 depth 특징들을 설계하여 큰 3D 후보 상자들을 점수매긴다. Mono3D는 ground 평면을 먼저 활용하고, 일부 분할 특징을 사용하여 단일 이미지에서 3D proposal을 생성한다. Deep Sliding Shapes는 더 강력한 딥러닝 특징들을 사용한다. 그러나, 이는 계산적으로 비싼 3D convolution을 사용하고 3D voxel grid안에서 작동된다. 그래서 bird`s eye view 표현을 생산하고, 더 정확한 3D proposal 발생시키기 위한 2D convolution을 생산함으로써 더 효과적인 접근 방식을 제안한다.</p><p><br /></p><h1 id="3-mv3d-network">3. MV3D Network</h1><p>MV3D 네트워크는 이미지와 3D point cloud를 입력으로 하여 다중 뷰를 표현한다. 이는 먼저 bird`s eye view map와 region 기반의 표현을 통해 깊게 융합한 다중 뷰 특징들로부터 3D object proposal을 생성한다. 이 융합된 특징들은 classification과 3D box regression에 사용된다.</p><h2 id="31-3d-point-cloud-representation">3.1 3D point cloud Representation <a href="#31-3d-point-cloud-representation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>현존하는 모델들은 보통 3D LIDAR point cloud를 3D grid나 FV(front view) map로 인코딩한다. 대부분의 point cloud의 기초적인 정보들은 3D grid 표현들을 보존하는데, 이는 그 다음의 특징 추출을 위해 더 복잡한 계산을 요구하게 된다. 그래서 저자는 아래 그림에서 볼 수 있듯이 bird`s eye view와 FV에 대한 3D point cloud를 투영함으로써 더 간단한 표현을 제안한다.</p><p><img data-src="/assets/img/autodriving/MV3D/fig2.png" data-proofer-ignore></p><ul><li>Bird’s eye view representation</ul><p>bird’s eye view, 즉 조감도는 <strong>height</strong>(높이), <strong>intensity</strong>(강도), <strong>density</strong>(밀도)로 인코딩되어 있다. 저자는 투영된 point cloud를 0.1m의 해상도의 2D grid로 나눈다.</p><p>각 셀의 <strong>height</strong> 특징들은 셀 안의 point들의 <code class="language-plaintext highlighter-rouge">최대 높이</code>로 계산되어 있다. 좀 더 자세한 높이 정보를 인코딩하기 위해, point cloud는 M개의 slices로 균등하게 나누어진다. height map은 각각의 slice로 계산되어 있기 때문에 M height maps를 얻을 수 있다. <strong>intensity</strong> 특징들은 각 셀의 최대 높이를 가지는 점의 반사도이다. <strong>density</strong>는 각 셀의 점들의 갯수를 나타낸다.</p><p>특징들을 정규화시키기 위해 <em>min(1.0, log(N+1)/log(64))</em> 로 계산되는데 이때 N은 셀 안의 점들의 갯수를 나타낸다. 이 때, 중요한 것은 height feature은 M개의 slice에 의해, 즉 각 셀을 계산하는 반면, intensity와 density feature은 전체 point cloud에 대해 계산된다. 그래서 전체 조감도 map은 <em>(M+2) - channel</em>로 계산된다.</p><ul><li>Front View Representation</ul><p>전방 뷰는 조감도 표현에 대한 상호 보완적인 정보를 제공한다. LIDAR point cloud가 매우 밀도가 희박함에 따라, 이미지 평면에 이를 투영하는 것은 밀도가 희박한 2D point map를 얻는다. 따라서 밀도있는 전방 뷰 map을 발생시키기 위해 평면 대신 실린더 면(원통 면)에 투영한다. 3D point, <em>p = (x,y,z)</em>라고 할 때, 전방 뷰에 대한 point 표현은 <em>pfv = (r,c)</em>이고, r과 c에 대한 계산은 다음과 같다.</p><p><img data-src="/assets/img/autodriving/MV3D/fvpoint.png" data-proofer-ignore></p><p>∆ϴ와 ∆ø는 각각 레이저의 수평, 수직에 대한 표현이다. fig2애서 볼 수 있듯이 저자는 height, distance, intensity의 3채널 특징을 통해 FV map을 인코딩한다.</p><p><img data-src="/assets/img/autodriving/MV3D/fig2.png" data-proofer-ignore></p><p><br /></p><h2 id="32-3d-proposal-network">3.2 3D Proposal Network <a href="#32-3d-proposal-network" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>최첨단 2D object detector의 핵심 구성요소인 RPN(Region Proposals Network)에서 영감을 받아, 먼저 3D object proposal을 발생시키는 네트워크를 설계했다. 일단 조감도 맵을 입력으로 사용한다. 3D object detection에서 조감도 맵은 전방 뷰나 이미지 평면에 비해 몇 가지 장점이 있다. 먼저, 객체는 사이즈 변화가 작은 조감도 맵으로 투영했을 때 <code class="language-plaintext highlighter-rouge">물리적 사이즈가 보존</code>된다. 두번째로, 조감도 맵에서의 물체는 서로 다른 공간을 차지하기 때문에 <code class="language-plaintext highlighter-rouge">occlusion 문제</code>를 피할 수 있다. 세번째로 로드 뷰에서 물체는 대부분 지면에 놓여있거나 수직적 위치의 변화가 작기 때문에, 조감도 맵은 더 <code class="language-plaintext highlighter-rouge">정확한 3D bounding box</code>를 얻을 수 있다. 따라서 조감도 맵을 사용하면 3D 위치 예측이 더욱 실현 가능해진다.</p><p>네트워크는 3D prior boxes로부터 3D box proposals를 발생시킨다. 각 3D box는 <em>(x,y,z,l,w,h)</em>로 구성되어 있고, 각각 LIDAR 좌표계에서의 3D box의 중심 좌표(x,y,z)와 사이즈(meters)(l,w,h)를 나타낸다. 각 사전 box(prior box)에 해당하는 조감도 anchor<em>(xbv, ybv, lbv, wbv)</em>는 (x,y,l,w)를 통해 얻을 수 있다. 훈련 셋에서 GT(Ground Truth) object size를 군집 분류함으로써 N개의 3D prior boxes를 설계한다. 자동차 감지의 경우, (l,w)의 prior box는 {(3.9, 1.6), (1.0, 0.6)}의 값을 가지며, 높이 h는 1.56m를 가진다. 조감도 앵커를 90도 회전함으로써, N = 4의 prior boxes를 얻는다. (x,y)는 조감도 특징 맵에서의 위치이고, z는 카메라 높이와 물체 높이에 의해 계산된다. 저자는 제안 생성(proposal generation)에서 방향 회귀(orientation regression)를 하지 않고, 다음 예측 단계로 넘어간다. 3D box의 방향은 대부분의 로드 뷰에서의 물체의 실제 방향에 가까운 0~90도로 제한되어 있다. 이 간단함은 제안 회귀의 훈련을 쉽게 만들 것이다.</p><p>0.1m 해상도에 따라, 조감도의 물체 박스들은 오직 5~40 픽셀만 차지한다. 초소형 물체를 감지하는 것은 아직 어려운 문제지만, 하나의 해결책이 있다면 입력을 높은 해상도로 하는 것이다. 그러나 이는 더더욱 큰 계산을 요구한다. 그래서 <em>A unified multi-scale deep convolutional neural network for fast object detection</em> 논문의 업샘플링 방법을 채택하여, proposal network안의 마지막 convolution layer이후에, 2배 이선형(bilinear) 업샘플링을 사용한다. front-end convolution은 3번의 pooling작업을 하여 1/8배 다운샘플링을 진행한다. 이를 2배 업샘플링과 결합하여 proposal network에 제공된 특징 맵은 조감도 입력과 관련하여 1/4배 다운샘플링된다.</p><p>RPN과 유사하게 <em>t = (∆x,∆y,∆z,∆l,∆w,∆h)</em>에 대해 3D box regression한다. (∆x,∆y,∆z)는 anchor 크기에 의해 정규화된 중심 좌표이고, (∆l,∆w,∆h)는 다음과 같이 계산된 값이다.</p><p><img data-src="/assets/img/autodriving/MV3D/sizecompute.png" data-proofer-ignore></p><p>물체/배경을 동시에 분류하고, 3D box regression을 수행하기 위해 multi-task loss를 사용한다. 특히, objectness loss와 smooth <em>l1</em>에 대한 cross-entropy를 사용하여 3D box regression loss를 구한다. box regresson loss를 구할 때는 배경 anchor는 무시한다.</p><p>훈련하는 동안에, anchors과 GT 조감도 boxes들 사이의 IoU를 계산한다. IoU가 0.7이상이면 anchor가 positive이고, 0.5미만이면 negative가 된다. 그 사이의 값은 무시한다. LIDAR point cloud data가 많이 빈 anchor가 많기 때문에, 훈련과 테스트에서 계산을 줄이기 위해 모든 빈 anchor들을 제거한다.</p><p>마지막 convolution 특징 맵의 각 위치에서의 비지 않은 anchor에서 네트워크가 3D box를 발생시킨다. 불필요한 반복을 줄이기 위해, NMS(Non-Maximum Suppression)을 조감도 박스들에 적용시킨다. 물체가 지면에서 각각 다른 공간을 차지해야 하기 때문에 3D NMS는 적용하지 않았다.</p><p>저자는 NMS를 위해 0.7 threshold의 IoU를 사용한다. 훈련에는 2000개의 top boxes를, 테스트에서는 300개의 top boxes를 유지시킨다.</p><p><br /></p><h2 id="33-region-based-fusion-network">3.3 Region-based Fusion Network <a href="#33-region-based-fusion-network" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>저자는 다중 뷰로부터 특징들을 효과적으로 결합하고, 객체 제안들을 분류하고, oriented 3D box regression을 하기 위해 region-based fusion network를 사용한다.</p><ul><li>Multi-View ROI Pooling</ul><p>다른 뷰/방식들에 대한 특징들은 다른 해상도들을 가지고 있기 때문에, 같은 길이에 대한 특징 벡터를 얻기 위해 각각의 뷰에 ROI pooling을 적용한다. 생성된 3D proposals를 고려할 때, 3D 공간에서 모든 뷰에 대해 투영할 수 있게 된다. 이 논문에서는 BV(bird’s eye view), FV(front view), RGB(image plane) 총 3개의 뷰에 대해 투영하였다.</p><p><img data-src="/assets/img/autodriving/MV3D/ROIproposal.png" data-proofer-ignore></p><p>3D proposal, P<em>3D</em>를 고려할 때, T<em>3D -&gt; v</em>는 각각 LIDAR 좌표계에서 조감도, 정면 뷰, 이미지 평면으로의 변환 기능을 나타내고, 위의 식에 따라 각 뷰에 대해 ROI를 얻을 수 있다. 각 뷰의 front-end network로부터 입력 특징 맵 x를 고려하여 ROI pooling에 의해 고정된 길이 특징,<em>fv</em>를 얻는다.</p><p><img data-src="/assets/img/autodriving/MV3D/roipooling.png" data-proofer-ignore></p><p><br /></p><ul><li>Deep Fusion</ul><p><img data-src="/assets/img/autodriving/MV3D/fig3.png" data-proofer-ignore></p><p>다른 특징들과 정보를 결합하기 위해, 사전 작업은 보통 <em>초기 결합</em>이나 <em>후기 결합</em>을 사용한다. 하지만 저자는 다중 뷰 특징을 계층적으로 융합하는 <em>deep fusion</em> 방식을 적용한다. 초기/후기 결합 네트워크와 deep fusion 네트워크 아키텍쳐의 비교는 위의 그림에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/MV3D/earlyfusion.png" data-proofer-ignore></p><p>L개의 layers를 가진 네트워크의 경우, <strong>초기 결합</strong>은 입력 단계에서 다중 뷰들의 특징, <em>fv</em>들을 결합한다. 이 떄, {Hl,l = 1,…L} 은 특징 변환 함수이고, ⊕는 concatenation, summation과 같은 결합 연산이다.</p><p><img data-src="/assets/img/autodriving/MV3D/latefusion.png" data-proofer-ignore></p><p>대조적으로, <strong>후기 결합</strong>은 별도의 서브 네트워크를 사용하여 특징 변환을 독립적으로 수행하고 예측 단계에서 출력을 결합한다.</p><p><img data-src="/assets/img/autodriving/MV3D/intermediate.png" data-proofer-ignore></p><p>그러나, 이 논문에서는 다른 뷰들에서 <strong>중간 레이어</strong>의 특징들 사이에 더 많은 상호작용을 가능하게 하기 위해 다음과 같은 <strong>deep fusion</strong> 프로세스를 설계했다. drop-path 훈련과 결합할 때 더 유연하기 때문에 이 논문에서는 deep fusion을 위한 결합 작업에 요소별 평균을 사용한다.</p><p><br /></p><ul><li>Oriented 3D box Regression</ul><p>다중 뷰 네트워크의 특징 결합을 고려할 때, 3D 제안들로부터 oriented 3D boxes를 회귀한다. 특히, regression(회귀) 타겟들은 3D boxes에 대한 8개의 모서리들이다. (t = (∆x0,…,∆x7,∆y0,…,∆y7,∆z0,…,∆z7)). 제안 박스의 대각선의 길이에 의해 정규호된 모서리 좌표들로 인코딩되어 있다. 총 24D 벡터 표현은 oriented 3D box를 나타내는 데 중복되지만, 저자는 이 인코딩 접근 방식이 축 정렬 3D box로 회귀하는 중심 및 크기 인코딩 접근 방식보다 더 잘 작동한다는 것을 발견했다고 한다. 이 논문의 모델에서, 객체 방향은 예측된 3D box 모서리로부터 계산될 수 있다. 우리는 객체 카테고리 및 oriented 3D boxes를 예측하는 multi-task loss를 사용한다.</p><p>제안 네트워크에서 category loss는 cross-entropy를 사용하고, 3D box loss는 smooth <em>l1</em>을 사용했다. 훈련할 때 positive/negative ROI들은 조감도 boxes의 IOU 중첩을 기준으로 결정된다. 조감도 IOU 중첩이 0.5 이상일 때는 3D 제안이 positive가 되고, 나머지는 negative가 된다. 추론에서는 3D bounding box regression이후에 3D boxes에 NMS를 적용한다. 조감도에 대한 3D box들을 투영함으로써 IOU 중첩을 계산한다. 조감도에서 같은 공간을 차지할 수 없는 객체들을 가리키는 불필요한 박스를 제거하기 위해 0.05 IOU threshold를 사용한다.</p><p><br /></p><ul><li>Network Regularization</ul><p>저자는 지역 기반의 결합 네트워크를 정규화하기 위해 <code class="language-plaintext highlighter-rouge">drop-path training</code>과 <code class="language-plaintext highlighter-rouge">auxiliary losses</code>, 두 가지 접근 방법을 사용했다. 각 반복마다 무작위로 50% 확률로 <strong>global drop-path</strong>와 <strong>local drop-path</strong>를 무작위로 선택한다. 만약 global drop path가 선택되면 동일한 확률로 3개의 뷰안에서 1개의 단일 뷰를 선택한다. 반대로 local drop-path가 선택되면, 각 결합 노드의 입력 경로는 50% 확률로 무작위로 drop된다. 적어도 1개의 입력 경로는 유지되게 한다. 그리고, 각 뷰의 표현 능력을 더욱 강화하기 위해 네트워크에 보조 경로와 손실을 추가한다.</p><p><br /></p><p><img data-src="/assets/img/autodriving/MV3D/fig4.png" data-proofer-ignore></p><p>위의 그림에서 볼 수 있듯이, 보조 경로는 메인 네트워크와 같은 수의 layer를 가지고 있다. 보조 경로의 각 layer는 메인 네트워크의 연관된 층의 가중치를 공유한다. 그리고 동일하게 각 보조경로를 역전파하기 위해 classification loss와 3D box regression loss를 더한 multi-task loss를 사용한다. 보조 loss을 포함한 모든 loss는 동일하게 가중치를 부여한다. 보조 경로들은 추론에서는 삭제된다.</p><p><br /></p><h2 id="34-implementation">3.4 Implementation <a href="#34-implementation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Network Architecture</ul><p>다중 뷰 네트워크에서 각 뷰는 같은 아키텍처를 가진다. backbone 네트워크는 16-layer VGGNet으로 만들었고, 몇가지를 모델에 맞게 수정했다. 수정한 내용은 다음과 같다.</p><ol><li>채널을 원래 네트워크의 반으로 줄였다.<li>초소형 객체를 다루기 위해 특징 근사를 사용하여 고해상도 특징 맵을 얻었다. 특히, 우리는 마지막 convolution 특징 맵과 3D porposal 네트워크 사이에 2배 이선형 업샘플링 layer를 삽입했다. 또한, BV/FV/RGB의 ROI pooling layer 앞에 4x/4x/2x 업샘플링 layer도 삽입했다.<li>VGG 네트워크 안에 있던 4번째 pooling 작업을 제거하여, convolution 부분에서 8배 다운샘플링을 진행한다.<li>다중 뷰 결합 네트워크에서 원래는 FC 6 layer이나 FC 7 layer 였던 FC에 1개 FC layer를 추가했다.</ol><p>imageNet으로 pretrained된 VGG16 네트워크의 가중치를 샘플링하여 파라미터를 초기화했다. 우리의 네트워크는 3개의 브런치를 가짐에도 불구하고 파라미터의 수는 VGG16 네트워크의 약 75%이다. 또, 1개의 이미지에 대한 네트워크의 추론 시간은 Titan X GPU에서 약 0.36s정도였다.</p><p><br /></p><ul><li>Input Representation</ul><p>FV(front view)안에서의 객체 정보(annotations)가 제공되는 KITTI의 경우, (0, 70.4) X (-40, 40) meters의 범위 안의 point cloud를 사용했고, 이미지 평면에 투영했을 때 이미지 경계를 넘어가는 point들은 제거했다. 조감도에서 각 해상도는 0.1m로 나누었기 때문에, 조감도 입력은 704x800의 입력사이즈를 가진다. KITTI는 64-beam Velodyne 레이저 스캐너를 사용하기 때문에 우리는 64x512 map의 FV point를 얻는다. RGB 이미지는 가장 짧은 크기가 500 사이즈가 되도록 업스케일링한다.</p><p><br /></p><ul><li>Training</ul><p>네트워크는 end-to-end로 훈련되고, 각 mini-batch마다 1개의 이미지와 128개의 ROI 샘플링하여 ROI의 25%가 positive로 유지되도록 한다. 그리고, iteration = 100K, learning rate = 0.001의 SGD를 사용하여 훈련한다. 그리고 나서 다른 훈련에서는 iteration = 20K, learning rate = 0.0001로 줄인다.</p><p><br /></p><h1 id="4-experiments">4. Experiments</h1><p>KITTI object detection benchmark에서 MV3D 네트워크를 평가했다. 그 데이터셋은 트레이닝을 위한 7481개의 이미지와 테스트에 사용될 7581개 이미지로 구성되어 있다. 테스트에서는 2D detection만 평가하기에 training data를 1:1 비율로 training set과 validation set으로 나눈다. 그리고, validation set에서 3D box 평가를 구성했다. KITTI가 충분한 자동차 인스턴스를 제공하기 때문에, 우리는 자동차에만 초점을 맞춰 실험을 진행했다. KITTI 셋팅에 따라 easy, moderate, hard에 대한 어려움 정도를 세가지로 나눠 평가를 진행할 것이다.</p><ul><li>Metrics</ul><p>metrics에 따라 3D box recall을 사용한 3D object proposals를 평가한다. 두 직사각형의 IOU 중첩을 계산하는 2D box recall와 달리, 여기서는 두 직육면체의 IOU 중첩을 계산한다. 직육면체는 축과 평행하는 것이 아니라 3D boxes의 방향을 나타낸다. 평가에서는 3D IOU threshold를 0.25와 0.5로 설정한다. 최종적인 3D detection 결과를 위해, <code class="language-plaintext highlighter-rouge">3D localization</code>과 <code class="language-plaintext highlighter-rouge">3D bounding box detection</code>의 정확성을 측정하기 위해 두 방법을 사용한다. 3D localization에서는 방향성을 가진 조감도 box를 얻기 위해 지면에 3D boxes를 투영한다. 그런 다음 조감도 boxes에 대한 APloc(Average Precision)을 계산한다. 3D bounding box detection을 위해 저자는 full 3D bounding boxes를 평가하기 위한 방법인 AP3D(Average Precision)을 사용한다. 조감도 boxes와 3D boxes 둘 다 방향성이 있기 때문에, 객체 방향은 이 두 가지 지표에서 고려된다. 이미지 평면에 대한 3D boxes를 투영함으로써 2D detection의 수행을 평가한다. AP2D(Average Precision)은 측정 기준으로도 사용된다. KITTI 규칙에 따라 2D boxes는 IOU threshold를 0.7로 설정한다.</p><p><br /></p><ul><li>Baselines</ul><p>이 작업이 3D Object Detection을 목표로 하고 있기 때문에, LIDAR 기반의 방법들인 VeloFCN, Vote3Deep, Vote3D뿐만 아니라 이미지 기반의 방법들인 3DOP와 Mono3D들과도 비교했다. 공정한 비교를 위해 조감도와 전면도(BV+FV)를 입력으로 사용하는 순수 LIDAR 기반의 모델과 LIDAR과 RGB 데이터(BV+FV+RGB)를 결합한 멀티모달에만 초점을 맞췄다. 3D box 평가에서는 validation set이 제공되는 VeloFCN, 3DOP, Mono3D와 비교했다. 공개적으로 사용할 수 있는 결과가 없는 Vote3Deep과 Voto3D에서는 테스트 셋에서 2D detection만을 비교했다.</p><p><br /></p><ul><li>3D Proposal Recall</ul><p><img data-src="/assets/img/autodriving/MV3D/fig5.png" data-proofer-ignore></p><p>위 그림은 3D box recall을 나타낸 것이다. 300개의 제안을 사용하여 IOU 임계값의 함수로서 recall을 구성한다. 또한, 위 그림에서 볼 수 있듯이, 이 논문의 접근 방식은 모든 threshold에서 3DOP와 Mono3D를 엄청나게 능가했다. 그리고 IOU threshold가 0.25와 0.5아래인 제안들의 갯수에 대한 함수로 3D recall을 볼 수 있다. 이 논문의 방식은 0.25 IOU threshold에서 99.1% recall을, 0.5 IOU threshold에서 91% recall을 얻어냈다. 대조적으로 3DOP에서의 0.5 IOU threshold에 대한 최대 recall은 73.9%를 얻는다고 한다. recall이 크다는 것은 이미지 기반 방법에 비해 LIDAR 기반 접근 방식이 더 좋다는 것을 의미한다.</p><p><br /></p><ul><li>3D Localization</ul><p>3D localization 평가를 위해 0.5와 0.7 IOU threshold을 사용한다.</p><p><img data-src="/assets/img/autodriving/MV3D/table1.png" data-proofer-ignore></p><p>KITTI validation set에서 APloc를 table 1에서 할 수 있다. 모든 LIDAR 기반 접근 방식은 스테레오 기반 방식인 3DOP 및 monocular 방식인 Mono3D보다 우수했다. LIDAR 기반 접근 방식중에서 BV+FV 방법은 0.5 IOU threshold에서 최대 25% APloc인 VeloFCN을 능가했다. IOU를 0.7로 기준을 잡으면, 이 모델의 성능은 더 높아지고, moderate와 hard 환경에서 최대 45% 더 높은 APloc를 달성할 수 있었다. RGB images와 결합하면, 성능은 더 개선된다. 아래 그림에서 몇 가지 사례의 localization result를 시각화했다.</p><p><img data-src="/assets/img/autodriving/MV3D/fig6.png" data-proofer-ignore></p><p><br /></p><ul><li>3D Object Detection</ul><p>3D 중첩에 대해 설명하자면 0.5 3D IOU threshold와 LIDAR 기반의 방법들에 대해 0.7 IOU threshold에 초점을 맞췄다. 이러한 threshold들은 이미지 기반 방법에는 다소 엄격하기 때문에, 0.25 IOU threshold도 평가에 사용했다.</p><p><img data-src="/assets/img/autodriving/MV3D/table2.png" data-proofer-ignore></p><p>Table2에서 볼 수 있듯이, 우리의 BV+FV 방법은 moderate setting에서 86.75%를 달성하는 0.5 IOU threshold를 적용할 때 VeloFCN보다 AP3D를 30% 더 높게 얻을 수 있다. 0.7 IOU threshold를 사용할 때, multimodal 접근 방법은 easy data에서 71.29%의 AP3D를 달성했다. moderate 셋팅에서는 0.25 IOU threshold를 사용한 3DOP에 의해 달성될 수 있는 최대 AP3D는 68.82%인 반면, 이 논문의 접근 방식은 0.5 IOU threshold를 사용할 때 89.05% AP3D를 달성했다.</p><p><br /></p><ul><li>Ablation Studies</ul><p><img data-src="/assets/img/autodriving/MV3D/table3.png" data-proofer-ignore></p><p>먼저 초기/후기 결합 방식과 이 논문의 deep fusion 방식을 비교해보자. 결합 연산은 초기/후기 융합 방식에서 연결과 함께 인스턴스화된다. table 3에서 볼 수 있듯이 초기/후기 융합 접근은 매우 단순하다. 근사 loss의 사용없이, deep fusion 방법은 초기/후기 결합 방식보다 0.5% 더 향상된 결과를 얻을 수 있다. 근사 loss를 추가하면 1% 더 향상된다.</p><p><img data-src="/assets/img/autodriving/MV3D/table4.png" data-proofer-ignore></p><p>table4에서 볼 수 있듯이, 입력으로 단일 뷰만 사용한다면 조감도, 즉 bird’s eye view 특징이 가장 높았고, front view가 가장 낮았다. 두개의 뷰를 섞는다면 단일 뷰보다 모든 뷰들이 더 높게 측정되었다. 이것은 서로 다른 뷰들의 특징들이 성호 보완적이라는 가정이 정당화된다. 세가지 뷰를 모두 결합하면 최상의 결과를 얻을 수 있다.</p><p><br /></p><ul><li>2D Object Detection</ul><p>우리는 마지막으로 KITTI 테스트 셋에서 2D detection 수행을 평가했다. 결과는 다음과 같다.</p><p><img data-src="/assets/img/autodriving/MV3D/table5.png" data-proofer-ignore></p><p>LIDAR 기반의 방법들중에서 BV+FV 방식이 hard 셋팅에서 최근 제안된 Vote3Deep에 비해 AP2D가 14.93% 더 높게 결과가 나왔다. 전체적으로 이미지 기반의 방법들은 보통 2D detection에서 LIDAR 기반의 방법들보다 더 좋다. 이는 이미지 기반의 방법들이 3D boxes를 최적화하는 LIDAR 기반의 방법들과 달리 2D boxes를 직접적으로 최적화하기 때문이다. 하지만 이 논문의 방법이 3D boxes를 최적화함에도 불구하고 최첨단 2D detection 방법들보다 더 경쟁력있는 결과를 얻었다.</p><p><br /></p><ul><li>Qualitative Results</ul><p>fig6에서 볼 수 있듯이, 접근 방식은 스테레오 기반의 방법인 3DOP과 LIDAR 기반의 방법인 VeloFCN에 비해 더 정확한 3D 위치, 사이즈, 객체의 방향성을 얻는다.</p><p><br /></p><h1 id="5-conclusion">5. Conclusion</h1><p>그래서 최종적으로 로드 뷰에서 3D object detection에 대한 멀티뷰 센서퓨전 모델을 제안한다. 우리의 모델은 LIDAR point cloud와 images 둘 다 강점을 보인다. 3D 제안을 생성하고 특징 추출을 위해 여러 뷰에 투영하여 서로 다른 양식을 정렬한다. region-based fusion network는 방향성을 가진 3D box regression을 하고, 다중 뷰 정보를 깊게 융합한다. 우리의 접근 방식은 KITTI benchmark에서 3D localization과 3D detection의 task에 대한 현존하는 LIDAR 기반과 image 기반 방법들을 엄청나게 능가한다. 3D detection으로부터 얻어진 2D box 결과들은 최첨단 2D detection 방법들과 비교해도 경쟁력이 있다.</p><p><br /></p><h1 id="reference">Reference</h1><ul><li><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf</a></ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/multiview/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/multiview.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/review/'>Review</a>, <a href='/categories/autonomous-driving/'>Autonomous Driving</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/autonomous-driving/" class="post-tag no-text-decoration" >Autonomous Driving</a> <a href="/tags/mv3d/" class="post-tag no-text-decoration" >MV3D</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/multiview/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/multiview/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/multiview/&amp;text=[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/multiview/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/pointcnn2/"><div class="card-body"> <em class="timeago small" date="2022-01-23 13:00:00 +0900" >Jan 23, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 코드 구현] PointCNN: Convolution on X-transformed points</h3><div class="text-muted small"><p> pointCNN 깃허브에 올라와 있는 코드를 직접 실행하고 리뷰한 내용입니다. 모든 코드는 colab에서 진행하였습니다. 논문에 대한 리뷰가 궁금하신 분들은 이 링크를 참고해주세요. PointCNN 깃허브 주소: https://github.com/yangyanli/PointCNN Classification ModelNet40 먼저 분류 모...</p></div></div></a></div><div class="card"> <a href="/posts/monocular/"><div class="card-body"> <em class="timeago small" date="2022-01-25 23:53:00 +0900" >Jan 25, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] Monocular 3D Object Detection for Autonomous Driving</h3><div class="text-muted small"><p> Monocular 3D Object Detection for Autonomous Driving 논문에 대한 리뷰입니다. 이 논문은 2016 CVPR에 투고된 논문이며 약 669회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstract 본 논문은 자율주행에서 단안 ...</p></div></div></a></div><div class="card"> <a href="/posts/pointrcnn/"><div class="card-body"> <em class="timeago small" date="2022-02-02 01:17:00 +0900" >Feb 2, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</h3><div class="text-muted small"><p> PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstrac...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/codingtestkakao/" class="btn btn-outline-primary" prompt="Older"><p>Coding Test[Python] - 카카오 및 코딩테스트 기출문제</p></a> <a href="/posts/voxelnet/" class="btn btn-outline-primary" prompt="Newer"><p>[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>