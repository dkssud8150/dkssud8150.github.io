<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 9. Hidden Markov Model" /><meta property="og:description" content="Chapter 9. Hidden Markov Model" /><link rel="canonical" href="https://dkssud8150.github.io/posts/kooc_week9/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/kooc_week9/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-11-09T22:10:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-11-12T00:27:51+09:00","datePublished":"2022-11-09T22:10:00+09:00","description":"Chapter 9. Hidden Markov Model","headline":"[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/kooc_week9/"},"url":"https://dkssud8150.github.io/posts/kooc_week9/"}</script><title>[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-11-09 22:10:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Wed, Nov 9, 2022, 10:10 PM +0900" >Nov 9, 2022</em> </span> <span> Updated <em class="timeago" date="2022-11-12 00:27:51 +0900 " data-toggle="tooltip" data-placement="bottom" title="Sat, Nov 12, 2022, 12:27 AM +0900" >Nov 12, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4914 words"> <em>27 min</em> read</span></div></div></div><div class="post-content"><h1 id="chapter-9-hidden-markov-model">Chapter 9. Hidden Markov Model</h1><ul><li>목차</ul><ol><li>Hidden Markov model<ul><li>static clustering to the dynamic clustering<li>difference of the graphical model</ul><li>three major questions of HMM<ul><li>evaluation question<li>decoding question<li>learning question</ul><li>link some method<ul><li>forward-backward algorithm to the message passing<li>baum-welch algorithm to the EM algorithm.</ul></ol><p> </p><p> </p><h2 id="9-1-hidden-markov-model">9-1. Hidden Markov Model <a href="#9-1-hidden-markov-model" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>이때까지는 시간에 관계없는 상황에서의 데이터들이었다. 그러나 만약 시간에 따라 변화하는 데이터에 대해서는 어떻게 다룰 수 있는지 살펴보고자 한다.</p><p>시간에 따라 변화한다는 것은 latent variable이 연관되어 있다는 뜻이고, 이를 그래프화하면 다음과 같다.</p><p><img data-src="/assets/img/kooc/week910/temporal_data.png" data-proofer-ignore></p><p>왼쪽 그래프는 이때까지 다뤄왔던 각 데이터들이 독립적인 관계에 대한 것이고, 오른쪽 그래프가 시간이라는 latent variable이 추가되어 각 포인트마다 관련이 되어 있는 상황이다.</p><p> </p><p><img data-src="/assets/img/kooc/week910/hidden_markov_model.png" data-proofer-ignore></p><p>관측값 x는 이산적일수도, 연속적일수도 있다. 이산적이라면, x는 특정 값을 가지고, 연속적이라면 확률 분포를 가진다. 이번 강의에서는 이산적인 경우에 대해서만 다뤄보고자 한다.</p><p>시간이 1~T까지 흘러감에 따라 x는 x1~xT로 존재한다.</p><p> </p><p>latent variable, z의 경우도 이산적일수도 있고, 연속적일수도 있다. latent variable이 연속적인 경우에 사용하는 기법을 <strong>Kalman filter</strong> 이라 한다. 따라서 이번 강의에서는 z도 이산적인 경우만 다룬다. z는 vector로서 k개의 elements를 가진다.</p><p> </p><p><img data-src="/assets/img/kooc/week910/hidden_markov_model2.png" data-proofer-ignore></p><p>initial probability, P(z1) 은 π1~πk 에 대한 multinomial distribution에 의해 생성된다.</p><p>그리고 HMM(Hidden Markov Model)에는 2가지 확률이 더 존재한다.</p><ol><li>Transition Probabilities</ol><p>z1에서 z2, z2에서 z3등과 같이 $ z_{i} $에서 $z_{j}$로 가는 관계를 나타내는 확률이다. 수식적으로 보면, t-1에서의 z에 대한 z_t의 condition probability를 나타내고, 이는 a라는 변수에 대한 multinomial distribution로 나타내어진다.</p><p>이 때, z_t=1이라는 특정 상황에 대한 확률을 $ a_{i,j} $ 로 나타낼 수 있다. i와 j는 z의 k개의 elements에서 i번째 elements와 j번째 elements에 대한 index를 나타낸다.</p><p> </p><ol><li>Emission Probabilities</ol><p>이는 z에서의 x에 대한 관계를 나타낸다. 이 또한, x_t=1이라는 특정 상황에 대한 확률을 $ b_{i,j} $ 로 나타낼 수 있다.</p><p> </p><p> </p><h2 id="9-2-forward-backward-probability">9-2. Forward-Backward Probability <a href="#9-2-forward-backward-probability" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>HMM의 과정을 자세하게 살펴보기에 앞서, 알고 가야할 개념들이 있다.</p><p> </p><h3 id="many-questions">Many Questions <a href="#many-questions" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>일단, ML(machine Learning)에서 몇가지 과제가 있는데, 이는 주어진 변수들에 따라 달라진다.</p><ol><li>Evaluation question<ul><li>π, a, b, X 가 주어진 경우<li>즉, 관측값과 latent factor에 대한 정보도 모두 가지고 있는 상태<li>P(X|Model,π,a,b)를 찾는 것이 목적<li>π,a,b,X가 주어졌을 때 학습된 모들에 의해 특정 X가 관측될 확률을 구함.</ul></ol><p> </p><ol><li>Decoding question<ul><li>π, a, b, X 가 주어진 경우<li>$ argmax_Z P(Z|X,M,π,a,b) $<li>π,a,b,X가 주어졌을 때, latent factor, Z의 확률 중 가장 확률이 높은 Z를 구하는 것, 즉 Z를 학습시키면서 가장 최적의 Z를 찾음.</ul></ol><p> </p><ol><li>Learning question<ul><li>X만 주어진 경우<li>$ argmax_{π, a, b} P(X|M,π,a,b) $<li>관측값만 가진 상태에서 특정 X가 관측될 확률을 구함.</ul></ol><p>decoding의 경우 관측값과 GT가 주어지므로 Supervised learning과 유사하고, learning 의 경우 관측값만 주어진 상황에서 확률을 구해야 하는 unsupervised learning과 유사하다.</p><p> </p><p> </p><ul><li>loaded dice</ul><p><img data-src="/assets/img/kooc/week910/loaded_dice.png" data-proofer-ignore></p><p>간단한 예시를 들어보자. 기울어진 주사위가 있어서 6이 나올 확률이 1/2이고 나머지는 모두 1/10 확률을 가진다고 하자. 일반적인 주사위는 모두 동일하게 1/6의 확률을 가진다.</p><p>latent factor로서, 기울어진 주사위(L)을 사용할지 평범한 주사위(F)를 사용할지에 대한 확률이 있고, 처음 L을 사용할 확률을 0.5, L을 사용한 후에 다시 L을 사용할 확률을 0.7, F를 사용한 후 F을 사용할 확률을 0.5라 가정한다.</p><p>X와 Z가 주어진 상황에서 dataset을 훈련시킬 때, 간편한 계산을 위해 Joint probability를 사용해볼 것이다. baysian network를 생각했을 때, P(X,Z)는 다음과 같이 풀어줄 수 있다.</p>\[P(X,Z) = P(x_1,...,x_t,z_1,...,z_t) = P(z_1)P(x_1|z_1)P(z_2|z_1)P(x_2|z_2)P(z_3|z_2)P(x_3|z_3)\]<p>아무것도 가정하지 않고도 joint와 conditinoal probability의 정의에 따라 풀어줄 수 있다.</p><p>앞서 배운 transition과 emission probability를 적용하면 다음과 같다.</p><p>$ P(X,Z) = \pi_{z_1} b_{x1=1,z_1=1} a_{z_1=1,z_2=1} \dots $</p><p> </p><p> </p><p>그렇다면, P(166,LLL) 과 P(166,FFF) 에 대한 값을 직접 구할 수 있다. 처음 L을 사용할 확률은 0.5, L을 사용했을 때 1이 나올 확률은 0.1, L을 사용했을 때 6이 나올 확률은 0.5이고, L을 사용한 후에 다시 L을 사용할 확률은 0.7이었으므로 다음과 같이 계산한다.</p><p>P(166,LLL) = 1/2 x 1/10 x 7/10 x 1/2 x 7/10 x 1/2 = 0.0061</p><p> </p><p>P(166,FFF) = 1/2 x 1/6 x 1/2 x 1/6 x 1/2 x 1/6 = 5.7870e - 04</p><p>F를 사용했을 때 1이 나올 확률과 6이 나올 확률은 동일하게 1/6이고 처음 F를 사용할 확률은 1/2, F에서 F를 다시 사용할 확률은 0.5였다.</p><p> </p><p>이처럼 직접 구해줄 수는 있지만, 횟수가 늘어남에 따라 경우의 수는 기하급수적으로 증가하므로 큰 횟수에 대해서는 직접 구해주기 힘들다.</p><p> </p><p> </p><p>이전에 다뤘던 marginalization 기법을 사용하고자 한다.</p><p>$ P(X|\theta) = \sum_Z P(X,Z|\theta) $</p><p> </p><p>HMM에서는 θ가 아닌, π,a,b이므로 이에 대해 다시 정의한다.</p><p>$ P(X|\pi,a,b) = \sum_Z P(X,Z|\pi,a,b) $</p><p> </p><p>$ P(X) = \sum_Z P(X,Z) = \sum_{z_1} \dots \sum_{z_t} P(x_1,…,x_t,z_1,…,z_t) = \sum_{z_1} \dots \sum_{z_t} \pi_{z_1} \prod_{t=2}^T a_{z_{t-1},z_t} \prod_{t=1}^T b_{z_t,x_t}$</p><p>이는 너무 많은 연산이 들어가므로, 반복적인 연산을 피하기 위해 하나의 시간에 대해서만 계산해보자. 특정 시간 t에 대해 구하기 위해서 t-1에서의 z를 marginalization 처리해준다.</p><p>$ P(x_1,…,x_t,z_t^k=1) = \sum_{z_{t-1}} P(x_1,…,x_{t-1},x_t,z_{t=1}, z_t^k=1) = \sum_{z_{t-1}} P(x_1,…,x_{t-1},z_{t-1})P(z_t^k=1|z_{t-1})P(x_t|z_t^k=1) $</p><p>이 때, z_t에 대한 확률을 구할 때는 x와는 모두 독립적이고, x_t에 대한 확률을 구할 때는 x_1~t-1에 대한 값들과 z_t-1에 대해서는 독립적이므로 지워줄 수 있다.</p><p>지워주고 나서 시그마에 관련 없는 값들은 밖으로 빼내고 나면, 이전에 구했던 transition과 emission probabilities를 적용할 수 있다.</p><p>적용하고 나면</p><p>$ P(x_1,…,x_t,z_t^k=1) = \alpha_t^k = b_{k,x_t} \sum_i \alpha_{t-1}^i a_{i,k} $</p><p> </p><p>즉, 시간 t에 대한 확률은 시간 t-1에서의 확률에 값을 곱한 것과 동일한 재귀적인 관계가 형성된다.</p><p> </p><p> </p><h3 id="dynamic-programming">Dynamic Programming <a href="#dynamic-programming" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>이 때, dynamic programming이라는 기법이 등장한다. 이는 특정 값을 연산하는 데 있어서 겹치는 값들에 대해서는 재사용(recurrence)하는 방식이다. 예를 들어, 피보나치 수열에서 n=4를 구하기 위해서는 F(3)과 F(2)를 구해야 하고, F(3)을 구하기 위해서는 F(2)와 F(1)을 구해야 한다.</p><p>이런 Top-down 방식으로 연산하는 것을 <code class="language-plaintext highlighter-rouge">Recursion</code>이라 하고, 이 recursion의 단점은 같은 값을 반복적으로 연산하는 것이다.</p><p>반대로, 아래에서부터 연산을 시작하여 F(4)가 될 때까지 연산하는 방식이 dynamic programming이다. 이런 방식으로 진행하게 되면, 먼저 F(0)을 계산하고 값을 저장한다. 그리고 F(1)을 계산하고 저장하고, F(2)를 연산할 때 저장해둔 F(0)과 F(1)을 가져와 F(2)를 계산한다. 이렇게 연산을 이어나가다가 원래 찾고자 했던 값인 F(4)를 연산하고 나면 과정을 종료한다.</p><p>dynamic programming의 장점은 반복적인 연산을 하지 않는다는 것이다. 이와 같이 반복적인 연산을 할 때 값을 저장하고 불러오는 과정을 Memoization이라 한다.</p><p> </p><p> </p><h3 id="forward-probability-calculation">Forward Probability Calculation <a href="#forward-probability-calculation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week910/forward_probability.png" data-proofer-ignore></p><p>만약 $ \alpha_t^k $ 를 안다면 Z를 알아낼 필요없이 P(X)를 계산할 수 있다. 따라서 이 $ \alpha_t^k $ 를 계산하기 위해 Forward Probability를 사용한다.</p><ol><li><p>Initialize</p><li>Iterate until time T<ul><li>$ \alpha_t^k = b_{k,x_t} \sum_i \alpha_{t-1}^i a_{i,k} $</ul><li>Return $ \sum_i \alpha_T^i $<ul><li>$ \sum_i \alpha_t^i = \sum_i P(x_1,…,x_t,z_t^i = 1) = P(x_1,…,x_t) $</ul></ol><p> </p><p>Forward probablity의 한계로는 t가 전체 데이터셋에 대한 T일 필요가 없다는 점이다. 예를 들어 t가 2라고 한다면, z3에 도달하지 않은 채로 x1,x2,z2에 대해 모델링된다. 그렇다 하더라도 문제가 발생하지 않는다. 따라서 전체 데이터셋인 X에 대한 확률도 계산해줘야 한다. 그를 위한 방법이 <strong>Backward probability calculation</strong> 이다.</p><p> </p><p> </p><h3 id="backward-probability-calculation">Backward Probability Calculation <a href="#backward-probability-calculation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week910/backward_probability.png" data-proofer-ignore></p><p>이 때는 P(X,z_t^k=1) 이 아닌 P(z_t^k=1|X) 를 계산한다. 전체 X에 대해 연산하기 위해 앞서 구했던 x_t에서 x_t+1~x_T를 추가한다.</p><p>$ P(z_t^k=1,X) = P(x_1,…,x_t,z_t^k=1,x_{t+1},…,x_T) = P(x_1,…,x_t,z_t^k=1)P(x_{t+1},…,x_T|x_1,…,x_t,z_t^k=1) $</p><p>이 때도 x_t+1~x_T과 x_1~x_t는 독립적이므로 제거할 수 있다.</p><p>$ P(x_1,…,x_t,z_t^k=1)P(x_{t+1},…,x_T|z_t^k=1) $</p><p>P(x_1,…,x_t,z_t^k=1)는 forward 계산할 때 α_t^k로 지정했다. 그리고 P(x_{t+1},…,x_T|z_t^k=1) 를 β_t^k 로 정의한다.</p><p> </p><p>$ P(x_{t+1},…,x_T|z_t^k=1) = \sum_{z_{t+1}} P(z_{t+1},x_{t+1},…,x_T | z_t^k=1) = \sum_i P(z_{t+1}^i =1 | z_t^k = 1) P(x_{t+1} | z_{t+1}^i = 1, z_t^k=1) P(x_{t+2},…,x_T|x_{t+1},z_{t+1}^i=1,z_{t}^k=1) $</p><p>이 떄, x_t+1에 대해 구할 때 z_t+1을 알고 있다면, z_t과는 독립적이 되어 제거되고, 그 뒤에 x_t+1도 x_t+2~x_T와는 독립적이므로 제거할 수 있다.</p><p>이렇게 다 제거하고 나면 이전에 구했던 a,b,β로 치환할 수 있다.</p><p>$ P(x_{t+1},…,x_T|z_t^k=1) = \beta_t^k = \sum_i a_{k,i}b_{i,x_t} \beta_{t+1}^i $</p><p>이 때, β_t+1 의 size는 t+1에서 T로 점점 다가가므로 감소된다.</p><p> </p><p> </p><h2 id="9-3-viterbi-decoding">9-3. Viterbi Decoding <a href="#9-3-viterbi-decoding" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>decoding question에서 가장 많이 사용되는 기법이 Viterbi decoding이다.</p><ul><li>$ \alpha_t^k $ : forward probability<li>$ \beta_t^k $ : backward probability</ul><p>$ P(z_t^k=1,X) = \alpha_t^k \beta_t^k = (b_{j,x_t}\sum_i \alpha_{t-1}^i a_{i,k}) \times (\sum_i a_{k,i}b_{i,x_t} \beta_{t+1}^i) $</p><p>이렇게 특정 time t에 대한 joint probability를 계산할 수 있고, joint를 통해 conditional probability도 계산할 수 있다. 그러나 이는 특정 시간 t인데, 우리가 구하고자 하는 것은 전체 sequence를 보고 전체 sequence에 대한 latent variable이다.</p><p>그래서 z_t가 아닌 전체 latent variable인 Z에 대해 구해보고자 하고, 이 과정을 decoding question이라 할 수 있다.</p><p> </p><p><img data-src="/assets/img/kooc/week910/viterbi_decoding.png" data-proofer-ignore></p><p>$ P(x_1,…,x_{t-1},z_1,…,z_{t-1},x_t,z_t^k=1) $ 를 $V_t^k $라 정의하고, 이는 t-1까지의 가장 확률이 높은 Latent vector을 나타낸다.</p><p>즉, estimation하고자 하는 latent factor(z_t)가 1이라는 클러스터에 속하고 있는 상황에서 $ x_1,…,x_{t-1} $과 $ z_1,…,z_{t-1}, x_t, z_t^k=1 $ 이 관측될 확률이 최대가 되는 z1~z_t-1을 최적화하겠다는 의미이다.</p><p>t-1까지의 과정을 dynamic programming을 통해 구한다.</p><p> </p><p>V_t^k 의 joint probability를 condition probability로 나타내어 간편화한다.</p><p>$ V_t^k = max_{z_1,…,z_{t-1}} P(x_t,z_t^k=1|x_1,…,x_{t-1},z_1,…,z_{t-1})P(x_1,…,x_{t-1},z_1,…,z_{t-1}) $</p><p>이 때, x_t와 z_t^k=1에 대해 구할 때, x1,…,x_t-1,z1,…,z_t-2와는 독립적이므로 제거된다. 이에 따라 maximize하는 값도 앞에는 z1~z_t-1이 아닌 z_t-1로 변환되고, 뒤에는 V_t^k 식과 동일한 형식이므로 z1~z_t-2로 바뀌게 된다.</p><p>이 때, time t-1의 latent factor가 i라는 라는 클러스터에 속해진다고 가정을 하게되면, 다음과 같이 나타내진다.</p><p>$ max_{i \in t-1} P(x_t,z_t^k=1|z_{t-1}^i = 1)V_{t-1}^i = max_{i \in t-1} P(x_t|z_t^k=1)P(z_{t-1}^i = 1)V_{t-1}^i $</p><p>forward, backward probability 인 a와 b로 나타내면</p><p>$ b_{k,x_t} max_{i \in z_{t-1}} a_{i,k} V_{t-1}^i $</p><p> </p><p>이 식을 통해 t=T일 때까지 반복하여 V_(t-1)^i 를 업데이트한다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week910/viterbi_decoding_example.png" data-proofer-ignore></p><p>viterbi decoding에 대한 예제가 하나 있다. 제품을 생산하는 시간을 최적화하는 알고리즘으로, 각 원(station)안에 들어 있는 숫자는 프로세스 시간이고, assembly line이 2개 존재한다. 각 station에서는 같은 line으로 갈수도 있고, 다른 line으로 갈수도 있는데, 다른 line으로 갈 경우 추가 시간이 발생된다.</p><p>이러한 경우 최소한의 시간이 드는 루트는 어떻게 되는지 확인하는 과정이다.</p><p>아래의 두 개의 테이블에서 첫번째 table은 각 시간마다 line별 걸린 시간을 나타낸 것이고, 오른쪽 table은 해당 시간에서 이전의 line이 어디였는지를 나타내는 값이다. 해당 시간에서 소요된 시간이 더 적은 line에서 이동된 것으로서, 이 숫자들을 역추적해서 이으면 최적의 루트가 된다.</p><p>즉, 마지막이 line1에서 끝났으므로 1을 선택하고, t=6에서 2가 되어 있으므로 t=5에서는 line2가 소요된 시간이 적게 걸렸다고 볼 수 있다. t=5에서는 L2에 2로 되어 있으므로 t=4에서도 line2에 위치하는 것이 최적의 루트이다. 이렇게 따라가다보면, 빨간색 화살표의 경로처럼 된다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week910/viterbi_decoding_summary.png" data-proofer-ignore></p><p>이러한 경우를 확률적으로 계산해보자. 일단 먼저 확률을 계산하는 table과 trace에 대한 memoization table이 존재해야 한다. trace란 이전 시간에서 최적의 루트를 나타내는 값이다.</p><p> </p><ul><li>Viterbi Decoding Algorithm<ol><li>Initialize</ol><ul><li>$ V_1^k = b_{k,x_1} \pi_k $<li>π라는 초기값을 통해 V_1^k를 계산한다.<ol><li>Iterate until time T</ol><li>$ V_t^k = b_{k,x_t} max_{i \in z_{t-1}} a_{i,k} V_{t-1}^i $<li>$ trace_t^k = argmax_{i \in z_{t-1}} a_{i,k} V_{t-1}^i $<li>V_t^k 는 앞서 말한 확률 table에 대한 값이고, trace는 trace table에 대한 값이다.<ol><li>Return $ P(X,Z<em>) = max_k V_T^k, z_T^</em> = argmax_k V_T^k,z_{t-1}^* = trace_t^{z_t^*}$</ol></ul></ul><p> </p><p>viterbi decoding에도 한계가 존재한다. a,b가 확률이므로 데이터가 100개 200개와 같이 많은 경우 값이 매우 작아져서 0에 가까워진다.</p><p>따라서 이러한 곱셈 연산에 <strong>log</strong>를 씌워 곱셈을 덧셈으로 바꿔 연산을 하게 해야 한다.</p><p> </p><p>최종적으로 이러한 decoding algorithm을 통해 학습된 파라미터(π,a,b)와 X를 통해 새로운 sequence(데이터)에 대해 최적의 클러스터링을 할 수 있게 된다.</p><p>자연어 처리를 예로 들면, 모델을 학습하여 π,a,b를 얻은 후 새로운 데이터(문장)이 들어왔을 때, viterbi decoding 과정을 통해 pos-tagging(품사 태깅)을 할 수 있다.</p><p> </p><p> </p><h2 id="9-4-baum-welch-algorithm">9-4. Baum-Welch algorithm <a href="#9-4-baum-welch-algorithm" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>decoding알고리즘은 supervised learning에 해당되었다. π,a,b,X 를 모두 알고 있다는 가정하에 새로운 데이터에 대해 fitting하는 작업을 했다. 이제는 unsupervised learning, 즉 X만 주어진 상황에서 모델을 학습하는 동시에 새로운 데이터에 대해 fitting하는 작업을 해보고자 한다. 이를 수행하기 위한 방법이 <strong>Baum-Welch</strong> 알고리즘이다. 이 Baum-Welch 알고리즘은 이전에 배웠던 EM 알고리즘과 동작이 거의 동일하다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week910/baum-welch.png" data-proofer-ignore></p><p>clustering을 하는 데 있어서 HMM의 파라미터인 π,a,b는 Forward algorithm과 Viterbi algorithm(decoding)에서 사용된다.</p><p>그러나 π,a,b는 X와 Z를 알고 있다는 가정 하에 추론이 가능해진다. 그러나 현실에서는 Z를 관찰하는 것은 매우 어렵다. 이 Z를 추론하기 위해 학습을 시킨다면, annotation이 필요하게 된다.</p><p>만약 Z를 모르고, X만 관찰할 수 있다면 clsutering을 위해서는 Z와 π,a,b를 모두 찾아야 한다.</p><p> </p><p>파라미터들을 최적의 값으로 찾아내기 위해 <strong>EM algorith</strong>을 사용하고자 한다. EM 알고리즘의 전략은 다음과 같다.</p><ol><li>X에 대해 π,a,b를 최적화한다.<li>X,π,a,b에 대해 가장 확률이 높은 Z를 찾는다.<li>이를 반복적으로 작업하여 Z,π,a,b를 찾는다.</ol><p> </p><p><img data-src="/assets/img/kooc/week910/EM_algorithm.png" data-proofer-ignore></p><p>EM algorithm에 대해 조금 더 자세하게 살펴보자. EM 알고리즘의 순서는 다음과 같다.</p><ol><li>무작위 값으로 $ \theta^0 $ 를 초기화한다.<li>likelihood가 수렴이 될때까지 아래 과정을 반복한다.<li>Expectation step<ul><li>$ q^{t+1}(z) = argmax_q Q(\theta^t, q) = argmax_q L(\theta^t, q) = argmin_q KL(q||P(Z|X,\theta^t)) $<ul><li>q는 latent variable 할당에 대한 값으로, X와 θ^t가 주어졌을 때, Z를 할당하는 확률 그대로 q를 할당한다.</ul></ul><li>Maximization step<ul><li>$ θ^{t+1} = argmax_θ Q(θ, q^{t+1}) = argmax_θ L(θ, q^{t+1}) $<li>할당된 Z와 관측값 X에 대해 MLE를 수행하여 파라미터들을 최적화한다.</ul></ol><p> </p><p>이 과정을 HMM(Hidden Markov Model)에 적용하면 아래와 같게 된다.</p><ol><li>무작위 값으로 $ π^0, a^0, b^0 $ 를 초기화한다.<li>likelihood가 수렴될 때까지 아래 과정을 반복한다.<li>Expectation step<ul><li>$ q^{t+1}(z) = P(Z|X,π^t, a^t, b^t) $</ul><li>Maximization step<ul><li>$ π^{t+1}, a^{t+1}, b^{t+1} = argmax_{π,a,b} Q(π,a,b,q^{t+1}) $<ul><li>지난 주 KL divergence를 배울 때, Q에 대해 정의했다. ln함수는 concave에 해당하므로 최소값과의 차이이 H(q)와 함께 ln안에 있던 <code class="language-plaintext highlighter-rouge">∑</code>를 밖으로 빼냈다.<li>E는 expectation값 즉, 기대값을 의미하고, 이 기대값 q^t+1(Z)는 Expectation step에서 P(Z|X,π^t, a^t,b^t) 로 정의하기로 했으므로 파라미터의 업데이트 식은 다음과 같이 정의된다.<li>$ π^{t+1}, a^{t+1}, b^{t+1} = argmax_{π,a,b}E_{q^{t+1}(z)} lnP(X,Z|π,a,b) + H(q) $</ul></ul></ol><p>이 때, P(X,Z|π,a,b)는 joint를 condition probability으로 바꿈과 동시에 marginalization을 통해</p><p>$ P(X,Z|π,a,b) = \pi_{z_1} \prod_{t=2}^T a_{z_{t-1},z_t} \prod_{t=1}^T b_{z_t,x_t} = ln\pi_{z_1} + \sum_{t=2}^T lna_{z_{t-1},z_t} + \sum_{t=1}^T lnb_{z_t, x_t} = Q(π,a,b,q^{t+1}) $</p><p>로 정의된다.</p><p> </p><p><img data-src="/assets/img/kooc/week910/derivation_hmm.png" data-proofer-ignore></p><p>Q 함수를 최적화해야 하므로, 이에 대해 미분을 수행하는데, 제약조건이 하나 있다. π를 multinomial distribution을 따른다고 정의했으므로 $ \sum_i π_i = 1 $ 이다. 따라서 <em>lagrange method</em>를 사용하여 미분해야 한다.</p><p>$ L(π,a,b,q^{t+1}) = Q(π,a,b,q^{t+1}) - \lambda(\sum_{i=1}^k \pi_i - 1) - \sum_{i=1}^K \lambda_{a_i} (\sum_{j=1}^K a_{i,j} -1) - \sum_{i=1}^K \lambda_{b_i} (\sum_{j=1}^M b_{i,j} -1) $</p><p> </p><p>이 식에서 π에 대해 편미분을 수행해보자.</p><p>$ \frac{dL(π,a,b,q^{t+1})}{dπ_i} = \frac{d}{dπ_i} \sum_Z P(Z|X,π^t, a^t, b^t) lnπ_{z_1} - \lambda_π (\sum_{i=1}^K π_i - 1) = 0 $</p><p>이 식은 z_1이 i 일 때만 성립되므로, z_1 = i로 두어 π의 최대값을 찾는다.</p><p>$ \frac{P(z_1^i = 1| X,π^t, a^t, b^t)}{π_i} = \lambda_\pi $</p><p> </p><p>이 식을 바로 풀기는 어려우므로 L함수를 $ \lambda_\pi $ 에 대해 편미분을 수행한다.</p><p>$ \frac{d}{d\lambda_\pi} L(\pi,a,b,q^{t+1}) =\gt : \sum_{i=1}^K \pi_i = 1 $</p><p>즉, π_i를 i=1부터 K까지 모두 더하면 1이 된다는 것이다.</p><p>그렇다면 $ \lambda_\pi $ 는 분자에 있는 값을 1~K만큼 합한 것과 같고, 이것이 곧 $ π^{t+1} $ 이 된다.</p><p>이 프로세스를 동일하게 a와 b에 적용하면 우상단의 값들로 추출이 된다.</p><p> </p><p>Baum Welch algorithm은 결국 HMM에 대한 learning question이자 HMM을 위한 EM 알고리즘이다.</p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week9/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week9.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/kooc/'>kooc</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/kooc/" class="post-tag no-text-decoration" >kooc</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/kooc_week9/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/kooc_week9/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/kooc_week9/&amp;text=[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/kooc_week9/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/kooc_start/"><div class="card-body"> <em class="timeago small" date="2022-09-27 00:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 - Abstract</h3><div class="text-muted small"><p> KOOC란? KOOC(KAIST Massive Open Online Course)는 KAIST 무료 온라인 강좌를 말하는 것으로, KAIST 교수님들의 강의를 무료로 수강할 수 있는 플랫폼입니다. KOOC에는 다양한 주제로 강좌가 열려 있고, 온라인으로 진행되는 강좌이므로 언제 어디서든 강의를 들을 수 있습니다. 저는 그 중, 인공지능을 전공할 때 ...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week12/"><div class="card-body"> <em class="timeago small" date="2022-09-27 01:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy</h3><div class="text-muted small"><p> Chapter 1. Motivation and Basics 목차 Motivation Machine Learning, AI, Datamining What is Machine Learning? MLE MAP Basics Probabil...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week34/"><div class="card-body"> <em class="timeago small" date="2022-09-29 01:40:00 +0900" >Sep 29, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression</h3><div class="text-muted small"><p> Chapter 3. Motivation and Basics 목차 optimal classification optimal predictor concept of Bayes risk concept of desicion boundary Naive Bayes Classifier ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/kooc_week8/" class="btn btn-outline-primary" prompt="Older"><p>[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm</p></a> <a href="/posts/kooc_week10/" class="btn btn-outline-primary" prompt="Newer"><p>[KOOC] 인공지능 및 기계학습 개론 10주차 - Sampling Based Inference</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>