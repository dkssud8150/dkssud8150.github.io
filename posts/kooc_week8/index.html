<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 8. K-Means Clustering and Gausssian Mixture Model" /><meta property="og:description" content="Chapter 8. K-Means Clustering and Gausssian Mixture Model" /><link rel="canonical" href="https://dkssud8150.github.io/posts/kooc_week8/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/kooc_week8/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-24T22:10:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-11-12T00:28:37+09:00","datePublished":"2022-10-24T22:10:00+09:00","description":"Chapter 8. K-Means Clustering and Gausssian Mixture Model","headline":"[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/kooc_week8/"},"url":"https://dkssud8150.github.io/posts/kooc_week8/"}</script><title>[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-10-24 22:10:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 24, 2022, 10:10 PM +0900" >Oct 24, 2022</em> </span> <span> Updated <em class="timeago" date="2022-11-12 00:28:37 +0900 " data-toggle="tooltip" data-placement="bottom" title="Sat, Nov 12, 2022, 12:28 AM +0900" >Nov 12, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5109 words"> <em>28 min</em> read</span></div></div></div><div class="post-content"><h1 id="chapter-8-k-means-clustering-and-gausssian-mixture-model">Chapter 8. K-Means Clustering and Gausssian Mixture Model</h1><ul><li>목차</ul><ol><li>Clustering task and K-Means algorithm<ul><li>unsuperveised learning<li>K-Means iterative process<li>limitation of K-Means algorithm</ul><li>Gaussian mixture model<ul><li>multinomial distribution and multivariate Gaussian distribution<li>why mixture model<li>parameter updates are derived from the Gaussian mixture model</ul><li>EM algorithm<ul><li>fundamentals of the EM algorithm<li>how to derive the EM updates of a model</ul></ol><p> </p><p> </p><h2 id="8-1-k-means-algorithm">8-1. K-Means algorithm <a href="#8-1-k-means-algorithm" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week78/machine_learning.png" data-proofer-ignore></p><p>1~6주차에서는 supervised learning에 대해 배웠다.</p><p>이제는 k-means라는 unsupervised learning을 배워보고자 한다. unsupervised learning이란 label이 존재하지 않은 데이터셋을 활용하여 task를 수행하는 것이다. true value를 모르는 상황에서 패턴을 찾아야 한다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/clustering.png" data-proofer-ignore></p><p>만약, 우리가 이 데이터셋에 대해 색상 데이터를 가지고 있다면, supervised task가 될 것이다. 그러나 오른쪽과 같이 어떤 군집이 만들어지는지에 대해 모르고, 군집을 찾아야 한다면 이는 unsupervised learning이 된다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/k_means.png" data-proofer-ignore></p><p>k-means algorithm이란, 잠재적으로 생각하기에 내부적으로 k개의 동력이 있어서 n개의 데이터 포인트들을 clustering하는 기법이다. 위의 데이터들에 대해 k=3개의 동력원이 존재하여 n개의 데이터들이 3개의 동력원들에 의해 군집화된다.</p><p> </p><p>여기서 중요한 것은 K-Means와 K-Nearest Neightbor algorithm은 동작방식이 비슷하나, 다른 기법이다. K-Means는 군집을 찾기 위한 unsupervised task에 사용되는 기법이고, KNN은 supervised task에서 사용되는 기법이다. KNN는 한 데이터의 주변에 가장 가까운 점 k개를 찾아 그에 대해 분류하고, 한 데이터를 주변 점들이 많이 분포되어 있는 값으로 지정해주는 것이다. 그러나 K-Means는 실제 컴퓨터가 보는 정보는 분류되지 않은 검은색 점들이고, 무엇보다 중심점들조차 주어지지 않는다. 이러한 상황에서 중심점들을 찾고, 그에 관련된 데이터들을 분류해야 한다.</p><p>따라서 <strong>K-Means는 크게 두 단계로 구성되어 있다. 첫번째로는 중심점을 찾는 것이고, 그 후에 중심점들과의 거리를 통해 데이터들을 분류한다.</strong></p><p> </p><p> </p><p>clustering에 대한 식은 다음과 같다.</p><p><img data-src="/assets/img/kooc/week78/clustering_function.png" data-proofer-ignore></p><ul><li>$ r_{nk} $ : a assignment variable of data point to cluster<li>$ \mu_k $ : location of centroids</ul><p>$ r_{nk} $ 는 cluster에 대한 파라미터로서, 각 데이터 포인트는 1개의 centroid(중심점)에 대해서만 할당된다. 그러므로, 할당된 centroid에 대해서는 1로, 나머지 centroid에 대한 것은 0으로 할당하여 연산한다. 그리고, $ \mu_k $ 는 중심점의 위치이다.</p><p>최종적으로 <em>J</em> 를 최소화하는 것이 최적화의 목적이다. 그러나 이 때는 파라미터가 2개이므로, 이러한 경우는 반복적인 최적화를 통해 먼저 $ r_{nk} $를 최적화하고, 최적화된 $ r_{nk} $ 를 통해 $ \mu_k $를 최적화하고, 다시 최적화된 $ \mu_k $를 통해 $ r_{nk} $를 최적화하는 방식을 통해 J를 최소화한다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/EM.png" data-proofer-ignore></p><ul><li>Expectation<ul><li>파라미터($ r_{nk} $)에 의해 구해지는 확률을 통해 centroid를 중심으로 한 데이터 포인트들을 할당해준다.</ul><li>Maximization<ul><li>likelihood에 대한 파라미터($ \mu_k $)를 최대화시킴으로써 centroid 위치를 업데이트한다.</ul></ul><p>$ \mu_k $ 에 대해 optimization하는 과정은 다음과 같다. 최적화하는 방법은 동일하게 미분하여 =0인 값을 찾는 것이다.</p><p><img data-src="/assets/img/kooc/week78/mu_optimization.png" data-proofer-ignore></p><p>결론적으로 구해보면, $ \mu_k $는 할당된 데이터 포인트들의 평균값이 된다.</p><p> </p><p>k-means algorithm의 과정은 다음과 같다.</p><p>특정 반복 횟수만큼 아래 과정을 반복한다.</p><ol><li>데이터들 중 random 하게 n개를 골라 centroid에 대한 파라미터인 $ \mu_k $ 를 지정한다.<li>$ r_nk $를 가까운 centroid( $ \mu_k $)에 대해 assign한다. ( 0 또는 1 )<li>assign된 데이터들의 평균값을 구해 $ \mu_k $ 를 업데이트한다.<li>다시 $ r_nk $를 업데이트한다.</ol><p> </p><p>이러한 반복 과정을 통해, 점차 최적의 centroid를 찾을 수 있다.</p><p><img data-src="/assets/img/kooc/week78/example_kmean.png" data-proofer-ignore></p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/limitation_kmeans.png" data-proofer-ignore></p><p>이러한 간단한 k-means algorithm에는 한계가 존재한다.</p><ol><li>centroid의 <strong>개수</strong>가 명확하지 않다.<li>centroid의 <strong>초기 위치</strong>가 잘못 지정되면 잘못된 optimization이 수행될 수도 있다.<li>limitation of distance metrics<ul><li>euclidean distance만으로는 데이터의 특정을 정확하게 파악하기 힘들다.</ul><li>hard clustering<ul><li>r_nk가 0 또는 1이므로 <strong>discrete하다</strong>는 단점이 있다.</ul></ol><p> </p><p> </p><p> </p><h2 id="8-2-gaussian-mixture-model">8-2. Gaussian Mixture Model <a href="#8-2-gaussian-mixture-model" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p> </p><h3 id="multinomial-distribution">Multinomial Distribution <a href="#multinomial-distribution" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>gaussian mixture model을 배우기에 앞서, Multinomial distribution에 대해 다시 한 번 짚고 넘어가자.</p><p> </p><p><img data-src="/assets/img/kooc/week78/multinomial_distribution.png" data-proofer-ignore></p><p>먼저 binomial distribution이란, 0 또는 1에 대해서만 나타내어지는 distribution이다. binomial distribution에 있어서 X=(0,0,1,0,0,0) 에 대해 3개를 선택하는 것과 같이, 확률에 대한 식은 다음과 같다.</p>\[P(X|\mu) = \prod_{k=1}^K \mu_k^{x_k}, \sum_k x_k = 1\]<p>이 때의 제약조건(Subject)는</p><ul><li>$ \mu_k \geq 0 $<li>$ \sum_k \mu_k = 1 $</ul><p> </p><p>이 0 또는 1의 binary한 값을, 0,1,2,3,… 에 대한 값으로 늘린 것이 <strong>multinomial distribution</strong> 이다.</p><p> </p><p>N개의 선택지($x_1$,…,$x_n$)를 가진 데이터셋 D가 있다고 할 때의 확률은 다음과 같다.</p>\[P(X|\mu) = \prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{\sum_{n=1}^N x_{nk}} = \prod_{k=1}^K \mu_k^{m_k}\]<p>이 때, $ m_k = \sum_{n=1}^N x_{nk} $ 이다.</p><p>MLE, 즉 확률이 최대가 되는 μ를 구하기 위한 식은 다음과 같다.</p><ul><li>Maximize $ P(X|\mu) = \prod_{k=1}^K \mu_k^{m_k} $<li>제약조건 $ \mu_k &gt;= 0, \sum_k \mu_k = 1 $</ul><p> </p><p>MLE task에서 제약조건이 존재할 때에는 <strong>lagrange function</strong>을 활용하여 MLE를 수행할 수 있다. lagrange function, L은 다음과 같다.</p><p>$ L(μ,m,𝜆) = \sum_{k=1}^K m_k :ln\mu_k + \lambda(\sum_{k=1}^K \mu_k - 1) $</p><p>lagrange function을 μ_k 에 대해 미분하면</p><p><img data-src="/assets/img/kooc/week78/lagrange_function_deriative.png" data-proofer-ignore></p><p> </p><p>이 때, 제약조건인 $ \sum_k \mu_k = 1 $를 활용하면</p><p><img data-src="/assets/img/kooc/week78/constraint_function.png" data-proofer-ignore></p><p>이 때, $ \sum_k \sum_{n=1}^N x_{nk} = N $ 인 이유는 모든 선택지에 대한 summation인 k와 개별 선택지마다 모든 데이터 포인트가 선택되는지에 대한 summation인 N에 대해 x_nk를 summation하는 것이므로, N이 된다. 이 때 x는 각 instance k에 대해 n개의 선택지가 선택될 확률이다.</p><p> </p><p>결론적으로 MLE에 대한 μ_k는 m_k/N이 된다. 이는 특정 선택지가 선택된 개수 / 관측한 선택지 전체를 의미한다.</p><p> </p><p> </p><h3 id="multivariate-gaussian-distribution">Multivariate Gaussian Distribution <a href="#multivariate-gaussian-distribution" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>Gaussian Mixture model을 위해 또 다른 재료가 필요하다. 그것이 <strong>Multivariate Gaussian Distribution</strong>이다.</p><p> </p><p>single dimension에서의 Gaussian Distribution 식은 다음과 같다.</p><p><img data-src="/assets/img/kooc/week78/single_gaussian_distribution.png" data-proofer-ignore></p><p>이를 multi dimension으로 바꿀 때는 variance가 covariance matrix로 변환되어 만들어진다. μ값 또한, 벡터의 형태가 된다.</p><p><img data-src="/assets/img/kooc/week78/multi_gaussian_distribution.png" data-proofer-ignore></p><p>이 multivariate gaussian distribution 식에 대한 MLE를 찾아보자.</p><p> </p><p>먼저 log를 씌워서 식을 단순화시킨다.</p><p><img data-src="/assets/img/kooc/week78/log_multivariate_gaussian_distribution.png" data-proofer-ignore></p><p>그리고 이 때, 사용되는 기법이 <em>trace trick</em> 이다. trace trick은 선형 대수에서 등장하는 기법으로</p><p><img data-src="/assets/img/kooc/week78/trace_trick.png" data-proofer-ignore></p><p>Trace trick은 위와 같이, <code class="language-plaintext highlighter-rouge">Tr</code> 이라는 기호를 통해 나타낸다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/mle_log_multivariate.png" data-proofer-ignore></p><p>이렇게 간단해진 식을 미분한다. 변수는 μ와 $\sum$ 이므로 이 둘에 대해 미분한다. trace_trick에 대해 미분하면 간단하게 값을 구할 수 있다.</p><p><img data-src="/assets/img/kooc/week78/trace_trick_deriative.png" data-proofer-ignore></p><p> </p><p> </p><p>covariance matrix에 대한 예시가 있다.</p><p><img data-src="/assets/img/kooc/week78/covariance_matrix.png" data-proofer-ignore></p><p>covariance_matrix를 다양하게 변화시켜가면서 샘플링한 결과이다. 행렬에서 1행 1열과 2행 2열의 위치의 값은 variation이고, 행렬에서 2행 1열과 1행 2열 위치의 값은 correlation 즉 연관성에 대한 상수이다. 만약 분산이 둘다 1인 상황에서 correlation도 둘다 1이라면, 기울기가 양수인 직선의 형태로 나오게 될 것이다. x방향으로도 분산이 1, y방향으로도 분산이 1이고, 서로 상관관계가 크게 작용하고 있다는 의미이기 때문이다.</p><p>모든 값을 0으로 두면, 분산이 0이므로 점으로 모이게 될 것이고, 분산은 둘다 존재하나 연관성이 없다면 원의 형태로 둥글게 생긴다.</p><p>correlation이 음수인 경우는 반대 방향으로 작용될 것이다.</p><p> </p><p> </p><h3 id="mixture-model">Mixture Model <a href="#mixture-model" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>이렇게 구한, Multinomial distribution에 대한 MLE estimation 식과 Multivariate gaussian distribution에 대한 MLE estimation을 융합한 것이 Mixture model이다.</p><p><img data-src="/assets/img/kooc/week78/mixture_model.png" data-proofer-ignore></p><p>mixture model을 히스토그램 측면에서 바라본다면, 여러 개의 데이터가 하나로 모여 있는 것으로 보인다. 그래서 이를 1개의 normal distribution으로 만들면 다소 부정확한 모델이 된다.</p><p>따라서, 이러한 모델의 경우 여러 개의 normal distribution으로 만들어서 나타내고자 한다. 이를 <strong>Mixture distribution</strong>이라 한다.</p><p> </p><p>mixture distribution에 대한 식은 다음과 같다.</p>\[P(x) = \sum_{k=1}^K \pi_k N(x|\mu_k, \sigma_k)\]<ul><li>$ \pi_k $ : Mixing coefficients<ul><li>multinomial distribution에 대한 값으로, K개의 옵션이 존재하고 그 중 하나가 선택될 확률을 나타낸다. 이 때의 K는 normal distribution의 개수이다.<li>weight의 역할을 수행<li>$ \sum_{k=1}^K \pi_k = 1 $<li>0 &lt;= π_k &lt;= 1<li>k-means에서의 weight 변수는 0 또는 1의 값을 가지지만, 이 때는 확률적인(stochastic) 값을 가진다.</ul><li>$ N(x|μ_k, σ_k) $ : Mixture component<ul><li>multivariate gaussian distribution에 대한 값으로, 개별 normal distribution를 나타낸다.</ul></ul><p>z라는 새로운 variable이 있다 생각한다면, x에 대한 확률 P(x)는 다음과 같다.</p>\[P(x) = \sum_{k=1}^K P(z_k)P(x|z)\]<p>k개 중 z가 선택될 확률과, z가 주어졌을 때, 즉 어떤 normal distribution을 선택했는지에 대한 값이 주어진 상태에서의 x의 확률, 즉 normal distribution을 나타낸다.</p><p> </p><h3 id="gaussian-mixture-model">Gaussian Mixture Model <a href="#gaussian-mixture-model" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week78/gaussian_mixture_model.png" data-proofer-ignore></p><p>이제 데이터 포인트들이 multiple multivariate Gaussian Distribution의 mixture distribution에 대해 구성된다고 생각해보자.</p><p>그러면, P(x)는</p><p>$ P(x) = \sum_{k=1}^K \pi_k N(x|\mu_k, \sum_k) $</p><p>로 나타낼 수 있다. 이때의 $ \pi_k $는 $ P(z_k = 1) = \pi_k $ 이므로 P(Z)는 k개 안에서 선택지($ z_k $)를 선택하는 것이므로 확률 P(x)를 다음과 같이 P(Z)에 대해 나타낼 수 있다.</p><p>$ P(Z) = \prod_{k=1}^K \pi_k^{z_k} $</p><p> </p><p>또한, mixture component인 $ N(x|\mu_k, \sum_k) $ 에 대해 $ P(X|z_k = 1) = N(x|\mu_k, \sum_k) $ 인데, 이 z_k가 k개 있으므로, 이를 일반화시키면 다음과 같이 나타낼 수 있다.</p><p>$ P(X|Z) = \prod_{k=1}^K N(x|\mu_k, \sum_k)^{z_k} $</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/baysian_graph.png" data-proofer-ignore></p><p>위와 같은 baysian network를 가진 모델이 있다고 생각해보자. 파란색은 파라미터에 대한 노드를 나타낸 것이고, N이라는 박스는 안의 내용들이 N번 반복적으로 생성된다는 의미이다.</p><p>이 때, conditional probability는 다음과 같이 정의된다.</p>\[p(z_k=1|x_n) = \cfrac{P(z_k=1)P(x|z_k=1)}{\sum_{j=1}^K P(z_j=1)P(x|z_j=1)}\]<p>로 나타낼 수 있으므로, 위에서 구한 값들을 그대로 대입한다.</p>\[p(z_k=1|x_n) = \cfrac{\pi_k N(x|\mu_k, \sum_k)}{\sum_{j=1}^K \pi_j N(x|\mu_j, \sum_j)}\]<p> </p><p><img data-src="/assets/img/kooc/week78/mle_estimation.png" data-proofer-ignore></p><p>기존의 MLE를 구하는 방식은 distance를 사용하여 확률을 사용하지 않았다. 이번에는 확률을 가정했으므로, <strong>log likelihood</strong>를 활용하여 MLE를 하고자 한다.</p><p> </p><p> </p><h3 id="expectation-and-maximization-of-gmm">Expectation and Maximization of GMM <a href="#expectation-and-maximization-of-gmm" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>GMM은 K-Means algorithm과 비슷하다. k-means에서는 assignment variable과 location of cetroid, 2개의 parameter가 존재한다.</p><p>GMMM에서는 location of centroid 와 동일한 역할을 하는 μ와 σ로 표현되는 Mixture component, π_k로 나타내지는 multinomial distribution으로 모델링된 값인 Mixing coefficients, k-means의 파라미터들과 동일한 역할을 하는 2개의 parameter가 있다.</p><p> </p><p>이전에 Expectation and Maximization step에 대해 잠깐 살펴보았다. Expectation에서는 data point들을 centroid에 대해 할당하고, Maximization에서는 파라미터들을 업데이트한다.</p><p> </p><ul><li>Expectation step</ul><p>기존에 K-means는 가장 가까운 centroid에 대해 각 데이터 포인트들을 0 또는 1로 할당했지만, 이제는 확률에 대해 계산할 것이므로 확률을 할당한다. 이전에 구했던 marginalized probability을 할당하면 된다.</p><p>$ \gamma(z_{nk}) = p(z_k = 1 | x_n) = \cfrac{P(z_k = 1)P(x|z_k = 1)}{\sum_{j=1}^K P(z_j=1)P(x|z_j=1)} = \cfrac{\pi_k N(x|\mu_k, \sum_k)}{\sum_{j=1}^k \pi_j N(x|\mu_j, \sum_j)} $</p><ul><li>x와 파라미터(π,μ,∑) 가 주어졌을 때, $\gamma(z_{nk})$ 를 계산함.<li>$\gamma(z_{nk})$ 는 수많은 iteration을 통한 지속저긍로 갱신되는 파라미터에 의해 업데이트된다.</ul><p> </p><p> </p><ul><li>Maximization step</ul><p>그 다음, Maximization step에서는 Expectation에서 계산한 $\gamma(z_{nk})$ 를 통해 파라미터들을 업데이트한다. 업데이트할 파라미터는 π,μ,∑ 이고, 이에 대한 업데이트 식은 다음과 같다.</p><p>$ lnP(X|\pi,\mu,\sum) = \sum_{n=1}^N ln{\sum_{k=1}^K \pi_k N(x|\mu_k, \sum_k)} $</p><p>이 파라미터들을 최적화를 하기 위해서는 이 식을 미분한다. 만약 제약조건이 존재한다면 lagrange method를 활용하여 미분한다. 각 파라미터들에 대해 미분하므로 총 3번 수행한다.</p><p><img data-src="/assets/img/kooc/week78/maximization.png" data-proofer-ignore></p><ul><li>이 때, π_k에 대한 미분에서 한 데이터 포인트에 대해 k개의 대안이 존재할 때, r(z_nk)를 k개 다 더하면 1이 되고, 이 1을 N개의 데이터 포인트 개수만큼 더하므로 N이 된다. 그리고, pi_k는 확률인데, 이를 k에 대해 다 더하면 1이 된다. 따라서 λ = -N 이 된다.</ul><p> </p><p>이렇게 총 3개의 r(z_nk)에 대한 식이 나오므로 이를 통해 파라미터들을 업데이트하고, 이러한 과정을 Maximization이라 할 수 있다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/example_gmm.png" data-proofer-ignore></p><p>GMM이 k-means에 대해 soft한 이유는 데이터 포인트들이 확률로 할당되기에 확실한 색상이 아닌 융합된 색상으로 나타내어진다.</p><p>centroid(μ_k) 또한 gaussian distribution이므로 정확한 위치가 아닌 분포를 따르고 있다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/property_gmm.png" data-proofer-ignore></p><p>GMM의 속성에 대해 알아보자.</p><p>GMM의 장점으로는 Soft clustering이 가능해지고, 각각의 분포들에 대해 이전에는 서로의 관계를 알 수 없었으나, GMM에서는 서로의 분포간의 관계를 파악할 수 있으므로 더 많은 정보를 가질 수 있다.</p><p>단점으로는 연산이 더 많아지고, local maximum에 빠질 수 있다. 또한 k-means에서도 존재했던 centroid의 개수를 지정해주어야 한다는 점이다.</p><p> </p><p> </p><h3 id="relation-between-k-means-and-gmm">Relation between K-Means and GMM <a href="#relation-between-k-means-and-gmm" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week78/relation_kmeansandGMM.png" data-proofer-ignore></p><p>multivariate gaussian distribution에 대한 식이 있었다. 이 때, 특정 k에 대한 상황일 때, ∑_k = 𝜖Ι 라고 가정해보자. I는 identity matrix를 의미하고, 𝜖는 EM process에 대한 값이다.</p><p>identity matrix는 역행렬과 형태가 동일하다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/assignment_probability.png" data-proofer-ignore></p><p>이러한 상황에서 지수함수 안의 값을 살펴보자. 지수함수의 특성상 x가 -inf 로 가면 y는 0에 가까워진다. 𝜖가 0에 가까워질수록 -1/2𝜖는 -inf로 갈 것이다. 이러한 상황에서 데이터 포인트가 centroid와 가깝다면 x - μ_k 는 다소 작으므로 다소 천천히 0에 다가가게 되고, 데이터 포인트가 centroid와 멀다면 x - μ_k 는 크므로 빠르게 0에 다가간다.</p><p>그렇다면, 특정 상황에서 0 또는 1 로 binary하게 분류할 수 있게 되고, 이러한 경우에는 GMM이 k-means와 동일한 역할을 수행하게 된다.</p><p> </p><p> </p><h2 id="em-algorithm">EM Algorithm <a href="#em-algorithm" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>EM 알고리즘에 대해 자세하게 살펴보자.</p><p>P(X|θ) 가 있을 때, <em>Z</em>라는 hidden variable이 존재한다면, 이에 대해 marginalization을 수행하여</p><p>$ P(X|\theta) = \sum_Z P(X,Z|\theta) $</p><p>라는 식이 성립된다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/probability_decomposition.png" data-proofer-ignore></p><p>이 때, q(Z)라는 임의의 변수를 생성하고, 이를 분모에 만들어준다.</p><p> </p><p>그리고 <strong>Jensen’s inequality</strong> 라는 기법을 활용한다.</p><p>jensen’s inequality란 y = φ(x) 라는 식이 있을 때, x에 대한 평균값을 함수에 넣어서 얻은 y값과 각 x를 함수로 넣어 얻은 y들의 평균을 비교한다.</p><p>전자가 더 크다면 <code class="language-plaintext highlighter-rouge">concave</code>, 후자가 더 크다면 <code class="language-plaintext highlighter-rouge">convex</code> 라 한다.</p><p><img data-src="/assets/img/kooc/week78/jensen_inequality.png" data-proofer-ignore></p><p>그림에서 특정 함수 f가 있을 때, x1과 x2에 대해 각각 f(x1), f(x2)를 구하여 평균한 것보다 f(x1+x2)/2) 가 더 크면 concave, 더 작으면 convex이다. log(ln) 함수는 concave 에 해당된다.</p><p>만약에, $ \sum_Z q(Z) ln \frac{P(X,Z|\theta)}{q(Z)}$ 이 커지면, 좌항도 최소값이 커지는 것이므로 전체적으로 값들이 증가될 것이다.</p><p>$ \sum_Z q(Z) ln \frac{P(X,Z|\theta)}{q(Z)}$ 를 풀어보면 $ \sum_Z q(Z) ln P(X,Z|\theta) - q(Z)lnq(Z) $ 가 되고, q(Z)lnq(Z) 는 entropy 식과 동일하므로 entropy, H로 변환할 수 있다.</p><p>H로 변환하기 위해 q(Z)도 변하므로 이를 E_q(z) 로 표현할 수 있다. 이렇게 변환한 식을 Q(θ,q) 라는 새로운 함수로 정의한다. 이 때, q는 확률밀도함수여야 한다는 제약조건이 있다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/maximizing_lower_bound.png" data-proofer-ignore></p><p>또는, P(X,Z|θ) 에 대해 bayes rule을 적용하여 변환할 수 있다. 이 때, q는 확률밀도함수로 가정했기 때문에 모든 z에 대해 q를 더하면 1이 된다.</p><p>이렇게 변환하게 되면,</p><p>$ lnP(X|\theta) - \sum_Z{q(Z)ln \frac{q(Z)}{P(Z|X,\theta)}} $</p><p>가 되고, 동일한 lnP(X|θ) 라는 값과 함께 inequality의 원인을 확인할 수 있게 된다. 이 원인을 제거한다면 부등호가 등호가 될 수 있을 것이다.</p><p>$ \sum_Z{q(Z)ln \frac{q(Z)}{P(Z|X,\theta)}} $ 를 자세히 살펴보게 되면, KL divergence의 형태와 매우 유사하다.</p><p><img data-src="/assets/img/kooc/week78/kl_divergence.png" data-proofer-ignore></p><p>KL divergence란 kullback leiber divergence를 줄인 말로, 두 확률 분포를 비교하는 기법이다.</p><p>비교하는 식은 다음과 같다.</p><p>$ KL(P||Q) = \sum_i P(i) ln(\frac{P(i)}{Q(i)}) $</p><p>P와 Q라는 확률 분포에 대해 비교하는데, KL(P||Q)와 KL(Q||K) 가 다르다는 비대칭(Non-symmetric)한 특징이 있다. 또다른 특징으로는 KL(P||Q)는 항상 0보다 크다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/maximizing_lower_bound2.png" data-proofer-ignore></p><p>jensen’s inequality를 활용하여 표현한 L라는 식에서 θ^t라는 현재 시간에서 가지는 theta를 가져와서 그대로 q^t(Z)로 정의하게 되면 second(KL divergence) term이 0이 된다. 이를 통해 Q를 업데이트하고, 업데이트된 Q를 통해 theta를 최대화하여 theta^(t+1) 을 계산한다.</p><p> </p><p><img data-src="/assets/img/kooc/week78/graphical_lowerbound_maximization.png" data-proofer-ignore></p><p>이를 그래프화하면 다음과 같다. KL이라는 제거할 term이 존재하여 이를 줄이기 위해 q를 최적화한다. 최적화된 q를 통해 다시 theta를 최적화할 수 있고, 최적화된 theta에 대해 다시 KL term이 생겨난다. 생겨난 KL term을 제거하기 위해 다시 q를 최적화한다.</p><p>이러한 반복을 통해 최적의 값으로 도달한다. 그러나 목표로 하는 곳은 global maxima이지만, 이 과정을 통해 local maxima에 빠질 수 있다.</p><p> </p><p>EM 알고리즘을 정리하면 다음과 같다.</p><ul><li>목표 : latent variable(Z)이 존재할 때 likelihood를 최대화한다.<li>과정<ol><li>무작위의 점으로 theta_0를 초기화한다.<li>likelihood가 수렴될 때까지 아래 과정을 반복한다.<li>Expectation step<ul><li>$ q^{t+1}(z) = argmax_qQ(\theta^t, q) = argmax_qL(\theta^t,q) = argmin_qKL(q||P(Z|X,\theta^t)) $<ol><li>Z를 할당한다.<li>$ \gamma(z_{nk}) = p(z_k=1|x_n) = \frac{P(z_k=1)P(x|z_k=1)}{\sum_{j=1}^kP(z_j=1)P(x|z_j=1)} = \frac{\pi_k N(x|\mu_k, \sum_k)}{\sum_{j=1}^k \pi_j N(x|\mu_j, \sum_j)} $</ol></ul><li>Maximization step<ul><li>$ \theta^{t+1} = argmax_{\theta} Q(\theta, q^{t+1}) = argmax_{\theta} L(\theta, q^{t+1}) $<li>MLE와 동일한 과정의 최적화<ol><li>미분하여 파라미터를 최적화한다.<li><img data-src="/assets/img/kooc/week78/deriative_parameter.png" data-proofer-ignore></ol></ul></ol></ul><p><img data-src="/assets/img/kooc/week78/summary_gmm.png" data-proofer-ignore></p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week8/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week8.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/kooc/'>kooc</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/kooc/" class="post-tag no-text-decoration" >kooc</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/kooc_week8/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/kooc_week8/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/kooc_week8/&amp;text=[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/kooc_week8/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/kooc_start/"><div class="card-body"> <em class="timeago small" date="2022-09-27 00:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 - Abstract</h3><div class="text-muted small"><p> KOOC란? KOOC(KAIST Massive Open Online Course)는 KAIST 무료 온라인 강좌를 말하는 것으로, KAIST 교수님들의 강의를 무료로 수강할 수 있는 플랫폼입니다. KOOC에는 다양한 주제로 강좌가 열려 있고, 온라인으로 진행되는 강좌이므로 언제 어디서든 강의를 들을 수 있습니다. 저는 그 중, 인공지능을 전공할 때 ...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week12/"><div class="card-body"> <em class="timeago small" date="2022-09-27 01:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy</h3><div class="text-muted small"><p> Chapter 1. Motivation and Basics 목차 Motivation Machine Learning, AI, Datamining What is Machine Learning? MLE MAP Basics Probabil...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week34/"><div class="card-body"> <em class="timeago small" date="2022-09-29 01:40:00 +0900" >Sep 29, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression</h3><div class="text-muted small"><p> Chapter 3. Motivation and Basics 목차 optimal classification optimal predictor concept of Bayes risk concept of desicion boundary Naive Bayes Classifier ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/kooc_week7/" class="btn btn-outline-primary" prompt="Older"><p>[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network</p></a> <a href="/posts/kooc_week9/" class="btn btn-outline-primary" prompt="Newer"><p>[KOOC] 인공지능 및 기계학습 개론 9주차 - Hidden Markov Model</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>