<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="CS231N chapter 8 - Deep Learning Softwares" /><meta name="author" content="JaeHo-YooN" /><meta property="og:locale" content="en" /><meta name="description" content="7강 리뷰 1) Facier Optimization : 많이 사용되는 강력한 손실함수 최적화 알고리즘 SGD, Momentum, Nesterov, RMSProp, Adam 2) Regularization : 네트워크의 Train/Test error 간의 격차를 줄이기 위해 사용 (과적합시 이 값을 조정함 - 손실함수에 Regulaization term을 더해 함수를 일반화시킨다) dropout 3) Transfer Learning : 미리 학습된 모델을 불러와 데이터만 사용하여 학습시키는 방법" /><meta property="og:description" content="7강 리뷰 1) Facier Optimization : 많이 사용되는 강력한 손실함수 최적화 알고리즘 SGD, Momentum, Nesterov, RMSProp, Adam 2) Regularization : 네트워크의 Train/Test error 간의 격차를 줄이기 위해 사용 (과적합시 이 값을 조정함 - 손실함수에 Regulaization term을 더해 함수를 일반화시킨다) dropout 3) Transfer Learning : 미리 학습된 모델을 불러와 데이터만 사용하여 학습시키는 방법" /><link rel="canonical" href="https://dkssud8150.github.io/posts/cs231n8/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/cs231n8/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-09-28T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="CS231N chapter 8 - Deep Learning Softwares" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo-YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo-YooN"},"dateModified":"2022-03-22T22:14:30+09:00","datePublished":"2021-09-28T13:00:00+09:00","description":"7강 리뷰 1) Facier Optimization : 많이 사용되는 강력한 손실함수 최적화 알고리즘 SGD, Momentum, Nesterov, RMSProp, Adam 2) Regularization : 네트워크의 Train/Test error 간의 격차를 줄이기 위해 사용 (과적합시 이 값을 조정함 - 손실함수에 Regulaization term을 더해 함수를 일반화시킨다) dropout 3) Transfer Learning : 미리 학습된 모델을 불러와 데이터만 사용하여 학습시키는 방법","headline":"CS231N chapter 8 - Deep Learning Softwares","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/cs231n8/"},"url":"https://dkssud8150.github.io/posts/cs231n8/"}</script><title>CS231N chapter 8 - Deep Learning Softwares | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>CS231N chapter 8 - Deep Learning Softwares</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>CS231N chapter 8 - Deep Learning Softwares</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo-YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-09-28 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Tue, Sep 28, 2021, 1:00 PM +0900" >Sep 28, 2021</em> </span> <span> Updated <em class="timeago" date="2022-03-22 22:14:30 +0900 " data-toggle="tooltip" data-placement="bottom" title="Tue, Mar 22, 2022, 10:14 PM +0900" >Mar 22, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6540 words"> <em>36 min</em> read</span></div></div></div><div class="post-content"><blockquote><p>7강 리뷰</p><p>1) Facier Optimization : 많이 사용되는 강력한 손실함수 최적화 알고리즘</p><ul><li>SGD, Momentum, Nesterov, RMSProp, Adam</ul><p>2) Regularization : 네트워크의 Train/Test error 간의 격차를 줄이기 위해 사용 (과적합시 이 값을 조정함 - 손실함수에 Regulaization term을 더해 함수를 일반화시킨다)</p><ul><li>dropout</ul><p>3) Transfer Learning : 미리 학습된 모델을 불러와 데이터만 사용하여 학습시키는 방법</p></blockquote><p><br /></p><p><br /></p><ul><li>CPU vs GPU</ul><p>딥러닝을 진행할 때는 GPU를 사용하는 것이 좋다. GPU 종류 중에서는 NVIDIA 와 AMD가 있는데, AMD의 경우 딥러닝을 수행할 때 문제가 더 많이 발생할 수 있다.</p><p>NVIDIA는 딥러닝에 적합한 하드웨어를 만들기 위해 공을 들였기 때문에, 딥러닝에서는 <em>NVIDIA</em>가 독점적으로 많이 사용된다.</p><p><br /></p><p>CPU는 기본적으로 GPU보다 core의 수가 적다. GPU의 경우 고성능 상업 GPU는 수천개의 코어를 가지고 있다. 코어 수를 가지고 직접적으로 비교하지는 않지만, 코어가 많다는 것은 어떤 일을 수행할 때 병렬로 수행하기 더 적합하다.</p><p>또한, CPU는 대부분의 메모리를 RAM에서 끌어다 쓴다. 반면 GPU는 칩 안에 RAM이 내장되어 있다.</p><p><strong>따라서 GPU는 딥러닝 연산에 적합하다.</strong></p><p><br /></p><p>GPU에서 실행되는 코드를 직접 작성할 수 있으나 상당히 어렵고 복잡하다. CUDA, OpenCL, Udacity 등이 있긴 하나 그 대신 편리한 라이브러리를 사용하여 구현하고, GPU를 통해 계산만 실행하면 된다.</p><p><br /></p><p><br /></p><h1 id="deep-learning-frameworks">Deep Learning Frameworks</h1><p>딥러닝 프레임워크에는 Pytorch, TensorFlow, Caffe2, MXNet 등이 있지만, 가장 유명한 것은 tensorflow와 pytorch이다.</p><p>프레임워크를 사용하는 이유는</p><ul><li>computational graphs를 직접 만들지 않아도 된다.<li>forward pass만 잘 구현해놓으면 backpropagation은 알아서 구성된다.<li>GPU를 효율적으로 사용할 수 있다.</ul><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-28/0028.jpg" alt="image" data-proofer-ignore></p><p>여기 입력 x,y,z에 대한 computational graphs가 있다. 이를 numpy로 작성하면 위와 같은 코드가 될 것이다.</p><p>그래프를 numpy로 작성한다면 backward도 직접 작성해야 한다. 이는 매우 까다로운 일이 될 것이다. 또, numpy는 GPU에서 동작하지 않는다.</p><p><br /></p><ul><li>Tensorflow</ul><p>이를 tensorflow로 구현해보자.</p><p><img data-src="/assets/img/cs231n/2021-09-28/0029.jpg" alt="image" data-proofer-ignore></p><p>tensorflow 코드를 보면 numpy와 유사하다. 하지만 tensorflow에는 gradient를 계산해주는 코드가 존재한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">,</span> <span class="n">grad_z</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">])</span>
</pre></table></code></div></div><p><br /></p><p>또, tensorflow의 장점은 명령어 한 줄이면 CPU/GPU 전환이 가능하다는 것이다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'/cpu:0'</span><span class="p">):</span> 
<span class="o">=&gt;</span> <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'/gpu:0'</span><span class="p">):</span>
</pre></table></code></div></div><p>CPU:0을 GPU:0으로 바꿔주기만 하면 된다.</p><p><br /></p><ul><li>Pytorch</ul><p>pytorch로 구현해보자.</p><p><img data-src="/assets/img/cs231n/2021-09-28/0034.jpg" alt="image" data-proofer-ignore></p><p>비슷한 코드로 구성되어 있다.</p><p>pytorch 또한 1줄이면 gradient를 계산할 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">c</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</pre></table></code></div></div><p>GPU를 사용하기 위해서 .cuda()를 붙인다.</p><p><br /></p><p><br /></p><p>각 프레임워크를 좀 더 깊게 살펴보자.</p><h2 id="tensorflow">Tensorflow <a href="#tensorflow" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre><td class="rouge-code"><pre><span class="s">'''
tensorflow neural net
'''</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow.compat.v1</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">tf</span><span class="p">.</span><span class="nf">disable_v2_behavior</span><span class="p">()</span> <span class="c1"># placeholder 을 위해 사용
</span>
<span class="c1">#### computational graphs 구성
</span>
<span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="o">=</span> <span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span> <span class="c1"># number of node, input_size, hidden_size
</span>
<span class="s">''' 그래프의 입력 노드 생성, 그래프 밖에서 데이터를 넣어줄 수 있도록 해줌 =&gt; session 시에 직접 값을 지정
    그래프만 구성하고 실제적인 메모리할당은 일어나지 않는다. '''</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>    <span class="c1"># 입력데이터
</span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>    <span class="c1"># 입력데이터
</span><span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="p">))</span>   <span class="c1"># 가중치
</span><span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>   <span class="c1"># 가중치
</span>
<span class="s">''' x와 w1 행렬곱 연산 후 maximum을 통한 ReLU 구현 '''</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w1</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>             <span class="c1"># x와 w1의 행렬곱 연산 -&gt; max(,0)을 이용해 ReLU 함수 구현
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w2</span><span class="p">)</span>                      <span class="c1"># h와 w2의 행렬곱 연산 -&gt; 최종 출력값 y_pred 계산
</span>
<span class="s">''' L2 Euclidean '''</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">-</span><span class="n">y</span>                                                 <span class="c1"># 예측값과 실제 정답과의 차 
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>            <span class="c1"># 예측값과 실제 정답값 y 사이의 유클리디안 거리를 계산
</span>
<span class="s">''' gradient 함수를 통해 loss와, w1, w2의 gradient 계산. backprop 직접 구현할 필요가 없다. '''</span>
<span class="n">grad_w1</span><span class="p">,</span><span class="n">grad_w2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,[</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">])</span>

<span class="s">''' ㅡㅡㅡㅡㅡㅡㅡㅡ  그래프 구성 =&gt; 아직까지 실제 계산이 이루어지지는 않았다.  ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ'''</span>


<span class="c1">#### Tensorflow session : 데이터 입력, 실제 그래프를 실행 및 계산
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'/gpu:0'</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="s">''' 그래프에 들어갈 value 입력. tensorflow는 numpy를 지원한다. '''</span>
    <span class="n">values</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">),</span>
           <span class="n">w1</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="p">),</span>
           <span class="n">w2</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D</span><span class="p">),</span>
           <span class="n">y</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">)}</span>
    
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-5</span>

    <span class="s">''' 실제 그래프 실행하는 부분, 출력 값은 numpy array '''</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>                                 <span class="c1"># 50번 반복하여 네트워크를 학습한다.
</span>        <span class="n">out</span><span class="o">=</span><span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span><span class="n">grad_w1</span><span class="p">,</span><span class="n">grad_w2</span><span class="p">],</span><span class="n">feed_dict</span><span class="o">=</span><span class="n">values</span><span class="p">)</span>   <span class="c1"># 첫번째 인자를 통해 그래프의 출력으로 어떤 부분을 원하는지 볼 수 있다. =&gt; loss, grad_w1, grad_w2 
</span>                                                                <span class="c1"># feed_dict를 통해 실제 값을 전달
</span>        <span class="n">loss_val</span><span class="p">,</span> <span class="n">grad_w1_val</span><span class="p">,</span><span class="n">grad_w2_val</span> <span class="o">=</span> <span class="n">out</span>                 <span class="c1"># output에는 loss와 gradient가 numpy array형태로 반환되어 있기에 각각에 반환한다.
</span>        <span class="n">values</span><span class="p">[</span><span class="n">w1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1_val</span>               <span class="c1"># 가중치 업데이트를 위해 gradient를 계산해서 수동으로 gradient diescent하고 있다.
</span>        <span class="n">values</span><span class="p">[</span><span class="n">w2</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2_val</span>               <span class="c1"># 이 경우 forward pass에서 그래프가 실행될 때마다 가중치를 넣어줘야 한다.
</span></pre></table></code></div></div><p>두 개의 fc layer + ReLU 네트워크가 있다. 그리고 손실함수로는 L2 Euclidean을 사용했다.</p><p>위의 코드는 크게 2 stage로 나눌 수 있다. 1) computational graph 정의 2) 그래프 실행</p><p><br /></p><p>위의 코드는 GPU/CPU간의 데이터 교환은 엄청 느리고 비용도 크다.</p><p>따라서 아래와 같이 코드를 수정하여 해결한다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
</pre><td class="rouge-code"><pre>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow.compat.v1</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">tf</span><span class="p">.</span><span class="nf">disable_v2_behavior</span><span class="p">()</span>

<span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="o">=</span> <span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span>

<span class="c1">#### 그래프 구성 
</span>

<span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>

<span class="s">''' variables로 변경, 변수가 그래프 내부에 있도록 해주는 함수. =&gt; session 전에 값을 지정
    tf.random_normal로 초기화 설정  '''</span>
<span class="n">w1</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">random_normal</span><span class="p">((</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="p">)))</span>
<span class="n">w2</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">random_normal</span><span class="p">((</span><span class="n">H</span><span class="p">,</span><span class="n">D</span><span class="p">)))</span>

<span class="n">h</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w1</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w2</span><span class="p">)</span>
<span class="n">diff</span><span class="o">=</span><span class="n">y_pred</span><span class="o">-</span><span class="n">y</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># L2 loss를 직접 구현하지 않고 함수를 불러와 계산
</span>                                                <span class="c1"># 이전에는 loss를 계산하는 연산을 직접 만들었다.
</span>                                                <span class="c1"># loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))
</span>
<span class="s">''' loss 계산, gradient 계산. backprop 직접구현 X '''</span>
<span class="n">grad_w1</span><span class="p">,</span><span class="n">grad_w2</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,[</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">])</span>

<span class="s">''' assign 함수를 통해 그래프 내에서 업뎃이 일어날 수 있도록 해줌 '''</span>
<span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span>

<span class="c1">#  tensorflow는 output에 필요한 연산만 수행한다. 그래서 w1와 w2를 업뎃하라고 명시적으로 넣어주어야 한다.
#  new_w1, new_w2를 직접적으로 session 안으로 추가해줄 수 있으나, 
#  사이즈가 큰 tensor의 경우 tensorflow가 직접 출력을 하는 것은 cpu/gpu간 데이터 전송이 필요하므로 좋지 않다.
#  따라서 dummy node인 updates 변수를 만들어 그래프에 추가
#  update는 어떤 값이 아니라, 특수한 데이터 타입으로 assign operation(작업)을 수행하기 위한 함수 '''
</span><span class="n">new_w1</span><span class="o">=</span><span class="n">w1</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">w1</span><span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span> <span class="n">grad_w1</span><span class="p">)</span>
<span class="n">new_w2</span><span class="o">=</span><span class="n">w2</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">w2</span><span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span> <span class="n">grad_w2</span><span class="p">)</span> 
<span class="n">updates</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">group</span><span class="p">(</span><span class="n">new_w1</span><span class="p">,</span><span class="n">new_w2</span><span class="p">)</span>


<span class="c1">#### Tensorflow session
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'/gpu:0'</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">global_variables_initializer</span><span class="p">())</span> <span class="c1"># 그래프 내부의 변수들을 초기화, 즉 ,w1,w2를 초기화
</span>    
        <span class="n">values</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">),</span>             <span class="c1"># 데이터와 레이블만 넣어줌, 가중치는 그래프 내부에 항상 상주
</span>            <span class="n">y</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">),}</span>                <span class="c1"># x,y도 가중치처럼 그래프에 넣어줘도 되지만, x,y가 mini-batch인 경우가 대부분이다. 
</span>                                                    <span class="c1"># 그렇다는 것은 매번 데이터가 바뀌므로 매번 데이터를 넣어줘야 한다.
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">loss_val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span><span class="n">updates</span><span class="p">],</span><span class="n">feed_dict</span><span class="o">=</span><span class="n">values</span><span class="p">)</span> <span class="c1"># 그래프를 실행시키면 loss와 dummy node 계산
</span>                                                                    <span class="c1"># _ 는 원래 변수로 사용하던 값을 무시한다는 말이다.
</span>                                                                    <span class="c1"># updates 값은 받지 않고, loss 값만 출력
</span></pre></table></code></div></div><p>placeholder을 써서 굳이 매번 가중치를 넣어줄 필요가 없다. 그 대신 variables를 선언해준다. variable은 computational graph안에 서식하는 변수이다.</p><p><br /></p><p>하지만 이 경우 그래프안에 살기 때문에, 우리가 tensorflow에게 어떻게 초기화시킬 것인지를 알려줘야 한다. 그래프 밖에 있다면 그래프에 넣어주기 전에 numpy를 통해 초기화(tf.random)시킬 수 있지만, 그래프 안에 있다면 초기화시킬 권한은 tensorflow에게 있다.</p><p>이전 예제에서는 gradient를 계산하고 그래프 외부(session)에서 numpy로 가중치를 업데이트했다. 계산된 가중치를 다음 스텝에 다시 넣어주었다.</p><p><br /></p><p>그러나 그래프 안에서 하려면 그 연산 자체가 그래프에 포함되어야 한다. 이를 위해 assign함수를 이용하여 변수가 그래프 내에서 업데이트 되도록 한다.</p><p><br /></p><p>updates는 tensorflow의 트릭중 하나다. 그래프를 구성하는 동안에는 복잡한 객체를 반환하지만, session.run을 동작시키면 none을 반환(_)하게 된다.</p><p><br /></p><p>하지만, 사실 위의 방법은 좋은 방법은 아니다. 대신에 optimizer을 사용할 수 있다. assigns줄부터 updates줄까지를 아래와 같이 바꾸기만 하면 된다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="nc">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>    <span class="c1"># 이 외에도 Adam, RMSprop 등 optimization 알고리즘 사용 가능
</span>                                                                <span class="c1"># 그래프에 w1,w2의 gradient를 계산하는 노드도 알아서 추가하고, 
</span>                                                                <span class="c1"># w1,w2의 update opertation도 알아서 추가한다. 
</span>                                                                <span class="c1"># assign을 위한 grouping opertation도 추가된다.
</span><span class="n">updates</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>                              <span class="c1"># loss는 최소값을 찾아야 하므로 minimize
</span></pre></table></code></div></div><p><br /></p><p><br /></p><p>입력과 가중치를 정의하고 행렬 곱연산으로 묶는 일이 번거롭기에 <strong>tf.layers</strong>로 간편하게 변경시킬 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="o">=</span> <span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span>

<span class="s">''' 그래프의 입력노드 생성 '''</span>
<span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>

<span class="s">''' Xavier initializer로 초기화 '''</span>
<span class="n">init</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">xavier_initializer</span><span class="p">()</span>                                         <span class="c1"># 초기화 방법 선언, 기존에는 tf.randomnormal로 일일이 초기화시켰다.
</span>
<span class="s">''' w1, b2를 변수로 만들어주고 초기화 '''</span>
<span class="n">h</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="n">units</span><span class="o">=</span><span class="n">H</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">init</span><span class="p">)</span>   <span class="c1"># w1과 w2를 variables로 만들어주고, 그래프 내부에 적절한 shape으로 만들어준다. 
</span>                                                                                    <span class="c1"># activation을 통해 layer에 relu를 추가해준다. 
</span><span class="n">y_pred</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">h</span><span class="p">,</span><span class="n">units</span><span class="o">=</span><span class="n">D</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">init</span><span class="p">)</span>                    <span class="c1"># 이전 h를 받아서 똑같이 수행한다. 
</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># optimizer을 이용해서 gradient를 계산하고 가중치를 업뎃할 수 있다
</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="nc">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">updates</span><span class="o">=</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1">## Tensorflow session
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">global_variables_initializer</span><span class="p">())</span>
    
    <span class="n">values</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">),</span>
           <span class="n">y</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">)}</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
        <span class="n">loss_val</span><span class="o">=</span><span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span><span class="n">updates</span><span class="p">],</span><span class="n">feed_dict</span><span class="o">=</span><span class="n">values</span><span class="p">)</span>
</pre></table></code></div></div><p>위는 tf.contrib.layer과 tf.layer을 사용한 편리한 방법이다. 하지만 더 다양하고 향상된(high level) 라이브러리가 있다.</p><p>그리고, 위의 코드들은 computational graph를 다루고 있다. 이는 low level이지만, 우리가 앞으로 다루어야 할 것들은 Neural Network인데, 이는 high level이다.</p><p><br /></p><p>그래서 tensorflow와 keras를 활용한 Network를 만들어보았다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="n">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>                                                    <span class="c1"># 모델을 구성하기 위한 큰 틀과 같은 것
</span><span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span> <span class="n">H</span><span class="p">))</span>                            <span class="c1"># 한 층 추가
</span><span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>                                           <span class="c1"># 활성함수 추가
</span><span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span> <span class="n">D</span><span class="p">))</span>                            <span class="c1"># 한 층 추가
</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e0</span><span class="p">)</span>                                               <span class="c1"># optimizer 선언
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s">'mean_squared_error'</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">)</span>       <span class="c1"># loss와 optimizer를 사용하여 그래프 생성
</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>                                               <span class="c1"># 데이터 입력
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>                                               <span class="c1"># 데이터 입력
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">nb_epoch</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">N</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>   <span class="c1"># model 50번 실행
</span></pre></table></code></div></div><p><br /></p><blockquote><p>tensorflow 기반 high level wrapper(함수? 라이브러리?)</p><ul><li>Keras <a href="https://keras.io/">https://keras.io/</a><li>TFLearn <a href="http://tflearn.org/">http://tflearn.org/</a><li>TensorLayer <a href="http://tensorlayer.readthedocs.io/en/latest/">http://tensorlayer.readthedocs.io/en/latest/</a><li>tf.layers <a href="https://www.tensorflow.org/api_docs/python/tf/layers">https://www.tensorflow.org/api_docs/python/tf/layers</a><li>TF-Slim <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">https://github.com/tensorflow/models/tree/master/inception/inception/slim</a><li>tf.contrib.learn <a href="https://www.tensorflow.org/get_started/tflearn">https://www.tensorflow.org/get_started/tflearn</a><li>Pretty Tensor <a href="https://github.com/google/prettytensor">https://github.com/google/prettytensor</a></ul></blockquote><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-28/0070.jpg" alt="image" data-proofer-ignore></p><p><strong>tensorboard</strong>를 통해, tensorflow를 사용할 때 training 하는 동안 loss와 같은 통계값들을 시각화할 수 있다.</p><p><br /></p><p><br /></p><p><br /></p><h2 id="pytorch">Pytorch <a href="#pytorch" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>pytorch에는 3가지 중요한 요소들이 있다</p><ol><li>tensor: imperative(명령형) array, gpu에서 사용가능, tensorflow의 numpy array와 같은 역할<blockquote><p>명령형 언어 vs 선언형 언어의 차이 설명 참고 : <a href="https://blog.naver.com/66dlwjddbs66/221826016905">https://blog.naver.com/66dlwjddbs66/221826016905</a></p></blockquote><li>variable: 그래프의 노드, 그래프를 구성하고 gradient를 계산할 수 있음, tensorflow의 variable,placeholder과 같은 역할<li>module: Neural Network를 구성, tf.layer과 같은 역할</ol><p><br /></p><p><br /></p><p>아래는 pytorch tensor로 구성한 2-layer Network이다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>


<span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">FloatTensor</span>    <span class="c1"># gpu에서 돌아가도록 데이터 타입을 변경
</span>                                <span class="c1"># gpu사용을 위해 FloatTensor 대신 cuda.FloatTensor로 변경
</span>
<span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span>

<span class="s">''' random data 선언 '''</span>
<span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span> 
<span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_out</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">w1</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">w2</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="s">''' forward pass '''</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>                            <span class="c1"># mm : 행렬 곱 연산 함수 -&gt; x와 w1을 행렬 곱 연산하여 h에 저장
</span>    <span class="n">h_relu</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                 <span class="c1"># clamp : min/max 를 통해 값을 바꾸는 함수, min = 0 이라면 0보다 작은 값들은 0으로 치환 -&gt; relu 형태로 만듦
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h_relu</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>                  <span class="c1"># h와 w2를 연산하여 pred로 출력
</span>    <span class="n">loss</span><span class="o">=</span><span class="p">(</span><span class="n">y_pred</span><span class="o">-</span><span class="n">y</span><span class="p">).</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>            <span class="c1"># loss 계산, pow는 제곱 함수 -&gt; L2 euclidean distance
</span>
    <span class="s">''' backward pass 
        backward 직접 구현 - &gt; gradient 계산 '''</span>
    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="p">.</span><span class="nf">t</span><span class="p">().</span><span class="nf">mm</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
    <span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">.</span><span class="nf">t</span><span class="p">())</span>    <span class="c1"># 행렬 곱 연산
</span>    <span class="n">grad_h</span><span class="p">[</span><span class="n">h</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>                         <span class="c1"># relu
</span>    <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">t</span><span class="p">().</span><span class="nf">mm</span><span class="p">(</span><span class="n">grad_h</span><span class="p">)</span>              
    
    <span class="s">''' 가중치 업데이트 '''</span>
    <span class="n">w1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
    <span class="n">w2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>
</pre></table></code></div></div><p>따로 그래프를 구성하는 코드가 없다.</p><p><br /></p><p>아래는 variable을 사용하여 구현한 것이다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span> <span class="c1"># number of node, input_size, hidden_size, output_size
</span>
<span class="s">''' variable은 computational graph를 만들고 이를 통해 gradient를 자동 계산하는 목적으로 사용
    x는 variable, x.data는 tensor, x.grad는 variable, x.grad.data도 tensor
    x.grad.data는 gradient 정보를 담고 있다. '''</span>
<span class="n">x</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>   <span class="c1"># 이 값의 gradient를 계산할 것인지 지정할 수 있다.
</span><span class="n">y</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_out</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 앞서 복잡한 gradient 계산 과정을 압축시킬 수 있음
</span><span class="n">w1</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>   <span class="c1"># 가중치에 대한 gradient만 계산
</span><span class="n">w2</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="s">''' forward pass '''</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>                 <span class="c1"># 출력값 계산 후 relu 연산
</span>    <span class="n">loss</span><span class="o">=</span><span class="p">(</span><span class="n">y_pred</span><span class="o">-</span><span class="n">y</span><span class="p">).</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>                        <span class="c1"># loss 계산
</span>    
    <span class="s">''' backward pass '''</span>
    <span class="k">if</span> <span class="n">w1</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span> <span class="n">w1</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>                    <span class="c1"># w1.grad 초기화
</span>    <span class="k">if</span> <span class="n">w2</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span> <span class="n">w2</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>                    <span class="c1"># w2.grad 초기화
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>                                     <span class="c1"># gradient가 알아서 반환되어 backward 진행
</span>    
    <span class="s">''' w.grad.data를 이용해 가중치 업데이트 '''</span>
    <span class="n">w1</span><span class="p">.</span><span class="n">data</span><span class="o">-=</span><span class="n">learning_rate</span><span class="o">*</span> <span class="n">w1</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span>
    <span class="n">w2</span><span class="p">.</span><span class="n">data</span><span class="o">-=</span><span class="n">learning_rate</span><span class="o">*</span> <span class="n">w2</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span>
</pre></table></code></div></div><p><br /></p><p><br /></p><ul><li>Tensorflow vs Pytorch</ul><p>먼저 tensorflow의 경우 그래프를 명시적으로 구성한 후 그래프를 돌린다. 즉 <code class="language-plaintext highlighter-rouge">session 전에 그래프를 구성</code>하고 session을 통해 계산한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="s">''' 그래프의 입력노드 생성 '''</span>
<span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>

<span class="s">''' Xavier initializer로 초기화 '''</span>
<span class="n">init</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">xavier_initializer</span><span class="p">()</span>                                         

<span class="s">''' w1, b2를 변수로 만들어주고 초기화 '''</span>
<span class="n">h</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="n">units</span><span class="o">=</span><span class="n">H</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">init</span><span class="p">)</span>   
                                                                                
<span class="n">y_pred</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">h</span><span class="p">,</span><span class="n">units</span><span class="o">=</span><span class="n">D</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">init</span><span class="p">)</span>                    

<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></table></code></div></div><p>반면 pytorch의 경우 forward pass 할 때마다 <code class="language-plaintext highlighter-rouge">매번 그래프를 다시 구성</code>한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="s">''' forward pass '''</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>                 <span class="c1"># 출력값 계산 후 relu 연산
</span>    <span class="n">loss</span><span class="o">=</span><span class="p">(</span><span class="n">y_pred</span><span class="o">-</span><span class="n">y</span><span class="p">).</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>                        <span class="c1"># loss 계산
</span></pre></table></code></div></div><p><br /></p><p><br /></p><p>추가적으로 autograd를 직접 제작할 수 있다.</p><p><img data-src="/assets/img/cs231n/2021-09-28/0094.jpg" alt="image" data-proofer-ignore></p><p>하지만, 대부분의 경우 직접 구현하기보다, 원래 있는 모델을 사용하는 것이 좋다.</p><p><br /></p><p><br /></p><p>pytorch에는 <code class="language-plaintext highlighter-rouge">nn module</code>이 존재한다. 이는 DNN 구성시 매우 많이 사용된다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span>

<span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span>

<span class="n">x</span><span class="o">=</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_out</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>                                         <span class="c1"># layer 생성
</span>        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">))</span>              

<span class="n">loss_fn</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">size_average</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>                       <span class="c1"># common loss function, mean squared error loss(평균제곱오차)
</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># optimizer를 위해 Adam 알고리즘 선언
</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="s">''' forward pass, prediction, loss 계산 '''</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>             <span class="c1"># model에 x 넣고 prediction
</span>    <span class="n">loss</span><span class="o">=</span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>      <span class="c1"># prediction과 실제값 y를 통해 loss 계산
</span>    
    <span class="s">''' backward pass, gradient 계산 '''</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>       <span class="c1"># grad 초기화
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>             <span class="c1"># gradient 자동 계산
</span>    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>            <span class="c1"># model parameter 업데이트
</span>    <span class="s">''' optimizer.step을 풀어쓰면 아래와 같다.
    for param in model.parameters():
        param.data-=learning_rate*param.grad.data
    '''</span>
</pre></table></code></div></div><p><br /></p><p>자신만의 모델을 구축해보자.</p><p>여기 2-layer Network 예제가 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="s">''' 단일 모듈(class)로 model 생성
    backward는 autograd가 알아서 해줌 '''</span>
<span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">):</span>         <span class="c1"># layer 정의
</span>        <span class="nf">super</span><span class="p">(</span><span class="n">TwoLayerNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>     <span class="c1"># super : 부모 클래스 함수의 메서드를 실행 가능하게 해줌, 
</span>                                                <span class="c1"># __init__ :  우리는 객체(인스턴스) 생성시 매번 해주어야 하는 번거로운 작업을 간단하게 만들어줌
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">)</span>    <span class="c1"># Linear 구성
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="p">)</span>   <span class="c1"># Linear 구성
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>                        <span class="c1"># forward pass 정의
</span>        <span class="n">h_relu</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>     <span class="c1"># 1층 layer 출력을 relu 연산 후 h_relu에 저장
</span>        <span class="n">y_pred</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">h_relu</span><span class="p">)</span>             <span class="c1"># 2층 layer 출력을 y_pred 에 저장
</span>        <span class="k">return</span> <span class="n">y_pred</span>                           <span class="c1"># y_pred 출력
</span>
<span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">D_out</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span>

<span class="n">x</span><span class="o">=</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D_out</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="s">''' DataLoader가 minibatching, shuffling, multithreading 관리
minibatch를 가져오는 작업들을 multi-threading을 통해 알아서 관리해준다. '''</span>
<span class="n">ds</span> <span class="o">=</span> <span class="nc">TensorDataset</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                    <span class="c1"># 미리 정리된 TensorDateset을 불러옴
</span><span class="n">loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>       <span class="c1"># dataset을 dataloader을 통해 loader함
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>         <span class="c1"># 정의했던 Net을 이용해 model을 불러옴
</span><span class="n">criterion</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 
<span class="n">optimizer</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="s">''' forward pass '''</span>
    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">x_var</span><span class="p">,</span> <span class="n">y_var</span><span class="o">=</span><span class="nc">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nc">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># mini-batch단위로 나뉘어진 loader를 각 epoch마다 x,y에 계속 입력시켜야 한다.
</span>        <span class="n">y_pred</span><span class="o">=</span><span class="nf">model</span><span class="p">(</span><span class="n">x_var</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span><span class="nf">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_var</span><span class="p">)</span>

    <span class="s">''' backward pass '''</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</pre></table></code></div></div><p><br /></p><p>pretrained model을 불러와 사용할 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">from</span> <span class="n">torchsummary</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="n">resnet101</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">resnet101</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="s">''' 불러온 모델 구조 확인 방법 '''</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">resnet101</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s">'cuda'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">resnet101</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span>
<span class="o">----------------------------------------------------------------</span>
        <span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>               <span class="n">Output</span> <span class="n">Shape</span>         <span class="n">Param</span> <span class="c1">#
</span><span class="o">================================================================</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">1</span>         <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">]</span>           <span class="mi">9</span><span class="p">,</span><span class="mi">408</span>
       <span class="n">BatchNorm2d</span><span class="o">-</span><span class="mi">2</span>         <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">]</span>             <span class="mi">128</span>
              <span class="n">ReLU</span><span class="o">-</span><span class="mi">3</span>         <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">]</span>               <span class="mi">0</span>
         <span class="n">MaxPool2d</span><span class="o">-</span><span class="mi">4</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>               <span class="mi">0</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">5</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>           <span class="mi">4</span><span class="p">,</span><span class="mi">096</span>
       <span class="n">BatchNorm2d</span><span class="o">-</span><span class="mi">6</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>             <span class="mi">128</span>
              <span class="n">ReLU</span><span class="o">-</span><span class="mi">7</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>               <span class="mi">0</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">8</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>          <span class="mi">36</span><span class="p">,</span><span class="mi">864</span>
       <span class="n">BatchNorm2d</span><span class="o">-</span><span class="mi">9</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>             <span class="mi">128</span>
             <span class="n">ReLU</span><span class="o">-</span><span class="mi">10</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>               <span class="mi">0</span>
           <span class="n">Conv2d</span><span class="o">-</span><span class="mi">11</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>          <span class="mi">16</span><span class="p">,</span><span class="mi">384</span>
      <span class="n">BatchNorm2d</span><span class="o">-</span><span class="mi">12</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>             <span class="mi">512</span>
           <span class="n">Conv2d</span><span class="o">-</span><span class="mi">13</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>          <span class="mi">16</span><span class="p">,</span><span class="mi">384</span>
      <span class="n">BatchNorm2d</span><span class="o">-</span><span class="mi">14</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>             <span class="mi">512</span>
             <span class="n">ReLU</span><span class="o">-</span><span class="mi">15</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>               <span class="mi">0</span>
       <span class="n">Bottleneck</span><span class="o">-</span><span class="mi">16</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">]</span>               <span class="mi">0</span>
</pre></table></code></div></div><p><br /></p><p><br /></p><p>tensorboard와 같이 pytorch에서도 visdom을 통해 loss에 대한 통계같은 값들을 시각화 할 수 있다.</p><p><img data-src="/assets/img/cs231n/2021-09-28/0111.jpg" alt="Half-width image" data-proofer-ignore></p><p><br /></p><p><br /></p><h2 id="statictensorflow---vs---dynamicpytorch-graph">Static(tensorflow) Vs Dynamic(pytorch) graph <a href="#statictensorflow---vs---dynamicpytorch-graph" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/cs231n/2021-09-28/0120.jpg" alt="image" data-proofer-ignore></p><p>먼저 tensorflow는 두 단계로 나눌 수 있다. 그래프를 구성하는 단계, 두번째는 그래프를 반복적으로 실행하는 단계이다.</p><p>그래프가 단 하나만 고정적으로 존재하는 형태를 <strong>static computational graph</strong>라고 한다.</p><p><br /></p><p>반면 pytorch의 경우 매번 forward pass할 때마다 새로운 그래프를 구성한다.</p><p>이를 <strong>dynamic computational graph</strong>라 한다.</p><p><br /></p><p>단순한 과정에서는 별 차이 없어보이지만, 차이점은 분명 존재한다.</p><p><br /></p><p>static graph는 한번 그래프를 구성해놓으면 학습시에 계속 똑같은 그래프를 재사용한다. 그렇다는 것은 그래프를 최적화시킬 수 있다는 것을 의미한다.</p><p>일부 연산들을 합치거나 재배열시키는 등으로 가장 효율적으로 연산하도록 최적화시킬 수 있다는 것이다.</p><p>처음에는 최적화작업이 오래걸리지만, 최적화한 그래프를 여러번 사용하기 때문에, 손해가 아니다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-28/0121.jpg" alt="image" data-proofer-ignore></p><p>예를 들어, 여기 Conv와 Relu를 합쳐버릴 수 있다.</p><p>반면 dynamic graph에서는 그래프를 최적화하기 어렵다.</p><p><br /></p><p><br /></p><p>또 다른 차이점으로는 serialization에 관한 것이다.</p><p>static graph를 사용한다고 해보자. 그래프를 한 번 구성해놓으면 메모리 내에 그 네트워크 구조를 가지고 있게 된다. 그렇다는 것은 그래프 자체, 네트워크 구조 자체를 disk에 저장할 수 있다. 이렇게 되면 전체 네트워크 구조를 파일 형태로 저장할 수 있다. 그러면 원본 코드 없이도 그래프를 다시 불러올 수 있다.</p><p>python에서 불러올 수 있을 뿐만 아니라 c++에서도 불러 올 수 있다.</p><p><br /></p><p>반면 dynamic graph의 경우 그래프 구성과 그래프 실행하는 과정이 묶여 있다. 그렇기에 그래프를 다시 불러오기 위해서는 항상 원본 코드가 필요하다.</p><p><br /></p><p>하지만 dynamic은 대다수의 경우에 코드가 훨씬 깔끔하고 작성하기 더 쉽다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-28/0125.jpg" alt="image" data-proofer-ignore></p><p>if문을 만들 때, pytorch는 단순하게 if문을 사용하면 된다.</p><p>하지만 tensorflow의 경우 우선 그래프를 만들고 난 후 그래프 내에 조건부 연산을 정의하는 코드가 더 필요하다. 때문에, if문 대신 control flow 자체를 넣어줘야 한다. 그래프가 실행되기 전에 연산을 넣는 것이다.</p><p><br /></p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-28/0127.jpg" alt="image" data-proofer-ignore></p><p>또한, 반복연산(loops)에서도 차이점이 있다.</p><p>우리의 데이터는 다양한 사이즈일 수 있다. 데이터의 길이가 얼마인지 신경쓰지 않고 재귀 연산을 할 수 있다면 좋을 것이다.</p><p>pytorch의 경우 기본적인 for문을 사용하면 된다.</p><p><br /></p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-28/0128.jpg" alt="image" data-proofer-ignore></p><p>그러나 tensorflow에서는 그래프를 앞에서 미리 만들어줘야 하기 때문에, 그래프에 명시적으로 loop를 넣어줘야만 한다.</p><p>특정 재귀적인 관계를 정의하기 위해 tf.fold1연산을 사용한다. fold는 dynamic graph처럼 만들어주기 위한 트릭이다.</p><p><br /></p><p><strong>정리하자면</strong></p><p><code class="language-plaintext highlighter-rouge">static graph</code></p><ol><li>그래프를 구성해놓으면 재사용이 가능, 즉 그래프를 최적화시킬 수 있다.</ol><p>ex) conv -&gt; relu -&gt; conv -&gt; relu ==&gt; conv+relu -&gt; conv+relu</p><ol><li>그래프를 구성 -&gt; 네트워크 구조 자체를 disk에 저장 가능 -&gt; 파일 형태로 네트워크 구조를 저장 가능 -&gt; 원본 코드 없이 그래프 다시 불러오기가 가능<li>조건부 연산(if 문) 그래프를 따로 만들어야 함</ol><p><br /></p><p><code class="language-plaintext highlighter-rouge">dynamic graph</code></p><ol><li>forward pass할때마다 그래프를 구성 -&gt; 그래프를 최적화하기 어려움<li>모델을 다시 불러오기 위해서는 항상 원본 코드 필요<li>대신 코드가 더 깔끔하고 작성하기 쉬움</ol><p><br /></p><p><br /></p><p><br /></p><p>그렇다면 어떤 상황에서 dynamic graph를 사용해야 할까?</p><p>최근 나온 network인 <strong>image captioning</strong>이 있다. image captioning은 다양한 길이의 sequences(출력)를 다루기 위해 RNN을 이용한다.</p><p>sequence는 입력 데이터에 따라서 다양하게 변할 수 있다. sequence의 크기에 따라 computational graph가 커질 수도, 작아질 수도 있다.</p><p>따라서 <strong>image captioning</strong>은 dynamic graph를 이용할 일반적인 예시 중 하나다.</p><p><br /></p><p>또는, <strong>자연어 처리</strong> 분야에서 문장을 파싱하는 문제에서 트리를 파싱하기 위해 <strong>recursive(반복)한 네트워크</strong>가 필요할 수 있다.</p><p>이런 경우 layer의 sequence 구조를 이용하기 보다 graph나 tree 구조를 이용한다. 즉, 데이터에 따라 다양한 graph나 tree 구조를 가질 수 있다.</p><p>이 때, tensorflow로 구성하려면 정말 복잡하고 까다롭다. 따라서 dynamic graph를 사용하는 것이 좋다.</p><p><br /></p><p>VQS를 다루는 neuro module(신경 모듈)이라는 연구가 있다. <strong>이미지와 질문을 던지면 적절한 답</strong>을 해주는 것이다.</p><p>질문에 따라 그 질문의 답을 위한 적절한 네트워크를 구성한다. 또, 질문이 물체 2개를 비교하는 것이라면 find, count와 compare을 수행해야 할 것이다.</p><p><br /></p><p><br /></p><h2 id="caffe">Caffe <a href="#caffe" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>다른 딥러닝 프레임워크랑 다소 다르게 코드를 작성하지 않아도 네트워크를 학습시킬 수 있다.</p><p>기존에 빌드된 바이너리를 호출하고 configuration 파일만 조금 수정하면 코드를 작성하지 않아도 데이터를 학습시킬 수 있다.</p><p>training 과정</p><ol><li>데이터를 HDF5 또는 LMDB포맷으로 변환시킨다.<li>코드 작성 대신 prototxt라는 텍스트파일을 만든다.<li>입력은 HDF5파일로 받는 부분도 있고 내적을 하는 부분도 있다. 이런 형식으로 그래프를 구성한다.<li>훈련시킨다.</ol><p>이런 방식은 네트워크의 규모가 커지면 상당히 보기 안좋다. 152layer 모델을 훈련시키려면 prototxt는 거의 7000줄이 된다.</p><p>그래서 prototxt생성 대신 python 스크립트를 작성한다.</p><p>caffe는 그래프를 하나하나 전부 다 기술해줘야 한다. 또, optimizer이나 solver을 정의하기 위해 또 다른 prototxt 파일을 작성해야 한다.</p><p>caffe에도 pretrained model을 제공한다. alexnet,vgg,googlenet,resnet 등등이 있다.</p><p>caffe는 산업이나 생산 개발에 많이 쓰인다.</p><p><br /></p><p><br /></p><h2 id="caffe2">Caffe2 <a href="#caffe2" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>caffe2는 caffe의 다음 버전이다.</p><p>caffe2는 static graph를 사용한다. caffe와 같이 C++로 작성되어 있고, python 인터페이스도 제공한다.</p><p>caffe와 차이점은 더이상 prototxt파일을 만들기 위해 python 스크립트를 작성할 필요가 없다는 것이다.</p><p><br /></p><p><br /></p><p><br /></p><h1 id="reference">Reference</h1><ul><li><a href="http://cs231n.stanford.edu/2017/syllabus.html">http://cs231n.stanford.edu/2017/syllabus.html</a><li><a href="https://blog.naver.com/asdjklfgh97/222157100126">https://blog.naver.com/asdjklfgh97/222157100126</a></ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/cs231n8/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/cs231n8.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/cs231n/'>CS231N</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/cs231n/" class="post-tag no-text-decoration" >CS231N</a> <a href="/tags/framework/" class="post-tag no-text-decoration" >framework</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=CS231N chapter 8 - Deep Learning Softwares - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/cs231n8/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=CS231N chapter 8 - Deep Learning Softwares - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/cs231n8/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/cs231n8/&amp;text=CS231N chapter 8 - Deep Learning Softwares - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/cs231n8/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/cs231n1/"><div class="card-body"> <em class="timeago small" date="2021-09-04 13:00:00 +0900" >Sep 4, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CS231N chapter 1 - Course Introduction</h3><div class="text-muted small"><p> A brief history of Computer Vision Biological Vision 아주 먼 옛날, 눈을 통해 동물들은 진화했다. Human Vision 카메라와 같이 사람의 눈과 유사하게 사진을 담을 수 있도록 만들어졌다. Computer Vision ** David Mars** 이미지를 인식하고 최종...</p></div></div></a></div><div class="card"> <a href="/posts/cs231n2/"><div class="card-body"> <em class="timeago small" date="2021-09-04 13:00:00 +0900" >Sep 4, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CS231N chapter 2 - Image Classification</h3><div class="text-muted small"><p> numpy를 이용한 텐서 사용이 익숙하지 않는 사람들을 위한 튜토리얼 Image Classification 이미지를 볼 때 컴퓨터는 거대한 숫자 그리드로 표현한다. 그것을 통해 컴퓨터는 미리 정의된 class 레이블 중 하나를 지정한다. 이 이미지의 경우 800x600 픽셀로 이루어져 있다. 또한, 3채널 즉, RGB로 표현되기 때문에 8...</p></div></div></a></div><div class="card"> <a href="/posts/cs231n3/"><div class="card-body"> <em class="timeago small" date="2021-09-12 13:00:00 +0900" >Sep 12, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CS231N chapter 3 - Loss function and Optimizations</h3><div class="text-muted small"><p> 2강 리뷰 1) image deformation 2) 그를 해결하기 위한 data-driven approach 3) Nearest Neighbor method, K-Nearest Neighbor(KNN) 4) cross validation 5) hyperparameter 6) linear cl...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/cs231n7/" class="btn btn-outline-primary" prompt="Older"><p>CS231N chapter 7 - Training Neural Network part 2</p></a> <a href="/posts/cs231n9/" class="btn btn-outline-primary" prompt="Newer"><p>CS231N chapter 9 - CNN Architectures</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>