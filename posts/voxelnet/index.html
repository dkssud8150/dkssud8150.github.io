<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 논문에 대한 리뷰한 글입니다. 이 논문은 2018 CVPR에 투고된 논문이며 약 1615회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><meta property="og:description" content="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 논문에 대한 리뷰한 글입니다. 이 논문은 2018 CVPR에 투고된 논문이며 약 1615회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><link rel="canonical" href="https://dkssud8150.github.io/posts/voxelnet/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/voxelnet/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-18T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-01-18T13:00:00+09:00","datePublished":"2022-01-18T13:00:00+09:00","description":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 논문에 대한 리뷰한 글입니다. 이 논문은 2018 CVPR에 투고된 논문이며 약 1615회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.","headline":"[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/voxelnet/"},"url":"https://dkssud8150.github.io/posts/voxelnet/"}</script><title>[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-src="/assets/img/autodriving/voxelnet/fig6.png" class="preview-img bg" alt="Preview Image" width="800" height="500" data-proofer-ignore><h1 data-toc-skip>[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-01-18 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Tue, Jan 18, 2022, 1:00 PM +0900" >Jan 18, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="8603 words"> <em>47 min</em> read</span></div></div></div><div class="post-content"><p>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 논문에 대한 리뷰한 글입니다. 이 논문은 2018 CVPR에 투고된 논문이며 약 1615회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.</p><p><br /></p><h1 id="abstract">Abstract</h1><p>3D point cloud안에서 정확한 객체 검출은 네비게이션이나 집 지킴이, AR/VR에 적용하기에는 아직 큰 문제들이 있다. 매우 희박한 LIDAR point cloud에 RPN(region proposal network)을 적용시키기 위해, 기존에는 조감도(bird’s eye view) 투영과 같이 수작업으로 제작된 피쳐 representation에 초점을 맞췄다. 이 논문에서는 3D point cloud를 위한 수동적인 특징 처리 대신, 심층 네트워크에서 훈련이 가능한 end-to-end이고, 단일 단계에서처럼 특징 추출과 bounding box 예측을 통합한 포괄적인 3D detection network인 <strong>VoxelNet</strong>을 제안한다. 특히 VoxelNet은 point cloud를 동등한 공간의 3D voxel로 나누고, 새로 도입된 VFE(voxel feature encoding) Layer를 통해 각 voxel내의 point 그룹을 통일된 특징 표현으로 변환한다. 이러한 방식으로, point cloud는 서술적 체적 표현으로서 인코딩되며, 이 표현은 RPN에 연결되어 detection을 생성한다. KITTI car detection benchmark에서의 실험에서 VoxelNet은 최첨단 LIDAR 기반의 3D detection 방법을 큰 차이로 능가했다. 게다가, 이 네트워크는 다양한 형상을 가진 물체의 효과적인 차별적 표현을 학습하여, LIDAR만을 기반으로 하여 보행자와 자전거 이용자를 3D detection하는 결과를 도출했다.</p><h1 id="1-introduction">1. Introduction</h1><p>point cloud 기반의 3D 객체 검출은 미래의 세상을 구성하는데 중요한 요소다. 이미지 기반의 detection과 달리 LIDAR는 객체를 정확하게 위치화하고, 그들의 모양을 정확하게 구분하는데 사용되는 신뢰할 수 있는 depth 정보를 제공한다. 그러나, 이미지와 달리 LIDAR point cloud는 매우 희박하고, 3D 공간의 비균일한 샘플링과 센서의 유효 범위, 충돌, 상대적인 모습 때문에 점 밀도가 변화하기 너무 쉽다. 이런 까다로움을 다루기 위해 많은 접근 방식듣이 3D 객체 감지를 위해 조정된 point cloud를 위해 수동으로 특징 표현을 만들었다. 몇몇의 방법들은 point cloud를 투영하고, 이미지 기반의 특징 추출 기술을 적용한다. 다른 방법으로는 point cloud를 3D voxel grid로 전환시키고 각 voxel을 수작업으로 조작하여 기능을 인코딩한다. 하지만, 이러한 수작업 설계 방법은 이런 접근 방식들이 3D 특징 정보와 검출 작업에 필요한 불변성을 효과적으로 이용하는 것을 막는 정보 병목 현상을 초래한다. 수작업 기능 제작에서 기계 기능 학습으로 변화되고 있기 때문에 이미지에 대한 인지와 검출 작업이 주요 돌파구가 될 것이다.</p><p>최근에 point cloud에서 직접 포인트별 특징을 학습하는 end-to-end 심층 신경망인, PointNet을 제안했다. 이 접근 방식은 3D object detection, 3D object part segmentation 및 포인트별 semantic segmentation 작업에 대해서 인상적인 결과를 보여주었다. PointNet의 향상된 버전은 다른 크기들에게서 지역 구조를 배울 수 있다. 만족스러운 결과를 성취하기 위해 이 pointNet의 접근 방식들은 모든 입력 점들에 대해 특징 변환 네트워크를 훈련시켰다. 하지만 전형적인 point cloud방법들은 LIDAR를 사용하여 약 100K 점을 얻게 되는데, 이에 대해 아키텍처를 훈련시키는 것은 높은 계산과 메모리를 요구한다. 3D 특징 학습 네트워크를 훨씬 더 많은 점들과 3D detection작업으로 확장시키는 것은 본 논문에서 다루는 주요 과제이다.</p><p>RPN(Region Proposal Network)는 효과적인 객체 검출에 매우 최적화된 알고리즘이다. 그러나 이 접근 방식은 빽빽하고, 전형적인 LIDAR point cloud에 대한 구조가 아닌 이미지나 비디오와 같은 텐서구조 안에서 정돈된 데이터를 요구한다. 본 논문에서는 3D detection 작업에 대한 점들의 특징 학습과 RPN 사이의 격차를 좁히고자 했다.</p><p>그래서 본 논문에서는 point cloud에서 차별적 특징 표현을 동시에 학습하고 정확한 3D bounding boxes를 아래 그림과 같이 end-to-end방식으로 예측하는 일반적인 3D detection 프레임워크인 VoxelNet을 제안한다.</p><p><img data-src="/assets/img/autodriving/voxelnet/fig2.png" data-proofer-ignore></p><p>저자는 점 단위 특징을 지역적으로 집계된 특징과 결합하여 voxel 내에서 점간 상호작용을 가능하게 하는 새로운 VFE(voxel feature encoding) layer을 설계한다. 여러 VFE layer를 쌓으면 로컬 3D 형상 정보를 특성화하기 위한 복잡한 특징을 학습할 수 있다. 특히, VoxelNet은 point cloud를 균등한 3D voxel 공간으로 나누고, 쌓인 VFE layer을 통해 각 voxel을 인코딩하고, 그렇게 되면 3D convolution은 로컬 voxel 특징을 추가로 통합하여 point cloud를 고차원 공간적 표현으로 변환한다. 결국, RPN은 공간적 표현을 소비하고 검출 결과를 출력하게 된다. 이 효과적인 알고리즘은 밀도가 희박한 점 구조와 voxel grid에서 효과적인 병렬 프로세스 둘 다에게 이점을 가져다준다.</p><p>저자는 KITTI benchmark에서 제공되는 full 3D detection tasks와 조감도 검출에서 VoxelNet을 평가했다. 실험은 VoxelNet이 최첨단 LIDAR 기반의 3D detection을 큰 차이로 능가한다는 것을 보여준다. 또, VoxelNet이 LIDAR point cloud로부터 보행자와 자전거를 검출하는 결과를 잘 성취한다는 것을 보여준다.</p><h2 id="11-related-work">1.1 Related Work <a href="#11-related-work" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>이전에 나왔던 특징 표현의 방법들은 특징들을 수작업했고, 이는 많고 자세한 3D 형상 정보를 사용했을 때 좋은 결과를 야기했다. 하지만, 더 복잡한 형상과 장면들에 대해서는 추론하지 못했고, 데이터로부터 필요한 불변성을 학습할 수 없었기에 자율 탐색과 같은 통제되지 않는 시나리오에서는 다소 제한적인 성공을 거두었다.</p><p>더 자세한 텍스쳐 정보를 제공받았다고 한다면, 많은 알고리즘이 2D 이미지에서 3D bounding boxes를 추론했을 것이다. 하지만, 이미지 기반의 3D 검출 접근 방식의 정확도는 깊이 추정(depth estimation)에 따라 결정된다.</p><p>몇몇의 LIDAR 기반의 3D 객체 검출 기술들은 voxel grid 표현을 활용한다. 아래의 리스트는 voxel grid를 활용한 사례들이다.</p><ul><li>Vote3Deep: voxel 내에 포함된 모든 점에서 6개의 정보들을 통해 비어있는지를 판단한 후, 안 비어있는 voxel을 인코딩함<li>로컬 정보들을 융합하여 각 voxel을 표현함<li>voxel grid에서 부정확한 부호들을 계산함<li>3D voxel grid에 대한 이진 인코딩을 사용함<li>MV3D: 조감도에서 다채널 형상 맵과 정면뷰에서 원통형 좌표를 계산하여 LIDAR point cloud에 대한 다중 뷰 표현을 도입함</ul><h2 id="12-contributions">1.2 Contributions <a href="#12-contributions" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li><p>본 논문에서는 point cloud 기반의 3D 검출을 위한 새로운 훈련 가능한 end-to-end 심층 아키텍처인 VoxelNet을 제안했다. 이 VoxelNet은 희박한 3D 점들에서도 잘 동작하고, 수작업을 통한 특징 작업에 의해 발생되는 정보 병목(bottleneck) 현상을 방지한다.</p><li><p>희박한 점 구조와 voxel grid에서 효과적인 병렬 프로세싱에 대해 이점이 있는 VoxelNet을 구현하는 효과적인 방법을 소개했다.</p><li><p>KITTI benchmark에서 실험을 진행했다.</p></ul><p><br /></p><h1 id="2-voxelnet">2. VoxelNet</h1><p>이제부터 VoxelNet의 훈련에 사용된 loss function 구조를 설명하고, 네트워크를 구현하는 효과적인 알고리즘을 설명하고자 한다.</p><h2 id="21-voxelnet-architecture">2.1 VoxelNet Architecture <a href="#21-voxelnet-architecture" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/autodriving/voxelnet/fig2.png" data-proofer-ignore></p><p>VoxelNet은 위의 그림처럼 <code class="language-plaintext highlighter-rouge">Feature learning network</code>,<code class="language-plaintext highlighter-rouge">Convolutional middle layers</code>,<code class="language-plaintext highlighter-rouge">Region Proposal Network</code>의 3가지 블록을 가지고 있다.</p><h3 id="211-feature-learning-network">2.1.1 Feature Learning Network <a href="#211-feature-learning-network" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Voxel Partition</ul><p>fig2에서 볼 수 있듯이, point cloud의 경우, 3D 공간을 균등한 voxel 공간으로 나눈다. point cloud가 X,Y,Z축을 따라 W,H,D 범위의 3D 공간을 차지한다고 가정해보자. 그리고 각 voxel의 크기를 vD,vH,vW로 정의한다면 3D voxel grid의 크기 D’ = D/vD , H’ = H/vH , W’ = W/vW 가 될 것이다. 그렇다면 D, H, W가 vD, vH, vW의 배수라고 생각할 수 있다.</p><p><br /></p><ul><li>Grouping</ul><p>voxel에 따라 점들을 그룹화한다. 거리, 충돌, 객체의 상대적 모양, 비균일한 샘플링 등과 같은 요소들 때문에, LIDAR point cloud는 매우 희박하고, 공간 전체에 걸쳐 매우 가변적인 점 밀도를 가진다. 따라서, 그룹화후에는, voxel은 가변적인 점들을 포함할 것이다. fig2를 보면, Voxel-1은 Voxel-2, Voxel-4보다 매우 더 많은 점들을 포함하고 있고, Voxel-3는 점을 포함하고 있지 않다.</p><p><br /></p><ul><li>Random Sampling</ul><p>전형적으로 고차원의 LIDAR point cloud는 최대 약 100k의 점들로 구성되어 있다. 모든 점을 프로세싱하는 것은 계산에 대한 메모리와 효율을 증대시킬 뿐만 아니라 공간 전체에 걸쳐 매우 가변적인 점 밀도를 증가시킬 수 있다. 이를 해결하기 위해, T개의 점보다 더 많이 포함되어 있는 voxels에 대해서 고정된 점들의 수, <code class="language-plaintext highlighter-rouge">T</code>를 무작위로 샘플링한다. 이 샘플링 전략은 두가지의 목적을 가지는데, (1) 계산상 절약, (2) voxel 사이의 점의 불균형을 감소시켜 샘플링 편향을 줄이고 훈련에 더 많은 변화를 더하고자 한다.</p><p><br /></p><ul><li>Stacked Voxel Feature Encoding</ul><p><img data-src="/assets/img/autodriving/voxelnet/fig3.png" data-proofer-ignore></p><p>핵심은 VFE layer들의 나열이다. 단순성을 위해 fig2에서는 하나의 voxel에 대한 계층적 특징 인코딩 프로세스를 볼 수 있으며, 일반성을 잃지 않은 채로 VFE layer-1을 사용할 것이다. VFE layer-1는 위의 fig3에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/voxel-1.png" data-proofer-ignore></p><p>voxel V에 대해 t는 t &lt;= T 개의 LIDAR 점, pi는 i번째 점에 대한 X,Y,Z 좌표를 포함하며, ri는 수신된 반사율을 의미한다. 먼저 voxel V = (vx, vy, vz)안의 모든 점의 중심으로서, 지역 평균을 계산한다. 그러면 각 점 pi의 상대적 중심 좌표인 w,r,t로 판단해볼 수 있으며, 입력 특징셋 Vin을 얻을 수 있다. Vin에 대해서는 다음과 같다.</p><p><img data-src="/assets/img/autodriving/voxelnet/vin.png" data-proofer-ignore></p><p>그 다음으로 각 pˆi은 FCN(fully connected network)를 통해 형상 공간을 변환되며, 여기서 voxel에 포함된 표면의 모양을 인코딩하기 위해 점 특징인,fi ∈ R^m 으로부터 정보를 수집할 수 있다. FCN은 선형 layer, BN(batch normalization) layer, ReLU layer로 구성되어 있다. 점 단위 특징 표현들을 얻은 후, V에 대한 각 voxel별 집계된 특징인,˜f ∈ R^m 을 얻기 위해 V와 연관된 모든 fi에서 요소별 Map Pooling을 적용한다. 마지막으로, 각 fi를 ˜f로 만들어서 점 단위 연결 특징으로서 fi,out를 형성한다. fi,out에 대해서는 아래와 같다.</p><p><img data-src="/assets/img/autodriving/voxelnet/fout.png" data-proofer-ignore></p><p>그렇게 되면, 출력 특징셋인 Vout을 얻을 수 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/vout.png" data-proofer-ignore></p><p>모든 비어 있지 않은 voxel들은 같은 방법으로 인코딩되고, FCN의 파라미터셋을 같이 공유한다.</p><p>cin차원의 입력 피쳐를 Cout 차원의 출력 피쳐로 변환하는 i번째 VFE layer를 나타내는 VFE-i(Cin,Cout)을 사용한다. 선형 layer에서는 행렬 크기 Cin x Cout/2를 학습하고, 점 단위 결합은 Cout 차원의 출력을 리턴한다.</p><p>출력 피쳐들은 점 단위 피쳐들과 지역적으로 집계된 특징들을 모두 결합하여 가지고 있기 때문에, VFE layer를 쌓는 것은 voxel 내의 점 상호작용이 인코딩되고, 최종 피쳐는 서술적인 형상 정보를 학습할 수 있다. voxel 단위의 피쳐는 fig2에서 보이는 voxel 단위의 피쳐의 차원을 C라고 할 때 FCN을 통해 VFE-n의 출력을 R^C로 변환하고, 요소별 Maxpooling을 적용함으로써 얻어진다.</p><p><br /></p><ul><li>Sparse Tensor Representation</ul><p>비어 있지 않은 voxel들만을 프로세싱함으로써, 우리는 각각 비어 있지 않은 특정 voxel의 공간 좌표에 고유하게 연관된 voxel 피쳐 리스트를 얻는다. 얻어진 voxel 단위의 피쳐들은 fig2에서 볼 수 있듯이 C x D’ x H’ x W’의 크기를 가진 희박한 4D tensor로서 나타내진다. point cloud가 약 최대 100k 점들을 가지고 있지만, 90%가 비어 있다. sparse tensor로서 비어 있지 않은 voxel을 나타내는 것은 메모리 사용과 역전파 시에 계산 비용을 줄일 수 있으며, 이것은 효과적인 실행에 중요한 단계에 해당한다.</p><p><br /></p><h3 id="212-convolutional-middle-layers">2.1.2 Convolutional Middle Layers <a href="#212-convolutional-middle-layers" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>입력과 출력의 채널의 수를 나타내는 Cin,Cout과 각각 커널 사이즈, stride 사이즈, 패딩 사이즈에 해당하는 M차원의 벡터 k,s,p인 ConvMD(Cin,Cout,k,s,p)를 사용하여 M차원의 convolution 작업을 표현한다. M차원의 크기가 동일한 경우, k = (k,k,k)의 스칼라를 사용한다.</p><p>각 convolutional middle layer은 3D convolution, BN layer, ReLU layer를 순차적으로 적용한다. 이 layer들은 점진적으로 확장하는 수용 범위(receptive field)내에서 voxel 단위의 피쳐들을 집계하여 형상 설명에 더 많은 내용을 추가한다. 필터들의 자세한 설명은 section 3에서 더 설명할 것이다.</p><p><br /></p><h3 id="213-region-proposal-network">2.1.3 Region Proposal Network <a href="#213-region-proposal-network" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>최근에, 지역 제안 네트워크는 최고 성능의 객체 탐지 프레임워크의 중요한 구성 요소가 되었다. 이 논문에서는 RPN 아키텍쳐에서 몇가지 중요한 수정을 하고, 이를 feature learning network와 convolutional middle layer와 결합하여 end-to-end로 훈련가능한 파이프라인을 형성할 것이다.</p><p>이 논문에서의 RPN의 입력은 convolutional middle layer에서 제공된 피쳐 맵이다. 아키텍쳐는 아래 그림에서 자세히 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/fig4.png" data-proofer-ignore></p><p>이 네트워크는 fully convolutional layer의 3가지 블록이 있다. 각 블록의 첫번째 layer는 stride가 2인 convolution을 통해 특징맵을 1/2 크기로 다운샘플링한 다음, stride 1인 convolution을 차례로 진행한다. 각 convolution layer이후에, BN과 ReLU 작업을 수행한다. 다 진행하고 나면 모든 블록이 고정된 사이즈의 출력으로 업샘플링되고, 고해상도 피쳐 맵을 구성하기 위해 이들을 결합한다. 이 피쳐 맵은 (1) 확률 score 맵, (2) regression 맵에 각각 매핑된다.</p><p><br /></p><h2 id="22-loss-function">2.2 Loss Function <a href="#22-loss-function" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>positive anchor를 Npos와 negative anchor를 Nneg에 대해 각각</p><p><img data-src="/assets/img/autodriving/voxelnet/Npos.png" width="50%" data-proofer-ignore><img data-src="/assets/img/autodriving/voxelnet/Npos.png" width="50%" data-proofer-ignore></p><p>그리고 3D GT(ground Truth) box를 (xc^g,yc^g,zc^g,l^g,w^g,h^g,θ^g)라 할 때, 각 파라미터에 대해 다음과 같이 설명한다.</p><ul><li>θ^g: Z축을 중심으로 회전 각도 (이때 z축을 중심으로 회전하는 것을 yaw rotation이라 하고, x,y축을 중심으로 회전하는 것을 각각 roll, pitch rotation이라 함)<li>(l^g,w^g,h^g): box의 길이,너비,높이<li>(xc^g,yc^g,zc^g): 중심 좌표</ul><p>(xc^g,yc^g,zc^g,l^g,w^g,h^g,θ^g)로 정의되는 poisitive anchor과 GT box를 매칭시키기 위해, 중심 좌표(∆x, ∆y, ∆z)와, 3차원(∆l,( ∆w, ∆h), 회전(∆θ) 에 해당하는 7개의 regression targets를 포함하는 잔류 벡터인 u* ∈ R^7를 정의한다. 이 때, 각 파라미터는 다음과 같이 계산한다.</p><p><img data-src="/assets/img/autodriving/voxelnet/ustar.png" data-proofer-ignore></p><p>이 때, d^a = ((l^a)^2 + (w^a)^2)^(1/2) 는 앵커 박스 밑면의 대각선을 의미한다. 저자는 oriented 3D box를 직접 추정하고, ∆x, ∆y를 대각선 d^a로 균일하게 정규화하는 것을 목표로 하고 있다. 그리고 손실 함수는 다음과 같이 정의한다.</p><p><img data-src="/assets/img/autodriving/voxelnet/lossf.png" data-proofer-ignore></p><p>이때, pi^pos와 pj^neg는 각각 positive anchor ai^pos와 negative anchor aj^pos에 해당하며, ui ∈ R^7와 ui* ∈ R^7은 각각 regression 출력과 positive anchor ai^pos에 대한 GT에 해당한다.</p><p>위의 손실함수를 세분화하여 따져보면,</p><p><img data-src="/assets/img/autodriving/voxelnet/lossf1.png" data-proofer-ignore></p><p>앞의 2개 텀을 보게 되면, Lcls는 binary cross-entropy loss를 나타내고, α, β는 상대적 중요도의 균형을 유지하는 양의 상수, 즉 가중치를 나타낼 때, 각각 {ai^pos}i=1…Npos 와 {aj^neg}j=1…Nneg 에 대한 정규화된 classification loss이다.</p><p><img data-src="/assets/img/autodriving/voxelnet/lossf2.png" data-proofer-ignore></p><p>마지막 텀에서 Lreg는 smoothL1 함수를 사용한 regression loss이다.</p><p><br /></p><h2 id="23-efficient-implementation">2.3 Efficient Implementation <a href="#23-efficient-implementation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>GPU들은 빽빽한 텐서 구조를 프로세싱하는데 최적화되어 있다. point cloud를 직접적으로 작업하는데에 있어서 문제점은 공간에서 점들이 희박하게 분포되어 있고, 각 voxel은 점들에 대한 가변적인 수를 가지고 있기 때문이다. 그래서 저자는 점들와 voxel들을 병렬로 처리할 수 있는 stacked VFE layer를 통해 point cloud를 고밀도 텐서 구조로 변환하는 방법을 고안했다. 이에 대한 그림은 아래에 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/fig5.png" data-proofer-ignore></p><p>저자는 K x T x 7차원의 텐서 구조를 초기화하고, 여기에 비어 있지 않은 voxel들의 최대값을 K에, 각 voxel의 점의 최댓값을 T에, 각 점에 대해 입력 인코딩 차원을 의미하는 7을 voxel 입력 피쳐 버퍼를 저장한다. 그 점들은 프로세싱 전에는 무작위로 된다. point cloud의 각 점에 대해 해당voxel이 이미 존재하는지를 점검해봐야 한다. 이 점검 작업은 O(1)에서, voxel 좌표가 hash key로 사용되는 hash 테이블을 사용하여, 효율적으로 수행된다. 만약 voxel이 이미 초기화된 경우, 점의 갯수가 T보다 작으면 voxel에 점을 삽입하고, 그렇지 않으면 그 점은 무시한다. 그러나 voxel이 초기화되지 않았다면, voxel을 초기화하고, 이 voxel좌표를 voxel 좌표 버퍼에 저장하고, 이 voxel에 점을 삽입한다. voxel 입력 피쳐와 좌표 버퍼는 점 목록을 통과하여 구성될 수 있고, 그래서 그것의 복잡도는 O(n)이 된다. 메모리/계산 효율을 더 높이기 위해 제한된 voxel의 수 K만을 저장하고, 점이 별로 없는 voxel에 대한 점들은 무시할 수도 있다.</p><p>voxel 입력 버퍼가 구성된 후에 쌓여있는 VFE는 GPU에서 병렬로 계산할 수 있는 point 레벨과 voxel 레벨 밀도 연산만 포함한다. VFE에서 결합 작업이 끝난 후에 점이 없는 곳에 해당하는 피쳐를 0으로 설정하여 계산된 voxel 피쳐에 영향을 미치지 않도록 한다. 마지막으로, 저장된 좌표 버퍼를 이용하여 계산된 희박한 voxel 단위의 구조들을 밀집 voxel grid로 재구성한다. 이렇게 되면, 다음에 오는 convolutional middle layer와 RPN 작업은 GPU에서 효율적으로 동작할 수 있는 밀집된 voxel grid에서 작동하게 된다.</p><p><br /></p><h1 id="3-training-details">3. Training Details</h1><p>이번에는 VoxelNet의 구현 세부 정보와 트레이닝 절차를 설명한다.</p><h2 id="31-network-details">3.1 Network Details <a href="#31-network-details" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>실험은 KITTI 데이터 셋중 LIDAR 셋을 기반으로 한다.</p><ul><li>Car Detection</ul><p>이 태스크에서의 셋팅은 다음과 같다.</p><ul><li>X,Y,Z축에 대해 [0,70.4] x [-40,40] x [-3,1] meters 범위 안에 있는 point cloud를 고려<li>이미지 경계 범위 밖에 투영된 점들은 제거<li>voxel 사이즈 vD = 0.4, vH = 0.2, vW = 0.2를 선택했으며, 이에 따른 D’ = 10, H’ = 400, W’ = 352로 정함<li>비어 있지 않은 각 voxel에 대해 무작위로 샘플링된 점들의 최대 수로서, T = 35로 세팅<li>VFE-1(7,32)와 VFE-2(32,128) layer를 사용<li>마지막 FCN은 VFE-2 출력을 R^128에 매핑<li>sparse tensor = (128 x 10 x 400 x 352)<li>voxel 단위로 피쳐를 집계하기 위해 순차적으로 3개의 convolution middle layer을 적용하여 (64 x 2 x 400 x 352)의 4D 텐서 사이즈를 산출 =&gt; Conv3D(128, 64, 3,(2,1,1),(1,1,1)), Conv3D(64,64,3,(1,1,1),(0,1,1)), Conv3D(64,64,3,(2,1,1),(1,1,1))</ul><p>reshape 후에, RPN의 입력은 각 channel, height, width의 차원에 대해 (128 x 400 x 352)의 사이즈의 3D 텐서 피쳐 맵이다. 아래의 그림을 통해 아키텍처를 더 자세하게 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/fig4.png" data-proofer-ignore></p><p>또한, l^a = 3.9, w^a = 1.6, h^a = 1.56 meters이고, 중심이 zc^a = -1.0 meter에 있으며, 0과 90도의 회전을 가지는 1개의 앵커 박스만을 사용한다. 앵커는 조감도에서 GT와 IOU가 가장 높거나 GT와의 IOU가 0.6 초과인 것에 대해서는 양의 값을 가진다. 또한, GT와 IOU가 0.45 미만인 것들은 음의 값을 가진다. 그 사이의 값들, 즉 0.45 &lt;= IOU &lt;= 0.6 에 대해서는 고려하지 않는다. 그리고, 손실 함수에서의 α = 1.5, β = 1로 둔다.</p><p><br /></p><ul><li>Pedestrian and Cyclist Detection</ul><p>이 태스크의 셋팅은 다음과 같다.</p><ul><li>입력 범위는 X,Y,Z축에 대해 [0,48] x [-20,20] x [-3,1] meters의 범위로 둔다.<li>voxel 크기는 car detection과 동일하게 D = 10, H = 200, W = 240<li>형상 정보를 더 잘 포착하기 위해, 더 많은 LIDAR 점을 얻을 수 있는 T = 45로 한다.<li>feature learning network와 convolutional middle layer는 car detection에서 사용되는 네트워크와 동일하게 둔다.</ul><p>RPN에서는 fig4에서와 같이 첫 2D convolution에서의 stride 사이즈를 2에서 1로 바꿈으로써 1개의 블록으로 수정한다. 이를 통해 보행자 및 자전거 이용자를 감지하는데 필요한 앵커 박스에서 보다 미세한 해상도를 얻을 수 있다.</p><p>보행자에 대해서는 l^a = 0.8, w^a = 0.6, h^a = 1.73 meters, 중심은 zc^a = -0.6 meters에 위치하며, 0과 90도의 회전을 가지는 anchor box를 사용하고, 자전거 이용자에 대해서는 l^a = 1.76, w^a = 0.6, h^a = 1.73 meters, zc^a = -0.6 meters의 파라미터를 가지고, 0과 90도의 회전을 가지는 anchor box를 사용한다.</p><p>GT와의 IOU가 0.5 초과이면 양, 0.35 미만이면 음의 값을 가지며, 0.35 &lt;= IOU &lt;= 0.5의 값들은 고려하지 않는다.</p><p>훈련시에는 lr = 0.01, 150 epochs에 대한 SGD(stochastic gradient descent)을 사용하고, 그 다음 lr = 0.001, 10 epochs에 대한 SGD를 적용했다. 또한 batch size는 16으로 지정했다.</p><p><br /></p><h2 id="32-data-augmentation">3.2 Data Augmentation <a href="#32-data-augmentation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>4000개 미만의 training point cloud에서는 네트워크를 처음부터 훈련하면 과적합이 발생할 수 있으므로 데이터 증강을 통해 과적합을 방지하고자 한다. 본 논문에서는 총 3가지의 데이터 증강 방법을 소개한다. 증강된 훈련 데이터들은 디스크에 저장할 필요 없이 즉시 생성된다.</p><p>N개의 점들로 구성된 전체 point cloud M에 대해 다음과 같이 정의할 수 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/pointcloud.png" data-proofer-ignore></p><p>그리고, 3D bounding box, bi에 대해 (xc,yc,zc)의 중심 좌표를 가지고, (l,w,h)의 각각 길이,너비,높이, Z축을 중심으로 회전하는 각도 θ에 대한 (xc,yc,zc,l,w,h,θ)로 정의하고, 전체 셋 M안에서 특정 LIDAR 점을 나타내는 p = [x,y,z,r]과 bi안에 모든 LIDAR 점들을 포함하는 셋으로서 Ωi을 다음과 같이 정의할 수 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/umm.png" data-proofer-ignore></p><p>데이터 증강의 첫번째 방법은 독립적으로 각 GT 3D bounding box와 그 박스안의 LIDAR점들에게 작은 변화를 주는 것이다. 특히, Z축을 중심으로 균등하게 분포되어 있는 무작위 가변적인 ∆θ ∈ [-π/10, +π/10]에 의해, (xc,yc,zc)에 대해 bi와 그와 관련된 Ωi를 회전시킨다. bi의 X,Y,Z 성분과 Ωi의 각 점에 (∆x, ∆y, ∆z)을 추가한다. 여기서 (∆x, ∆y, ∆z)는 평균 0, 표준편차 1.0을 갖는 가우스 분포로부터 독립적으로 나타내진다. 물리적으로 불가능한 결과를 피하기 위해, 추가한 후에 모든 두 상자 사이에 대한 충돌 테스트를 진행하고, 충돌이 감지되면 추가하기 전의 원래의 값으로 되돌린다. 이 작은 변화가 각 GT box와 그와 연관된 LIDAR 점들을 독립적으로 적용시키기 때문에, 네트워크는 원래 훈련 데이터보다 훨씬 더 많은 가변성을 배울 수 있다.</p><p>두번째 방법은, 모든 point cloud M과 모든 GT box들에 전역 스케일링을 적용한다. 특히, X,Y,Z 좌표와 각 bi의 3개의 차원들, M에 있는 모든 점들의 X,Y,Z 좌표에 대한 균일한 분포[0.95,1.05]로부터 도출된 무작위 변수를 곱한다. 전역 스케일링 증강은 다양한 사이즈와 거리에 대한 객체 검출 네트워크의 강인함을 개선할 수 있다.</p><p>마지막으로, 모든 GT box bi와 모든 point cloud M를 전역 회전시킨다. 회전은 Z축과 (0,0,0)에 대해 적용된다. 이 회전 범위는 균일한 분포[−π/4, +π/4]로부터 샘플링된다. 모든 point cloud를 회전함으로써, 좌/우회전하는 차량을 시뮬레이션할 수 있다.</p><p><br /></p><h1 id="4-experiments">4. Experiments</h1><p>차, 보행자, 자전거 이용자에 대해서만 분류하고, 7481개의 훈련 이미지/point cloud와 7518개의 테스트 이미지/point cloud로 구성된 KITTI 3D Object detection benchmark에서 VoxelNet를 평가했다. 각 클래스마다 검출 결과는 객체 사이즈, 폐색 사이즈, 잘림 수준등에 따른 3가지 난도인 <em>easy, moderate, hard</em>에 기반하여 평가했다. 테스트셋의 GT는 이용할 수 없고, 테스트 서버는 제한되어 있기 때문에 훈련 데이터를 훈련과 검증 데이터셋으로 분할하여 3712개의 훈련 데이터 샘플, 3769개의 검증 데이터 샘플을 얻었다.</p><p>차량 카테고리에서 이미지 기반의 접근 방식인 Mono3D, 3DOP과 LIDAR 기반의 접근 방식인 VeloFCN, 3D-FCN, 그리고 멀티모달 접근 방식인 MV를 포함한 높은 수행 알고리즘과 비교했다. Mono3D와 3DOP, MV는 초기화를 위한 pretrain model을 사용했지만, VoxelNet은 KITTI에서 제공하는 LIDAR 데이터만을 사용하여 훈련시켰다.</p><p>end-to-end 학습의 중요성을 분석하기 위해 VoxelNet 아키텍쳐을 기본으로 하나, 위에서 제안한 feature learning network 대신에 수작업 제작된 피쳐들을 사용하는 강력한 베이스라인을 사용했다. 그래서 이 모델을 the hand-crafted baseline 이라 하여 HC-baseline라고 명명했다. HC-baseline은 MV3D에서처럼 0.1m의 해상도로 계산된 조감도 피쳐를 사용한다. 그러나 MV3D와 다른 점은 높이 채널의 수를 4에서 16으로 증가시켜 더 자세한 형상 정보를 얻는다. 그러나 이는 성능 개선으로는 이어지지 않았다. 또한, VoxelNet의 convolutional middle layer를 간단한 2D convolutional layer로 바꿨다. 2D convolutional layer는 Conv2D(16,32,3,1,1), Conv2D(32,64,3,2,1), Conv2D(64,128,3,1,1)로 구성되어 있다. 마지막으로 RPN은 voxelNet과 HC-baseline이 동일하다. VoxelNet과 HC-baseline의 총 파라미터의 수는 매우 간단하다. HC-baseline도 위의 VoxelNet 훈련 프로세스와 동일하게 진행했다.</p><h2 id="41-evaluation-on-kitti-validation-set">4.1 Evaluation on KITTI validation set <a href="#41-evaluation-on-kitti-validation-set" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Metrics</ul><p>차 IOU threshold는 0.7, 보행자와 자전거 이용자 IOU threshold는 0.5를 적용하는 공식적인 KITTI 평가 프로토콜을 따랐다. 그 IOU threshold는 조감도나 full 3D 평가에서도 동일하다. 많은 방법들과 AP(average precision)을 비교했다.</p><p><br /></p><ul><li>Evaluation in Bird’s eye view</ul><p>평가 결과는 아래 테이블과 같다.</p><p><img data-src="/assets/img/autodriving/voxelnet/table1.png" data-proofer-ignore></p><p>VoxelNet은 모든 비교 접근 방식들보다 모든 난도에 대해 능가했다. HC-baseline도 최첨단 기술인 MV3D와 비교해도 만족스러운 성능을 보인걸로 보아 본 논문의 RPN이 효과적이라는 것을 알 수 있다. RPN을 제외하고는 다 다르게 세팅했기 때문이다. 조감도안에서 보행자와 자전거 이용자 검출 태스크에서 VoxelNet과 HC-baseline을 비교했다. VoxelNet은 까다로운 범주들에 대해서 HC-baseline보다 상당히 높은 AP를 산출했고, 이는 end-to-end 학습이 point cloud 기반의 탐지에 필수적이라는 것을 볼 수 있다.</p><p><br /></p><ul><li>Evaluation in 3D</ul><p>2D 평면안에서 객체의 정확한 localization만을 요구하는 조감도 탐지를 비교할 때, 3D 탐지는 3D 평면안에서 형상의 미세한 localization을 요구하는 더 까다로운 작업이다.</p><p><img data-src="/assets/img/autodriving/voxelnet/table2.png" data-proofer-ignore></p><p>차 클래스에 대해서 VoxelNet은 모든 난도에 대해서 다른 모든 방법들보다 AP가 월등하게 높았다. 특히 LIDAR 데이터만 사용할 때, VoxelNet은 최신 기술인 LIDAR + RGB의 MV(BV+FV+RGB)를 기반으로 하는 기술들을 easy,moderate,hard 각각 10.68%, 2.78%, 6.29%의 차이를 보이며 월등하게 능가했다. HC-baseline은 MV 방법과 비슷한 성능을 보였다.</p><p>조감도 평가와 같이 3D 보행자와 자전거 이용자 탐지를 HC-baseline과 VoxelNet을 비교했다. 3D 포즈와 형상의 가변성 때문에 이 두가지 카테고리의 성공적인 탐지는 더 나은 3D 형상 표현을 요구한다. table2에서 볼 수 있듯이 3D 탐지에서 최대 약 12%, 조감도 측면에서 최대 약 8%나 더 높음으로써 수작업 제작보다 VoxelNet에서 3D 형상 표현을 잡아내는 것이 더 효과적이라는 것을 볼 수 있기 때문에, VoxelNet의 개선된 성능은 3D 탐지 작업에서 더 강조된다.</p><p><br /></p><h2 id="42-evaluation-on-kitti-tset-set">4.2 Evaluation on KITTI tset set <a href="#42-evaluation-on-kitti-tset-set" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>공식 서버에 검출 결과를 제출하여 KITTI 테스트 셋에서의 VoxelNet을 평가했다. 이 결과는 아래 테이블에 요약되어 있다.</p><p><img data-src="/assets/img/autodriving/voxelnet/table3.png" data-proofer-ignore></p><p>VoxelNet은 모든 task(조감도, 3D 탐지)와 모든 난도(easy,moderate,hard)에서 이전의 공개된 MV3D과 같은 최첨단 기술들보다 월등하게 높았다. VoxelNet은 LIDAR만 적용했지만, 나머지 LIDAR과 RGB를 융합한 모델보다 더 월등한 성능을 보일 수 있었다.</p><p>LIDAR를 사용한 3D box 탐지를 더 잘 보이게 하기 위해, fig6에서처럼 RGB 이미지로 투영해서 나타냈다.</p><p><img data-src="/assets/img/autodriving/voxelnet/fig6.png" data-proofer-ignore></p><p>TitanX GPU와 1.7Ghz CPU에서 VoxelNet의 추론 시간은 33ms인데, 입력 피쳐 계산은 5ms, convolutional middle layers는 1ms, region proposal network는 11ms 걸렸다.</p><p><br /></p><h1 id="5-conclusion">5. Conclusion</h1><p>현존하는 대부분의 LIDAR 기반의 탐지는 조감도 투영과 같이 수작업 피쳐 제작에 의존하지만, 본 논문에서는 수작업 제작으로 인한 병목 현상을 제거하고 point cloud 기반의 3D 탐지를 위한 새로운 end-to-end로 훈련이 가능한 심층 아키텍처인 VoxelNet을 제안했다. 이 논문에서의 접근 방식은 희박한 3D 점들을 직접적으로 작업할 수 있고, 3D 형상 정보를 효과적으로 잡을 수 있다. 또한, voxel grid에서의 병렬 프로세싱과 point cloud의 희소성으로부터 이익을 줄 수 있는 VoxelNet의 효과적인 구현을 제시했다. KITTI 차 탐지 benchmark에서의 실험은 VoxelNet이 최첨단의 LIDAR 기반의 3D 탐지 방법들을 큰 차이로 능가한다는 것을 보여준다. 보행자와 자전거 이용자에 대한 3D 탐지와 같은 더 까다로운 작업에서 VoxelNet은 더 나은 3D 표현을 제공하는 좋은 결과를 가져왔다.</p><p><br /></p><h1 id="reference">Reference</h1><ul><li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a></ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/voxelnet/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/voxelnet.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/review/'>Review</a>, <a href='/categories/autonomous-driving/'>Autonomous Driving</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/autonomous-driving/" class="post-tag no-text-decoration" >Autonomous Driving</a> <a href="/tags/voxelnet/" class="post-tag no-text-decoration" >VoxelNet</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/voxelnet/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/voxelnet/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/voxelnet/&amp;text=[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/voxelnet/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/pointcnn2/"><div class="card-body"> <em class="timeago small" date="2022-01-23 13:00:00 +0900" >Jan 23, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 코드 구현] PointCNN: Convolution on X-transformed points</h3><div class="text-muted small"><p> pointCNN 깃허브에 올라와 있는 코드를 직접 실행하고 리뷰한 내용입니다. 모든 코드는 colab에서 진행하였습니다. 논문에 대한 리뷰가 궁금하신 분들은 이 링크를 참고해주세요. PointCNN 깃허브 주소: https://github.com/yangyanli/PointCNN Classification ModelNet40 먼저 분류 모...</p></div></div></a></div><div class="card"> <a href="/posts/monocular/"><div class="card-body"> <em class="timeago small" date="2022-01-25 23:53:00 +0900" >Jan 25, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] Monocular 3D Object Detection for Autonomous Driving</h3><div class="text-muted small"><p> Monocular 3D Object Detection for Autonomous Driving 논문에 대한 리뷰입니다. 이 논문은 2016 CVPR에 투고된 논문이며 약 669회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstract 본 논문은 자율주행에서 단안 ...</p></div></div></a></div><div class="card"> <a href="/posts/pointrcnn/"><div class="card-body"> <em class="timeago small" date="2022-02-02 01:17:00 +0900" >Feb 2, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</h3><div class="text-muted small"><p> PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstrac...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/multiview/" class="btn btn-outline-primary" prompt="Older"><p>[논문 리뷰] Multi-View 3D Object Detection Network for Autonomous Driving</p></a> <a href="/posts/pointcnn/" class="btn btn-outline-primary" prompt="Newer"><p>[논문 리뷰] PointCNN: Convolution on X-transformed points</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>