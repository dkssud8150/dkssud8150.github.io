<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[데브코스] 11주차 - DeepLearning Many CNN models structure" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Do focus on developing ‘your ability’ rather than waste time on making you famous." /><meta property="og:description" content="Do focus on developing ‘your ability’ rather than waste time on making you famous." /><link rel="canonical" href="https://dkssud8150.github.io/posts/models/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/models/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-26T15:20:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[데브코스] 11주차 - DeepLearning Many CNN models structure" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-05-17T02:56:35+09:00","datePublished":"2022-04-26T15:20:00+09:00","description":"Do focus on developing ‘your ability’ rather than waste time on making you famous.","headline":"[데브코스] 11주차 - DeepLearning Many CNN models structure","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/models/"},"url":"https://dkssud8150.github.io/posts/models/"}</script><title>[데브코스] 11주차 - DeepLearning Many CNN models structure | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[데브코스] 11주차 - DeepLearning Many CNN models structure</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[데브코스] 11주차 - DeepLearning Many CNN models structure</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-04-26 15:20:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Tue, Apr 26, 2022, 3:20 PM +0900" >Apr 26, 2022</em> </span> <span> Updated <em class="timeago" date="2022-05-17 02:56:35 +0900 " data-toggle="tooltip" data-placement="bottom" title="Tue, May 17, 2022, 2:56 AM +0900" >May 17, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2822 words"> <em>15 min</em> read</span></div></div></div><div class="post-content"><p><br /></p><h1 id="alexnet">AlexNet</h1><ul><li>pytorch</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">AlexNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_classes</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AlexNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                              <span class="c1"># conv1
</span>                              <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                              <span class="c1"># conv2
</span>                              <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="mi">192</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
                              <span class="c1"># conv3
</span>                              <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span><span class="mi">384</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                              <span class="c1"># conv4
</span>                              <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                              <span class="c1"># conv5
</span>                              <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> 
                              <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
                              <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveavgPool2d</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                              <span class="c1"># dropout
</span>                              <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(),</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="o">*</span><span class="mi">6</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(),</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span><span class="mi">4096</span><span class="p">),</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">),</span>
                              <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="c1"># output shape : (batch size * 256, 6, 6)
</span>      <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># output shape : (batch_size, 256 * 6 * 6)
</span>      <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span>
</pre></table></code></div></div><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre><td class="rouge-code"><pre><span class="n">model1</span> <span class="o">=</span> <span class="nc">AlexNet</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">torchsummary</span>
<span class="n">torchsummary</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>

<span class="c1"># --------------------- #
</span>
<span class="o">----------------------------------------------------------------</span>
        <span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>               <span class="n">Output</span> <span class="n">Shape</span>         <span class="n">Param</span> <span class="c1">#
</span><span class="o">================================================================</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">1</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">63</span><span class="p">]</span>          <span class="mi">23</span><span class="p">,</span><span class="mi">296</span>
              <span class="n">ReLU</span><span class="o">-</span><span class="mi">2</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">63</span><span class="p">]</span>               <span class="mi">0</span>
         <span class="n">MaxPool2d</span><span class="o">-</span><span class="mi">3</span>           <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">31</span><span class="p">]</span>               <span class="mi">0</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">4</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">31</span><span class="p">]</span>         <span class="mi">307</span><span class="p">,</span><span class="mi">392</span>
              <span class="n">ReLU</span><span class="o">-</span><span class="mi">5</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">31</span><span class="p">]</span>               <span class="mi">0</span>
         <span class="n">MaxPool2d</span><span class="o">-</span><span class="mi">6</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>               <span class="mi">0</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">7</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>         <span class="mi">663</span><span class="p">,</span><span class="mi">936</span>
              <span class="n">ReLU</span><span class="o">-</span><span class="mi">8</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>               <span class="mi">0</span>
            <span class="n">Conv2d</span><span class="o">-</span><span class="mi">9</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>         <span class="mi">884</span><span class="p">,</span><span class="mi">992</span>
             <span class="n">ReLU</span><span class="o">-</span><span class="mi">10</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>               <span class="mi">0</span>
           <span class="n">Conv2d</span><span class="o">-</span><span class="mi">11</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>         <span class="mi">590</span><span class="p">,</span><span class="mi">080</span>
             <span class="n">ReLU</span><span class="o">-</span><span class="mi">12</span>          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>               <span class="mi">0</span>
        <span class="n">MaxPool2d</span><span class="o">-</span><span class="mi">13</span>            <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>               <span class="mi">0</span>
<span class="n">AdaptiveAvgPool2d</span><span class="o">-</span><span class="mi">14</span>            <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>               <span class="mi">0</span>
          <span class="n">Dropout</span><span class="o">-</span><span class="mi">15</span>                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9216</span><span class="p">]</span>               <span class="mi">0</span>
           <span class="n">Linear</span><span class="o">-</span><span class="mi">16</span>                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>      <span class="mi">37</span><span class="p">,</span><span class="mi">752</span><span class="p">,</span><span class="mi">832</span>
             <span class="n">ReLU</span><span class="o">-</span><span class="mi">17</span>                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>               <span class="mi">0</span>
          <span class="n">Dropout</span><span class="o">-</span><span class="mi">18</span>                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>               <span class="mi">0</span>
           <span class="n">Linear</span><span class="o">-</span><span class="mi">19</span>                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>      <span class="mi">16</span><span class="p">,</span><span class="mi">781</span><span class="p">,</span><span class="mi">312</span>
             <span class="n">ReLU</span><span class="o">-</span><span class="mi">20</span>                 <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>               <span class="mi">0</span>
           <span class="n">Linear</span><span class="o">-</span><span class="mi">21</span>                   <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>          <span class="mi">40</span><span class="p">,</span><span class="mi">970</span>
<span class="o">================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">57</span><span class="p">,</span><span class="mi">044</span><span class="p">,</span><span class="mi">810</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">57</span><span class="p">,</span><span class="mi">044</span><span class="p">,</span><span class="mi">810</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="o">----------------------------------------------------------------</span>
<span class="n">Input</span> <span class="nf">size </span><span class="p">(</span><span class="n">MB</span><span class="p">):</span> <span class="mf">0.75</span>
<span class="n">Forward</span><span class="o">/</span><span class="n">backward</span> <span class="k">pass</span> <span class="nf">size </span><span class="p">(</span><span class="n">MB</span><span class="p">):</span> <span class="mf">10.96</span>
<span class="n">Params</span> <span class="nf">size </span><span class="p">(</span><span class="n">MB</span><span class="p">):</span> <span class="mf">217.61</span>
<span class="n">Estimated</span> <span class="n">Total</span> <span class="nc">Size </span><span class="p">(</span><span class="n">MB</span><span class="p">):</span> <span class="mf">229.32</span>
<span class="o">----------------------------------------------------------------</span>
</pre></table></code></div></div><p>파라미터의 크기를 보면, linear layer에서 가장 크다. 그럼에도 불구하고, <code class="language-plaintext highlighter-rouge">linear(4096, 4096)</code>를 쓰는 이유는??</p><p>convolution 연산의 출력의 크기</p><ul><li>out_w = (in_w - k_w + 2 * pad) / stride + 1<li>out_h = (in_h - k_h + 2 * pad) / stride + 1<li>out_channels = num_kernels</ul><p>매개 변수의 수</p><ul><li>커널마다 (kernel size x kernel size x in_channels)개의 가중치와 1개의 bias를 가진다. 따라서 전체 매개변수의 수는 (kernel size x kernel size x in_channels) x num_kernels + 1 x num_kernels</ul><p><br /></p><p><br /></p><ul><li>tensorflow</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">AlexNet</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">classifier_activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span> <span class="p">:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="c1"># conv1
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">11</span><span class="p">),</span>
                          <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"valid"</span><span class="p">,</span> <span class="c1"># no zero padding
</span>                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span>
                          <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="s">"valid"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">(),</span>
    <span class="c1"># conv2
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
                          <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span> <span class="c1"># zero padding
</span>                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="s">"valid"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">(),</span>
    <span class="c1"># conv3
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
    <span class="c1"># conv4
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
    <span class="c1"># conv5
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>

    <span class="c1"># classifier
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="c1"># linear
</span>                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">softmax</span><span class="p">)</span>
  <span class="p">])</span>

  <span class="k">return</span> <span class="n">model</span>
</pre></table></code></div></div><p><br /></p><p><br /></p><h1 id="vggnet16">VGGNet16</h1><ul><li>pytorch</ul><p>https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span> <span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">init_weights</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span> <span class="c1"># make layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span> <span class="c1"># output shape : [in_channels, 7, 7]
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">init_weights</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">_initialize_weights</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">vgg16</span><span class="p">(</span><span class="n">pretrained</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">progress</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">VGG</span><span class="p">:</span>
  <span class="s">''' VGG 16 layer model
  very deep convolutional networks for large scale image recognition
  args:
    pretrained (bool) : if True, returns a model pretrained on imageNet
    progress (bool) : if True, displays a progress bar of the download to stderr
  '''</span>
  <span class="k">return</span> <span class="nf">_vgg</span><span class="p">(</span><span class="s">'vgg16'</span><span class="p">,</span> <span class="s">'D'</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="n">pretrained</span><span class="p">,</span> <span class="n">progress</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">vgg16_bn</span><span class="p">(</span><span class="n">pretrained</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">progress</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">VGG</span><span class="p">:</span>
  <span class="k">return</span> <span class="nf">_vgg</span><span class="p">(</span><span class="s">'vgg16'</span><span class="p">,</span> <span class="s">'D'</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">pretrained</span><span class="p">,</span> <span class="n">progress</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_layers</span><span class="p">(</span><span class="n">cfg</span> <span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">batch_norm</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">:</span>
  <span class="n">layer</span> <span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">3</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
    <span class="c1"># max pooling
</span>    <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="s">"M"</span><span class="p">:</span>
      <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)]</span>

    <span class="k">else</span><span class="p">:</span>
      <span class="n">v</span> <span class="o">=</span> <span class="nf">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
      <span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># conv
</span>      <span class="c1"># batch_norm
</span>      <span class="c1"># activation
</span>      <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
      <span class="c1"># conv
</span>      <span class="c1"># activation
</span>      <span class="k">else</span><span class="p">:</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>

      <span class="c1"># 다음 conv input channel 
</span>      <span class="n">in_channels</span> <span class="o">=</span> <span class="n">v</span>
  <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

<span class="n">cfgs</span> <span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'A'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
  <span class="s">'B'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
  <span class="s">'D'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
  <span class="s">'E'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">]</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">_vgg</span><span class="p">(</span><span class="n">arch</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">cfg</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">batch_norm</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">pretrained</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">progress</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span> <span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">VGG</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'init_weights'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

  <span class="n">model</span> <span class="o">=</span> <span class="nc">VGG</span><span class="p">(</span><span class="nf">make_layers</span><span class="p">(</span><span class="n">cfgs</span><span class="p">[</span><span class="n">cfg</span><span class="p">],</span> <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="p">...</span>
</pre></table></code></div></div><p><br /></p><ul><li>tensorflow</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">VGG16</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">pooling</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">classifier_activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span> <span class="p">:</span>
  <span class="s">''' 
  args:
    input_shape : classifier를 포함할지 말지에 대한 인자
  '''</span>
  <span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="c1"># Block1
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block1_conv1'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block1_conv2'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="s">'block1_pool'</span><span class="p">),</span>

    <span class="c1"># Block2
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block2_conv1'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span> 
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block2_conv2'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="s">'block2_pool'</span><span class="p">),</span>

    <span class="c1"># Block3
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block3_conv1'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block3_conv2'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block3_conv3'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="s">'block3_pool'</span><span class="p">),</span>

    <span class="c1"># Block4
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block4_conv1'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block4_conv2'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block4_conv3'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="s">'block4_pool'</span><span class="p">),</span>

    <span class="c1"># Block5
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block5_conv1'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block5_conv2'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                          <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'block5_conv3'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="s">'block5_pool'</span><span class="p">),</span>

    <span class="k">if</span> <span class="n">include_top</span><span class="p">:</span>

      <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'flatten'</span><span class="p">),</span>

      <span class="c1"># classifier
</span>      <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="c1"># linear
</span>                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'fc1'</span><span class="p">),</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="c1"># linear
</span>                          <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'fc2'</span><span class="p">),</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span> <span class="c1"># linear
</span>                          <span class="n">activation</span><span class="o">=</span><span class="n">classifier_activation</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="s">'prediction'</span><span class="p">),</span>
  <span class="p">])</span>

  <span class="k">if</span> <span class="n">weights</span> <span class="o">==</span> <span class="s">'imagenet'</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">include_top</span><span class="p">:</span>
      <span class="n">weight_path</span> <span class="o">=</span> <span class="n">data_utils</span><span class="p">.</span><span class="nf">get_file</span><span class="p">(</span>
        <span class="s">'vgg16_weights_tf_dim_ordering_tf_kernels.h5'</span><span class="p">,</span>
        <span class="n">WEIGHTS_PATH</span><span class="p">,</span>
        <span class="n">cache_subdir</span><span class="o">=</span><span class="s">'models'</span><span class="p">,</span>
        <span class="n">file_hash</span> <span class="o">=</span> <span class="s">''</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">weight_path</span> <span class="o">=</span> <span class="n">data_utils</span><span class="p">.</span><span class="nf">get_file</span><span class="p">(</span>
        <span class="s">'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'</span><span class="p">,</span>
        <span class="n">WEIGHTS_PATH_NO_TOP</span><span class="p">,</span>
        <span class="n">cache_subdir</span><span class="o">=</span><span class="s">'models'</span><span class="p">,</span>
        <span class="n">file_hash</span> <span class="o">=</span> <span class="s">''</span><span class="p">)</span>

    <span class="n">model</span><span class="p">.</span><span class="nf">load_weights</span><span class="p">(</span><span class="n">weights_path</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">load_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">model</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>
<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">layer</span><span class="p">:</span>
  <span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"prediction"</span><span class="p">))</span>
</pre></table></code></div></div><p><br /></p><p><br /></p><h1 id="googlenet">GoogLeNet</h1><ul><li>1x1 커널</ul><p>이 때, 사용했던 1x1 커널을 설명하고 넘어가자.</p><p>1x1 커널이란 차원의 통합 및 축소를 할 수 있는 효과를 가지고 있다.</p><p>예를 들어 input이 [in_b, in_c, in_h, in_w] 를 가지고 있고, filter를 [1, in_c, 1, 1] 을 사용하게 되면 output의 shape은 [in_b, 1, (in_h - 1 + 2p) // stride + 1, (in_h - 1 + 2*pad) // stride + 1] 을 가진다. 만약 pad=0, stride=1이라면 (in_h - 1) + 1 = in_h, in_w가 된다. 따라서 1x1 커널을 사용하게 되면 크기는 그대로지만, 차원을 축소시킬 수 있는 효과를 가지고 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">fc0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="p">...</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># --------------- #
</span><span class="n">Linear</span><span class="o">-</span><span class="mi">33</span>                   <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>         <span class="mi">655</span><span class="p">,</span><span class="mi">360</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
  <span class="n">self</span><span class="p">.</span><span class="n">conv6</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
  <span class="p">...</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># ---------------- #
</span><span class="n">Conv2d</span><span class="o">-</span><span class="mi">33</span>             <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>          <span class="mi">10</span><span class="p">,</span><span class="mi">250</span>
</pre></table></code></div></div><p>이 두 경우의 weight를 비교해보았더니 fc layer를 사용할 경우 약 65.5만개의 weight가 존재했지만, 1x1 conv를 통해 차원을 축소했더니 1만개에 불과했다.</p><p><code class="language-plaintext highlighter-rouge">(kernel size x kernel size x in_channels) x num_kernels + 1 x num_kernels</code> 를 통해 직접 파라미터의 수를 구해보면, (1 x 1 x 1024 x 10 + 1 x 10) = 10,250개에 불과하므로 약 65배가 차이나는 것을 확인할 수 있다.</p><p><br /></p><ul><li>inception module</ul><p>또 하나 googLeNet의 핵심은 인셉션 모듈이라는 다양한 특징을 추출하기 위해 NIN 구조를 확장한 복수의 병렬적인 컨볼루션 층을 사용했다. NIN 구조에서는 기존의 컨볼루션 연산을 MLPConv 연산으로 대체하여 커널 대신 비선형 함수를 활성함수로 포함하는 MLP를 사용하여 특징을 추출한다.</p><p><img data-src="/assets/img/dev/week11/day2/nin.png" data-proofer-ignore></p><p>a가 기존의 방법으로, 하나의 receptive field를 커널과 연산하여 output을 내는 방식이었다. 그러나 MLPconv 층의 경우에는 한 receptive field를 MLP의 입력으로 넣어 연산을 한다. 이렇게 하면 해당하는 위치에 대해 완전 연결, FC layer를 사용하므로 특징을 조금 더 잘 추출할 수도 있고, receptive field안의 또 다른 다양한 receptive field 크기를 만들 수 있게 된다.</p><p>NIN의 또다른 특징으로는 전역 평균 풀링을 사용했다. MLPconv layer에서의 특징맵 채널을 분류하고자 하는 클래스의 수만큼으로 만들고, 각각의 클래스가 1개의 채널만을 가리킬 수 있도록, 즉 1개의 채널마다의 평균을 구해서 확률값으로 넣어서 모든 채널을 평균내면 클래스의 수만큼의 길이로 output이 생성된다.</p><p><img data-src="/assets/img/dev/week11/day2/gapool.png" data-proofer-ignore></p><p>예를 들어, 이와 같이 3채널일 때, 이를 각각의 채널을 평균 내서 각각의 클래스에 해당되도록 만든다. 그러면 최종 output은 [batch, n_classes, 1, 1]의 shape을 가지게 된다. 이를 fc layer에 넣은 후 softmax를 적용하여 확률값으로 만들어 출력한다. 이와 같은 방법의 가장 큰 장점은 전역 평균 풀링으로 만들어진 feature map에서 평균내는 것에는 매개변수가 존재하지 않는다는 것이다. 이전의 방법인 VGGNet에서는 3개의 fc layer를 사용했고, 이에 대한 매개변수는 1억2천2백만 개의 매개변수를 가지는데, 그에 반해 전역 평균 풀링과 1개의 fc layer를 사용하면 1억개의 매개변수가 사라지게 된다.</p><p><br /></p><p>이러한 MLPconv 개념을 googLeNet에 활용한 방법이 인셉션 모듈이다. MLP 대신 네 종류의 컨볼루션 연산을 사용하여 다양한 특징을 추출한다.</p><p><img data-src="/assets/img/dev/week11/day2/inception.png" data-proofer-ignore></p><p>또한, 그림을 보면, a처럼 1x1,3x3,5x5,7x7… 을 하게되면 계속 연산량이 증가하게 된다. 그래서 b처럼 1x1 conv를 사용해서 차원을 줄여 연산량을 줄인 후 연산을 한다. 마지막에는 3x3 maxpooling을 추가했다. 이렇게 구해진 4개 각각의 feature map을 concat하기 위해서는 차원을 맞춰줘야 한다. 그를 위해 maxpooling에도 1x1 convolution을 하여 차원을 변경한다.</p><p><br /></p><p><img data-src="/assets/img/dev/week11/day2/googlenet.png" data-proofer-ignore></p><p>그래서 googLeNet은 inception 모듈을 9개 결합했다. 그리고 예전에는 학습이 잘 진행되지 않아서 보조 분류기를 사용해서 역전파의 결과를 결합해서 경사 소멸 문제를 완화했다. 학습할 때 도움을 주고, 추론할 때는 제거한다.</p><p><br /></p><p><br /></p><h1 id="resnet">ResNet</h1><p>ResNet은 residual block을 사용했다. 층이 깊을수록 좋은 것은 확인했으나 실제 20개의 layer와 50개의 layer를 사용했을 때 학습을 진행해보면, 20layer가 오히려 loss가 더 낮게 나오는 경향이 발생했다. 이러한 이유는 최적화가 잘 되지 않았다는 것이라 생각했고, 이를 해결하는 방법으로 residual learning을 생각했다.</p><p><img data-src="/assets/img/dev/week11/day2/resnet.png" data-proofer-ignore></p><p>residual block에 사용되는 residual learning의 특징으로는 원래의 학습이 입력 x를 넣으면 conv-relu-conv 를 거쳐 출력 H(x)가 된다고 할 때, 입력 x에서 H(x)가 되기 위한 변화량을 F(x)라 하여 <code class="language-plaintext highlighter-rouge">H(x) = F(x) + x</code>로 식을 새로 만들었다. 이 때 F(x)를 잔류(잔차)(residual)이라 한다.</p><p><img data-src="/assets\img\dev\week11\day2\resnet_archi.png" data-proofer-ignore></p><p>https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py</p><p><br /></p><p><br /></p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/models/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/models.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/devcourse/'>devcourse</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/devcourse/" class="post-tag no-text-decoration" >devcourse</a> <a href="/tags/deeplearning/" class="post-tag no-text-decoration" >deeplearning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[데브코스] 11주차 - DeepLearning Many CNN models structure - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/models/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[데브코스] 11주차 - DeepLearning Many CNN models structure - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/models/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/models/&amp;text=[데브코스] 11주차 - DeepLearning Many CNN models structure - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/models/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/numpy/"><div class="card-body"> <em class="timeago small" date="2022-04-11 14:40:00 +0900" >Apr 11, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 9주차 - DeepLearning Numpy & Matplotlib</h3><div class="text-muted small"><p> Numpy Numpy 설치 및 불러오기 pip install numpy import numpy as np 파이썬의 리스트는 머신러닝에서 가장 많이 사용되는 구조 중 하나일 것이다. 그러나 이 리스트는 연산 속도가 느리다. 그래서 연산을 효과적으로 할 수 있도록 하기 위해 만든 것이 Numpy이다. Numpy 연산 속도 nump...</p></div></div></a></div><div class="card"> <a href="/posts/dplearn/"><div class="card-body"> <em class="timeago small" date="2022-04-12 15:40:00 +0900" >Apr 12, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 9주차 - DeepLearning AI & machine learning</h3><div class="text-muted small"><p> 기계 학습 어떤 컴퓨터 프로그램이 T라는 작업을 수행할 때, 이 프로그램의 성능이 P라는 척도로 평가했을 때 경험 E를 통해 성능이 개선된다면 이 프로그램은 학습한다고 말할 수 있다. 따라서 최적의 알고리즘을 찾는 행위를 기계 학습이라 할 수 있다. 기계 학습의 중심은 경험, 과업, 성능에 있다. 예전에는 지식 기반의 학습을 했다. 즉, 인간이...</p></div></div></a></div><div class="card"> <a href="/posts/mlmath/"><div class="card-body"> <em class="timeago small" date="2022-04-13 15:40:00 +0900" >Apr 13, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 9주차 - DeepLearning Mathematics for Machine Learning</h3><div class="text-muted small"><p> 기계 학습에서 수학은 손실함수를 정의하고 손실함수의 최저점을 찾아주는 최적화 이론에 사용된다. 제어를 함에 있어서도 수학이 필요하다. 선형대수 데이터는 벡터나 행렬, 텐서 형태로 되어 있는데, 이에 대한 공간을 이해하고, 연산을 하기 위해서는 선형대수가 필요하다. 벡터와 행렬 벡터는 요소의 종료와 크기를 표현한다. [x \in R^n] ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/yolo2/" class="btn btn-outline-primary" prompt="Older"><p>[데브코스] 10주차 - DeepLearning Yolo v3 coding (2)</p></a> <a href="/posts/tusimple/" class="btn btn-outline-primary" prompt="Newer"><p>[논문 리뷰] Ultra Fast Structure-aware Deep Lane Detection</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>