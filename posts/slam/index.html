<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[데브코스] 15주차 - Visual-SLAM Introduction" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Do focus on developing ‘your ability’ rather than waste time on making you famous." /><meta property="og:description" content="Do focus on developing ‘your ability’ rather than waste time on making you famous." /><link rel="canonical" href="https://dkssud8150.github.io/posts/slam/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/slam/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-05-23T15:20:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[데브코스] 15주차 - Visual-SLAM Introduction" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-07-02T22:33:41+09:00","datePublished":"2022-05-23T15:20:00+09:00","description":"Do focus on developing ‘your ability’ rather than waste time on making you famous.","headline":"[데브코스] 15주차 - Visual-SLAM Introduction","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/slam/"},"url":"https://dkssud8150.github.io/posts/slam/"}</script><title>[데브코스] 15주차 - Visual-SLAM Introduction | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[데브코스] 15주차 - Visual-SLAM Introduction </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[데브코스] 15주차 - Visual-SLAM Introduction</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-05-23 15:20:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Mon, May 23, 2022, 3:20 PM +0900" >May 23, 2022</em> </span> <span> Updated <em class="timeago" date="2022-07-02 22:33:41 +0900 " data-toggle="tooltip" data-placement="bottom" title="Sat, Jul 2, 2022, 10:33 PM +0900" >Jul 2, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="10929 words"> <em>60 min</em> read</span></div></div></div><div class="post-content"><p><br /></p><h1 id="introduction">Introduction</h1><p>SLAM을 수행하게 되면 지금까지 스캔했었던 공간을 기억할 수 있어서 공간에 대한 지도를 생성할 수 있게 된다. 지도를 가지게 된다는 것은 기존의 딥러닝을 통해 즉각적인 추론만으로 판단하는 것보다 훨씬 더 높은 수준의 제어가 가능해진다. 또한 현재의 위치를 파악할 수 있게 되고, 그렇게 되면 주변의 벽이나 객체의 위치도 파악이 가능해진다. 하지만 현재의 위치가 틀리게 추론된다면 주변의 벽이나 객체의 위치도 틀리게 된다는 단점이 존재한다.</p><p>Visual SLAM을 사용하게 되면 주변 정보와 위치정보를 동시에 추론하게 되면서 서로 상호 보완이 가능해진다. 그로 인해 더 정확한 추론이 가능하다.</p><p><br /></p><p><br /></p><h1 id="slam-이란">SLAM 이란?</h1><p>SLAM이란 Simultaneous Localization and Mapping 을 줄인 말로 동시적(simultaneous) 위치 추정(localization) 및 지도 작성(mapping)이다. SLAM은 원래는 로보틱스에서 출발했는데, 로봇 제어를 하기 위해서는 위치를 추정하고 지도를 작성할 줄 알아야 한다.</p><p>지도 작성을 할 때는 센서를 통해 들어온 데이터를 perception(딥러닝)을 통해 주변 환경을 파악한다.</p><p>센서는 로봇의 다양한 감각을 담당한다. 카메라, 라이다, 레이다, 초음파, IMU 등등이 존재하고, 이를 통해 다양한 환경을 파악한다.</p><p>사전 정보가 존재한다면 사전 정보를 통해 현재의 위치를 추정하고, 그를 통해 더 정확한 추정이 가능할 것이다. 그러나 SLAM은 사전 정보가 없이 위치를 추정을 하고, 지도를 제작하는 것이 특징이다. SLAM을 수행한다는 것 자체가 사전 정보를 제작한다고 할 수 있다.</p><p><br /></p><p><br /></p><h2 id="로보틱스-기술의-진화">로보틱스 기술의 진화 <a href="#로보틱스-기술의-진화" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>SLAM의 뿌리는 로보틱스에 있다. 특히 고정되어 있지 않고 움직일 수 있는 moblie robotics에서부터 시작되었는데, 이는 위험하거나 사람이 가기 어려운 곳과 같이 사람이 가지 못하는 곳을 대신 탐색하거나 사람 대신 기계가 대신하도록 만들기 위한 기술이다. 가장 기초에는 고정되어 있는 로봇을 바퀴나 레일을 달아 움직일 수 있도록 했는데, 움직이는 로봇으로 만들기 위해서, 즉 자율이동체로 만들기 위한 조건으로는 스스로 <strong>인지</strong>하고 <strong>결정</strong>하고 <strong>행동</strong>해야 한다. 그러나 단순하게 바퀴나 레일을 통해 한정적인 움직임으로는 mobile robotics라 할 수 없다.</p><p>스스로 공간을 인지하는 것을 연구한 것이 SLAM이다. 공간을 인지한다는 것은 이동을 할 수 있는 공간과 없는 공간을 인지할 줄 알아야 하고, 벽이나 장애물도 인지해야 한다. 벽과 같이 외부 환경을 인지하기 위해 사용되는 센서를 exteroceptive sensing(외부 감각)이라 한다.</p><p>센서는 항상 노이즈를 가지고 있는데, 센서를 통해 정확하게 추정하려면 노이즈를 제거해주어야 한다. 수많은 센서로부터 들어오는 데이터의 노이즈를 잘 처리하는 과정이 복잡하고, 어렵다. 노이즈로 인해 아주 정확한 SLAM은 불가능하기에 SLAM은 <strong>확률적인 프로세스</strong>라고 할 수 있다.</p><p><br /></p><p>mobile robotics에서 주변 환경을 파악하는 것도 중요하지만, 자신의 움직임도 잘 인식해야 한다. 자신의 움직임을 인식하는 센서를 Proprioceptive sensing이라 하고, IMU, GPS 등이 있다.</p><p><br /></p><p>움직이면서 주변을 파악하기 위해서는 이 두 sensing을 반복하면 된다. 주변 환경을 탐지하고 벽이 없는 곳으로 이동하고, 제대로 이동했는지 위치를 추정하고, 다시 주변 환경을 탐지하고 , 이동을 반복한다. 이를 <code class="language-plaintext highlighter-rouge">perception &amp; control feedback loop</code>이라 하는데, 극 초기 단계의 mobile robotics는 이러한 형태를 띄었다. 현재는 사용하지 않는 이유는 여러 가지가 있다.</p><ol><li>proprioceptive sensing은 확률적인 분포를 가지기에 안정성 확보가 어렵다.<li>exteroceptive sensing과 proprioceptive sensing은 항상 노이즈를 가지므로 신뢰도가 떨어질 수 있다.<li>노이즈 분석을 하는 동안에는 이동이 불가능하다.</ol><p>이들을 보완하기 위해서는 여러 개의 센서값들을 확보해서 중간값을 추정하면 되지만, 이는 여러 개의 센서를 확보해야 하므로 가격이 올라가고, 각각의 센서마다 정확도가 다를 수 있어서 어떤 것이 맞고, 어떤것이 틀린지 알수가 없다.</p><p>또 다른 방법으로는 exteroceptive sensing을 통해 proprioceptive sensing이 맞는지 틀린지 확인해볼 수 있다. 로봇이 1m 전진했다면 벽과의 간격이 1m가 줄어야 할 것이다. 그러나 평소에는 오차가 0.2m의 오차를 가지지만, 갑자기 특정 환경에 의해 값이 뻥튀기 된다면 어떤 센서를 믿어야 할지 알 수 없을 것이다. exteroceptive sensing을 하기 위해서는 움직이지 않고 멈춰야 한다. 그러면 움직이다가 샘플링을 위해 멈추고, 다시 움직이고, 샘플링을 위해 다시 멈추고를 반복한다면 평균 이동속도가 엄청나게 감소될 것이다.</p><p><br /></p><p><br /></p><h1 id="localization-mapping-and-slam">Localization, Mapping,, and SLAM</h1><p>우리가 생각하는 완벽한 mobile robotics는 지속적으로 움직이면서 모션과 주변 환경을 인지할 줄 알아야 하고, 두 sensing이 안정적으로 취득되어야 한다. 이 완벽한 시스템을 만들기 위해 proprioceptive sensing을 집중적으로 보는 localization과 exteroceptive sensing을 집중적으로 보는 mapping, 두 가지 기술이 개발되었다. 이 두 기술은 각각 개발되다가 추후에 혼합되면서 SLAM으로 개발되었다.</p><p><br /></p><p>모든 센서는 확률 분포를 가지고 있다. 그렇다면 exteroceptive sensing과 proprioceptive sensing을 조합할 수는 없을까?</p><p>두 확률 분포를 조합할 때는 조심해야 하는 부분이 있다. proprioceptive sensing이 상대적으로 부정확하므로 exteroceptive sensing을 통해 약간의 보정이 가능했다. 그러나 반대로 exteroceptive sensing도 부정확하다면 proprioceptive sensing을 보정해줄 수 없다. exteroceptive을 보정하기 위해 proprioceptive을 활용해줄 수도 있다.</p><p>이처럼 두 확률 분포 중 하나만 정확하다면 다른 하나를 보정해줄 수 있다. 높은 정확도를 가진 proprioceptive sensing을 통해 exteroceptive sensing을 보정해주는 과정을 <code class="language-plaintext highlighter-rouge">mapping</code>, 반대로 높은 정확도의 exteroceptive sensing을 통해 proprioceptive sensing을 보정해주는 과정을 <code class="language-plaintext highlighter-rouge">localization</code>이라 한다.</p><p><br /></p><h2 id="mapping">Mapping <a href="#mapping" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/dev/week15/day1/map.png" data-proofer-ignore></p><p>대동여지도는 사람이 직접 움직이면서 주변을 살펴보고 작은 지도를 계속 그려나가 전체 지도를 만들었을 것이다. 이처럼 주변 환경을 다양한 시점에서 바라보면서 나 자신의 위치를 정확하게 알고 있다는 가정하에(정확한 proprioceptive sensing 값을 알고 있을 때) 불안정한 주변 환경을 보정해 나가며 주변 환경을 그려나가는 것을 <code class="language-plaintext highlighter-rouge">Mapping</code>이라 할 수 있다.</p><p><br /></p><p><br /></p><h2 id="localization">Localization <a href="#localization" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/dev/week15/day1/lotteworld.png" data-proofer-ignore></p><p>롯데월드를 갔다고 생각했을 때, 정확한 지도를 보며 자신의 위치를 파악할 수 있을 것이다. 즉, 정확한 exteroceptive sensing의 결과인 지도를 통해 불안정한 proprioceptive sensing의 결과인 나의 위치와 움직임을 보정하여 나의 위치와 움직임을 추론하는 것을 <code class="language-plaintext highlighter-rouge">localization</code>이라 할 수 있다.</p><p><br /></p><p><br /></p><p>이 두 기술을 자율주행에 빗대어 말하면, 지도를 정확하게 그려내야 자동차가 어떤 차선에서 달리고, 적절한 시기에 차선을 바꿀 수 있고, 경로를 미리 계획할 수 있다. 또한 지도를 통해 내가 갈 수 있는 곳과 가면 안되는 곳을 파악할 수도 있다. 주행에 필요한 신호등이나 표지판도 잘 파악할 수 있다.</p><p>지도를 정확하게 그리는 것 뿐만 아니라 위치 정보를 정확하게 파악하는 것도 중요하다. 인도와의 경계, 신호등 위치 등을 정확하게 알아도 내 위치가 어디에 있는지 모르면 사고가 난다.</p><p><br /></p><p><br /></p><p>그런데 만약 prior 정보가 주어지지 않거나 이 사전 정보가 정확하지 않다면 localization과 mapping이 불가능할까?</p><h3 id="monte-carlo-localization">Monte Carlo Localization <a href="#monte-carlo-localization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>localization에서 가장 유명한 기법인 monte carlo localization의 작동 방법은 지도가 사전적으로 주어졌을 때, particle filter를 통해 위치를 추정한다. particle filter는 exteroceptive sensing의 확률 분포와 proprioceptive sensing의 확률 분포를 융합하여 최적의 값을 찾는 기법 중 하나이다.</p><p><img data-src="/assets/img/dev/week15/day1/monte_carlo_localization.png" data-proofer-ignore></p><p>monte carlo localization의 단계는 다음과 같다.</p><ol><li>initialization : 파티클(사전 정보)를 분포시킨다. 사전 정보란 로봇이 존재할 수 잇는 모든 위치를 의미한다.<li>motion update : 뿌려진 파티클마다 사전 센서로부터 들어온 모션 정보를 추가해서 위치 정보를 업데이트시킨다. 이 때 파티클이 벽으로 들어간다던가, 존재할 수 없는 위치에 있는 파티클은 전부 삭제한다.<li>measurement : 뿌려진 파티클마다 exteroceptive 센서로부터 들어온 정보를 덧씌운다.<li>weight update : 해당 위치에 실제로 이 관측값, exteroceptive센서로부터 들어온 정보가 나올 수 있는지, 즉 현재 위치와 주변 환경의 정보가 맞아 떨어지는지 계산한다.<li>resampling : 잘 맞아 떨어지는 파티클만 남기고, 그 주변에서 다시 파티클을 새로 뿌려서 resampling을 수행한다.</ol><p>조금 복잡하지만, 로봇이 어디에 존재해야 proprioceptive센서의 값과 exteroceptive센서의 값이 어디에 위치해야 잘 맞아떨어지는지를 찾는 것이 이 알고리즘의 핵심이다.</p><p>로봇이 존재할 수 있는 위치는 단 한 곳 뿐일 것이고, 이 알고리즘을 통해 지도 정보와 여러 센서를 조합하면서 점점 더 정답에 가까워질 것이다.</p><p>그러나 위의 그림에서 resampling을 보면 여러 곳에 점이 찍혀있다. 이는 측정값들이 나올 수 잇을 만한 위치를 모두 찍은 것으로 꽤 다양한 위치로 예측이 되는 것을 볼 수 있다. 이렇게 되면 위치를 헷갈려할 수 있게 된다.</p><p><br /></p><p>이 알고리즘의 문제점은 지도를 전적으로 믿고 파악을 하기 때문에 지도가 틀리면 monte carlo localization은 다른 위치의 파티클을 믿게 될 수도 있고, 이곳저곳 우왕좌왕 할수도 있을 것이다.</p><p>또는 지도가 주어지지 않으면 이 알고리즘을 수행할 수 없다. 정확한 위치를 알기 위해 알고리즘을 수행하는데, 이 알고리즘에서 정확한 지도가 필요하다는 역설적인 상황이 발생된다. 예전에는 매우 비싼 센서를 통해 이를 해결했다. 좋은 센서를 통해 정확한 위치 정보를 얻고, 이 정보를 기반으로 정확한 매핑을 해서 지도를 만든다. 그를 통해 정확한 localization을 수행했다.</p><p>이는 매우매우 비싼 센서를 요하기도 하고, 실내에서만 사용이 가능하다던지, 실외에서만 사용 가능하다고 하는 여러 제약 조건이 존재했다. 또는 엄청 비싼 센서로 작업을 했지만, 주변 환경이 달라지면 다시 매핑을 해야 했다.</p><p><br /></p><p>주변 환경이 급격하게 자주 바뀌는 상황 속에서는 이 방법을 사용할 수 없었다. 그래서 SLAM이라는 기술이 개발되게 된 것이고, SLAM은 사전 정보없이 최적의 지도와 최적의 위치 정보를 동시에 추정한다.</p><p><br /></p><p><br /></p><p>SLAM을 통해 사전 정보없이 exteroceptive센서와 proprioceptive센서만으로 최적의 지도외 위치 정보를 추론할 수 있다. 고품질의 사전 정보를 가지고 있다면, SLAM을 굳이 사용하지 않고 Localization을 하거나 Mapping을 하면 되고, 사전 정보가 없을 경우에만 SLAM을 사용한다고 볼 수 있다.</p><p><br /></p><p><br /></p><h1 id="slam에서-사용할-수-있는-센서">SLAM에서 사용할 수 있는 센서</h1><h2 id="proprioceptive-sensors">proprioceptive Sensors <a href="#proprioceptive-sensors" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>자율주행에 사용되는 대표적인 센서로는 IMU, Wheel encoder가 있다.</p><h3 id="wheel-encoder">Wheel encoder <a href="#wheel-encoder" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/dev/week15/day1/wheel_encoder.jpg" data-proofer-ignore></p><p>wheel encoder는 자동차 바퀴에 탑재되어 바퀴의 회전량을 측정하는 센서이다. 회전량을 측정하는 방법에는 brush encoder, 위 그림과 같이 빛을 이용하는 optical sensor, 자기장이나 전기장을 사용하는 magnetic sensor 등이 있다.</p><p>측정량에 바퀴의 둘레를 곱하면 이동량도 계산할 수 있다. wheel encoder를 통해 거리를 계산하는 알고리즘으로는 dead recoding 기법이 있는데, 모션값을 누적해 나가서 차체의 위치를 추정하는 방법이다. 그러나 이 방법은 시간이 오래 지나면 지날수록 에러가 점차 누적된다. GPS 등을 통해 이 에러값을 보정해줄 수 있지만, 루프를 빠르게 돌아야 하는 제어 시스템에서는 dead recoding 기법은 위험할 수 있다.</p><p>wheel encoder는 다양한 이유로 에러가 생길 수 있다. 기본적으로 나타나는 센서의 노이즈도 있고, 비나 눈이 오는 날에 바퀴와 바닥의 접지가 좋지 않아 바퀴가 헛돌아서 오차가 발생할 수 있다. 또는 타이어가 눌려서 둘레가 바뀌는 경우에도 바뀔 수 있고, 코너를 돌 때 왼쪽과 오른쪽의 타이어 둘레가 달라질수도 있고, 고속도로 주행중 타이어 마찰에 의해 팽창되어 오차가 생길수도 있다. 최근에는 CAN 통신을 통해 타이어의 변화를 보정해주기도 한다.</p><p><br /></p><h3 id="imu">IMU <a href="#imu" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>선형 가속도를 측정하는 Linear accelerator와 각속도를 측정하는 Angular gyroscope 센서를 혼합한 센서다. 간단하게 IMU 센서는 관성을 측정하는 센서로, 관성을 측정하는 기술로 기계공학에서는 Spring-damper system이라고 부른다. 이 시스템을 칩으로 만든 것을 IMU라고 할 수 있다. 미세한 진동에도 변화를 감지할 수 있고, 자동차에 장착되는 IMU는 빛을 이용하는 optical system을 사용하여 온도나 자기장 변화에 더 강인하다.</p><p><br /></p><p>최근 slam에서는 카메라와 IMU를 결합하는 visual inertial geometry 기법이 유행하고 있고, 라이다와 IMU를 결합하는 Lidar inertial geometry 기법도 유행하고 있다. 이와 같이 IMU를 여러 센서를 결합하는 이유는 imu로 얻은 prior 정보를 카메라나 라이다에 적용해서 정확한 계산 결과를 더 빠르게 얻기 위함이다.</p><p>IMU 센서는 자율주행 이외 제품에서는 저렴한 편이고, 높은 민감도를 가지고 있으며 높은 FPS(100Hz ~ 4000Hz)를 가진다. 자동차 안에서의 IMU센서는 안전 규제를 위한 수많은 검증 프로세스를 거치게 되면서 가격이 올라가게 되었다.</p><p>단점으로는 에러에 대한 오차가 굉장히 커서 보정을 꼭 거쳐야 한다는 것이다.</p><p><br /></p><p><br /></p><h2 id="exteroceptive-sensors">exteroceptive Sensors <a href="#exteroceptive-sensors" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>자율주행에서 사용할 수 있는 exteroceptive sensor로는 <code class="language-plaintext highlighter-rouge">GNSS</code>, <code class="language-plaintext highlighter-rouge">LiDAR</code>, <code class="language-plaintext highlighter-rouge">Camera</code>, <code class="language-plaintext highlighter-rouge">Ultrasound</code>, <code class="language-plaintext highlighter-rouge">RADAR</code>, <code class="language-plaintext highlighter-rouge">Microphone</code> 등이 있다.</p><p><br /></p><h3 id="gnssgps">GNSS(GPS) <a href="#gnssgps" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>GPS는 사실 미국에서 사용하는 GNSS 시스템을 지칭하는 말이다. GNSS(Global Navigation Satellite System)은 인공위성과의 통신을 통해 삼각측량을 하여 localization을 수행한다.</p><p>GNSS는 싸고 사용하기 쉽다는 편이지만, 부정확하고(10~20m 오차), 인공위성과 일직선으로 신호가 도달해야 하는데, 고층빌딩 사이에서는 빌딩 벽에 반사되어 위치를 잘못 추정할 수 있는데 이를 multi-path라 하고, 실내나 지하에서는 사용할 수 없다.</p><p><br /></p><h3 id="lidar">LiDAR <a href="#lidar" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>LiDAR(Light detection and ranging sensors)는 적외선 레이저를 쏘고 반사 시간을 측정하여 거리를 추정하는 센서다. 주변 환경을 3D point cloud 형태로 바로 알 수가 있다.</p><p>장점으로는 exteroceptive센서중에서는 가장 정확한 편이고, 자율주행용 라이다는 ~100m 의 유효거리를 가진다. 빛의 파장이 이렁나지 않기 때문에 낮/밤 둘다 사용이 가능하다.</p><p>단점으로는 비싸고, 카메라에 비해 해상도가 낮다. 그리고 눈이나 비/안개에 영향을 받기도 한다. 주변을 전부 파악하는 lidar도 있지만, 한 방향만 바라보는 solid-state LiDAR도 있는데, 이를 사용하기 위해서는 모든 방향을 보기 위해 여러 방향으로 여러 개를 탑재해야 한다. 또한, 레이저를 쏘기 때문에 여러 대를 사용할 경우 서로 간섭이 일어날 수도 있다. 또는 반사가 잘 이루어지는 물질에 의해서 물체의 위치를 잘못 추정할수도 있다.</p><p><br /></p><h3 id="camera">Camera <a href="#camera" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>카메라는 광센서를 이용해서 빛 신호를 받고, 아날로그 신호를 0~255값의 디지털 신호로 바꾸어 RGB 색으로 재구성시킨다. 3채널의 값을 가지게 하기 위해 debayering 프로세스를 사용한다.</p><p>장점으로는 저렴하고, 모든 픽셀들이 값을 가지기 때문에 밀도있는 데이터를 생성하고 컬러를 가지며 높은 FPS를 가지기 때문에 좋은 성능을 가진다고 할 수 있다. 렌즈를 교환함으로써 시야각 변경이 가능하다. 사람이 보는 것과 거의 유사하기 때문에 시각화하기 가장 좋다.</p><p>단점으로는 Depth 정보가 손실된다. 깊이 정보를 추정하기 위해 depth camera나 rgb-d 카메라가 개발되기도 했고, 최근에는 카메라와 라이다를 퓨전하기도 하고, monocular depth estimation을 통해 픽셀이 얼마나 거리값을 가지고 있는 지 추정하는 방법도 개발되고 있다. 또는 조명에 영향을 많이 받기 때문에 조명이 변함에 따라 다른 값들이 들어오기 된다.</p><p><br /></p><h3 id="ultrasound">Ultrasound <a href="#ultrasound" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>초음파(Ultrasound) 센서는 레이다와 작동 방식이 동일하다. 장점으로는 저렴하고, 가까운 범위에서 잘 동작한다.</p><p>단점으로는 물체의 형태를 잘 추정하지 못하고, 그래서 거리 센서로만 사용한다. 그리고 노이즈가 많다.</p><p><br /></p><h3 id="radar">RADAR <a href="#radar" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>전파를 쏘고 반사되어 돌아오는 전파를 통해 거리를 재는 센서이다. doppler 효과를 이용해서 이동중인 물체의 속도를 추정 가능하다. 테슬라에서도 예전에는 레이다를 사용하기도 했다. 테슬라에서 레이다를 사용하지 않는 이유는 테슬라 보드를 통해 딥러닝 네트워크의 성능이 엄청 올라가서 레이다를 사용하는 것보다 비전을 사용하는 것이 훨씬 더 높은 정확도를 가지기 때문이다.</p><p>장점으로는 빛을 사용하지 않기 때문에 날씨의 영향을 받지 않고, 타 센서에서는 얻지 못하는 속도 값을 추정할 수 있다. 속도를 추정할 수 있다는 것은 카메라나 라이다로는 추정이 불가능하기 때문에, 차량이 주차되어 있는 것인지 움직이지만 같은 속도로 달려서 정지되어 보이는 것인지를 확인할 수 없다. 그래서 레이다를 통해 이를 추정할 수 있다.</p><p>단점으로는 작은 물체들은 검출이 불가능하고, lidar보다 더 낮은 해상도를 가지고 있다.</p><p><br /></p><h3 id="microphone">Microphone <a href="#microphone" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>마이크는 유일하게 소리를 측정할 수 잇는 센서로, 공기의 진동을 transducer 센서를 통해 전기 신호로 변환하는 센서다. 마이크를 하나만 사용하는 것이 아닌 여러 개의 마이ㅡ를 통해 소리의 근원에 대한 위치를 계산할 수 있다.</p><p>장점으로는 저렴하다는 것이고, 단점으로는 노이즈가 너무 심하다.</p><p><br /></p><p><br /></p><h1 id="slam의-종류">SLAM의 종류</h1><p>사용하는 센서에 따라 종류가 달라진다.</p><ol><li>Visual SLAM/VSLAM</ol><p>카메라를 사용하는 SLAM 방법이다.</p><ol><li>LiDAR SLAM<li>RADAR SLAM</ol><p>추가적으로 마이크를 사용하면 microphone slam 이라고 할 수 있을 것이다. 또, 여러 개의 센서를 사용할수도 있는데, 이 때는 여러 개의 센서 타입을 나열해서 이름을 붙여준다.</p><ul><li>e.g. Visual-LiDAR SLAM(camera / LiDAR), Visual-inertial odometry(camera / IMU)…</ul><p><br /></p><div class="table-wrapper"><table><tbody><tr><td>여러 가지가 있지만, 나의 경우는 Visual SLAM을 주로 다룰 예정이다.</table></div><p><br /></p><h2 id="visual-slam">Visual SLAM <a href="#visual-slam" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>visual slam은 카메라를 사용하는 slam이다.</p><p>장점</p><ul><li>저렴한 센서<li>센서의 성능을 조절하기 쉽다. (e.g. 렌즈 교체 -&gt; 시야각, 초점, 노출 시간)<li>빠른 FPS<li>이미지 기반 딥러닝 적용 가능 ( object detection / segmentation )<li>이미지로 사람이 이해하기 쉬운 시각화 가능</ul><p>단점</p><ul><li>갑작스러운 빛 변화에 대응 불가능<li>시야가 가려지거나 어두운 곳에서 사용 불가능</ul><p>카메라는 3D 공간을 2D로 담는 것인데, 이를 통해 깊이 정보가 소실된다. VSLAM은 3D 공간을 재구축하기 위해 깊이 정보를 추정한다.</p><p><br /></p><p>SLAM을 잘 하기 위해서는 알고리즘 뿐만 아니라 센서에 대해서도 이해도가 높아야 한다. 모든 센서는 노이즈를 가지고 있고, SLAM은 확률적인 프로세스이므로 센서가 어떤 노이즈를 가지고 있는지 파악해서 노이즈를 제거해야 한다.</p><p><br /></p><p>아까 말했듯이 카메라는 3D 공간을 2D로 담아내는 것이므로, 카메라의 노이즈는 이 과정에서 생기는 한계점에 의해 나타나는 노이즈일 것이다. 이는 카메라의 칩에서 발생하는 노이즈일수도 있고, 렌즈에서 나타나는 노이즈일수도 있다. 칩에서 발생하는 노이즈는 아날로그를 디지털로 변환하는 과정에서 발생하는 것과 같이 전자기학적인 문제가 발생하는 것이다. 렌즈에서 발생하는 노이즈는 3D에서 2D로 차원 변화를 하면서 나타나는 노이즈다. 차원을 축소할 때 렌즈에 대한 수학적인 모델링이 완벽해야 하지만, 그렇지 않은 경우 노이즈가 발생한다.</p><p>이를 파악하기 위해서는 칩의 종류도 파악해야 하고, 렌즈의 종류도 파악해야 한다. 이 종류에 따라 기하학적 모델링이 다 달라지기 때문이다.</p><p>칩의 종류에 따른 카메라의 종류로는</p><ul><li>RGB Camera<li>Grayscale Camera<li>Multi-spectral Camera<li>Polarized camera (편광 카메라)<li>Event Camera</ul><p><br /></p><p>렌즈의 종류에 따른 카메라 종류로는</p><ul><li>perspective Camera<li>Wide FOV camera (광각 렌즈)<li>Telecentric Camera (망원 렌즈)<li>Fisheye Camera (어안 렌즈)<li>360 degree Camera</ul><p><br /></p><p><br /></p><p>Visual SLAM에서도 카메라 1대만 사용할수도 있지만, 다양한 카메라 종류를 사용할 수 있다.</p><ul><li>카메라 1대 : Monocular visual SLAM<li>카메라 2대 / 카메라 여러 대 : Stereo visual SLAM / Multi visual SLAM<li>depth 카메라 : Depth visual SLAM</ul><p>카메라 2대를 사용하는 것과 여러 대를 사용하는 것은 거의 동일하므로 저 두개를 묶어서 말하기도 한다.</p><blockquote><p><strong>ORB-SLAM pipeline</strong> <img data-src="/assets/img/dev/week16/day4/orb_slam.png" data-proofer-ignore></p></blockquote><p><br /></p><p><br /></p><h3 id="monocular-vslam">Monocular VSLAM <a href="#monocular-vslam" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>1대의 카메라에서만 이미지를 받는 것이 특징이다.</p><ul><li>장점</ul><p>다른 VSLAM에 비해 가격, 전력 소비량, 이미지 데이터 송수신 대역폭 등의 측면에서 저렴하다.</p><p><br /></p><ul><li>단점</ul><ol><li>1장의 이미지로는 depth를 추정할 수가 없다. 그래서 monocular vslam의 경우는 depth가 없는 2D 이미지로부터 실제 세상에 대한 스케일정보를 받을 수 없기 때문에, 재구축되는 3D 환경은 임의의 스케일로 생성되게 된다. 그렇게 되면 구축한 환경 내에서는 동일한 스케일이 유지되지만 실제 세상되는 얼마나 다른지는 확인할 수 없다. 이를 <code class="language-plaintext highlighter-rouge">Scale ambiguity problem</code>라 한다. 실제 세상에 대한 스케일을 <code class="language-plaintext highlighter-rouge">Metric scale</code>이라 하는데, 이는 실제 미터 단위를 의미한다. 이처럼 실제 스케일이 아닌 임의의 스케일을 up-to-scale이라 부르기도 한다.</ol><p><br /></p><p>이 문제를 해결하기 위해 카메라와 IMU 센서를 융합해서 3d 세상을 구축할 수 있도록 해주기도 한다. 또한 융합을 통해 모션 사전 정보를 통해 더 정확한 정보를 더 빠르게 사용할 수 있다. 그러나 IMU에 사용되는 wheel odometry는 가벼운 알고리즘을 사용하는데 이 경우 모션 정보가 앞/뒤/좌/우, yaw인 3축만 나오게 된다. 6축을 요구하는 VSLAM에 대해서는 충분한 정보를 제공하지 못한다는 단점이 존재한다.</p><p>또는 최근 딥러닝 기반에 monocular depth estimation을 통해 이 문제를 해결하려는 시도도 있다.</p><p><br /></p><ol><li>카메라만 사용하는 VSLAM은 구현하기도 어렵고, 시스템적으로 더 어려운 시스템을 구축해야 한다.</ol><p><br /></p><p>이와 같이 많은 단점이 존재하기 때문에 아직은 잘 사용하지 않지만, 많은 상용화된 시스템들이 stereo 시스템(카메라 2대)를 사용하고 있고, 1대만을 사용하는 monocular VSLAM으로 가려는 움직임이 많다.</p><p><br /></p><p><br /></p><p><img data-src="/assets/img/dev/week15/day1/dso.jpg" data-proofer-ignore></p><p>Monocular VSLAM의 가장 유명한 논문으로는 <code class="language-plaintext highlighter-rouge">DSO</code>가 있다. 이 논문에서는 2016년에 나온 논문이고, direct tracking 기법을 통해 모션을 추정하고 있다. 그리고 픽셀값을 통해 밝기값이 많이 차이나는 edge들만 tracking하고 있고, 과거의 값들과 현재의 값들을 조합해서 3D 공간을 추론한다. 실제의 색상을 볼 수는 없지만, 대략적인 지도를 표현할 수 있다.</p><p><img data-src="/assets/img/dev/week15/day1/sparse_slam.jpg" data-proofer-ignore></p><p>이 3d 공간 특징들을 point cloud 형태로 표현을 하고 있으며, 3D point cloud를 띄엄띄엄 표현을 한 방식을 sparse SLAM이라 한다. monocular vslam은 대부분 sparse slam을 한다. dense slam을 못하는 것은 아니지만, monocular vslam의 특징을 봤을 때, 카메라가 싸고, 이미지가 단 1개를 통해 연산을 수행하므로 데이터가 적고, <strong>컴퓨팅 보드가 가벼워도 충분히 돌 수 있다는 장점</strong>을 활용하기 위해 sparse slam을 수행한다.</p><p><br /></p><p><br /></p><h3 id="stereo--multi-camera-vslam">Stereo / Multi camera VSLAM <a href="#stereo--multi-camera-vslam" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/dev/week15/day1/stereo.jpg" data-proofer-ignore></p><p>2대~ 여러 대를 사용하는 VSLAM 방법을 말하는데, 인접한 카메라들간의 거리 값(baseline 거리)을 metric scale로 알고 있어야 거리와 깊이 추정이 가능하다. 이 baseline거리가 정확하게 구해져야 한다. 조금이라도 오차가 있다면 slam의 모든 과정에 에러가 포함된다.</p><p>문제는 완벽한 baseline은 구할수가 없다. 그럼에도 정확한 측량을 위해 섬세한 calibration 과정을 자동차 하나하나마다 모든 카메라에 수행해야 한다. 그래서 자율주행 회사들은 3D 룸을 만들어서 여러 개의 사진을 찍어서 calibration을 수행한다.</p><p>그렇다고 해서 calibration을 대충하게 되면 모든 slam 정보가 잘못 추정될 수 있기에 정확하게 수행되어야 한다.</p><p><br /></p><ul><li>장점</ul><p>두 이미지간의 disparity 정보를 이용해서 픽셀마다 depth를 추정할 수 있다. 픽셀마다 depth를 추정하는 것은 상당히 많은 연산을 필요로 하는데, 예를 들어 1280 x 720 의 해상도를 가지는 이미지라고 하면 2장이므로 최소 1280 x 720 x 2 의 연산을 필요로 하게 된다. cpu에서 이를 수행할 수가 없으므로 gpu에서 이를 수행한다.</p><p><br /></p><p><img data-src="/assets/img/dev/week15/day1/vio.png" data-proofer-ignore> <img data-src="/assets/img/dev/week15/day1/vio2.png" data-proofer-ignore></p><p>Stereo VSLAM 의 예시로는 <code class="language-plaintext highlighter-rouge">stereo VIO</code> 라고 하는 알고리즘이 있다. 이 알고리즘은 모션 추정에 특화되어 있는 가벼운 알고리즘이다.</p><p><br /></p><p><img data-src="/assets/img/dev/week15/day1/omni_slam.jpg" data-proofer-ignore></p><p>Multi VSLAM 의 예시로는 <code class="language-plaintext highlighter-rouge">OmniSLAM</code>이 있다. 이는 dense mapping에 초점을 둔 vslam 알고리즘이다. 주변 환경을 완벽하게 매핑하기 위해서 초광각 카메라 4개를 사용했고, 생성되는 맵이 매우 dense한 특징이 있다.</p><p><br /></p><p>이처럼 목적에 따라 파이프라인이 다 다르므로 알고리즘을 살펴볼 때는 어떤 목적의 알고리즘인지 파악하는 것이 중요하다.</p><p><br /></p><p><br /></p><h3 id="rgb-d-vslam">RGB-D VSLAM <a href="#rgb-d-vslam" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/dev/week15/day1/rgb-d.gif" data-proofer-ignore></p><p>구조광(structured light) 또는 ToF(time-of-Flight) 센서를 이용한 카메라를 사용한다. 센서가 depth를 직접 얻어주기 때문에 계산이 필요없다. 두 센서 모두 대략 10m의 유효거리를 가진다.</p><p>depth 데이터를 통해 3D 공간을 metric scale로 실시간으로 복원이 가능하다.</p><p>단점으로는 10m안에서만 dpeth 데이터가 정확하고, FOV가 작다. 그리고 적외선 파장이 햇빛과 간섭이 발생하므로 실외에서 사용이 불가능하다.</p><p><br /></p><p>RGB-D 카메라에서의 이미지와 depth 정보는 동일한 스케일이 아니라서 동일하게 맞춰주는 과정이 필요하다.</p><p><br /></p><p><br /></p><h1 id="slam-기술의-적용">SLAM 기술의 적용</h1><h2 id="자율주행-로보틱스">자율주행 로보틱스 <a href="#자율주행-로보틱스" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>로봇 청소기</ul><p><img data-src="/assets/img/dev/week15/day1/cleaner.png" data-proofer-ignore></p><p>가장 상용화가 잘 된 제품 중에 하나가 로봇 청소기다. 로봇은 지도를 가지고 있지 않기 때문에 집을 직접 돌아다니면서 탐색작업을 해야 한다. 이 과정에서도 청소가 가능할 것이고, 처음 보는 맵에 대해 최소 거리를 움직여서 맵을 완성해야 하는 최적화 문제를 수행해야 한다. 이를 active SLAM이라 하고, 이 SLAM은 SLAM과 path planning이 융합되어 있다고 볼 수 있다.</p><p>예전에는 카메라, 2D 라이다 등을 사용했는데, 최근에는 전방 카메라와 2D 라이다, 천장을 바라보는 카메라 등이 존재한다.</p><p><br /></p><ul><li>산업 현장 측량 로봇</ul><p><img data-src="/assets/img/dev/week15/day1/measurement.jpg" data-proofer-ignore></p><p>로봇이 돌아다니면서 매핑을 하고 측량을 수행한다. 이를 통해 바닥이 기울어져 있는지, 벽이 잘 세워졌는지를 측량하기도 한다. 여기에는 라이다가 탑재되어 있고, RGB-D 카메라가 탑재되어 있으나 내비게이션을 위해 탑재되어 있다.</p><p><br /></p><ul><li>배달 / 물류 로봇</ul><p><img data-src="/assets/img/dev/week15/day1/deliver.jpg" data-proofer-ignore></p><p>병원에서 약품을 배달하기도 하고, 실내 뿐만 아니라 실외를 배달하기도 한다.</p><p><br /></p><p><br /></p><h2 id="자율주행-자동차">자율주행 자동차 <a href="#자율주행-자동차" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/dev/week15/day1/autodriving.png" data-proofer-ignore></p><p><br /></p><p><br /></p><h2 id="자율비행-드론">자율비행 드론 <a href="#자율비행-드론" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/dev/week15/day1/drone.png" data-proofer-ignore></p><p><br /></p><p><br /></p><h2 id="메타버스---vrar">메타버스 - VR/AR <a href="#메타버스---vrar" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/dev/week15/day1/ar_vr.jpg" data-proofer-ignore></p><p><br /></p><p><br /></p><p>각각의 분야에서 SLAM의 특성이 존재한다.</p><ol><li>자율주행 로보틱스<ul><li>제품화를 위한 규제</ul><ul><li>e.g. 배터리 폭발로 인한 위험성으로 인도에 다닐 수 없다 - 자유로운 알고리즘 개발<li>개발 언어, 개발 프로세스, 개발 알고리즘 등 다양하개 개발이 가능하다. - 전세계적으로 경쟁</ul><li>자율주행 자동차<ul><li>높은 수준의 딥러닝 솔루션 존재<li>딥러닝 + SLAM<li>안전한 소프트웨어 요구</ul><ul><li>오픈소스 알고리즘의 신뢰도<li>믿을 수 없다면 알고리즘을 직접 짜야함<li>다양한 환경에 의해 센서가 고장났을 때의 유연성</ul><li>자율비행 드론<ul><li>제품화를 위한 규제<li>가장 빠른 SLAM 요구 -&gt; 가장 어려움<li>전력이 많이 소요됨 -&gt; 컴퓨팅 전력이 작아짐 -&gt; 가벼운 알고리즘 요구<li>배터리 폭발과 같은 위험성에 대해 안정성</ul><li>메타버스 - VR/AR<ul><li>가장 자유로운 개발 규제</ul><ul><li>익명 사고가 존재하지 않기 때문에 매우 자유로움 - 디바이스가 제한될 수 있음 - 빠르고, 정확하고, 가벼운 SLAM -&gt; GPU를 사용해야 빠른 속도를 이룰 수 있지만, VR/VR과 같이 그래픽 렌더링을 위해 CPU를 사용해야 함</ul></ol><p><br /></p><p><br /></p><h1 id="slam-이론-공부-팁">SLAM 이론 공부 팁</h1><p>SLAM 분야는 국내 자료가 다소 부족하기 때문에 대부분 해외 자료를 찾아봐야 한다. SLAM을 이해하기 위해 공부해야 하는 것 중 대표적으로 3가지가 있다. 그리고 그에 대해 참고 자료를 같이 소개하려고 한다.</p><ol><li>Mathematics<ul><li>Linear algebra (선형 대수)</ul><ul><li>[youtube] 이상엽Math - 선형대수학<ul><li>SLAM에서 사용하는 다양한 방법들을 설명하지는 않지만 개념을 익히기에 좋다.</ul><li>[교재] gilbert strang - Introduction to Linear algebra 5th Ed.<ul><li>SLAM에서 사용하는 다양한 방법들을 설명하지는 않으나 적절한 깊이까지 내용을 소개한다.</ul><li>[교재] Deisenroth et al - Mathematics for Machine Learning part1<ul><li>수학과를 타겟으로 하기에 난이도가 상당히 높다.<li>Part2는 머신러닝 기법이다. - probability &amp; statistics (확률과 통계)</ul><li>[교재] Evans and Rosenthal - Probability and Statistics<ul><li>기본적인 통계를 배우고, SLAM에서 사용하는 다양한 방법들을 소개해준다.</ul><li>가까운 서점에서 책을 시작해도 좋다.</ul><li>Computer Vision<ul><li>Imaging theory</ul><ul><li>영상 처리와는 다르게 카메라가 이미지를 어떻게 얻어내는지에 대한 이론<li>[youtube] Marc Levoy - Lectures on Digital Photography<ul><li>카메라의 컨트롤 기법과 깔끔한 이미지를 얻는 방법을 소개한다. 깔끔한 이미지를 얻어야 노이즈가 적어진다. - Image Processing</ul><li>[교재] 황선규 박사님 - OpenCV 4로 배우는 컴퓨터 비전과 머신 러닝<li>OpenCV 공식 document</ul><li>Optimization<ul><li>Optimization Theory</ul><ul><li>SLAM은 최적의 지도를 얻어야 함<li>e.g. gradient descent<li>[교재] Wright et al - Numerical Optimization ch8, ch10<ul><li>전체적인 수치 최적화가 소개되고, SLAM에서 사용되는 이론은 ch10에서 소개된다.<li>ch8은 수학적인 내부 구조를 보는 이론</ul><li>[교재] Press et al - Numerical Recipes<ul><li>2007년에 나온 책이라 공부는 말고, 최적화 이론과 이론을 코드로 옮기는 방법을 배우는 것 - eigen library<li>추후 실제 자율주행을 개발할 때는 라이브러리를 사용하지 않고 구현해야 하므로 1번 정도는 참고해보는 것도 좋다. - Numerical Optimization</ul><li>전과 동</ul><li>SLAM<ul><li>[youtube] Cyrill Stachniss</ul><ul><li>대학강의를 온라인으로 진행하게 되면서 공개했다.<li>visual slam 뿐만 아니라 lidar slam도 어느정도 포함되어 있어서 가장 추천 - [교재] Gao - Introduction to Visual SLAM<li>visual slam에 대해 깊게 파고 드는 책으로, 국내에서 커뮤니티에 번역판이 있어서 참고하면 좋다. - [교재] Ma - An Invitation to 3D vision<li>옛날 책이긴 하나 예전에 어떻게 풀었는지 확인할 수 있고, 현재까지 이어져오는 부분도 꽤 있다.</ul></ol><p>SLAM 공부는 이 자료들로 시작하는 것을 매우 추천한다. 위의 1~3번도 필요하지만 최종 목표는 각각의 이론들을 통달하는 것이 목표가 아니라 SLAM이 목표이기 때문에 이 3가지를 먼저 공부하는 것을 추천한다.</p><p><br /></p><div class="table-wrapper"><table><tbody><tr><td>slam이란 굉장히 어려운 분야다. 시작하기는 어렵지만 공부하다보면 메리트가 있을 것이다. 논문을 봐도 이해가 되지 않거나, 방향성을 확정짓기 어려울수도 있다.</table></div><p><br /></p><p><br /></p><h1 id="c--소프트웨어-공부-꿀팁">C++ / 소프트웨어 공부 꿀팁</h1><ul><li>C++</ul><p>python에 비해 더 어려운 언어이고, 에러 메시지도 꽤나 어렵게 되어 있다. 대학교 강의에서 배우는 C++도 C언어와 융합된 부분을 가르치기도 하면서 더 헷갈리기도 한다. 모던 C++는 2011년에 만들어진 C++로, 기업에서 사용하는 C++는 대부분이 <code class="language-plaintext highlighter-rouge">모던 C++</code>이다. 모던 C++는 C++ 11,14,17,20,22 등이 있고, 대체로 자율주행에서는 14,17을 사용하고, 드론은 17, AR 분야에서는 17,20 등을 사용한다.</p><p>C++을 사용하는 이유는 포인터나 동적할당에 큰 장점이 있기 때문이다. 그러나 이러한 포인터나 동적할당에서 에러가 굉장히 많이 발생하여 오히려 더 안좋은 효율을 보이기도 해서 이를 사용하려면 매우 깊이 알아야 했었다. 모던 C++에서 언어적으로 방지하는 기능을 추가해서 더 안전하게 프로그래밍하고, 더 간결하게 표현할 수 있게 되었다.</p><p>모던 C++ 참고 자료</p><ul><li>[교재] <strong>Marc Gregory - 전문가를 위한 C++</strong><ul><li>굉장히 체계적이고, 디테일하게 설명해주므로 필수라고 할 수 있다.</ul><li>[교재] Jason Turner - C++ Best Practices<ul><li>어떻게 하면 C++ 코드를 짤 수 있는지에 대한 책으로, 코드를 개선할 수 있는 방법을 소개한다.</ul><li>[youtube] CppCon - Back To Basics 시리즈<ul><li>cpp conference<li>언어에 있는 다양한 기능을 소개하고, 목적이 무엇인지에 대한 초보자의 눈높이에 맞게 소개해준다.</ul></ul><p><br /></p><ul><li>소프트웨어</ul><p>파이썬과 C++의 가장 큰 차이점은 python은 개발자의 시간을 아끼는 프로그래밍이고, c++는 컴퓨터의 시간을 아끼는 프로그래밍이라 할 수 있다. C++는 cpu사이클이 덜 돌고 메모리를 덜 차지하는 것이 목표라고 할 수 있다.</p><p>자료 구조 / 알고리즘 참고 자료</p><ul><li>[블로그] 장형기 강사님 - https://www.cv-learn.com/20210214-data-structures-and-algorithms/<ul><li>자료구조에 대한 overview로 어떤 자료구조가 있고, 어떻게 작동하는지 이해할 수 있다.</ul><li>[교재] 코딩 테스트를 위한 자료 구조와 알고리즘 with C++<ul><li>여러 가지 자료구조를 직접 구현해보고, 예시문제를 따라가면 좋다. 이를 통해 c++로 여러 가지 자료구조를 사용하는데 익숙해질 수 있다.</ul><li>[youtube] 나동빈 - 이것이 취업을 위한 코딩 테스트다. with Python (C++도 있음)<ul><li>기업 코테 수준의 문제들을 풀어보고 따라가며 익히면 좋다.</ul></ul><p><br /></p><p>개발자 역량 강화 참고 자료</p><ul><li>알고리즘 개발자는 효율성이 좋고, 성능이 좋은 알고리즘을 만드는 것이 목표일 것이다. 내가 짠 코드가 지속적으로 사용되게 하기 위해서는 좋은 소프트웨어를 짜야 한다. 버그가 없어야 하고, 쉽게 유지보수가 되어야 하며, 어느 곳에서든 쉽게 쓸 수 있는 코드여야 한다.<li>[교재] 모던 C++ 디자인 패턴<ul><li>우리가 짜는 소프트웨어 모듈이 어떤 디자인을 가져야 사용자가 쉽게 사용할 수 있고, 쉽게 유지보수가 될 수 잇는지에 대한 책이다.<li>디자인 패턴은 주니어 단계에서 시니어 단계로 넘어갈 때 숙지해야 할 부분이다. solid 법칙과 같이 깔끔한 코드를 적기 위한 룰이 몇가지 존재한다. 어느 정도 연차가 쌓이면 미팅을 할 때, 어떻게 디자인할지 어떻게 코드를 사용할지 서로 논의를 해야 한다. 이를 위해 디자인 패턴을 잘 익혀놓아야 한다.</ul><li>[교재] Robert C. Martin - the Clean Coder, The clean code<ul><li>the clean coder<ul><li>코드를 잘 작성하는 개발자가 되기 위한 방법에 대해 소개</ul><li>the clean code<ul><li>코드를 잘 작성하는 방법에 대해 소개</ul><li>사회 초년생에게 추천되는 책<li>알고리즘이나 프레임워크를 만들 때는 여러 모듈들이 함께 돌아가는 큰 스케일이 구성될 것이다. 작은 단위의 모듈들부터 큰 단위의 프레임워크를 다뤄야 하므로 이들을 다루는 방법과 마음가짐을 소개한다.</ul></ul><p><br /></p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/slam/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/slam.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/devcourse/'>devcourse</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/devcourse/" class="post-tag no-text-decoration" >devcourse</a> <a href="/tags/visual-slam/" class="post-tag no-text-decoration" >Visual-SLAM</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[데브코스] 15주차 - Visual-SLAM Introduction - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/slam/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[데브코스] 15주차 - Visual-SLAM Introduction - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/slam/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/slam/&amp;text=[데브코스] 15주차 - Visual-SLAM Introduction - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/slam/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/motion_estimation/"><div class="card-body"> <em class="timeago small" date="2022-05-30 17:20:00 +0900" >May 30, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 16주차 - Visual-SLAM motion estimation</h3><div class="text-muted small"><p> Epipolor Geometry 3D point estimation을 수행할 때, 왜 2장의 이미지가 필요할까? 카메라 투영 시 3D에서 2D로의 mapping 관계는 이전 글에서 많이 다뤘다. 그러나 2D에서 3D로의 mapping 관계를 풀기 위해 intrinsic과 extrinsic matrix를 활용할 수 있지만, extrinsic을 알...</p></div></div></a></div><div class="card"> <a href="/posts/triangulation/"><div class="card-body"> <em class="timeago small" date="2022-06-01 15:20:00 +0900" >Jun 1, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 16주차 - Visual-SLAM triangulation and perspective</h3><div class="text-muted small"><p> Triangulation 지난 글에서 F-matrix와 E-matrix를 구하는 방법을 배웠고, 이 matrix를 분해하면 두 이미지 간의 translation과 rotation matrix를 구할 수 있었다. 이 후의 작업으로 3D 구조를 복원해주는 mapping 단계가 수행되어야 한다. mapping의 첫단계가 triangulation 이다....</p></div></div></a></div><div class="card"> <a href="/posts/optimizer/"><div class="card-body"> <em class="timeago small" date="2022-06-01 20:20:00 +0900" >Jun 1, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 16주차 - Visual-SLAM MAP and Non-linear Optimizer, Loop Closure</h3><div class="text-muted small"><p> 이때까지는 Projection에 대해 공부했다. 이제 SLAM으로 다시 돌아와보자. Least Squares SLAM이란 Simultaneous Localization And Mapping으로 동시적 위치 추정 및 지도 작성이다. 최소자승법(Least Squares)은 SLAM 문제를 풀 때 사용하는 최적화 방법론이다. 최적화란 어떤 방정식...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/pingpong/" class="btn btn-outline-primary" prompt="Older"><p>[detection] DeepLearning Ping-Pong Ball Detection</p></a> <a href="/posts/motion_estimation/" class="btn btn-outline-primary" prompt="Newer"><p>[데브코스] 16주차 - Visual-SLAM motion estimation</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>