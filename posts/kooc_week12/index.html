<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 1. Motivation and Basics" /><meta property="og:description" content="Chapter 1. Motivation and Basics" /><link rel="canonical" href="https://dkssud8150.github.io/posts/kooc_week12/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/kooc_week12/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-27T01:40:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-10-10T18:24:35+09:00","datePublished":"2022-09-27T01:40:00+09:00","description":"Chapter 1. Motivation and Basics","headline":"[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/kooc_week12/"},"url":"https://dkssud8150.github.io/posts/kooc_week12/"}</script><title>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-09-27 01:40:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Tue, Sep 27, 2022, 1:40 AM +0900" >Sep 27, 2022</em> </span> <span> Updated <em class="timeago" date="2022-10-10 18:24:35 +0900 " data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 10, 2022, 6:24 PM +0900" >Oct 10, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5240 words"> <em>29 min</em> read</span></div></div></div><div class="post-content"><h1 id="chapter-1-motivation-and-basics">Chapter 1. Motivation and Basics</h1><ul><li>목차</ul><ol><li>Motivation<ul><li>Machine Learning, AI, Datamining</ul><li>What is Machine Learning?<ul><li>MLE<li>MAP</ul><li>Basics<ul><li>Probability<li>Distribution<li>And some Rules</ul></ol><p> </p><p> </p><h2 id="1-1-motivation">1-1. Motivation <a href="#1-1-motivation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week12/example_ml.png" data-proofer-ignore></p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week12/type_ML.png" data-proofer-ignore></p><p>Machine Learning에는 학습 방법에 따라 supervised learning, unsupervised learning, reinforcement learning 이 있다. 먼저 supervised learning에는 요약하거나 예측과 같은 것들이 이에 해당하고, 요약하고 정리하고 군집을 찾는 것이 unsupervised learning에 해당한다. 마지막으로 어떤 것이 지능적이고, 원하는 계획인가에 대한 학습이 reinforcement learning이다.</p><p>더 자세하게 살펴보자.</p><p> </p><ul><li>supervised learning</ul><p><img data-src="/assets/img/kooc/week12/supervised_learning.png" data-proofer-ignore></p><p>사람이나, 기계가 지정한 가이드가 있는 것이 supervised라 할 수 있다. 예를 들어, 스팸 필터링은 매우 많은 데이터들에 의해 어떤 것을 필터링할지 판단한다. 또한, 자동 카테고리화도 수많은 데이터의 축적에 의해 분류하는 모델을 생성하는 것이다.</p><p>또는, 특정 값에 대해 가격을 예측하는 것과 같은 것들은 regression, 또는 prediction 도 supervised learning task에 해당한다.</p><p> </p><ul><li>unsupervised learning</ul><p><img data-src="/assets/img/kooc/week12/unsupervised_learning.png" data-proofer-ignore></p><p>그에 반해, 순수히 주어진 데이터에 의해 기계가 군집을 찾고, 패턴을 찾도록 하는 것이 unsupervised learning이다. 또는 잠재적인 현상을 분석할 때도 활용한다. 예를 들어 신문기사가 엄청 많을 때, 주제를 10개로 추려보는 것은 직접 가이드를 제작해주기 힘들어서, 기계가 직접 군집을 찾아야 하므로 unsupervised learning에 해당한다.</p><p> </p><h2 id="1-2-what-consists-of-machine-learning">1-2. What consists of Machine Learning? <a href="#1-2-what-consists-of-machine-learning" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week12/thumbpack.png" data-proofer-ignore></p><p>Thumbtack, 압정을 활용하여 게임을 해보자. 동전과 달리, 압정은 앞과 뒤가 50:50이라 하기 어렵다. 그래서 앞 또는 뒤가 나올 확률을 직접 구해보고자 한다. 가장 먼저 해야 할 방법은 직접 던져보는 것이다.</p><p> </p><p><img data-src="/assets/img/kooc/week12/experience_trial.png" data-proofer-ignore></p><p>뾰족한 부분이 아래일 때를 <code class="language-plaintext highlighter-rouge">Head</code>, 뾰족한 부분이 위일 때를 <code class="language-plaintext highlighter-rouge">Tail</code> 이라 하고, 5번을 던져봤을 때 Head가 2번 Tail이 3번 나온다면, 확률은 각각 2/5, 3/5 인 것은 당연한 결과다. 그러나 이 2/5, 3/5라는 값은 사실 그리 간단하게 구해지는 것이 아니다.</p><p> </p><p><img data-src="/assets/img/kooc/week12/binomial_distribution.png" data-proofer-ignore></p><p>이 2/5, 3/5로 구하는 것은 Binomial distribution이라 하는데, 이는 뚜렷한, 이산적인 사건에 대한 확률 분포를 말한다. 즉, Head가 나오든, Tail이 나오든 2가지의 경우만 존재하므로 Binomial distribution이라 한다.</p><p>먼저 head가 나올 경우, 다음에 Tail이 나올 때 앞서 나온 값이 영향을 주지는 않으므로 독립적인 상황이다. 만약 Head가 나올 경우를 <code class="language-plaintext highlighter-rouge">P(H) = Theta</code>, Tail이 나올 경우를 <code class="language-plaintext highlighter-rouge">P(T) = 1 - Theta</code> 라 하자. 이럴 때, 방금 전 상황인 HHTHT에 대한 확률은 $ P(HTTHT) = \theta (1-\theta) (1-\theta) \theta (1-\theta) = \theta^2 (1-\theta)^3 $ 이다. 전체 Data를 D, Head가 나온 횟수를 a_H, 반대의 경우를 a_T라 한다면 theta, 즉 head가 나올 확률은 다음과 같다.</p>\[P(D | \theta) = \theta^{a_H}(1-\theta)^{a_T}\]<p>이러한 추정값이 최적의 $ \theta $ 라는 것을 증명해야 하는데, 이를 증명하는 것이 <strong>확률</strong>이라 할 수 있다. 이를 증명할 때 사용하는 대표적인 방법에는 MLE, MAP이 있다.</p><p> </p><h3 id="mlemaximum-likelihood-estimation">MLE(Maximum Likelihood Estimation) <a href="#mlemaximum-likelihood-estimation" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>관측된 데이터의 확률이 최대가 되는 값 theta를 찾는 방법이 MLE이다. MLE를 수식적으로 나타내면 다음과 같다.</p>\[\hat{\theta} = argmax_{\theta} P(D|\theta)\]<p>즉, P(D|theta)가 최대가 되는 theta를 theta hat이라 표현한다는 것이다. 여기에 위에서 구한 식을 대입하면</p>\[\hat{\theta} = argmax_{\theta} P(D|\theta) = argmax_{\theta} \theta^{a_H}(1-\theta)^{a_T}\]<p> </p><p>이를 구할 때, 쉽게 구하는 방법은 log를 씌우는 것이다. log를 씌운 값이 최대가 되면, 그 log 안의 값도 최대가 된다는 특성이 있다.</p>\[\hat{\theta} = argmax_{\theta} ln\theta^{a_H}(1-\theta)^{a_T} = argmax_{\theta} (a_H ln\theta + a_T ln(1-\theta))\]<p> </p><p>그 후, 미분을 통해 최솟값을 찾는다. 그러면 <code class="language-plaintext highlighter-rouge">theta_hat = a_H / (a_T + a_H)</code> 가 된다.</p><p> </p><p> </p><p>그런데, 만약 5번이 아닌 50번을 수행해서 동일하게 head가 나올 확률이 0.6이 나온다면, 효율상 5번이 더 좋다고 생각할 수 있다. 여기에는 조금의 오류가 존재한다. 우리가 이때까지 구한 것은 정답(Ground Truth)이 아니라 추론(estimation)값이므로, 오류가 항상 존재한다. 정답인 $ \theta* $ 와 $ \hat{\theta} $ 사이의 오차값과 error boundary($ \epsilon $) 에 대한 수식이 있다.</p>\[P(|\hat{\theta} - \theta*| \geq \epsilon) \leq 2e^{-2N\epsilon^2}\]<p>정답과 추론의 차이가 특정 에러보다 클 확률은 우항보다 작다는 것이다. N은 trial, 즉 반복횟수인데, N이 커질수록 우항은 작아지므로 오차가 특정 error boundary보다 클 확률도 작아지게 될 것이다.</p><p>이러한 방식이 PAC(Probably Approximate Correct)이라 하는데, 즉 아마도 특정 확률과 오차범위 안에서 존재하는 theta를 추정하는 기법이다.</p><p> </p><p> </p><h3 id="map">MAP <a href="#map" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>MLE에서 구한 확률은 사전정보가 포함되어 있지 않다. 그러나 만약 사전정보를 가미하여 확률을 구하고자 한다면 다른 방식을 통해 구해야 할 것이다. Bayes라는 학자는 다음과 같이 확률에 대한 식을 정의했다.</p>\[P(\theta | D) = \cfrac{P(D | \theta)P(\theta)}{P(D)} \: == \: Posterior = \cfrac{Likelihood \: x \: Prior \: Knowledge}{Normalizing\:Constant}\]<p><code class="language-plaintext highlighter-rouge">theta가 주어졌을 때 data를 관측할 확률 x theta에 대한 사전정보 / 데이터를 관측할 확률</code> 를 하면 데이터가 주어졌을 때 theta의 확률을 구할 수 있다. 이 떄 우리는 theta에 대해 구하고 있으므로, P(D)는 상수가 된다. 그리고 P(D|theta)는 이전에 구했으므로 P(theta)만 지정해둔다면 p(theta|D)를 구할 수 있다.</p><p> </p><p>또한, Bayes는 P(theta)를 binomial distribution이 아닌 Beta Distribution을 통해 구하고자 했다.</p>\[P(\theta) = \cfrac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)},\: B(\alpha,\beta) = \cfrac{r(\alpha)r(\beta)}{r(\alpha+\beta)},\:r(\alpha) = (\alpha - 1)!\]<p>이 때, B(alpha, beta)는 theta와 관련이 없으므로 상수 취급할 수 있다. 그렇다면, P(theta|D)는</p>\[P(\theta|D) \propto P(D|\theta)P(\theta) \propto \theta^{a_H}(1-\theta)^{a_T} \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{a_H+\alpha-1} (1-\theta)^{a_T+\beta-1}\]<p>이 식은 P(D|theta) 와 유사한 것으로 보아, theta_hat은 쉽게 추정할 수 있다.</p>\[\hat{\theta} = \cfrac{a_H+\alpha-1}{a_H+\alpha+a_T+\beta-2}\]<p> </p><p>이 때, MLE와 MAP의 값이 다를 수 있다. 그러나 만약 반복 횟수를 증가시킨다면, alpha와 beta에 비해 a_H와 a_T는 증가하여 비중이 커질 것이므로 같아질 수 있다.</p><p> </p><p> </p><h2 id="1-3-basics">1-3. Basics <a href="#1-3-basics" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="normal-distribution">Normal Distribution <a href="#normal-distribution" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week12/normal_distribution.png" data-proofer-ignore></p><p>가장 많이 사용되는 기본적인 확률분포의 형태이다. 이 평균(mean) 또는 분산(variance)를 통해 함수의 모양을 바꿀 수 있을 것이다. Normal distribution에는 양쪽에 롱테일, 즉 무한대로 가는 긴 꼬리가 존재한다.</p><p> </p><h3 id="beta-distribution">Beta Distribution <a href="#beta-distribution" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week12/beta_distribution.png" data-proofer-ignore></p><p>beta에서는 롱테일이 존재하지 않는다. 따라서 확률을 모델링할 때 0과 1이라는 확실한 범위가 존재하므로 beta distribution을 사용할 수 있다.</p><p> </p><h3 id="binomial-distribution">Binomial Distribution <a href="#binomial-distribution" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week12/binomial_distribution_2.png" data-proofer-ignore></p><p>이 binomial distribution은 앞서 배운 함수 형태를 가지고 있고, 이산적인(discrete) 함수 형태를 가지고 있다.</p><p> </p><h3 id="multinomial-distribution">Multinomial Distribution <a href="#multinomial-distribution" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>binomial distribution은 2가지의 경우만 판단하지만, 만약 2가지 이상의 선택지가 존재한다면 더 다양한 예제를 판단할 수 있어야 한다. 대화를 할 때, 수많은 단어 중 하나를 선택하므로 수많은 데이터를 가진 이산적인 함수를 사용한다.</p><p><img data-src="/assets/img/kooc/week12/multinomial_distribution.png" data-proofer-ignore></p><p> </p><p> </p><h1 id="chapter-2-fundamentals-of-machine-learning">Chapter 2. Fundamentals of Machine Learning</h1><ul><li>목차</ul><ol><li>classical method overview<ul><li>rule based approach<li>classical statistics approach<li>information theory approach</ul><li>rule based machine learning<ul><li>how to find generalized rules</ul><li>decision tree<ul><li>how to create decision tree<li>weakness of decision tree given new dataset</ul><li>linear regression<ul><li>how to infer a parameter set</ul></ol><p> </p><p> </p><h2 id="2-1-rule-based-machine-learning-overview">2-1. Rule Based Machine Learning Overview <a href="#2-1-rule-based-machine-learning-overview" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week12/rulebasedlearning.png" data-proofer-ignore></p><p>perfect world라고 가정해보자. 즉 관측에 대한 에러가 존재하지 않고, 관측한 정보가 전부라 가정하는 것이다. 이러한 상황에서 날씨에 대해 관측하여 밖으로 나가 운동을 할지에 대한 태스크를 진행한다.</p><p> </p><ul><li>function approximation</ul><p><img data-src="/assets/img/kooc/week12/function_approximation.png" data-proofer-ignore></p><p>function approximation이란 몇 개의 데이터가 있을 때, 어떤 함수에 입력하면 나오는 결과가 정답인 함수를 추정하는 것이다. 그래서 머신러닝이란 더 나은 함수를 생성하는 기법이 된다.</p><p>데이터의 개수, instance,<code class="language-plaintext highlighter-rouge">X</code> 가 존재하고, 그 instance에 대한 feature가 <code class="language-plaintext highlighter-rouge">O=&lt;Sunny, Warm, Normal, Strong, Warm, Same&gt;</code> 이라는 값을 가지고, 원하는 정답인 <code class="language-plaintext highlighter-rouge">Y=yes</code> 라 해보자. 이러한 instance가 여러 개 존재할 때, dataset D가 될 것이다. 데이터를 통해 가설, <code class="language-plaintext highlighter-rouge">H</code>를 세워 X를 집어넣을 떄 Y가 나오는 함수를 생성할 수 있다. 예를 들어 sunny, warm, same만 일치한다면 나머지에는 상관없이 yes라는 Y를 출력하는 h_i를 가정할 수 있다. 이는 가설한 것이므로 수많은 가설이 존재할 수 밖에 없다.</p><p>그리고, 우리가 진짜 원하는 함수인, target function, <code class="language-plaintext highlighter-rouge">c</code> 가 있다면, 항상 <code class="language-plaintext highlighter-rouge">c(X) = Y</code>라는 식이 참이 된다. 이 c는 알 수 없으므로 추론하여 알아내야만 한다. 그래서 데이터의 개수가 많을수록 우리가 원하는 정답인 Y를 정확하게 추출할 수 있게 된다.</p><p><img data-src="/assets/img/kooc/week12/graph_function_approximation.png" data-proofer-ignore></p><p>그래서 instance가 x1,x2,x3가 있고, instance에 대한 가설인 h1,h2,h3가 있다고 해보자. 그렇다면, 가설이 정확한지 아닌지를 판단하기 위해서는 h1,h2,h3에 각각의 instance를 집어넣어 정답이 잘 추론되었는지 역계산해야 한다. 위의 h1,h2,h3를 살펴보면, h1은 x1,x2,x3가 모두 yes라는 값을 추출하는 가설이 되고, h2와 h3는 각각 2개씩만 yes라는 값을 추출하는 가설이다. 그러면 h1은 다소 일반적인 가설이므로 generalized function이라 할 수 있고, h2와 h3는 h1보다 specificed function이라 할 수 있다.</p><p> </p><ul><li>Find-S Algorithm</ul><p><img data-src="/assets/img/kooc/week12/find_s.png" data-proofer-ignore></p><p>그래서 정답의 가설인 c를 잘 추론하기 위한 알고리즘으로 Find-S 라는 알고리즘이 있다. 이 알고리즘은 먼저, 가장 specific한 가설인 H = &lt;None, None, None, None, None, None&gt; 라는 H를 세운다. 그 후 각 instance마다 각 feature들을 현재의 가설과 비교하여 업데이트한다.</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">if </span>x is positive
  For feature <span class="k">in </span>O
    <span class="k">if </span>h_i <span class="k">in </span>H <span class="o">==</span> feature_i
      pass
    <span class="k">else
      </span>update to more generalize
</pre></table></code></div></div><p>처음에는 모두 None이었다가 x1에 의해 H에 있는 모든 feature들을 업데이트하고, 다음 x2에서 각 feature들과 H에 있는 feature들을 비교하여 같지 않으면 둘다 포함이 되도록 업데이트한다.</p><p> </p><p>이러한 방법으로 하면 가설을 확정지을 수는 있지만, 만약 이렇게 나온 h1,2,3,4가 정답과 다를수도 있을 것이다. 그래서 생각한 방법이 범위를 정하여 그 범위 안에 존재하는 가설은 모두 정답의 후보라 할 수 있게 하는 알고리즘을 생각했다.</p><p> </p><ul><li>Version Space</ul><p><img data-src="/assets/img/kooc/week12/version_space.png" data-proofer-ignore></p><p><img data-src="/assets/img/kooc/week12/candidate_elimination_algorithm.png" data-proofer-ignore></p><p>VS(Version Space)란 가능한 가설의 집합을 나타내고, 이를 구하기 위해서 가장 일반적인 가설과 가장 구체적인 가설을 정의하여 범위를 좁혀나가는 <strong>Candidate Elimination Algorithm</strong>을 사용해보고자 한다. 일반적인 가설을 <code class="language-plaintext highlighter-rouge">G</code>, 구체적인 가설을 <code class="language-plaintext highlighter-rouge">S</code> 라 하고, dataset에 포함되어 있는 각 instance마다 값을 비교해나가며 업데이트한다.</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">if </span>y of x is positive
  <span class="c"># update first S, before G</span>

<span class="k">if </span>y of x is negative
  <span class="c"># update first G, before S</span>
</pre></table></code></div></div><p>y값이 참이라고 한다면, S의 범위를 x에 존재하는 feature들을 커버할 수 있을만큼만 일반적으로 업데이트해야 하는데, 만약 G에 대해 거짓이 나오게 된다면 G를 한 단계 일반적인 방향으로 떨어뜨려야 한다. 반대로, 거짓이라면 G의 범위를 커버할 수 있을만큼만 구체적으로 업데이트해야 하고, 만약 S에서 참이라 한다면 S를 한 단계 구체적인 방향으로 올려야 한다. 구체적인 동작방식은 다음과 같다.</p><p><img data-src="/assets/img/kooc/week12/cea_example.png" data-proofer-ignore></p><p>출력값이 yes라고 한다면, S를 먼저 업데이트하므로 G0==G1==G2가 될 것이다. 그리고 G3에서 same의 경우 마지막 instance에서 forest가 change이지만, yes가 나오는 결과로 인해 forest라는 feature는 y에 대해 관련이 없다라고 업데이트가 되야 하므로, 후보에서 제외가 된다.</p><p> </p><p><img data-src="/assets/img/kooc/week12/new_instance.png" data-proofer-ignore></p><p>이 때, 만약 새로운 instance가 등장했고, 한가지의 instance인 세번째 값이 Specific boundary에는 만족하지 않으나, General boundary에는 만족하는 값이라면, 즉 VS안의 수많은 가설 중 하나라면 판단하기 어려우므로 이러한 rule based 방식은 적합하지 않다. perfect world에서는 rule based가 정확하지만, real world에서는 정확하지 않다.</p><p> </p><p> </p><h2 id="2-2-desicion-tree">2-2. Desicion Tree <a href="#2-2-desicion-tree" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week12/desicion_tree.png" data-proofer-ignore></p><p>perfect world에서는 정확하나 real world에서는 불안정한 이유는 바로 노이즈 때문이다. real world에서는 노이즈는 항상 존재하는데, 이러한 노이즈를 제거하기 위해 수많은 연구들이 진행되어 왔다. 그 중 하나가 <strong>decision tree</strong>이다.</p><p>sky가 sunny라고 하면, temp를 고려하여 warm일 때만 다음을 고려하고, 그렇게 feature들을 모두 고려하는 방식이 desicion tree이다.</p><p> </p><p><img data-src="/assets/img/kooc/week12/credit_approval_dataset.png" data-proofer-ignore></p><p>조금 더 구체적인 예시를 위해 흔히 사용되는 uci dataset을 사용해보고자 한다. uci 데이터셋 중에서 credit approval, 신용평가에 대한 데이터셋을 사용할 것이다. 총 690개의 instance가 존재하고, 307개는 positive, 나머지는 negative로 구성되어 있다. 그리고 신용평가에 사용되는 feature들은 15개가 존재한다. 여기서 A1에 대해서만 판단하여 a를 항상 negative, b는 항상 positive를 준다고 판단했을 때, 실제 결과는 a에 대해 112개는 negative, 98개는 positive이고, b에 대해서는 206개는 positive, 262개는 negative로 구성되게 된다. 그렇다면 a의 98개와 b의 262개는 틀린 값이 된다. 이 때, <code class="language-plaintext highlighter-rouge">?</code> 는 don’t care가 아니라, 데이터가 존재하지 않는 instance에 대한 값이다. A9에 대해 t를 positive, f에 대해 negative를 주게 된다면, A1보다는 적게 틀리게 되겠지만 에러는 존재하게 된다.</p><p> </p><p> </p><h2 id="2-3-entropy-and-information-gain">2-3. Entropy and Information Gain <a href="#2-3-entropy-and-information-gain" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="entropy">Entropy <a href="#entropy" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>불확실성(uncertainty), 즉 에러를 줄이기 위해 어떤 속성이 좋은지를 측정하기 위해 <strong>Entropy</strong>를 사용한다. entropy, H는 다음과 같이 구할 수 있다. 이 때, entropy가 높을수록 불확실성도 높아진다.</p>\[H(X) = -\sum_X P(X = x)log_bP(X = x)\]<p> </p><ul><li>Conditional Entropy</ul><p>조건부를 적용한 상황에서 entropy도 구할 수 있다. 우리는 특정 feature가 주어졌을 때의 entropy를 구하고 싶어 한다. 그래서 X가 주어졌을 때, Y에 대한 entropy는 다음과 같다.</p>\[H(Y|X) = \sum_X P(X = x)H(Y|X = x) = \sum_X P(X = x)(\sum_Y P(Y = y | X = x)log_bP(Y = y | X = x))\]<p> </p><h3 id="information-gain">Information Gain <a href="#information-gain" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="/assets/img/kooc/week12/information_gain.png" data-proofer-ignore></p><p>information gain은 또 다른 측정기법이다. 우리는 A1 또는 A9에 대한 entropy를 구할 수도 있지만, A1에서 a,b,? 각각의 클래스에 대한 entropy도 구할 수 있을 것이다.</p><p>+,- 각각에 대한 확률을 구하여 A1에 대한 entropy를 구할 수 있다.</p>\[P(Y = +) = \cfrac{307}{307 + 383}, \: P(Y = -) = \cfrac{383}{307 + 383}\]<p> </p><p>그렇다면, A1 또는 A9이라는 조건을 주고 conditional entropy를 측정한다면 H(Y|A1) 과 같은 형태를 띌 것이다. 여기서 우리가 구해봐야 할 것은 단지 conditional entropy가 아니라, 이를 활용하여 원래의 entropy인 H(Y)에 비해 불확실성이 얼마나 줄어드는지를 측정해야 한다. 그래서 이러한 값이 <strong>information gain</strong>이라 하는 값이 되고, 이는 원래의 값에서 conditional entropy를 빼주면 된다.</p>\[IG(Y, A_i) = H(Y) - H(Y|A_i)\]<p>당연히 IG가 클수록 불확실성이 그만큼 줄어들었다는 것이므로 더 좋은 지표가 될 것이다.</p><p> </p><p>이렇게 각 클래스별로 information gain을 가지고, 최적의 루트를 정할 수 있다.</p><p> </p><p><img data-src="/assets/img/kooc/week12/id3.png" data-proofer-ignore></p><p>이렇게 decision tree를 만드는 방식에는 여러 가지가 있는데, 대표적으로 <strong>ID3 algorithm</strong>이 있다. 먼저 하나의 클래스를 정해서 오픈 노드를 생성한다. 이에 대해 최적의 루트를 판단하기 위해 위에서 배운 Information gain을 활용하여 어느 방향이 더 좋은지 확인한다.</p><p> </p><p> </p><p>여기서 tree가 깊을수록 좋을 것이라 생각할 수 있지만, 현실은 그렇지 않다. 왜냐하면 이러한 tree는 주어진 데이터셋에 대해 너무 많이 최적화되기 떄문이다. 이를 overfitting이라 하고, 그래서 최적의 tree 깊이를 잘 정해줘야 한다.</p><p><img data-src="/assets/img/kooc/week12/moredepthtree_problem.png" data-proofer-ignore></p><p> </p><p> </p><h2 id="2-4-linear-regression">2-4. Linear Regression <a href="#2-4-linear-regression" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week12/statistical_approach.png" data-proofer-ignore></p><p>지금까지는 Rule 기반의 모델을 구축했지만, 이제는 조금 더 <strong>통계 기반</strong>의 모델을 구축해보고자 한다. 머신러닝이란 더 나은 함수를 추정하는 것인데, 그 중 Linear Regression은 선형으로 decision boundary를 추정해보는 것이다.</p><p>rule 기반의 모델에서는 뚜렷한 구별 형태로 가설을 세웠지만, 이제는 함수 형태로 가설해볼 것이다. 지금은 선형의 모델을 구성할 예정이므로, 가중치(weight)인 theta를 잘 정의하여 최적화한다.</p><p>가설 h는 $ \hat{f}(x;\theta) = \theta_0 + \sum_{i=1}^n \theta_i x_i = \sum_{i=0}^n \theta_i x_i $ 로 정의할 수 있다. 이때, n은 feature의 개수이다. 이러한 h를 matrix 형태로 변환해보면 다음과 같다.</p><p><img data-src="/assets/img/kooc/week12/matrix.png" data-proofer-ignore></p><p>따라서 $ \hat{f} = X\theta $ 가 된다. 그리고, real world에서는 노이즈가 항상 존재하므로, error term인 <code class="language-plaintext highlighter-rouge">e</code>를 추가하여 정답의 f를 정의한다. 그리고 우리는 이 error term을 최소인 가설이 최적의 가설이라 판단할 수 있다. 그러므로 e가 최소가 되는 theta를 구한다.</p>\[f = X\theta + e = Y\] \[\hat{\theta} = argmin_{\theta} (f-\hat{f})^2 = argmin_{\theta} (Y - X\theta)^2\]<p>이 때, 이 식을 행렬의 특성을 활용하여 다시 나타내면 다음과 같다.</p>\[argmin_{\theta} (Y - X\theta)^T (Y-X\theta) = argmin_{\theta} (Y-X\theta)^T (Y-X\theta) = argmin_{\theta} (\theta^T X^T X\theta - 2\theta^TX^TY + Y^TY) = argmin_{\theta} (\theta^TX^TX\theta - 2 \theta^T X^T Y)\]<p>우리가 최소화시키고자 하는 값은 theta이므로 theta와 관련이 없는 Y^T Y 는 제거할 수 있다. MLE를 구할 때 사용했던 테크닉을 그대로 사용하여 argmin 안에 있는 값을 미분하여 최소가 되는 theta를 구한다.</p>\[\theta = (X^TX)^{-1} X^TY\]<p> </p><p><img data-src="/assets/img/kooc/week12/linear_regression.png" data-proofer-ignore></p><p>이를 통해 값을 추정할 수 있다. 빨간색 점이 GT 즉 정답인 Y값이고, 파란색 점들이 우리가 선형으로 추정한 Y^ 이다. 선형으로 추정을 하니 생각보다 잘 맞지 않는 모습을 보이고 있다. 따라서 이러한 linear 특성을 수정하여 비선형 가설을 세우고자 한다.</p><p><img data-src="/assets/img/kooc/week12/nonlinear_regression.png" data-proofer-ignore></p><p>원래의 식에서 x value에 제곱, 세제곱 네제곱… 을 취하여 새로운 variable을 생성한다.</p><p><img data-src="/assets/img/kooc/week12/nonlinear_regression_graph.png" data-proofer-ignore></p><p>이를 통해 새로운 곡선을 만들었는데, 뒤에 오는 값들이 거의 희박하게 존재하는데도 이들을 위해 더 깊게 만들어야 하는지에 대한 의문이 든다. 따라서 최적의 깊이를 정하는 것이 또 다시 중요해진다.</p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week12/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week12.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/kooc/'>kooc</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/kooc/" class="post-tag no-text-decoration" >kooc</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/kooc_week12/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/kooc_week12/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/kooc_week12/&amp;text=[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/kooc_week12/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/kooc_start/"><div class="card-body"> <em class="timeago small" date="2022-09-27 00:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 - Abstract</h3><div class="text-muted small"><p> KOOC란? KOOC(KAIST Massive Open Online Course)는 KAIST 무료 온라인 강좌를 말하는 것으로, KAIST 교수님들의 강의를 무료로 수강할 수 있는 플랫폼입니다. KOOC에는 다양한 주제로 강좌가 열려 있고, 온라인으로 진행되는 강좌이므로 언제 어디서든 강의를 들을 수 있습니다. 저는 그 중, 인공지능을 전공할 때 ...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week34/"><div class="card-body"> <em class="timeago small" date="2022-09-29 01:40:00 +0900" >Sep 29, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression</h3><div class="text-muted small"><p> Chapter 3. Motivation and Basics 목차 optimal classification optimal predictor concept of Bayes risk concept of desicion boundary Naive Bayes Classifier ...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week56/"><div class="card-body"> <em class="timeago small" date="2022-10-05 00:01:00 +0900" >Oct 5, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 5,6주차 - Support Vector Machine, Training, Regularization</h3><div class="text-muted small"><p> Chapter 5. Support Vector Machine 목차 Support Vector Machine Classifier maximum margin idea of the SVM formulation of the optimization problem soft-margin and pen...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/kooc_start/" class="btn btn-outline-primary" prompt="Older"><p>[KOOC] 인공지능 및 기계학습 개론 - Abstract</p></a> <a href="/posts/kooc_week34/" class="btn btn-outline-primary" prompt="Newer"><p>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>