<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 7. Bayesian Network" /><meta property="og:description" content="Chapter 7. Bayesian Network" /><link rel="canonical" href="https://dkssud8150.github.io/posts/kooc_week7/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/kooc_week7/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-15T19:10:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-10-25T10:40:56+09:00","datePublished":"2022-10-15T19:10:00+09:00","description":"Chapter 7. Bayesian Network","headline":"[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/kooc_week7/"},"url":"https://dkssud8150.github.io/posts/kooc_week7/"}</script><title>[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-10-15 19:10:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Sat, Oct 15, 2022, 7:10 PM +0900" >Oct 15, 2022</em> </span> <span> Updated <em class="timeago" date="2022-10-25 10:40:56 +0900 " data-toggle="tooltip" data-placement="bottom" title="Tue, Oct 25, 2022, 10:40 AM +0900" >Oct 25, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4299 words"> <em>23 min</em> read</span></div></div></div><div class="post-content"><h1 id="chapter-7-bayesian-network">Chapter 7. Bayesian Network</h1><ul><li>목차</ul><ol><li>Probability<ul><li>recover the probability concepts<li>recover the probability theorems<li>recover the concepts of the marginal and the conditional independencies</ul><li>Bayesian networks<ul><li>syntax and semantics of Bayesian Networks<li>how to factorize Bayesian networks<li>calculate a probability with given conditions</ul><li>inference of Bayesian networks<ul><li>calculate parameters of Bayesian networks<li>list the exact inference of Bayesian networks</ul></ol><p> </p><p> </p><h2 id="7-1-theorems-of-probability">7-1. theorems of Probability <a href="#7-1-theorems-of-probability" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>bayesian network에 들어가기 전에, probability에 대한 복습을 하고자 한다.</p><p> </p><ul><li>Conditional Probability</ul><p>conditional probability란 특정 상황이 주어졌을 때의 확률을 구하는 것이다.</p><p><img data-src="/assets/img/kooc/week78/conditional_probability.png" data-proofer-ignore></p><p>예를 들어, A와 B에 대한 상황이 존재한다고 생각해보자. B가 주어졌을 때의 A의 확률을 구한다는 것은 P(F=True)인 영역 속에서 P(H=True)인 영역에 해당한다.</p><p>이는 P(A = true | B = true) 와 같이 표현한다.</p>\[P(A = true | B = true) = \cfrac{P(A,B)}{P(B)}\]<p> </p><ul><li>Joint Probability</ul><p>비슷한 개념으로 Joint Probability라는 것이 있다. P(A = true, B = true) 와 같이 표현할 수 있고, 이는 A = true 와 B = true 인 확률을 나타낸다.</p>\[P(A = true, B = true) = P(A|B)P(B)\]<p><img data-src="/assets/img/kooc/week78/joint_probability.png" data-proofer-ignore></p><p> </p><p>이 두 개념을 활용하여 개별 확률인 marginal probability를 구할 수 있다.</p>\[P(a) = \sum_b P(a,b) = \sum_b P(a|b)P(b)\]<p> </p><p>만약, joint distribution인 P(a,b,c,d) 가 주어졌을 때, 우리는 P(b)에 대해 식을 세울 수 있다.</p>\[P(b) = \sum_a \sum_c \sum_d P(a,b,c,d)\]<p>또는, joint distribution을 통해 conditional probability를 구할 수 있다.</p>\[P(c|b) = \sum_a \sum_d P(a,c,d | b) = 1/P(b) \sum_a \sum_d P(a,c,d,b)\]<p>이 때, 1/P(b) 는 normalization constant(정규화 상수) 이다.</p><p> </p><p>이렇게만 보면 joint probablity가 매우 강력한 개념이라 생각이 되지만, joint probability를 구하기 위해서는 개별 확률들을 알아야 하므로, feature의 개수와 개별 경우의 수에 따라 기하급수적으로 파라미터가 증가한다.</p><p>만약 feature 개수가 4개이고, 개별 feature이 true/false로 구성된다면 총 16-1 개의 파라미터를 구해야 한다.</p><p> </p><p>joint probability의 또다른 특징으로는 <strong>chain rule</strong> 이다.</p>\[P(a,b,c,...z) = P(a|b,c,...,z)P(b,c,...,z) = P(a|b,c,...,z)P(b|c,...,z)P(c|...,z)...P(z)\]<p> </p><p> </p><ul><li>Independence</ul><p>variable A와 B가 독립적일 때는 다음과 같은 관계가 성립된다.</p><ul><li>P(A|B) = P(A)<li>P(A,B) = P(A)P(B)<li><div class="table-wrapper"><table><tbody><tr><td>P(B<td>A) = P(B)</table></div></ul><p> </p><p>따라서 P(C1,…,Cn) 의 joint distribution을 계산하는 것은 간단하다.</p>\[P(C_1,...,C_n) = \prod_{i=1}^n P(C_i)\]<p> </p><ul><li>Conditional Independence &amp; Marginal Independence<ul><li>Marignal independence<ul><li>P(A=true|B=true) &gt; P(A=true) 라면 이 경우는 marginally independent가 아니다.<li>즉, A와 B는 P(A) = P(A|B) 일때만 독립적이어야 한다.</ul><li>Conditional Independence<ul><li>P(A=true|B=true, C=true) = P(A=true|C=true) 이면, conditional independent하다.</ul></ul></ul><p> </p><p> </p><h2 id="7-2-bayesian-network">7-2. Bayesian Network <a href="#7-2-bayesian-network" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>이전에 Naive Bayes Classifier을 배웠고, 이에 대한 function을 정의했다.</p>\[f_{NB}(x) = argmax_{Y=y}P(Y=y) \prod_{1 \leq i \leq d} P(X_i = x_i | Y = y)\]<p><img data-src="/assets/img/kooc/week78/bayesian_network.png" data-proofer-ignore></p><p>bayesian network는 graphical notation 한 특성을 가지고 있다. 즉</p><ul><li>random variable<li>conditional independence<li>obtain a compact representation of the full joint distribution</ul><p>에 대한 정보가 들어있다.</p><p> </p><ul><li>Syntax(문법)</ul><p>bayesian network는</p><ul><li>acyclic(사이클이 존재x), directed graph<li>nodes<ul><li>random variable<li>P(X_i| Parents(X_i)) (e.g. P(X1|Y))</ul><li>link<ul><li>Direct influence from the parent to the child</ul></ul><p>네트워크의 구조는 conditional independence를 가정하고 있다.</p><p><img data-src="/assets/img/kooc/week78/conditional_independence.png" data-proofer-ignore></p><p>즉, toothache(치통)과 stench(악취)는 충치(cavity)와는 관계가 있지만, weather과는 관계가 없다. 따라서 weather은 variable과는 독립적이고, toothache와 stench는 cavity와 conditionally independent하다.</p><p> </p><p>유명한 예시를 하나 들어보자.</p><p><img data-src="/assets/img/kooc/week78/bayesian_network_example.png" data-proofer-ignore></p><p>도둑(buglary)이 들거나, 지진(earthquake)이 발생했을 때, 알람(Alarm)이 울리는데, 알람이 울리면, 이웃인 John과 Mary가 전화를 하는 상황이다.</p><p>알람에 대한 확률은 P(A|B,E) 로 나타낼 수 있고, John이 전화하는 것에 대한 확률은 P(J|A), Mary가 전화할 확률은 P(M|A) 이다.</p><p>이 network에서의 Variable은 Burglary, Earthquake, Alarm, JohnCalls, MaryCalls, 모두이다.</p><p> </p><p>정성적(Qualitative)인 요소로는</p><ul><li>인과관계에 대한 사전정보<li>데이터로부터의 학습<li>네트워크의 구조</ul><p> </p><p>정량적(Quantitative)인 요소로는</p><ul><li>조건부 확률 테이블(이미지에서의 우하단 표)</ul><p> </p><p> </p><p><img data-src="/assets/img/kooc/week78/local_structure.png" data-proofer-ignore></p><p>위의 구조는 간단했지만, 이러한 network를 복잡하게 구성할 수도 있다. 이럴 때, 지역적인 variable들에 대한 관계를 정의할 수 있다.</p><ul><li>Common parent<ul><li>동일한 parent variable을 가진 다른 variable에 대해서는 독립적이다. 즉, John이 전화할 확률과 Mary가 전화할 확률은 서로 독립적이다.<li>𝐽 ⊥ 𝑀|𝐴<ul><li>P(J,M|A) = P(J|A)P(M|A)</ul></ul><li>Cascading<ul><li>연쇄작용으로서, 연결되어 있는 variable일 때, A가 주어졌다면, B와 M은 독립이다. mary가 전화할 확률은 A를 알고 있다면 B는 필요가 없다. 즉 direct influence에 대한 확률을 안다면 indirect influence variable과는 독립적이다.<li>B ⊥ M|A<ul><li>P(M|B,A) = P(M|A)</ul></ul><li>V-structure<ul><li>B와 E는 공통의 child를 가지는 상황을 V-structure이라 하는데, A가 주어지지 않았다면, B와 M은 관계가 없으므로, 독립적이나, A가 주어진다면, B와 E는 관계가 생기는 것이므로 독립이 되지 않는다.<li>~ (B ⊥ E|A)<ul><li>P(B,E,A) = P(B)P(E)P(A|B,E)</ul></ul></ul><p> </p><p><img data-src="/assets/img/kooc/week78/bayes_ball_algorithm.png" data-proofer-ignore></p><p>이러한 복잡한 관계를 쉽게 이해하기 위해 <strong>Bayes Ball Algorithm</strong> 이라는 개념을 도입한다. 만약 $ X_A ⊥ X_B | X_C $ 를 확인하고자 할 때, 즉 X_C가 주어졌을 때, X_A와 X_B가 독립인지에 대해 판단하고자 할 때 사용할 수 있다.</p><p>공을 굴린다고 생각해서, 만약 방향이 -&gt; 일 때, 오른쪽 variable이 주어진다면(회색) 공이 굴러가지지 않고, variable이 주어지지 않는다면(무색) 굴러갈 수 있다. 반대로 방향이 &lt;- 이라면, 오른쪽 variable이 주어졌을 때는 굴러가지고, variable이 주어지지 않았을 때는 굴러갈 수 없다.</p><p> </p><p>위의 local structure들에 적용해보자.</p><ul><li>Common parent</ul><p>두 개의 variable이 공통의 한 개 parent를 가지는데, parent가 주어진다면, 공이 굴러갈 수 없으므로 이 두 variable은 독립적이고, 주어지지 않는다면, 독립이 아니다.</p><p> </p><ul><li>Cascading</ul><p>세 개의 variable이 연속적으로 존재하는데, 중간의 variable이 주어진다면, 나머지 두 개의 variable은 공이 굴러갈 수 없어서 독립이고, 주어지지 않는다면, 공이 굴러갈 수 있으므로 독립이 아니다.</p><p> </p><ul><li>V-structure</ul><p>두 개의 variable이 공통의 한 개 child를 가지는데, child가 주어진다면, V-structure은 common parent와 반대로, 공이 굴러 갈 수 있어서 두 variable은 독립이 아니다. 그러나 주어지지 않는다면 공이 굴러갈 수 없어서 독립이다.</p><p> </p><p>간단한 예시를 들어보자.</p><p><img data-src="/assets/img/kooc/week78/example_of_bayesball.png" data-proofer-ignore></p><p>x1~x6의 variable이 존재하고, 위와 같은 관계로 형성되어 있다고 해보자.</p><ol><li>x2가 주어졌을 때, x1과 x4의 관계 (x1 ⊥ x4 | x2)</ol><p>x2가 주어졌을 때, x1과 x4는 x2 방향으로는 cascading구조이므로 지나갈 수 없다. x3방향으로 지나간다 하더라도, x1과 x6는 v-structure구조이므로 지나갈 수 없다.</p><p>따라서 x1과 x4는 독립이다.</p><p> </p><ol><li>x1이 주어졌을 때, x2와 x5의 관계 (x2 ⊥ x5 | x1)</ol><p>x1이 주어졌을 때, x2와 x5는 v-structure 구조이므로, x6가 주어졌을 때만 지나갈 수 있다. x1방향으로 지나간다 하더라도, common parent 구조이므로 x1이 알려져 있지 않아야 지나갈 수 있다.</p><p>따라서 x1가 주어졌을 때는 x2와 x5는 독립이다.</p><p> </p><ol><li>x2,x3가 주어졌을 때, x1과 x6의 관계 (x1 ⊥ x6 | {x2,x3})</ol><p>x2와 x3가 주어졌을 때, x1과 x6는 x2방향으로는 cascading 구조이므로 x2가 주어졌을 때는 지나갈 수 없다. x3방향으로 지나간다 하더라도, cascading 구조이므로 x3가 주어졌을 때는 지나갈 수 없다.</p><p>따라서 x2와 x3가 주어졌을 때는 x1와 x6는 독립이다.</p><p> </p><ol><li>x1,x6가 주어졌을 때, x2와 x3의 관계 (x2 ⊥ x3 | {x1,x6})</ol><p>x1과 x6가 주어졌을 때, x2와 x3는 x1방향으로는 common parent 구조이므로 x1이 주어졌을 때는 지나갈 수 없다. x6방향으로 지나간다 하면, x2와 x5가 v-structure 구조이고, x6가 주어졌으므로 지나갈 수 있고, x6,x5,x3가 cascading 구조인 상황에서 x5가 주어지지 않았으므로 x3로 도착할 수 있다.</p><p>따라서 x1와 x6가 주어졌을 때는 x2와 x3는 독립이 아니다.</p><p> </p><p> </p><p>이러한 bayes ball algorithm을 활용하여 다양한 기법을 정의할 수 있다.</p><ol><li>Markov Blanket</ol><p><img data-src="/assets/img/kooc/week78/markov_blanket.png" data-proofer-ignore></p><p>A라는 특정 variable이 bayesian network안에 존재한다고 할 때, 주변 특정 관계에 있는 variable만 알면, 나머지의 variable에 대해서는 conditional independent하다.</p><p>특정 관계에는 parents, children, children’s other parents 이다.</p><ul><li>parents -&gt; cascading 관계에 있는 variable과 관련<li>children -&gt; cascading 관계에 있는 varable과 관련<li>children’s other parents -&gt; v-structure 관계에 있는 variable과 관련</ul><p> </p><p> </p><ol><li>D-Seperation(directly-seperated)</ol><ul><li>Y가 주어졌을 때, Z와 X는 d-seperated 이다. (X ⊥ Z | Y)</ul><p> </p><p> </p><p><img data-src="factorization.png" alt="" data-proofer-ignore></p><p>bayesian network가 있을 때, 이러한 joint probability를 구할 때, conditional independent 를 고려하면 다음과 같이 단순화할 수 있다.</p>\[P(X) = \prod_i P(X_i | X_{\pi_i})\] \[P(X1,X2,X3,X4,X5,X6,X7,X8) = P(X1)P(X2)P(X3|X1)P(X4|X2)P(X5|X2)P(X6|X3,X4)P(X7|X6)P(X8|X5,X6)\]<p>이렇게 나타낼 수 있는 이유는 joint probability와 conditional probability의 관계와, cascading, common parent 등의 구조로 인해 가능하다.</p><p> </p><p><img data-src="plate_notation.png" alt="" data-proofer-ignore></p><p>오른쪽과 같이 $ \mu $ 와 $ \sigma $ 의 공통의 parent를 가지는, X 들이 있다고 하면, 사각형의 공간을 만들어 간단하게 표현할 수 있다. 수식도 간단하게 나타낼 수 있다.</p>\[P(D|\theta) = P(X_1,...,X_N|\mu, \sigma) = \prod_N P(X_1|\mu, \sigma)\]<p> </p><p> </p><h2 id="7-3-inference-on-bayesian-networks">7-3. Inference on Bayesian Networks <a href="#7-3-inference-on-bayesian-networks" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="1-likelihood">1. Likelihood <a href="#1-likelihood" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="inference_question1.png" alt="" data-proofer-ignore></p><p>모든 variable, X = {X_1,…,X_N} 이 있을 때, X는 $ X_H$ 와 $ X_V $ 로 나눌 수 있다. $ X_V $는 evidence variable, 즉 관측된 variable이고, $ X_H $ 는 hidden variable, 즉 관측하지 않은 variable이다.</p><p>evidence value, $ x_V $ 에 대한 확률(Likelihood)은 다음고 같다.</p>\[P(x_V) = \sum_{X_H} P(X_H, X_V) = \sum_{x_1} \dots \sum_{x_k} P(x_1, ..., x_k, x_V)\]<p>bayesian network와 conditional probability table이 존재하면, Joint Probability를 구하는 것이 가장 적합하다. 따라서, 구하고자 하는 것이 아닌 X_H에 대해 marginalization을 수행하여, X_H에 대한 관계를 모두 포함시킨다.</p><p>도둑과 지진에 대한 알람 task에서의 P(X_H, X_V) 는 다음과 같다.</p>\[P(X_H, X_V) = P(B)P(E)P(A|B,E)P(M|A)P(J|A) = P(B,E,A,M,J)\]<p> </p><h3 id="2-conditional-probability">2. Conditional Probability <a href="#2-conditional-probability" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="inference_question2.png" alt="" data-proofer-ignore></p><p>B와 M에 대해 주어졌을 때 알람의 conditional probability를 구해보자. hidden variable을 또다시 2가지로 세분화할 수 있다.</p><p>X_H = {Y,Z}</p><ul><li>Y : 관측이 되지 않았지만, 관심은 있는 variable (interested hidden variable)<li>Z : 관측도 되지 않았고, 관심도 없는 variable (uninterested hidden variable)</ul><p>B와 M은 관측이 되었으므로 X_V에 해당하고, A에 대해 구하고 싶으므로, Y는 A, 나머지는 Z에 해당한다.</p><p>그래서 관측된 variable들에 대한 interested hidden variable에 대한 식은 다음과 같다.</p>\[P(Y|x_V) = \sum_z P(Y, Z=z | x_V) = \sum_z \cfrac{P(Y, Z, x_V)}{P(x_V)} = \sum_z \cfrac{P(Y, Z, x_V)}{\sum_{y,z} P(Y=y, Z=z, x_V)}\]<p>full joint로 만들기 위해 Z를 삽입하고, P(x_V)에 대해서는 앞서 정의했으므로, x_V 가 주어졌을 때의 Y의 conditional probability는 위와 같다.</p><p> </p><h3 id="3-most-probable-assignment">3. Most Probable Assignment <a href="#3-most-probable-assignment" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-src="inference_question3.png" alt="" data-proofer-ignore></p><div class="table-wrapper"><table><tbody><tr><td>마지막으로, P(Y<td>x_V)가 최대가 될 파라미터를 구할 수 있다. ( $argmax_a P(A|B=true, M=true)$)</table></div><p>만약 P(A|B,E) 에 대한 posteriori를 구하면, prediction task에 해당하고, 반대로 P(B,E|A)에 대한 posteriori를 구하면 diagnosis task가 된다.</p><p> </p><p> </p><p>joint probability를 구하는 것이 중요한데, 이를 구하기 위해서는 너무 많은 곱셈과 덧셈이 존재한다.</p>\[P(a = true, b=true, mc=true) = \sum_{JC}\sum_{E} P(a,b,E,JC,mc) = \sum_{JC} \sum_{E} P(JC|a)P(mc|a)P(a|b,E)P(E)P(b)\]<p> </p><p><img data-src="variable_elimination.png" alt="" data-proofer-ignore></p><p>조금 더 간단하게 구할 수 있는 방법은 없을까? 이러한 확률(P)를 함수의 형태로 생각해보자. 소문자가 evidence variable, 대문자가 hidden variable에 대한 값이다.</p>\[P(e,jc,mc,B,A) = P(e)\sum_B P(b)\sum_A P(a|b,e)P(jc|a)P(mc|a) =&gt; f_E(e)\sum_B f_B(b) \sum_A f_A(a,b,e) f_J(a) f_M(a)\]<p>MC에 대한 함수 $ f_m $ 와 JC에 대한 함수 $ f_j $ 를 곱해서 $ f_{JM} $ 을 만들 수 있다.</p><p> </p><p><img data-src="variable_elimination2.png" alt="" data-proofer-ignore></p><p>그리고, $ f_A $ 와 $ f_JM $ 을 결합하고, A에 대해 marginalization을 수행하게 되면, $ f_{\bar{A}JM}(b,e) $ 가 된다. 이렇게 계속 단순화시키면, $ f_E\bar{B}\bar{A}JM(e) $ 를 만들어 낼 수 있다.</p><p> </p><p> </p><p><img data-src="potential_function.png" alt="" data-proofer-ignore></p><p>이번에는 간단한 bayesian network인 <code class="language-plaintext highlighter-rouge">A &lt;- B &lt;- C &lt;- D </code> 를 정의하고, 이에 대한 full joint를 구하면,</p><p>$ P(A,B,C,D) = P(A|B)P(B|C)P(C|D)P(D) $</p><p>가 된다. 이 때, A,B,C,D에 대한 network를 다르게 표현하고자 한다. <strong>clique</strong>와 <strong>separator</strong> 개념을 사용하여, 노드(clique)를 A,B/B,C/C,D 로 구성하고, 그 사이에 링크(separator)를 B,C 로 정의한다.</p><p>그리고나서, <em>potential function</em> 을 정의한다.</p><ul><li>potential function on nodes</ul>\[\psi(a,b), \psi(b,c), \psi(c,d)\]<ul><li>potential function on links</ul>\[\phi(b), \phi(c)\]<p> </p><p>그 후, potential function을 활용하여 P(A,B,C,D)를 정의한다. 정의하는 방법에는 2가지가 있다.</p><ol><li>$ P(A,B,C,D) = \cfrac{\prod_N \psi(N)}{\prod_L \phi(L)} = \cfrac{\psi(a,b)\psi(b,c)\psi(c,d)}{\phi(b)\phi(c)} $</ol><ul><li>ψ(a,b) = P(A|B), ψ(b,c) = P(B|C), ψ(c,d) = P(C|D)P(D)<li>φ(b) = 1, φ(c) = 1</ul><p> </p><ol><li>$ P(A,B,C,D) = \cfrac{\prod_N \psi(N)}{\prod_L \phi(L)} = \cfrac{\psi(a,b)\psi(b,c)\psi(c,d)}{\phi(b)\phi(c)} $</ol><ul><li>ψ(a,b) = P(A,B), ψ(b,c) = P(B,C), ψ(c,d) = P(C,D)<li>φ(b) = P(B), φ(c) = P(C)</ul><p> </p><p> </p><p><img data-src="absorption_in_clique_graph.png" alt="" data-proofer-ignore></p><p>위의 potential function을 clique graph를 적용하게 되면</p><ul><li>P(B) = $ \sum_A \psi(A,B) $<ul><li>A에 대해 marginalization을 수행하면 P(B)가 된다.</ul><li>P(B) = $ \sum_C \psi(B,C) $<li>P(B) = φ(B)</ul><p>이 때, 만약 A가 관찰된다면, $ \phi(B) $ 값도 바뀔 것이고, 그렇다면, 그 뒤에 있는 B,C/C/C,D 에 대한 것들도 모두 바뀌게 된다. 이를 <strong>belief propagation</strong>이라 한다.</p><p> </p><p>belief를 전파(propagation)하기 위해서는 <em>Absorption(update) rule</em> 을 적용해야 한다. update된 separator와 update된 clique를 다음과 같이 정의한다.</p><ul><li>φ^<em>(B) = $ \sum_A \psi^</em>(A,B) $<li>ψ^<em>(B,C) = $ \psi(B,C)\frac{\phi^</em>(B)}{\phi(B)} $</ul><p> </p><p>이렇게 정의가 되면, 다음과 같이 적용된다.</p><p><img data-src="local_consistency.png" alt="" data-proofer-ignore></p><p>이를 <em>local consistency</em>, 지역적 일관성이라 한다.</p><p> </p><p> </p><p>이제 실제 예제에 적용할 것인데, 우리는 conditional probability는 알고 있지만, joint probability는 모르는 상태이므로, 일전에 정의했었던 P(A,B,C,D)에서 conditional probability를 활용한 방식을 사용할 것이다.</p><p> </p><p><img data-src="example_belief_propagation.png" alt="" data-proofer-ignore></p><ol><li>P(b) 를 구해보자.</ol><ul><li>$ \phi^*(b) = \sum_a \psi(a,b) = 1 $<ul><li>absorption rule을 수행할 때, φ*(b)를 구하기 위해서 a에 대해 marginalization을 하면 된다.<li>이렇게 되면, 모든 case에 대한 확률이 되므로 1이 된다.</ul><li>$ \psi^<em>(b,c) = \psi(b,c)\frac{\phi^</em>(b)}{\phi(b)} = P(b|c)P(c) \times 1 = P(b,c) $<ul><li>앞서 ψ*에 대해 정의했으므로 그대로 정의하고, 위에서 b separator에 대한 값은 둘다 1.<li>conditional probability의 정의에 따라 joint probability로 정의된다.</ul><li>$ \phi^{**}(b) = \sum_c \psi(b,c) = \sum_c P(b,c) = P(b) $<ul><li>이번에는 B,C 에서 A,B로 진행해본다.<li>동일하게 c에 대해 marginalization을 수행하여 apsorption rule이 적용된 b를 계산한다.</ul><li>$ \psi^<em>(a,b) = \psi(a,b)\frac{\phi^{**}(b)}{\phi^</em>(b)} = \frac{P(a|b)P(b)}{1} = P(a,b) $<ul><li>ψ*에 대해 앞서 정의했으므로, 그대로 정의하여 사용한다.</ul><li>$ \phi^{<em>**}(b) = \sum_a \psi^</em>(a,b) = P(b) $<ul><li>A,B에서 B,C로 한번 더 진행해보면 동일한 값이 나오는 것을 확인할 수 있다.</ul></ul><p> </p><ol><li>P(b|a=1,c=1) 를 구해보자.</ol><p>관측이 있는 상태에서 알지 못하는 variable에 대한 확률을 구하고, 추가로 확률이 최대가 되는 값을 찾는 것이 중요하다.</p><ul><li>$ \phi^*(b) = \sum_a \psi(a,b) \delta(a=1) = P(a = 1|b) $<ul><li>동일하게 a에 대해 marginalization을 수행하지만, a=1이라는 관측이 이미 있었으므로 a=1일 때의 값만 사용한다.<li>이로 인해, 모든 case라서 나온 1이 아닌, a=1일때의 확률</ul><li>$ \psi^<em>(b,c) = \psi(b,c)\frac{\phi^</em>(b)}{\phi(b)} = P(b|c=1)P(c=1)\frac{P(a=1|b)}{1} $<ul><li>앞서 absorption rule에서 정의한 것을 사용하되, c=1이라는 관측이 있었으므로 이로 제한하여 사용한다.</ul><li>$ \phi^{**}(b) = \sum_c \psi(b,c) \delta(c=1) = ψ^*(b,c) = P(b|c=1)P(c=1)P(a=1|b) $<ul><li>B,C에서 A,B로 진행하므로 φ**는 c를 marginalization을 수행한 것과 단다. 따라서 위에 것이 그대로 내려온다. «««&lt; HEAD:_posts/Classlog/KOOC/2022-10-15-kooc_week7.md</ul><li>$ \psi^<em>(a,b) = \psi(a,b)\frac{\phi^{**}(b)}{\phi^</em>(b)} = P(a=1|b) \frac{P(b|c=1)P(c=1)P(a=1|b)}{P(a=1|b)} = P(b|c=1)P(c=1)P(a=1|b) $<ul><li>absorption rule에서 정의한 것을 그대로 사용한다.</ul><li>$ \phi^{***}(b) = \sum_a \psi^*(a,b) \delta(a=1) = P(b|c=1)P(c=1)P(a=1|b) $<ul><li>A,B에서 다시 B,C로 이동을 시켜보면 동일한 값이 나온다는 것을 알 수 있다.</ul></ul><p>이렇게 구해진 seperator로 P(b|a=1,c=1) 를 구할 수 있다.</p><p> </p><p> </p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week7/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week7.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/kooc/'>kooc</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/kooc/" class="post-tag no-text-decoration" >kooc</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/kooc_week7/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/kooc_week7/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/kooc_week7/&amp;text=[KOOC] 인공지능 및 기계학습 개론 7주차 - Bayesian Network - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/kooc_week7/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/kooc_start/"><div class="card-body"> <em class="timeago small" date="2022-09-27 00:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 - Abstract</h3><div class="text-muted small"><p> KOOC란? KOOC(KAIST Massive Open Online Course)는 KAIST 무료 온라인 강좌를 말하는 것으로, KAIST 교수님들의 강의를 무료로 수강할 수 있는 플랫폼입니다. KOOC에는 다양한 주제로 강좌가 열려 있고, 온라인으로 진행되는 강좌이므로 언제 어디서든 강의를 들을 수 있습니다. 저는 그 중, 인공지능을 전공할 때 ...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week12/"><div class="card-body"> <em class="timeago small" date="2022-09-27 01:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy</h3><div class="text-muted small"><p> Chapter 1. Motivation and Basics 목차 Motivation Machine Learning, AI, Datamining What is Machine Learning? MLE MAP Basics Probabil...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week34/"><div class="card-body"> <em class="timeago small" date="2022-09-29 01:40:00 +0900" >Sep 29, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression</h3><div class="text-muted small"><p> Chapter 3. Motivation and Basics 목차 optimal classification optimal predictor concept of Bayes risk concept of desicion boundary Naive Bayes Classifier ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/kooc_week56/" class="btn btn-outline-primary" prompt="Older"><p>[KOOC] 인공지능 및 기계학습 개론 5,6주차 - Support Vector Machine, Training, Regularization</p></a> <a href="/posts/kooc_week8/" class="btn btn-outline-primary" prompt="Newer"><p>[KOOC] 인공지능 및 기계학습 개론 8주차 - K-Means Algorithm, Gaussian Mixture Model and EM Algorithm</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>