<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[데브코스] 9주차 - DeepLearning Multi Layer Perceptron" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Do focus on developing ‘your ability’ rather than waste time on making you famous." /><meta property="og:description" content="Do focus on developing ‘your ability’ rather than waste time on making you famous." /><link rel="canonical" href="https://dkssud8150.github.io/posts/multiperceptron/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/multiperceptron/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-14T15:40:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[데브코스] 9주차 - DeepLearning Multi Layer Perceptron" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-05-17T02:56:35+09:00","datePublished":"2022-04-14T15:40:00+09:00","description":"Do focus on developing ‘your ability’ rather than waste time on making you famous.","headline":"[데브코스] 9주차 - DeepLearning Multi Layer Perceptron","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/multiperceptron/"},"url":"https://dkssud8150.github.io/posts/multiperceptron/"}</script><title>[데브코스] 9주차 - DeepLearning Multi Layer Perceptron | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[데브코스] 9주차 - DeepLearning Multi Layer Perceptron</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[데브코스] 9주차 - DeepLearning Multi Layer Perceptron</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-04-14 15:40:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Thu, Apr 14, 2022, 3:40 PM +0900" >Apr 14, 2022</em> </span> <span> Updated <em class="timeago" date="2022-05-17 02:56:35 +0900 " data-toggle="tooltip" data-placement="bottom" title="Tue, May 17, 2022, 2:56 AM +0900" >May 17, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="9232 words"> <em>51 min</em> read</span></div></div></div><div class="post-content"><p><br /></p><h1 id="신경망">신경망</h1><p>사람의 뉴런의 집합을 신경망이라 한다. 뉴런은 두뇌의 가장 작은 정보처리 단위이고, 세포체는 연산을하고, 수상돌기는 신호를 수신, 축삭은 처리 결과를 전송한다. 이러한 사람의 뉴런을 따럿 컴퓨터에 인공지능으로 구성하여 퍼셉트론, 인공신경망을 만들었다. 세포체, 수상돌기, 축삭, 시냅스가 인공신경망에서 각각 노드, 입력, 출력, 가중치에 해당한다.</p><p><br /></p><p>인공 신경망에서는 다양한 분류 방식에 따라 많은 종류가 있다. 신호 처리 방식에 따라 전방 신경망/순환 신경망로 나뉠 수 있고, 신경망의 깊이에 따라 얕은 신경망/깊은 신경망로 나뉠 수 있다. 또는 결정론적 신경망과 확률론적 신경망으로도 나뉠 수 있다.</p><ul><li><p>결정론 신경망 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망을 말한다. 즉, 동일한 입력에서는 반드시 동일한 출력이 나오는 신경망이다.</p><li><p>확률론 신경망 고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력을 가지는 신경망이다.</p></ul><p><br /></p><h1 id="퍼셉트론">퍼셉트론</h1><p><img data-src="/assets/img/dev/week9/day4/perceptron.png" data-proofer-ignore></p><p>node, weight, layer 과 같은 새로운 개념의 구조를 도입한 모델이다. 처음으로 <code class="language-plaintext highlighter-rouge">학습</code>이라는 알고리즘을 제안했다. 현재의 딥러닝 모델은 퍼셉트론을 깊게 만들어서 결과를 도출하는 것이므로 중요한 기반이 되는 모델이다.</p><h2 id="퍼셉트론의-구조">퍼셉트론의 구조 <a href="#퍼셉트론의-구조" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ol><li><p>입력층 퍼센트론을 포함한 모든 모델은 d차원의 입력 벡터를 받는다. i번째 노드는 특징 벡터 $ x = (x_1, x_2, \cdots , x_d)^T $ 의 요소 x_i를 담당한다. 항상 1이 입력되는 평향(bias) 노드도 있다. 이 bias을 통해 임계점이 0으로 옮겨진다.</p><li><p>연산(입력과 출력 사이) i번째 입력 노드와 출력 노드를 연결할 때 가중치 w_i를 가진다. 퍼셉트론은 단일 층 구조로 간주한다.</p><li><p>출력층 계단함수를 사용했으므로 한 개의 노드에 의해 수치(+1 or -1)을 출력한다.</p></ol><h2 id="퍼셉트론의-동작">퍼셉트론의 동작 <a href="#퍼셉트론의-동작" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>선형 연산과 비선형 연산이 있다. 선형 연산에는 입력값과 가중치가 곱해지고 모두 더하는 연산이고, 비선형 연산에는 활성함수(계단함수)가 이에 해당된다.</p><p>$ s = w^Tx + w_0 $</p><p>편향항은 b또는 w_0라고 표기하는데, 이를 w안의 벡터로 추가하게 되면, $ x = (1,x_1,x_2, \cdots, x_d)^T, w = (w_0,w_1,w_2,\cdots,w_d)^T $ 가 된다. 따라서 이를 간단하게 만들면 다음과 같은 식이 만들어진다.</p><p>$ y = \tau(w^Tx) $</p><p><br /></p><h3 id="or-논리-게이트">OR 논리 게이트 <a href="#or-논리-게이트" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>OR 논리 게이트란 입력값이 (0,0)만 -1, 나머지인 (1,0),(0,1),(1,1) 은 모두 1로 출력되는 퍼셉트론을 말한다. 그렇다면 AND 논리 게이트는 (1,1)은 1이고, 이외에는 다 -1이 출력될 것이다.</p><p><img data-src="/assets/img/dev/week9/day4/orgate.png" data-proofer-ignore> <img data-src="/assets/img/dev/week9/day4/andgate.png" data-proofer-ignore></p><p>첫번째 그림은 OR, 두번째 그림은 AND 논리 게이트에 대한 좌표 그림이다.</p><p>결정 직선 d(x)는 $ d(x) = d(x_1,x_2) = w_1x_1 + w_2x_2 + w_0 = 0 $ 이다. 이 때, w_1,w_2는 직선의 기울기, w_0는 절편(편향)을 결정한다. 결정 직선이란 특징 공간을 두 부분 공간으로 이분할 하는 분류기 역할을 하는 직선이다. 이 때는 결정 직선이 (0,0)이외에는 다 1이 나오기 때문에 (0,0)과 나머지를 구분해야 한다.</p><p>식에서 0이 나오는 이유는 원래는 $ w_1x_1 + w_2x_2 = -w_0 $ 와 같이 w_0가 임계값에 대한 값인데, 이를 좌항으로 옮긴 것이다. -w_0인 이유는 임계값을 0으로 좌표를 옮기기 위해서이다.</p><p>이를 d차원 공간으로 일반화하면 $ d(x) = w_1x_1 + w_2x_2 + \cdots + w_dx_d + w_0 = 0 $</p><p><br /></p><p>AND 논리 게이트를 코드화하면 다음과 같다. 하지만 이는 벡터가 아닌 상수값으로 만들어진 것이다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">AND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x1</span><span class="o">*</span><span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span><span class="o">*</span><span class="n">w2</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">tmp</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    
<span class="nf">print</span><span class="p">(</span><span class="nc">AND</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nc">AND</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nc">AND</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nc">AND</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></table></code></div></div><p>theta는 편향값을 말한다.</p><p>상수가 아닌 벡터로 표현하게 되면 다음과 같다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># 함수로 표현
</span><span class="k">def</span> <span class="nf">AND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">):</span>                 <span class="c1"># 모두 1일때만 1, 나머지는 0
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.7</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">NAND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>               <span class="c1"># 모두 1일때만 0, 나머지는 1
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span> <span class="c1"># and와 부호만 반대
</span>    <span class="n">b</span> <span class="o">=</span> <span class="mf">0.7</span>                    <span class="c1"># and와 부호만 반대
</span>    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">OR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>                 <span class="c1"># 1이 존재하면 1, 나머지는 0
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>   <span class="c1"># and와 가중치만 다르다.
</span>    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.2</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</pre></table></code></div></div><p><br /></p><p><br /></p><h2 id="퍼셉트론의-학습">퍼셉트론의 학습 <a href="#퍼셉트론의-학습" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="목적함수">목적함수 <a href="#목적함수" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>목적함수 상세 설계</ul><p>$ J(w) = \sum_{x_k in Y} -y_k(w_Tx_k) $</p><p>이 때, Y는 w가 틀리는 샘플의 집합, 즉 오판단하는 샘플의 집합을 말한다. 이 식이 퍼셉트론의 목적함수로 적합한지를 판단해보면</p><ul><li>임의의 샘플 x_k 가 Y에 속한다면 퍼셉트론의 예측값 $w^Tx_k$와 실제값 y_k는 부호가 달라야 한다. 즉 정답이 1인데, 예측값이 -1이거나, 답이 -1인데 예측값이 1이므로 실제값과 예측값은 항상 부호가 다르다.<li>Y가 크다는 것은 틀리는 개수가 크다는 것이고, Y가 커질수록 J(w)가 커진다.<li>Y가 공집합, 즉 틀린 것이 없다면 J(w) = 0 이다</ul><p>따라서 목적함수로 사용이 가능하다.</p><p><br /></p><h3 id="경사하강법">경사하강법 <a href="#경사하강법" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>J(w)의 기울기를 이용하여 최소값을 찾는다. 이를 위해 가중치를 갱신할 때는 $ \theta = \theta - \rho g $를 사용하는데, 이 때 $\theta$는 w와 같다. 경사도 g를 얻기 위해 w_i에 대해 편미분을 한다.</p><p>$ \frac{\partial J(w)}{\partial w_i} = \sum_{x_k \in Y} \frac{\partial(-y_k(w_0x_{k0} + w_1x_{k1} + \cdots + w_ix_{ki} + w_dx_{kd}))}{\partial w_i} = \sum_{x_k \in Y} -y_kx_{ki} $</p><p>w_i는 w벡터의 i번째 요소이므로 가중치 w는 $ w = {w_0,w_1,\cdots,w_i,\cdots,w_d} $ 이다. 그렇기 때문에</p><p>가중치 갱신 식은 $ w_{i+1} = w_i + \rho\sum_{x_k \in Y} y_kx_{ki} $ 가 된다. 이를 <code class="language-plaintext highlighter-rouge">델타 규칙</code>이라 부른다. 델타 규칙은 퍼셉트론의 학습 방법에서만 해당된다. 여기서 $ \rho $는 learning rate(학습률)이다. 이 learning rate는 학습을 할 때 가장 중요한 파라미터이다. 이 값이 너무 크면 최저점을 넘어서서 계속 진동이 되기도 하고, 너무 작으면 지역 최저점에 갇혀버리거나 너무 느리게 학습이 될 수도 있기 때문이다.</p><blockquote><p>결정 직선과 가중치 w_k는 서로 직각을 이룬다.</p></blockquote><p><br /></p><p>퍼셉트론의 한계가 존재한다. 데이터가 직선을 통해 이분화가 된다면 퍼셉트론을 사용하면 되지만, 거의 모든 데이터는 직선으로 분류하기 어렵다. 이를 해결하기 위해 다층 퍼셉트론을 만들게 된다.</p><p><br /></p><p><br /></p><h1 id="다층-퍼셉트론">다층 퍼셉트론</h1><p>XOR 논리 게이트가 다층 퍼셉트론에 해당된다. 즉, 하나가 1이고, 하나가 0인 상태만 1을 출력한다.</p><div class="table-wrapper"><table><tbody><tr><td>x1<td>x2<td>output<tr><td>0<td>0<td>0<tr><td>1<td>0<td>1<tr><td>0<td>1<td>1<tr><td>1<td>1<td>0</table></div><p><img data-src="/assets/img/dev/week9/day4/xorgate.png" data-proofer-ignore></p><p>이 상황에서는 선형 분류기로는 한계가 존재한다.</p><p>다층 퍼셉트론의 특징</p><ul><li><strong>은닉층</strong>을 둔다. 즉 입력층, 출력층 이외에 중간에 층이 하나 더 존재한다.<li><strong>시그모이드 활성함수</strong>를 사용한다. 기존의 퍼셉트론은 계단함수로 활성함수를 만들었지만, 이 함수는 적절하지 않다. 그래서 다층 퍼셉트론에서는 시그모이드함수를 활성함수로 사용한다. 출력을 신뢰도로 간주함으로서 더 융통성 있게 의사결정이 가능해졌다.<li><strong>오류 역전파 알고리즘</strong>을 사용한다. 여러 층이 순차적으로 이어져 있기에 역방향을 진행하면서 한층씩 그레디언트를 계산하고 가중치를 갱신한다.</ul><p><br /></p><p>원래의 퍼셉트론을 2개를 병렬 결합하면 원래 공간 $ x = (x_1,x_2)^T $ 를 새로운 특징 공간 $ z = (z_1, z_2)^T $ 로 변환이 된다. 즉, 새로운 특징 공간 z에서 선형 분리가 가능해진다.</p><p><img data-src="/assets/img/dev/week9/day4/mlp.png" data-proofer-ignore></p><p><br /></p><h2 id="다층-퍼셉트론의-용량">다층 퍼셉트론의 용량 <a href="#다층-퍼셉트론의-용량" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>특징 공간 x가 2개의 벡터를 가진다면, x1,x2의 2차원 공간에서 w1,w2를 통해 직선의 방정식을 그릴 수 있다. 이를 새로운 영역 z에 점으로 투영한다면 3차원의 점으로 변환될 것이고, 층을 지날수록 더 높은 차원으로 변환이 될 수 있다.</p><p>그렇다면, 3개의 퍼셉트론을 결합한 경우 2차원 공간을 n개 영역으로 나누고, 각 영역을 3차원 점으로 변환하고, 계단함수를 활성함수로 가정한다면 3차원에 점으로 변환된다. 따라서 p개의 퍼셉트론을 결합하면 p차원 공간으로 변환되는 것과 같다.</p><p><br /></p><p>하나의 은닉층은 특징 공간을 다른 특징 공간으로 매핑하는 것이므로 함수의 근사표현으로 생각해볼 수 있고, 이 은닉층을 여러 개 쓴다면 내가 원하는 공간을 변환하는 근사 함수라고 표현할 수 있다.</p><h2 id="활성함수">활성함수 <a href="#활성함수" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>활성함수가 원래는 계단함수였으나 좀 더 부드러운 공간 분할을 위해 시그모이드 함수로 바꾸었다. 그렇게 되면 원래의 계단함수는 영역을 점으로 변환하지만, 시그모이드와 같은 함수들은 영역을 영역으로 변환한다.</p><p><img data-src="/assets/img/dev/week9/day4/sigmoid.png" data-proofer-ignore></p><p><br /></p><p>활성함수로 많이 사용되는 함수들은 다음과 같다.</p><p><img data-src="/assets/img/dev/week9/day4/function.png" data-proofer-ignore></p><p>시그모이드나 tanh는 a가 커질수록 계단함수에 가까워진다. 다층 퍼셉트론에서는 sigmoid나 tanh를 많이 사용했고, 딥러닝에서는 마지막에 ReLU를 가장 많이 사용한다. 딥러닝에서 sigmoid나 tanh를 사용하지 않는 이유는 그래프의 모양과 같이 0이나 1에 가까운 값을 내는 값들에 대해서는 gradient를 없앤다.(= vanishing gradient)</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-09-22/0018.jpg" data-proofer-ignore></p><p>예를 들어 데이터 x와 출력이 존재할 때 sigmoid를 사용한 모델에서 backpropagation를 위해 gradient를 계산한다면, 일단 (dL/dσ)가 있을 것이고, sigmoid gate를 지나 local sigmoid function의 gradient인 (dL/dx)를 구하기 위해 chain rule를 적용한다.</p><p>따라서 $ \frac{\partial L}{\partial x} = \frac{\partial σ}{\partial x} * \frac{\partial L}{\partial σ} $ 가 된다.</p><p><br /></p><p>x = 0 일 때는 backprop가 잘 진행될 것이다. 그러나 x = -10 일 때의 gradient($\frac{\partial L}{\partial x} $)는 그림에서 보듯이 <code class="language-plaintext highlighter-rouge">0</code>이다. 그렇게 되면, 0이 backpropagation 될 것이고, 그로 인해 뒤에 전달되는 모든 gradient는 모두 죽어(=0)버린다. x = 10 일 때도 마찬가지로 모든 gradient가 0 이 된다.</p><p>따라서 <strong>x가 아주 크거나 아주 작다면 gradient가 계속 0으로 죽어버린다.</strong> 이를 gradient vanish 또는 vanishing gradient라 한다.</p><blockquote><p><a href="https://dkssud8150.github.io/posts/cs231n6/">참고 자료</a></p></blockquote><p>사실 실제 뉴런의 출력도 sigmoid보다 ReLU의 형태가 더 가깝다. 따라서 ReLU를 많이 사용한다.</p><p><br /></p><h2 id="매개변수">매개변수 <a href="#매개변수" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>은닉 층이 1개인 퍼셉트론을 2층 퍼셉트론, 은닉층이 2개인 퍼셉트론을 3층 퍼셉트론이라 한다. 은닉층이 4개 이상인 퍼셉트론을 깊은 신경망이라 한다. 은닉층의 개수가 너무 많으면 과잉적합(overfitting), 너무 적으면 과소적합(underfitting)이 발생한다.</p><p>은닉층이 기하학적으로 봤을 때는 새로운 특징 공간으로 변환해주는 것과 같지만, 의미론적으로 바라봤을 때 이는 입력 벡터에 대해 내가 원하는 부분을 뽑아내겠다는 특징 추출기라 할 수 있다. 입력이 들어오면 내가 만들어놓은 가중치, 즉 벡터 값들과 곱해서 원하는 결과를 얻어내는 것이다. 예를 들어, 내가 이미지를 모델에 넣었을 때 학습된 가중치들과 곱해져서 컴퓨터가 해당 이미지의 특징들이 추상화된 형태가 될 것이다.</p><p>현대 기계학습에서는 이를 <code class="language-plaintext highlighter-rouge">특징학습</code>이라 부른다.</p><p><br /></p><p><br /></p><h2 id="오류-역전파-알고리즘">오류 역전파 알고리즘 <a href="#오류-역전파-알고리즘" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="손실함수">손실함수 <a href="#손실함수" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>가장 일반적인 손실함수는 MSE(Mean Squared Error)이 있다. L2 norm을 사용해서 다음과 같이 정의된다.</p><div class="table-wrapper"><table><tbody><tr><td>$ e = \frac{1}{2n}\sum_{i=1}^n<td> <td>y_i - o_i<td> <td>_2^2 $</table></div><p>여기서 o는 출력 벡터이고, y는 실제값인데, 이를 원핫 인코딩 형태로 만들어 <code class="language-plaintext highlighter-rouge">부류 벡터</code>라는 벡터를 만든다.. 즉, 0과 1로만 구성되어 있는데, 기댓값 즉 내가 원하는 값인지 아닌지에 대한 값을 나타내는 것으로 이를 통해 error를 구한다.</p><p><br /></p><h3 id="연산-그래프-computational-graph">연산 그래프 (computational graph) <a href="#연산-그래프-computational-graph" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>연산을 그래프로 표현한 것을 연산 그래프라 한다.</p><p><img data-src="https://media.vlpt.us/images/guide333/post/c2e92214-64a1-44ca-b7ec-b303f6866e9f/Screenshot%20from%202021-01-11%2023-58-02.png" data-proofer-ignore></p><p><br /></p><p>연산 그래프를 진행할 때, 그래디언트를 계산하기 위해서는 연쇄 법칙을 사용한다. 예를 들어, k(x) = f(g(h(i(x)))) 일 때, k’(x)를 구하기 위해서는 다음과 같은 식이 될 것이다.</p>\[i\prime(x) = f\prime(g(h(i(x)))) * g\prime(h(i(x))) * h\prime(i(x)) * i\prime(x)\]<p><br /></p><p>그렇다면 우리가 전방 계산을 할 때마다 prime을 구해놓으면 그것을 통해 역전파를 하면 된다.</p><p>동일한 f(활성함수)를 연속으로 사용하고, 입력이 w -&gt; x -&gt; y -&gt; z 일 때의 그래디언트를 구하면</p><p>$ \frac{\partial z}{\partial w} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \frac{\partial x}{\partial w} $ $ = f\prime(y)f\prime(x)f\prime(w) $ $ = f\prime(f(f(w)))f\prime(f(w))f\prime(w) $</p><p><img data-src="/assets/img/dev/week9/day4/chainrule.png" data-proofer-ignore></p><p><br /></p><p>우리가 실제로 궁금한 미분은 가중치에 대한 예측값의 그래디언트이다. 2층 퍼셉트론에서의 매개 변수 $ \theta = {U^1, U^2} $ 가 있다고 하면, 손실 함수 J($\theta$) 는 다음과 같다.</p><div class="table-wrapper"><table><tbody><tr><td>$ J(\theta) = \frac{1}{2}<td> <td>y - o(\theta)<td> <td>_2^2 $</table></div><p>이 때, y는 부류 벡터, o는 예측값이다. $J({U^1, U^2}) $의 최저점을 찾기위해 이를 미분하면</p><p>$ U^1 = U^1 - \rho \frac{\partial J}{\partial U^1} $ $ U^2 = U^2 - \rho \frac{\partial J}{\partial U^2} $</p><p>연산의 순서는 x -&gt; U^1 -&gt; U^2 -&gt; o 이다.</p><p><br /></p><p><img data-src="https://media.vlpt.us/images/lilpark/post/999cfcfb-b53f-40f4-b872-42f42da67a39/image.png" data-proofer-ignore></p><p>다시 한 번 살펴보자면, 우리가 궁금한 것은 가중치 x,y 에 대한 결과값 L의 그래디언트이다. 여기서 upstream은 뒤로 올라가는 방향의 gradient, downstream은 전방으로 전달되는 방향으로의 gradient이다.</p><p>$ \frac{\partial L}{\partial x} $ 은 chain rule에 의해 분해가 되어 $ \frac{\partial L}{\partial z} \frac{\partial z}{\partial x} $ 로 변환될 수 있다. z도 가중치이므로 x 와 z를 연산하여 출력이 되고, 그 출력이 L이 된다. y도 동일하다.</p><p>forward, backprop 두 가지를 따로 바라보게 되면</p><ul><li>forward</ul><p>입력값 <code class="language-plaintext highlighter-rouge">in</code>, 이 활성함수 <code class="language-plaintext highlighter-rouge">f</code>를 거쳐 <code class="language-plaintext highlighter-rouge">out</code>이 된다.</p><p><br /></p><ul><li>backward 이에 대해 미분을 해보면</ul><p>$ \frac{\partial \epsilon}{\partial in} = \frac{\partial \epsilon}{\partial out} \cdot \frac{\partial out}{\partial in} = \frac{\partial \epsilon}{\partial out} \cdot f\prime(in)$</p><p>이 때, $\epsilon$ 은 error이고, $ \frac{\partial \epsilon}{\partial out} $ 은 output gradient이다. $ \frac{\partial out}{\partial in} $ 은 local gradient이다.</p><p>그래서 입력에 대한 에러의 gradient는 output gradient(출력에 대한 에러의 gradient) * local gradient(입력에 대한 출력의 gradient) 이 된다. local gradient는 out = f(in) 이므로 in에 대한 out의 미분은 $f\prime(in)$ 이 된다.</p><p><br /></p><h4 id="곱셈의-역전파">곱셈의 역전파 <a href="#곱셈의-역전파" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p>forward의 식은 다음과 같다.</p><p>$ out = in_1 \cdot in_2 $</p><p><br /></p><p>이에 대해 역전파를 진행해보면</p><p>$ \frac{\partial \epsilon}{\partial in} = \frac{\partial \epsilon}{\partial out} \cdot \frac{\partial out}{\partial in} = \frac{\partial \epsilon}{\partial out} \cdot in_2$</p><p>in_2가 되는 이유는 곱셈의 연산을 생각해보면 <code class="language-plaintext highlighter-rouge">X * Y</code>에서 X에 대해 미분을 하면 <code class="language-plaintext highlighter-rouge">Y</code>, Y에 대해 미분하면 <code class="language-plaintext highlighter-rouge">X</code>이다. 따라서 in_1에 대해 미분을 하면 in_2가 남는다.</p><p><br /></p><p>이 연산을 코드로 구현하면 다음과 같다. 이는 pytorch에서 이미 구현되어 있다. 각 연산마다의 grad_z 즉 output gradient를 저장해놓고 이를 backprop에 사용한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Multiply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">z</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_x</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">grad_z</span> <span class="c1"># dz/dx * dL/dz
</span>        <span class="n">grad_y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">grad_z</span> <span class="c1"># dz/dy * dL/dz
</span>        <span class="k">return</span> <span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_y</span>
</pre></table></code></div></div><p><br /></p><h4 id="덧셈의-역전파">덧셈의 역전파 <a href="#덧셈의-역전파" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li>forward</ul><p>$ out = \sum_i in_i $</p><ul><li>backpropagation</ul><p>$ \frac{\partial \epsilon}{\partial in_i} = \frac{\partial \epsilon}{\partial out} \cdot 1 = \frac{\partial \epsilon}{\partial out}$</p><p>덧셈의 경우 <code class="language-plaintext highlighter-rouge">X+Y</code>를 X, Y 각각에 대해 미분하면 모두 1이 나온다.</p><p><br /></p><h4 id="s형-활성함수의-역전파">S형 활성함수의 역전파 <a href="#s형-활성함수의-역전파" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p>$ \frac{\partial \epsilon}{\partial in} = \frac{\partial \epsilon}{\partial out} \cdot \sigma \prime(in) = \frac{\partial \epsilon}{\partial out} \cdot [\sigma(in) (1 - \sigma(in))] $</p><p>시그모이드 함수의 형태를 생각해보면 다음과 같다.</p><p><img data-src="/assets/img/dev/week9/day4/sig.jpg" data-proofer-ignore></p><p>이를 미분하면 $ [\sigma(in) (1 - \sigma(in))] $ 이 만들어진다.</p><p><br /></p><h4 id="최대화-역전파">최대화 역전파 <a href="#최대화-역전파" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p>$ out = max_i{in_i} $</p><p>max{0,x} 일 경우, 0보다 작으면 0이 되고, 0보다 크면 output gradient를 전달해준다.</p><p><br /></p><p>ReLU가 이에 해당되는데, ReLU와 같은 형태는 미분을 하면 임계값까지는 0이다가, 임계값 이후에는 1이 나온다. 그래서 local gradient는 1 또는 0이 된다.</p><p><br /></p><h4 id="전개fanout-역전파">전개(fanout) 역전파 <a href="#전개fanout-역전파" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p><img data-src="/assets/img/dev/week9/day4/fanout.jpg" data-proofer-ignore></p><p>fanout이란 두 가지의 출력이 존재하고, 이 둘을 합쳐서 또 다른 출력을 생성한다.</p><p>$ x = x(t), y = y(t) =&gt; z = f(x,y) $</p><p>이 때는 역전파를 하기 위해서는 x,y 방향 각각을 gradient를 구해서 더하면 된다.</p><p>$ \frac{\partial z}{\partial t} = \frac{\partial z}{\partial x} \cdot \frac{\partial x}{\partial t} + \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial t} $</p><p><br /></p><p>위는 두 가지의 출력이지만, 이를 일반화 시키게 되면</p><p>$ z = f(n_1,n_2, \cdots , n_k, \cdots) , and , n_k = n_k(in)$</p><p>의 형태라면 이를 미분하게 되면 다음과 같다.</p><p>$ \frac{\partial z}{\partial in} = \sum_k \frac{\partial z}{\partial n_k} \cdot \frac{\partial n_k}{\partial in} = \sum_{k} \frac{\partial z}{\partial in_k} $</p><p><br /></p><h4 id="신경망-역전파">신경망 역전파 <a href="#신경망-역전파" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p><img data-src="/assets/img/dev/week9/day4/backprop.jpg" data-proofer-ignore></p><p>위의 덧셈, max, 곱셈에 대한 역전파를 모두 사용한다. 여기서 gradient vanish가 발생하는 이유는 중간에 시그모이드 함수가 있을 때, 값이 너무 크거나 너무 작으면 gradient가 0으로 나오고, 그 뒤로는 계속 0이 나오게 된다.</p><p><br /></p><p><img data-src="https://media.vlpt.us/images/lilpark/post/fc4c568a-d8d0-4e3c-8b29-8d851742f4b5/image.png" data-proofer-ignore></p><p><br /></p><p><br /></p><p>도함수의 종류를 바라보면 3가지가 있다</p><ol><li>scalar to scalar</ol><p>스칼라에 대한 스칼라는 상수가 나온다.</p><ol><li>vector to scalar</ol><p>출력값은 하나(L)지만, 입력값이 $ x = (x_1,x_2,x_3, …)^T $ 이라면 이에 대한 미분을 <strong>gradient</strong>라 한다. vector 각각이 영향을 주고, 그에 대해 L이 달라지므로 1~N개의 미분이 다 나올 것이고, 이는 벡터 형태이다.</p><p>이 때 표기를 $ \nabla_x z $ 라 한다. 즉, 스칼라 z에 대한 벡터 x의 미분 벡터이다.</p><ol><li>vector to vector</ol><p>출력도 벡터, 입력도 벡터라면 y1에 대한 모든 요소 x1,x2,x3..이 있고, y2도 x1,x2,x3 에 대한 미분이 다 있을 것이다. 그렇기에 출력되는 값은 행렬로 표현된다. 이 행렬을 <strong>Jacobian</strong>이라 한다.</p>\[\frac{\partial y}{\partial x} = [\frac{\partial y_1}{\partial x_1} \cdots \frac{\partial y_1}{\partial x_m} ] = [\frac{\partial y_1}{\partial x_1} \cdots \frac{\partial y_1}{\partial x_m} \vdots \ddots \vdots \frac{\partial y_n}{\partial x_1} \cdots \frac{\partial y_n}{\partial x_m}]\]<p><br /></p><p>만약 퍼셉트론이 2층이 있어서 x -&gt; y -&gt; z 인데, x가 vector, y도 vector, z 가 scalar이면 $ \frac{\partial y}{\partial x} $ 는 야코비안, $ \frac{\partial z}{\partial y} $는 gradient 형태이다.</p><p><br /></p><p>위의 vector는 다 2차원이지만, 실제 딥러닝에 사용되는 차원은 3차원이다. 미니 배치를 사용해서 3차원을 만든 형태를 집어넣는데, 이 때문에 연산량이 배치 단위에 따라 달라지는 것이다.</p><p><br /></p><p><br /></p><h1 id="mlp-코드-구현">MLP 코드 구현</h1><h3 id="pytorch">Pytorch <a href="#pytorch" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><p>autograd pytorch의 autograd라는 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공한다.</p><li><p>tensor</p><ul><li>torch.tensor 클래스에는 required_grad 속성이 있는데, 이를 true로 설정하면 해당 텐서에서 이루어진 모든 연산을 추적한다.<li>계산이 완료된 후 backward()를 호출하면 모든 그래디언트를 자동으로 계산하며 이 그래디언트는 .grad 속성에 누적된다.<li>tensor가 기록 추적하는 것을 멈추게 하려면 해당 줄에 .detach()를 사용하거나 .with torch_no_grad()를 사용한다. 이를 사용하는 이유는 모델을 추론할 때 사용하거나 numpy로 변환할 때는 grad를 추적하지 않아야 하기 때문이다.<li>각 tensor는 .grad_fn 속성을 가지고 있는데, 이는 tensor를 생성한 Function을 참조한다. 그러나 사용자가 만든 tensor는 예외고, 사용자가 만들지 않은 tensor에서의 연산으로 생긴 텐서는 모두 Function을 참조한다.<li>도함수를 계산할 때는 .backward를 호출한다.</ul></ul><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="c1"># -----------------------------#
</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="bp">None</span>
</pre></table></code></div></div><p>사용자가 직접 선언해준 tensor이므로 None으로 출력된다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>

<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="c1"># -----------------------------#
</span>
<span class="o">&lt;</span><span class="n">AddBackward0</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7ff8f2d294d0</span><span class="o">&gt;</span>
</pre></table></code></div></div><p>선언해주지 않았지만, tensor의 연산에 의해 만들어진 y는 True, 즉 requires_grad가 True로 설정된다.</p><p>requires_grad를 True로 설정했기 때문에 x의 grad_fn은 True로 출력된다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">x</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="c1"># -----------------------------#
</span>
<span class="bp">True</span>
<span class="bp">False</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">requires_grad_</code> 메서드를 통해 requires_grad를 변경시켜줄 수 있다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre><span class="n">x</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># 위에서 False로 설정했으므로 다시 True로 변경
</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">**</span><span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

<span class="n">y</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">z</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span> <span class="c1"># 이것이 leaf 노드인지 확인 -&gt; 즉 가장 끝단의 노드인지 확인
</span>
<span class="n">out</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># -----------------------------#
</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">6.7500</span><span class="p">,</span> <span class="mf">6.7500</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">6.7500</span><span class="p">,</span> <span class="mf">6.7500</span><span class="p">]])</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">6.7500</span><span class="p">,</span> <span class="mf">6.7500</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">6.7500</span><span class="p">,</span> <span class="mf">6.7500</span><span class="p">]])</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">0.2500</span><span class="p">,</span> <span class="mf">0.2500</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2500</span><span class="p">,</span> <span class="mf">0.2500</span><span class="p">]])</span>
<span class="bp">True</span>

<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">Trying</span> <span class="n">to</span> <span class="n">backward</span> <span class="n">through</span> <span class="n">the</span> <span class="n">graph</span> <span class="n">a</span> <span class="n">second</span> <span class="nf">time </span><span class="p">(</span><span class="ow">or</span> <span class="n">directly</span> <span class="n">access</span> <span class="n">saved</span> <span class="n">tensors</span> <span class="n">after</span> <span class="n">they</span> <span class="n">have</span> <span class="n">already</span> <span class="n">been</span> <span class="n">freed</span><span class="p">).</span> <span class="n">Saved</span> <span class="n">intermediate</span> <span class="n">values</span> <span class="n">of</span> <span class="n">the</span> <span class="n">graph</span> <span class="n">are</span> <span class="n">freed</span> <span class="n">when</span> <span class="n">you</span> <span class="n">call</span> <span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="ow">or</span> <span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">().</span> <span class="n">Specify</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span> <span class="k">if</span> <span class="n">you</span> <span class="n">need</span> <span class="n">to</span> <span class="n">backward</span> <span class="n">through</span> <span class="n">the</span> <span class="n">graph</span> <span class="n">a</span> <span class="n">second</span> <span class="n">time</span> <span class="ow">or</span> <span class="k">if</span> <span class="n">you</span> <span class="n">need</span> <span class="n">to</span> <span class="n">access</span> <span class="n">saved</span> <span class="n">tensors</span> <span class="n">after</span> <span class="n">calling</span> <span class="n">backward</span><span class="p">.</span>
</pre></table></code></div></div><p>중요한 것은 backward()를 여러 번 하려면 retain_graph = True로 설정해줘야 한다. 그렇지 않으면 에러가 난다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">**</span><span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

<span class="n">y</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">out</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> 

<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> 

<span class="c1"># -----------------------------#
</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">13.5000</span><span class="p">,</span> <span class="mf">13.5000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">13.5000</span><span class="p">,</span> <span class="mf">13.5000</span><span class="p">]])</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">13.5000</span><span class="p">,</span> <span class="mf">13.5000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">13.5000</span><span class="p">,</span> <span class="mf">13.5000</span><span class="p">]])</span>
<span class="bp">None</span>
</pre></table></code></div></div><p>z.retain_grad() 를 호출하지 않으면 grad를 저장하지 않으므로 grad가 없다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="c1"># 아무것도 지정하지 않으면 default로 1이 들어가서 값이 추출된다.
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># 미분 후에 결과값
</span>
<span class="c1"># -----------------------------#
</span>
<span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.5778</span><span class="p">,</span>  <span class="mf">1.1385</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4793</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">tensor</span><span class="p">([</span><span class="mf">2.0000e-01</span><span class="p">,</span> <span class="mf">2.0000e+00</span><span class="p">,</span> <span class="mf">2.0000e-04</span><span class="p">])</span>
</pre></table></code></div></div><p><br /></p><h4 id="신경망-구현하기">신경망 구현하기 <a href="#신경망-구현하기" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="c1"># batch normalization
</span>    <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>            <span class="c1"># activation function
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="nf">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="nf">l2</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">bn3</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="nf">l3</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">l4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">l5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

<span class="n">net</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>         <span class="c1"># 저장되어 있는 grad를 다 지워야 한다.
</span><span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">l5</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># zero grad 했으므로 none
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">l5</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span> <span class="c1"># leaf노드란 아래 자식 노드가 없는 노드
</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">l5</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>     

<span class="c1"># ----------------------------- #
</span>
<span class="mf">1.0462257862091064</span>
<span class="bp">None</span>
<span class="bp">True</span>
<span class="nf">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1753</span><span class="p">,</span>  <span class="mf">0.1098</span><span class="p">,</span>  <span class="mf">0.0655</span><span class="p">])</span>
</pre></table></code></div></div><p>역전파를 진행해주었기 때문에 grad가 기록되어 있다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">params</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="c1"># 각 층마다 weight 와 bias
</span><span class="nf">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">size</span><span class="p">())</span> <span class="c1">#  0번째 층의 weight 와 bias
</span> 
<span class="c1"># ----------------------------- #
</span>
<span class="mi">16</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></table></code></div></div><p>각 층마다의 weight와 bias를 저장해놓고 있으며, 이를 호출하면 모든 층의 weight와 bias를 리턴할 수 있다. 각 벡터마다 측의 weight를 저장하고 있다.</p><p><br /></p><h5 id="iris-데이터를-불러와서-mlp-모델에-학습시키기">iris 데이터를 불러와서 MLP 모델에 학습시키기 <a href="#iris-데이터를-불러와서-mlp-모델에-학습시키기" class="anchor"><i class="fas fa-hashtag"></i></a></h5></h5><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="n">pasdas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_iris</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">data</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">target</span>

<span class="nf">print</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="p">..</span> <span class="n">_iris_dataset</span><span class="p">:</span>

<span class="n">Iris</span> <span class="n">plants</span> <span class="n">dataset</span>
<span class="o">--------------------</span>

<span class="o">**</span><span class="n">Data</span> <span class="n">Set</span> <span class="n">Characteristics</span><span class="p">:</span><span class="o">**</span>

    <span class="p">:</span><span class="n">Number</span> <span class="n">of</span> <span class="n">Instances</span><span class="p">:</span> <span class="mi">150</span> <span class="p">(</span><span class="mi">50</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">of</span> <span class="n">three</span> <span class="n">classes</span><span class="p">)</span>
    <span class="p">:</span><span class="n">Number</span> <span class="n">of</span> <span class="n">Attributes</span><span class="p">:</span> <span class="mi">4</span> <span class="n">numeric</span><span class="p">,</span> <span class="n">predictive</span> <span class="n">attributes</span> <span class="ow">and</span> <span class="n">the</span> <span class="k">class</span>
    <span class="err">:</span><span class="nc">Attribute</span> <span class="n">Information</span><span class="p">:</span>
        <span class="o">-</span> <span class="n">sepal</span> <span class="n">length</span> <span class="ow">in</span> <span class="n">cm</span>
        <span class="o">-</span> <span class="n">sepal</span> <span class="n">width</span> <span class="ow">in</span> <span class="n">cm</span>
        <span class="o">-</span> <span class="n">petal</span> <span class="n">length</span> <span class="ow">in</span> <span class="n">cm</span>
        <span class="o">-</span> <span class="n">petal</span> <span class="n">width</span> <span class="ow">in</span> <span class="n">cm</span>
        <span class="o">-</span> <span class="n">class</span><span class="p">:</span>
                <span class="o">-</span> <span class="n">Iris</span><span class="o">-</span><span class="n">Setosa</span>
                <span class="o">-</span> <span class="n">Iris</span><span class="o">-</span><span class="n">Versicolour</span>
                <span class="o">-</span> <span class="n">Iris</span><span class="o">-</span><span class="n">Virginica</span>
                
    <span class="p">:</span><span class="n">Summary</span> <span class="n">Statistics</span><span class="p">:</span>

    <span class="o">==============</span> <span class="o">====</span> <span class="o">====</span> <span class="o">=======</span> <span class="o">=====</span> <span class="o">====================</span>
                    <span class="n">Min</span>  <span class="n">Max</span>   <span class="n">Mean</span>    <span class="n">SD</span>   <span class="n">Class</span> <span class="n">Correlation</span>
    <span class="o">==============</span> <span class="o">====</span> <span class="o">====</span> <span class="o">=======</span> <span class="o">=====</span> <span class="o">====================</span>
    <span class="n">sepal</span> <span class="n">length</span><span class="p">:</span>   <span class="mf">4.3</span>  <span class="mf">7.9</span>   <span class="mf">5.84</span>   <span class="mf">0.83</span>    <span class="mf">0.7826</span>
    <span class="n">sepal</span> <span class="n">width</span><span class="p">:</span>    <span class="mf">2.0</span>  <span class="mf">4.4</span>   <span class="mf">3.05</span>   <span class="mf">0.43</span>   <span class="o">-</span><span class="mf">0.4194</span>
    <span class="n">petal</span> <span class="n">length</span><span class="p">:</span>   <span class="mf">1.0</span>  <span class="mf">6.9</span>   <span class="mf">3.76</span>   <span class="mf">1.76</span>    <span class="mf">0.9490</span>  <span class="p">(</span><span class="n">high</span><span class="err">!</span><span class="p">)</span>
    <span class="n">petal</span> <span class="n">width</span><span class="p">:</span>    <span class="mf">0.1</span>  <span class="mf">2.5</span>   <span class="mf">1.20</span>   <span class="mf">0.76</span>    <span class="mf">0.9565</span>  <span class="p">(</span><span class="n">high</span><span class="err">!</span><span class="p">)</span>
    <span class="o">==============</span> <span class="o">====</span> <span class="o">====</span> <span class="o">=======</span> <span class="o">=====</span> <span class="o">====================</span>
<span class="p">...</span>

</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">from sklearn.datasets import load_iris</code>에는 여러 데이터셋이 저장되어 있다. <code class="language-plaintext highlighter-rouge">dataset.DESCR</code>을 하면 description 즉 설명들이 저장되어 있다.</p><p>target은 정답 라벨을 의미한다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">label</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">(</span><span class="mi">150</span><span class="p">,)</span>
</pre></table></code></div></div><p>150개의 데이터가 있고, 각각의 4개의 속성을 가지고 있는 것을 확인할 수 있다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklean.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="p">(</span><span class="mi">122</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="p">(</span><span class="mi">38</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></table></code></div></div><p>train_test_split은 데이터셋을 분리해주는 기능을 한다. test_size를 통해 test 데이터셋의 크기를 지정해준다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
</pre><td class="rouge-code"><pre><span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">x_train</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">l5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

    <span class="n">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span> 

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="nf">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="nf">l2</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">bn3</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="nf">l3</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">l4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">l5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>


<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accures</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">epoch_accur</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="n">confidence</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"pred"</span><span class="p">,</span><span class="n">prediction</span><span class="p">,</span> <span class="n">prediction</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"conf"</span><span class="p">,</span><span class="n">confidence</span><span class="p">,</span> <span class="n">confidence</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">accur</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">epoch_accur</span> <span class="o">+=</span> <span class="n">accur</span>

  <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
  <span class="n">epoch_accur</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

  
  <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
  <span class="n">accures</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">epoch_accur</span><span class="p">)</span>
</pre></table></code></div></div><ul><li>iris로 데이터를 불러오면 numpy로 되어 있으므로 이를 tensor로 변환한다.<li>dataloader이라는 함수를 통해 학습에 필요한 차원으로 변경시킨다. batch_size를 지정해주고, shuffle이란 데이터 샘플의 순서를 섞을지 말지에 대한 인자이다.<li>최적화 알고리즘은 SGD(stochastic gradient descent)를 사용했다.<li>손실함수로는 crossentropy를 사용했다.<li>epoch이란 학습 반복 횟수를 지정해주는 것이다.</ul><p>반복을 하면서 에측에 대한 loss를 구하고, 역전파를 진행하고, 그에 대해 가중치를 최적화 한다.</p><p><br /></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre>output :tensor<span class="o">([[</span><span class="nt">-0</span>.1946, <span class="nt">-0</span>.2287, <span class="nt">-0</span>.1576],
        <span class="o">[</span><span class="nt">-0</span>.4010, <span class="nt">-0</span>.0588,  0.0440],
        <span class="o">[</span><span class="nt">-0</span>.1065, <span class="nt">-0</span>.0667, <span class="nt">-0</span>.1870],
        <span class="o">[</span> 0.0362, <span class="nt">-0</span>.2685,  0.0521]], <span class="nv">grad_fn</span><span class="o">=</span>&lt;AddmmBackward0&gt;<span class="o">)</span> torch.Size<span class="o">([</span>4, 3]<span class="o">)</span>

pred : tensor<span class="o">([</span>2, 2, 1, 2]<span class="o">)</span> torch.Size<span class="o">([</span>4]<span class="o">)</span>

conf : tensor<span class="o">([</span><span class="nt">-0</span>.1576,  0.0440, <span class="nt">-0</span>.0667,  0.0521], <span class="nv">grad_fn</span><span class="o">=</span>&lt;MaxBackward0&gt;<span class="o">)</span> torch.Size<span class="o">([</span>4]<span class="o">)</span>

ground truth : tensor<span class="o">([</span>1, 2, 1, 0]<span class="o">)</span>
</pre></table></code></div></div><p>torch.max를 하게 되면, 2가지가 출력된다. confidence라는 최대 확률값들과 prediction이라는 최대값들의 index를 리턴하게 된다. output의 shape은 [batch size, n_classes]이다. 즉 1 batch마다의 출력값들이 존재하는데, 이는 총 class의 개수만큼의 길이를 가지고 있다. 이는 각 클래스마다의 분류 확률값을 의미한다. dim=1이므로 각 한 행마다, 즉 batch마다의 최대값에 대한 값(confidence), 인덱스(prediction)을 출력할 수 있다. prediction과 GT값을 비교하여 정확도를 계산할 수 있다.</p><p>이 때, argmax를 사용하면 confidence값은 받지 않고, prediction 값, 즉 index 값만 받아올 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">_</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><p><br /></p><p><br /></p><p>마지막에는 평균 loss와 accuracy를 알아야 하므로 데이터 샘플 개수만큼 나눠준다.</p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s">"loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s">"epochs"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s">"accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">accures</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s">"epochs"</span><span class="p">)</span>
</pre></table></code></div></div><p>결과를 그래프로 보면 다음과 같다.</p><p><img data-src="/assets/img/dev/week9/day4/matplot.png" data-proofer-ignore></p><p><br /></p><ul><li>inference</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="n">output</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nf">round</span><span class="p">((</span><span class="n">prediction</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span><span class="mi">4</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="c1"># -----------------------------#
</span>
<span class="n">torch</span><span class="p">.</span><span class="n">return_types</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span>
<span class="n">values</span><span class="o">=</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.2726</span><span class="p">,</span> <span class="mf">1.2251</span><span class="p">,</span> <span class="mf">0.5746</span><span class="p">,</span> <span class="mf">0.2792</span><span class="p">,</span> <span class="mf">0.4174</span><span class="p">,</span> <span class="mf">1.6891</span><span class="p">,</span> <span class="mf">0.2693</span><span class="p">,</span> <span class="mf">1.3956</span><span class="p">,</span> <span class="mf">0.2245</span><span class="p">,</span>
        <span class="mf">1.5383</span><span class="p">,</span> <span class="mf">0.5694</span><span class="p">,</span> <span class="mf">0.3931</span><span class="p">,</span> <span class="mf">1.6798</span><span class="p">,</span> <span class="mf">0.4401</span><span class="p">,</span> <span class="mf">0.8937</span><span class="p">,</span> <span class="mf">1.5574</span><span class="p">,</span> <span class="mf">1.4948</span><span class="p">,</span> <span class="mf">0.4101</span><span class="p">,</span>
        <span class="mf">2.0672</span><span class="p">,</span> <span class="mf">0.9091</span><span class="p">,</span> <span class="mf">0.3115</span><span class="p">,</span> <span class="mf">0.8186</span><span class="p">,</span> <span class="mf">1.6430</span><span class="p">,</span> <span class="mf">1.2262</span><span class="p">,</span> <span class="mf">1.7812</span><span class="p">,</span> <span class="mf">0.5254</span><span class="p">,</span> <span class="mf">1.0002</span><span class="p">,</span>
        <span class="mf">0.0993</span><span class="p">,</span> <span class="mf">0.4572</span><span class="p">,</span> <span class="mf">2.2780</span><span class="p">,</span> <span class="mf">0.4420</span><span class="p">,</span> <span class="mf">0.5622</span><span class="p">,</span> <span class="mf">0.4398</span><span class="p">,</span> <span class="mf">1.8892</span><span class="p">,</span> <span class="mf">0.9751</span><span class="p">,</span> <span class="mf">1.6513</span><span class="p">,</span>
        <span class="mf">0.4286</span><span class="p">,</span> <span class="mf">1.4288</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MaxBackward0</span><span class="o">&gt;</span><span class="p">),</span>
<span class="n">indices</span><span class="o">=</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>

<span class="mf">0.9211</span>
</pre></table></code></div></div><p>정확도가 0.92가 나왔다. 모델의 구성에 비해 너무 높게 나왔긴 했다.</p><p><br /></p><p><br /></p><h3 id="tensorflow">Tensorflow <a href="#tensorflow" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li><p>Autograd</p><li><p>그래디언트 테이프 텐서플로우는 자동 미분을 위한 <code class="language-plaintext highlighter-rouge">tf.GradientTape</code> API를 제공한다. 이는 컨텍스트 안에서 실행된 모든 연산을 테이프에 <strong>기록</strong> 한다.</p></ul><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
  <span class="n">t</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># x값을 본다.
</span>  <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="s">'y: '</span> <span class="p">,</span><span class="n">y</span><span class="p">)</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="s">"z :"</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

<span class="n">dz_dx</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dz_dx</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>
    <span class="k">assert</span> <span class="n">dz_dx</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">numpy</span> <span class="o">==</span> <span class="mf">8.0</span> <span class="c1">## 값이 틀릴 경우 assertionerror가 발생
</span>
<span class="n">dz_dy</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dz_dy</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">dz_dy</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span> <span class="o">==</span> <span class="mf">8.0</span> 

<span class="c1"># ----------------------------- #
</span>
<span class="n">y</span> <span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="n">z</span> <span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mf">16.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span>
<span class="p">[[</span><span class="mf">8.</span> <span class="mf">8.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">8.</span> <span class="mf">8.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></table></code></div></div><p>이 때, gradient를 호출하면 gradienttape에 포함된 리소스가 해제된다. 따라서 여러 그레디언트를 계산하려면 다음과 같이 정의해야 한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">t</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="c1"># z = x^4
</span>
<span class="n">dz_dx</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">dy_dx</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">dz_dx</span><span class="p">,</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">dy_dx</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mf">108.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span> 
<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="k">del</span> <span class="n">t</span>
</pre></table></code></div></div><p>반복적으로 본 후에는 삭제를 꼭 해주어야 한다.</p><p><br /></p><h4 id="고계도higher-order-그래디언트">고계도(Higher-order) 그래디언트 <a href="#고계도higher-order-그래디언트" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p>gradientTape 컨텍스트 매니저 안에 있는 연산들은 자동 미분을 위해 기록된다. 만약 이 컨텍스트 안에서 그래디언트를 계산하면 해당 그레디언트 연산 또한 기록된다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t1</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t2</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span>
    <span class="n">dy_dx</span> <span class="o">=</span> <span class="n">t2</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">d2y_dx2</span> <span class="o">=</span> <span class="n">t1</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">dy_dx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">dy_dx</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">d2y_dx2</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>

<span class="c1"># ----------------------------- #
</span>
<span class="mf">3.0</span>
<span class="mf">6.0</span>
</pre></table></code></div></div><p><br /></p><h3 id="신경망-구현하기-1">신경망 구현하기 <a href="#신경망-구현하기-1" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>sequential을 사용했을 때의 코드는 다음과 같다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
     <span class="c1"># 
</span>     <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'layer1'</span><span class="p">),</span>
     <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'layer2'</span><span class="p">),</span>
     <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'layer3'</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span>
<span class="p">[[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></table></code></div></div><p><br /></p><p>3x3 행렬이 노드가 2개인 층 -&gt; 3개인 층 -&gt; 4개인 층을 거쳐 출력값이 한 벡터당 4개로 출력이 되는 것을 볼 수 있다.</p><p><br /></p><p>sequential을 사용하지 않고 층을 쌓을 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">layer1</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'layer1'</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'layer2'</span><span class="p">)</span>
<span class="n">layer3</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'layer3'</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">layer3</span><span class="p">(</span><span class="nf">layer2</span><span class="p">(</span><span class="nf">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span>
<span class="p">[[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></table></code></div></div><p><br /></p><p>Sequential을 사용하는 것이 좋아보인다. 여기서 add 함수를 사용하여 층을 쌓을 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="n">tf</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.6980011</span> <span class="o">-</span><span class="mf">1.1421962</span>  <span class="mf">0.5842113</span>  <span class="mf">1.1211115</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.6980011</span> <span class="o">-</span><span class="mf">1.1421962</span>  <span class="mf">0.5842113</span>  <span class="mf">1.1211115</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.6980011</span> <span class="o">-</span><span class="mf">1.1421962</span>  <span class="mf">0.5842113</span>  <span class="mf">1.1211115</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></table></code></div></div><p><br /></p><p>add가 되는 것처럼 <code class="language-plaintext highlighter-rouge">pop</code> 메서드도 사용이 가능하다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">))</span>
<span class="mi">2</span>
</pre></table></code></div></div><p><br /></p><h5 id="패션-mnist-사용한-분류">패션 MNIST 사용한 분류 <a href="#패션-mnist-사용한-분류" class="anchor"><i class="fas fa-hashtag"></i></a></h5></h5><p>패션 MNIST데이터에는 10개의 카테고리와 70000개의 흑백이미지가 포함되어 있다. 이미지의 해상도는 28x28이다. 훈련 데이터셋은 6만장, 테스트 데이터셋은 1만장을 사용한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">fashion_mnist</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">fashion_mnist</span>

<span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'T-shirt/top'</span><span class="p">,</span> <span class="s">'Trouser'</span><span class="p">,</span> <span class="s">'Pullover'</span><span class="p">,</span> <span class="s">'Dress'</span><span class="p">,</span> <span class="s">'Coat'</span><span class="p">,</span> <span class="s">'Sandal'</span><span class="p">,</span> <span class="s">'Shirt'</span><span class="p">,</span> <span class="s">'Sneaker'</span><span class="p">,</span> <span class="s">'Bag'</span><span class="p">,</span> <span class="s">'Ankle boot'</span><span class="p">]</span>

<span class="n">train_images</span><span class="p">.</span><span class="n">shape</span>

<span class="c1"># ----------------------------- #
</span>
<span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></table></code></div></div><p><img data-src="/assets/img/dev/week9/day4/mnist.png" data-proofer-ignore></p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span> <span class="o">/</span> <span class="mf">255.0</span>
</pre></table></code></div></div><p>신경망 모델에 주입하기 전에 값의 범위를 0~1로 normalize한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([])</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">binary</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">train_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></table></code></div></div><p>정규화된 이미지를 여러 장 본다.</p><p><img data-src="/assets/img/dev/week9/day4/subplot.png" data-proofer-ignore></p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
          <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span>
          <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
          <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">),</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1875</span><span class="o">/</span><span class="mi">1875</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">7</span><span class="n">s</span> <span class="mi">3</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4982</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8246</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1875</span><span class="o">/</span><span class="mi">1875</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">6</span><span class="n">s</span> <span class="mi">3</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.3744</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8662</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1875</span><span class="o">/</span><span class="mi">1875</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">7</span><span class="n">s</span> <span class="mi">4</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.3359</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8782</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1875</span><span class="o">/</span><span class="mi">1875</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="n">s</span> <span class="mi">2</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.3115</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8861</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1875</span><span class="o">/</span><span class="mi">1875</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="n">s</span> <span class="mi">2</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.2932</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8922</span>
<span class="o">&lt;</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">History</span> <span class="n">at</span> <span class="mh">0x7ff871842590</span><span class="o">&gt;</span>
</pre></table></code></div></div><p>모델은 이와 같고, 입력을 위해 이미지 행렬을 벡터 형태로 편다. 그리고 마지막에는 확률값 출력을 위해 softmax를 사용했다.</p><p>compile은 모델에 도구들을 지정해준다. adam 이외에 SGD 등을 사용할 수 있다. loss에는 예측값이 정수값으로 나올 경우 sparse_categorical_crossentropy를 사용한다고 한다.</p><p>그 후 fit을 통해 실제 학습이 진행된다.</p><p><br /></p><p>케라스에는 모델을 시각화하는 함수가 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p><img data-src="/assets/img/dev/week9/day4/plot_model.png" data-proofer-ignore></p><p><br /></p><ul><li>validation</ul><p>모델을 통해 검증을 한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span><span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="s">"test loss : "</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"test accuracy : "</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">)</span>

<span class="c1"># ----------------------------- #
</span>
<span class="mi">313</span><span class="o">/</span><span class="mi">313</span> <span class="o">-</span> <span class="mi">1</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.3475</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8763</span> <span class="o">-</span> <span class="mi">792</span><span class="n">ms</span><span class="o">/</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">3</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span>
<span class="n">test</span> <span class="n">loss</span> <span class="p">:</span>  <span class="mf">0.3474982678890228</span>
<span class="n">test</span> <span class="n">accuracy</span> <span class="p">:</span>  <span class="mf">0.8762999773025513</span>
</pre></table></code></div></div><ul><li>inference</ul><p>그 후 평가를 진행한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>
<span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 9
</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">test_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># True
</span>
<span class="c1"># ----------------------------- #
</span>
<span class="nf">array</span><span class="p">([</span><span class="mf">2.8452354e-09</span><span class="p">,</span> <span class="mf">9.4890495e-09</span><span class="p">,</span> <span class="mf">9.5172211e-08</span><span class="p">,</span> <span class="mf">4.2439359e-09</span><span class="p">,</span>
       <span class="mf">1.4116972e-07</span><span class="p">,</span> <span class="mf">2.8057180e-03</span><span class="p">,</span> <span class="mf">4.7427403e-07</span><span class="p">,</span> <span class="mf">1.1753553e-02</span><span class="p">,</span>
       <span class="mf">1.5509060e-05</span><span class="p">,</span> <span class="mf">9.8542446e-01</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="mi">9</span>
<span class="bp">True</span>
</pre></table></code></div></div><p>prediction에는 모든 10개의 신뢰도를 나타낸다. 그 후 argmax를 사용하면 가장 예측값이 높은 레이블의 인덱스가 출력된다. 그것을 test_label과 비교하여 정답인지 확인한다.</p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/multiperceptron/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/multiperceptron.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/devcourse/'>devcourse</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/devcourse/" class="post-tag no-text-decoration" >devcourse</a> <a href="/tags/deeplearning/" class="post-tag no-text-decoration" >deeplearning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[데브코스] 9주차 - DeepLearning Multi Layer Perceptron - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/multiperceptron/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[데브코스] 9주차 - DeepLearning Multi Layer Perceptron - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/multiperceptron/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/multiperceptron/&amp;text=[데브코스] 9주차 - DeepLearning Multi Layer Perceptron - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/multiperceptron/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/numpy/"><div class="card-body"> <em class="timeago small" date="2022-04-11 14:40:00 +0900" >Apr 11, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 9주차 - DeepLearning Numpy & Matplotlib</h3><div class="text-muted small"><p> Numpy Numpy 설치 및 불러오기 pip install numpy import numpy as np 파이썬의 리스트는 머신러닝에서 가장 많이 사용되는 구조 중 하나일 것이다. 그러나 이 리스트는 연산 속도가 느리다. 그래서 연산을 효과적으로 할 수 있도록 하기 위해 만든 것이 Numpy이다. Numpy 연산 속도 nump...</p></div></div></a></div><div class="card"> <a href="/posts/dplearn/"><div class="card-body"> <em class="timeago small" date="2022-04-12 15:40:00 +0900" >Apr 12, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 9주차 - DeepLearning AI & machine learning</h3><div class="text-muted small"><p> 기계 학습 어떤 컴퓨터 프로그램이 T라는 작업을 수행할 때, 이 프로그램의 성능이 P라는 척도로 평가했을 때 경험 E를 통해 성능이 개선된다면 이 프로그램은 학습한다고 말할 수 있다. 따라서 최적의 알고리즘을 찾는 행위를 기계 학습이라 할 수 있다. 기계 학습의 중심은 경험, 과업, 성능에 있다. 예전에는 지식 기반의 학습을 했다. 즉, 인간이...</p></div></div></a></div><div class="card"> <a href="/posts/mlmath/"><div class="card-body"> <em class="timeago small" date="2022-04-13 15:40:00 +0900" >Apr 13, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[데브코스] 9주차 - DeepLearning Mathematics for Machine Learning</h3><div class="text-muted small"><p> 기계 학습에서 수학은 손실함수를 정의하고 손실함수의 최저점을 찾아주는 최적화 이론에 사용된다. 제어를 함에 있어서도 수학이 필요하다. 선형대수 데이터는 벡터나 행렬, 텐서 형태로 되어 있는데, 이에 대한 공간을 이해하고, 연산을 하기 위해서는 선형대수가 필요하다. 벡터와 행렬 벡터는 요소의 종료와 크기를 표현한다. [x \in R^n] ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/mlmath/" class="btn btn-outline-primary" prompt="Older"><p>[데브코스] 9주차 - DeepLearning Mathematics for Machine Learning</p></a> <a href="/posts/docker/" class="btn btn-outline-primary" prompt="Newer"><p>[데브코스] 9주차 - Docker (chroot, pseudo path)</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>