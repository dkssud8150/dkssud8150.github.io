<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="CS231N chapter 11 - Detection and Segmentation" /><meta name="author" content="JaeHo-YooN" /><meta property="og:locale" content="en" /><meta name="description" content="10강 리뷰 1) Recurrent Neural Network language model 2) CNN + RNN = image captioning 3) LSTM" /><meta property="og:description" content="10강 리뷰 1) Recurrent Neural Network language model 2) CNN + RNN = image captioning 3) LSTM" /><link rel="canonical" href="https://dkssud8150.github.io/posts/cs231n11/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/cs231n11/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-10-04T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="CS231N chapter 11 - Detection and Segmentation" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo-YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo-YooN"},"dateModified":"2022-07-20T00:49:28+09:00","datePublished":"2021-10-04T13:00:00+09:00","description":"10강 리뷰 1) Recurrent Neural Network language model 2) CNN + RNN = image captioning 3) LSTM","headline":"CS231N chapter 11 - Detection and Segmentation","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/cs231n11/"},"url":"https://dkssud8150.github.io/posts/cs231n11/"}</script><title> CS231N chapter 11 - Detection and Segmentation | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span> CS231N chapter 11 - Detection and Segmentation </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip> CS231N chapter 11 - Detection and Segmentation</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo-YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-10-04 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 4, 2021, 1:00 PM +0900" >Oct 4, 2021</em> </span> <span> Updated <em class="timeago" date="2022-07-20 00:49:28 +0900 " data-toggle="tooltip" data-placement="bottom" title="Wed, Jul 20, 2022, 12:49 AM +0900" >Jul 20, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4173 words"> <em>23 min</em> read</span></div></div></div><div class="post-content"><blockquote><p>10강 리뷰</p><p>1) Recurrent Neural Network</p><ul><li>language model</ul><p>2) CNN + RNN = image captioning</p><p>3) LSTM</p></blockquote><p><br /></p><p><br /></p><ul><li>Computer Vision Tasks</ul><p><img data-src="/assets/img/cs231n/2021-10-06/0017.jpg" alt="image" data-proofer-ignore></p><p>다양한 computer vision task가 있다. semantic segmentation / classification+localization / object detection / instance segmentation 등이 있다.</p><p><br /></p><h1 id="semantic-segmentation">Semantic Segmentation</h1><p><img data-src="/assets/img/cs231n/2021-10-06/0019.jpg" alt="image" data-proofer-ignore></p><p>semantic segmentation은 입력은 이미지, 출력은 이미지의 모든 픽셀에 카테고리를 정하는 것이다. 그 픽셀이 어떤 물체에 해당하는지 결정한다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0021.jpg" alt="image" data-proofer-ignore></p><p>semantic segmentation은 개별 객체를 구분하지 않는다. 픽셀의 카테고리만 구분할 뿐이다.</p><p>그에 반해 instance segmentation은 개별 객체까지 구분한다.</p><p>segmentation초기 모델에서는 sliding window를 적용했다. 입력 이미지를 아주 작은 단위로 쪼개어 classification을 하는 것이다.</p><p>이 방법은 비용이 엄청나게 크다. 모든 픽셀에 대해 작은 영역으로 쪼개고, 이 모든 영역을 forward/backward pass 하는 일은 상당히 비효율적이다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0022.jpg" alt="image" data-proofer-ignore></p><p>그래서 fully convolutional network를 사용해보았다. FC Layer을 없애고 convolution layer로 구성된 네트워크이다.</p><p>이 네트워크의 출력 tensor는 C x H x W이다. c는 카테고리 수에 해당한다. 이 출력값은 입력 이미지의 모든 픽셀 값에 대해 classification score을 매긴 값이다.</p><p>이 네트워크를 학습시키려면 모든 픽셀의 classification loss를 계산하고 평균 값을 취한다. 그리고 backpropagation을 수행한다.</p><p><br /></p><p>training data를 만드는 방법은 입력 이미지에 모든 픽셀에 대해 라벨링하면 된다. 사람이 툴을 만들기도 한다. 객체의 외관선만 그려주면 그 안을 채워넣는 식이다.</p><p>또한, 손실 함수는 출력의 모든 픽셀에 corss entropy를 적용한다. 출력의 모든 픽셀과 실제 값(ground truth)와의 cross entropy를 계산한다. 이 값들을 모두 더하거나 평균화시켜 loss를 구한다.</p><p><br /></p><p>문제는 이 네트워크는 입력 이미지의 spatial size(공간적 사이즈)를 계속 유지시켜야 한다. 그래서 비용이 아주 크다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0024.jpg" alt="image" data-proofer-ignore></p><p>실제 네트워크의 형태는 대부분 이렇다. 특징맵을 downsampling &amp; upsampling 한다.</p><p>이미지 전체를 계속 convolution 시키기보다 max pooling/ stride convolution을 통해 특징맵을 downsampling 한다. 그 후 다시 입력 이미지의 해상도와 같도록 upsampling 한다. 이 방법을 사용하면 계산 효율이 더 좋아진다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0026.jpg" alt="image" data-proofer-ignore></p><p>upsampling의 방법으로 nearest neighbor 이나 bed of nails 가 있다.</p><p>nearest neighbor upsampling의 경우는 해당하는 receptive field로 값을 그대로 복사한다. 주변의 값들을 복사하는 것이다.</p><p>bed of nails upsampling 의 경우는 원래 값을 가져오고 나머지 공간에는 0을 집어넣는다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0027.jpg" alt="image" data-proofer-ignore></p><p>Max unpooling 이라는 방법도 있다. pooling과 연관을 지어 downsampling 시에 maxpooling에 사용했던 요소들을 기억하고 그 자리에 값들을 집어넣는 것이다.</p><p>maxpooling을 하면서 특징맵의 공간 정보를 잃게 된다. 이 방법을 사용하면 공간정보 손실을 줄일 수 있기 때문에 더 좋은 결과를 얻을 수 있다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0028.jpg" alt="image" data-proofer-ignore></p><p>transpose convolution이라는 것도 있다. 앞서 배운 것(upsampling , max unpooling)들은 고정함수이기 때문에 학습하지는 않는다.</p><p>하지만, transpose convolution은 어떤 방식으로 upsampling할지 학습을 할 수 있다.</p><p>일반적인 3x3 <em>convolution filter</em>(stride=1,padding=1)은 입력도 출력도 4x4이다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0032.jpg" alt="image" data-proofer-ignore></p><p>하지만, <strong>strided convolution</strong> 의 경우 입력이 4x4이고, 출력이 2x2이다.</p><p>이유는 3x3 필터에 대한 strided convolution은 한 픽셀씩 이동하면서 계산하지 않고, 두 픽셀씩 움직여야 한다. 따라서 stride=2인 strided convolution은 학습 가능한 방법으로 2배 downsampling한다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0035.jpg" alt="image" data-proofer-ignore></p><p>transpose convolution은 반대로 입력이 2x2 이고, 출력이 4x4다.</p><p>여기서는 내적을 수행하지 않고, 우선 input 특징맵에서 값(빨간색)을 하나 선택한다. 이 하나의 값(스칼라)과 필터를 곱한 값을 3x3 영역에 넣는다.</p><p>transpose convolution은 필터와 입력의 내적의 계산이 아니라 입력 값이 필터에 곱해지는 가중치 역할을 한다. 따라서 출력은 <strong>필터 * 입력(가중치)</strong> 가 된다.</p><p>입력이 한칸씩 움직이면 출력은 2칸씩 움직인다. 이 때, 서로 겹치는 값이 존재할 경우 두 값을 더한다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0039.jpg" alt="image" data-proofer-ignore></p><p>transpose convolution의 구체적인 예시이다. 입력은 a,b 필터는 x,y,z 이다. 출력값을 계산해보면 오른쪽과 같은 형태가 된다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0041.jpg" alt="image" data-proofer-ignore></p><p>이름이 transpose인 이유를 설명하자면,</p><p>왼쪽은 일반적인 convolution 이고, 오른쪽은 transpose convolution이다. 같은 행렬이지만, 오른쪽의 행렬을 transpose(전치)를 취한 후 곱 연산한다. 왼쪽은 stride 1 convolution이고, 오른쪽은 stride 1 transpose convolution인 것이다.</p><p>왼쪽의 필터의 양 끝단의 0 은 padding으로 인한 값이다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0043.jpg" alt="image" data-proofer-ignore></p><p>하나 더 보자면, stride 1 transpose convolution을 보면 비슷하게 생겼다. padding 등을 고려하면 좀 달라지지만 기본적으로 비슷하다. 하지만 여기서 stride = 2로 설정되어 있다. 다른 점은 xyz가 2칸씩 움직인다.</p><p>이 때, stride가 1을 초과하면 transpose convolution과 convolution이 같지 않게 된다. 아마 a가 a,b,c,d가 아닌 a,b만 나오기 때문이 아닐까 싶다.</p><p><br /></p><p><br /></p><h1 id="classification--localization">Classification + Localization</h1><p><img data-src="/assets/img/cs231n/2021-10-06/0044.jpg" alt="image" data-proofer-ignore></p><p>이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체가 어디에 있는지 알기 위해 <strong>bounding box</strong>를 친다.</p><p><em>object detection</em>은 다중 객체를 인식할 때 사용되고, <em>localization</em>은 단일 객체를 분류할 때 자주 사용된다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0046.jpg" alt="image" data-proofer-ignore></p><p>localization의 기본 구조를 나타낸 것이다. 입력 이미지를 받아 네트워크에 넣고 출력 레이어 직전의 FC layer는 class score로 연결되어 카테고리를 정한다. 그리고 4개의 원소를 가진 vector와 연결된 FC layer가 하나 더 있다. 이 4개의 원소는 width/height/x/y로 bounding box의 위치를 나타내는 것이다.</p><p>따라서 localization을 할 때는 2개의 출력 값을 가진다. 하나는 클래스 스코어, 하나는 bounding box 좌표이다.</p><blockquote><p>이 때, 학습 이미지에는 카테고리 레이블과 해당 객체의 bounding box Ground Truth를 동시에 가지고 있어야 한다.</p></blockquote><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0048.jpg" alt="image" data-proofer-ignore></p><p>그래서 loss도 두 개 존재한다. 스코어에 대한 loss는 softmax, bounding box에 대한 loss는 L2 loss로 구한 후 두개를 더해 최종적인 loss를 구한다. L2 대신 smooth L1을 더 많이 사용되긴 한다.</p><p>이처럼 두 개의 loss를 합친 loss를 Multi-task loss라고 한다. gradient를 구하려면 네트워크 가중치들의 각각의 미분 값을 계산해야 한다. loss가 두 개이므로 미분 값도 두 개다.</p><p>loss의 수식을 보면 두 loss의 가중치를 조절하는 하이퍼파라미터가 존재한다. 이 두 값을 조절하는 것은 매우 까다롭다. 그래서 loss 값이 아닌 다른 지표(MAP(mean average precision), model size, speed, num of parameters)를 통해 성능을 비교하는 것이 좋다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0050.jpg" alt="image" data-proofer-ignore></p><p>classification + localizatino은 human pose estimation에도 적용할 수 있다. 사람의 관절이 어디에 위치하는지 예측한다. 관절의 위치로 사람의 포즈를 정의한다. 정의한 관절의 위치를 GT와 비교하여 regression Loss를 구한다.</p><p><br /></p><p><br /></p><h1 id="object-detection">Object Detection</h1><p>object detection은 엄청 많은 곳에 적용해볼 수 있는 방법이다. visual tracking, pose estimation, 등등 object detection 모델에 segmentation이나 tracking 모델을 더해서 사용할 수 있기 때문이다. 실제로 이런 방식을 사용하면 더 성능이 좋게 나온다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0057.jpg" alt="image" data-proofer-ignore></p><p>object detection을 할 때도 초기 모델에서는 sliding window를 많이 사용했다. 입력 이미지를 매우 작은 픽셀로 나누고 그 위를 window가 모든 픽셀을 읽어들이도록 한다.</p><p><img data-src="https://miro.medium.com/max/478/1*wbuckxPMCs4BEemAuqd94g.gif" alt="gif" data-proofer-ignore></p><p>silding window의 문제는 어떻게 영역을 추출할지와 이미지에 object가 몇 개 존재하는지, 어디에 존재하는지 파악하지 못한다. object의 크기도 알수가 없다. 또, 작은 영역 하나마다 거대한 CNN을 통과시키면 계산량이 엄청나다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0062.jpg" alt="image" data-proofer-ignore></p><p>그래서 <strong>Region Proposals</strong>를 개발했다. 이 방법은 현재 딥러닝에 사용되지 않지만 전통적인 방식이다.</p><p>object가 있을법한 2000개의 bounding box 후보군을 찾아내는데, 찾는 방법으로 selective search가 있다. selective search를 돌려 객체가 있을만한 2000개의 region proposal을 만들어내는 것이다. 찾아낸 region proposal을 CNN의 입력으로 한다.</p><p>이 방법을 사용하면 silding window방식보다 계산량이 줄어든다. 하지만, 이 방법은 노이즈가 심하다. 대부분은 실제 객체가 아닐지라도 recall이 매우 높다.</p><p>region proposal을 region of interest(ROI)라고도 한다. 실제로 ROI라는 말을 더 많이 사용한다.</p><p>여기서 ROI의 사이즈가 각양각색이다. 추출된 ROI로 CNN Classification을 수행하기 위해서는 FC-layer의 특성에 의해 사이즈가 다 동일해야 한다. 따라서 고정된 사이즈로 크기를 맞춰준다. 이를 <strong>warp</strong>시킨다고 한다.</p><p><br /></p><h2 id="r-cnn">R-CNN <a href="#r-cnn" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/cs231n/2021-10-06/0068.jpg" alt="image" data-proofer-ignore></p><p>R-CNN의 경우 ROI들의 최종 classification에 SVM classifier을 사용하여 score를 분류한다.</p><p>R-CNN은 Bounding Box를 보정해주기 위한 offset 값 4개도 예측한다.</p><p>고정된 알고리즘인 selective search를 사용하기 때문에 region proposal은 학습되지 않는다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0069.jpg" alt="image" data-proofer-ignore></p><p>R-CNN의 문제점으로는 계산비용이 크고 느리다. 2000개의 ROI를 독립적으로 CNN에 넣기 때문이다. 그리고 CNN에서 나온 feature 들을 디스크에 저장하므로 용량이 엄청나다. training time/test time 둘다 매우 느리다.</p><p><br /></p><h2 id="fast-r-cnn">Fast R-CNN <a href="#fast-r-cnn" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/cs231n/2021-10-06/0077.jpg" alt="image" data-proofer-ignore></p><p>위의 문제들을 해결하고자 Fast R-CNN을 개발했다.</p><ul><li>전체 알고리즘<ol><li>입력 이미지를 미리 학습된 CNN을 통과시켜 feature map을 추출한다.<li>selective search를 통해서 찾은 각각의 ROI에 대해 ROI Pooling을 진행하여 고정된 크기의 feature vector를 얻는다.<li>feature vector는 fully connected layer들을 통과한 뒤, 두 개의 브랜치로 나뉘게 된다.<li>1) 하나의 브랜치는 softmax를 통과하여 해당 ROI가 어떤 물체인지 classification한다. SVM은 사용하지 않는다.</ol><p>2) 나머지 브랜치는 bounding box regression을 통해서 selective search로 찾은 박스의 위치를 조정한다.</p></ul><p>R-CNN과 다른 점은 각 ROI마다 각각을 CNN에 넣지 않고, 입력 이미지에 CNN을 수행하여 고해상도 feature map을 얻고, 거기에 ROI를 적용시킨다. 이에 따라 여러 ROI가 서로 feature을 공유할 수 있게 되었다.</p><p><br /></p><p>feature map에서 가져온 ROI는 FC layer의 입력에 알맞게 크기를 조정해야 한다. 이를 위해 <strong>ROI pooling layer</strong>을 적용한다.</p><p><img data-src="/assets/img/cs231n/2021-10-06/0078.jpg" alt="image" data-proofer-ignore></p><p>feature map에서 가져온 ROI의 크기를 조정(wraped)한 후, FC layer의 입력으로 넣어 classification score과 regression box를 얻을 수 있다.</p><p><br /></p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0079.jpg" alt="image" data-proofer-ignore></p><p>R-CNN과의 차이는 CNN을 한 번만 통과시키기 때문에 계산이 줄어들고, SVM을 사용하지 않는다는 것이다.</p><p>CNN을 한번만 통과시킨 뒤 그 feature map을 공유하는 것은 SPPNet에서 고안한 방법이다.</p><p>fast R-CNN에서는 feature들을 서로 공유하기 때문에 계산이 엄청 빠르다. region proposal을 계산하는 시간이 대부분이다. faster R-CNN에서는 계산 시간을 줄이고자 했다.</p><p><br /></p><h2 id="faster-r-cnn">Faster R-CNN <a href="#faster-r-cnn" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/cs231n/2021-10-06/0081.jpg" alt="image" data-proofer-ignore></p><p>지금까지 사용되왔던 selective search대신 RPN(region proposal network)을 사용한다.</p><p>입력 이미지 전체를 CNN에 넣어 feature map을 얻는다. 여기서 RPN을 적용시켜 region proposal을 예측한다.</p><blockquote><ul><li>RPN 알고리즘<ol><li>feature map을 input으로 받는다.<li>feature map에 3x3 convolution을 256 또는 512 채널(depth)만큼 수행한다.<li>convolution된 map을 입력으로 받아 classification과 bounding box regression 예측 값을 계산한다.<li>앞서 얻은 값들로 ROI를 구한다.</ol></ul></blockquote><p>학습시에 RPN도 함께 학습시키는데, ground truth object(실제 객체)와 일정 threshold(한계 = 정도)이상 겹치는 proposal(제안)을 positive라 하고, 그 이하는 negative라고 한다.</p><p>Faster R-CNN에서는 최종 classification loss/bounding box regression loss도 구하고, proposal에 대한 classification loss와 bounding box regression loss까지 총 4개를 구하고 이들에 대한 loss를 합쳐 최종 loss를 계산한다.</p><p><br /></p><p><br /></p><p><br /></p><p>위의 R-CNN 시리즈는 2-stage model들이다. 즉, classification과 bounding box regression을 각각 수행한다. 하지만 YOLO/SSD의 경우 1-stage model로 이 둘을 함께 수행한다. 따라서 2stage는 정확도는 좋지만, 속도가 느리고, 1stage의 경우는 속도는 빠르나 정확도가 다소 떨어진다.</p><p><br /></p><h2 id="you-only-look-once--single-shot-detection">You Only Look Once / Single Shot Detection <a href="#you-only-look-once--single-shot-detection" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/cs231n/2021-10-06/0084.jpg" alt="image" data-proofer-ignore></p><p>YOLO(you only look once) 모델은 요즘 매우 핫한 모델로 2021년도에 개발된 yolov5 까지 있다. 이 두 모델은 거대한 CNN을 통과하면 모든 것을 담은 예측값이 한번에 나온다.</p><p><img data-src="/assets/img/cs231n/2021-10-04/yolo.png" data-proofer-ignore></p><p>region proposal 단계를 제거하고, bbox regression과 classification을 한 번에 수행하는 구조다.</p><p>먼저 입력 이미지를 S X S 그리드 영역으로 나눈다. 각 그리드 영역에서 물체가 있을만한 영역에 해당하는 B개의 bounding box를 예측한다. 그 다음 해당 박스의 신뢰도를 나타내는 confidence를 계산한다. 또한, bounding box 안에 객체가 존재할 가능성을 의미하는 classification score도 계산한다.</p><p>출력은 S x S x (5*B + C) [B: bounding box의 offset, C: 클래스 개수]의 3-dimension를 가지는 tensor이다. 이 값을 CNN으로 학습시킨다.</p><p><br /></p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0085.jpg" alt="image" data-proofer-ignore></p><p>base network(backbone network)에는 VGG, ResNet, MobileNet 등이 있다. 아키텍처에는 2stage인 R-CNN 시리즈와 1stage인 YOLO 등이 있다. 2stage와 1stage의 중간인 R-FCN도 있다.</p><p><br /></p><p><br /></p><ul><li>Dense Captioning</ul><p><img data-src="/assets/img/cs231n/2021-10-06/0086.jpg" alt="image" data-proofer-ignore></p><p>object detection + captioning 을 dense captioning으로 명명했다. 각 region에 대해 카테고리 대신 문장(caption)을 예측하는 것이다.</p><p>데이터셋으로는 각region에 caption이 있는 데이터셋이 필요하다.</p><p>이 모델을 end-to-end로 학습시켜 모든 것을 동시에 예측할 수 있도록 만들었다.</p><p><img data-src="/assets/img/cs231n/2021-10-06/0087.jpg" alt="image" data-proofer-ignore></p><p>네트워크에는 Region proposal을 사용했고, caption을 예측해야 하기에 softmax loss 대신 RNN Language model을 도입했다.</p><p><br /></p><p><br /></p><h1 id="instance-segmentation">Instance Segmentation</h1><p><img data-src="/assets/img/cs231n/2021-10-06/0089.jpg" alt="image" data-proofer-ignore></p><p>semantic segmentation과 object detection을 섞은 것이다. 이미지 내의 두 마리 개가 있을 때 2마리를 구분해야 한다. 또한 BBox 가 아닌 각 객체애 해당하는 segmentation mask를 예측해야 한다.</p><p><br /></p><p>Mask R-CNN 아키텍처가 instance segmentation에 속한다.</p><p><img data-src="/assets/img/cs231n/2021-10-06/0090.jpg" alt="image" data-proofer-ignore></p><p>입력 이미지는 CNN과 RPN을 거친다. 그 후 classification과 regression이 아닌 bbox마다의 segmentation mask를 예측한다. RPN으로 뽑은 ROI 영역 내에서 각각 semantic segmentation을 수행한다. feature map으로부터 ROI Pooling을 수행하면 두 갈래로 나뉜다. 첫번째는 각 region propasal이 어떤 카테고리에 속하는지 계산하고, region proposal의 좌표를 보정해주는 bbox regression도 예측한다. 다른 갈래는 각 픽셀마다 객체인지 아닌지 분류한다.</p><p><br /></p><p><img data-src="/assets/img/cs231n/2021-10-06/0092.jpg" alt="image" data-proofer-ignore></p><p>Mask R-CNN에 pose estimation도 융합할 수 있다. 관절의 좌표를 예측하는 부분을 추가하면 된다. region proposal에 한 갈래를 추가해 region proposal 안의 객체의 관절 좌표를 예측하면 된다.</p><p><br /></p><p><br /></p><ul><li>Mask R-CNN 코드 <a href="https://colab.research.google.com/drive/1dWg0nx7KEYGSH05heY2_z5hosHBK3EbP#scrollTo=iTL-OFlWNSWq">https://colab.research.google.com/drive/1dWg0nx7KEYGSH05heY2_z5hosHBK3EbP#scrollTo=iTL-OFlWNSWq</a></ul><p><br /></p><p><br /></p><p><br /></p><h1 id="reference">Reference</h1><ul><li><a href="http://cs231n.stanford.edu/2017/syllabus.html">http://cs231n.stanford.edu/2017/syllabus.html</a><li><a href="https://lsjsj92.tistory.com/416">https://lsjsj92.tistory.com/416</a></ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/cs231n11/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/cs231n11.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/cs231n/'>CS231N</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/cs231n/" class="post-tag no-text-decoration" >CS231N</a> <a href="/tags/classification/" class="post-tag no-text-decoration" >classification</a> <a href="/tags/detection/" class="post-tag no-text-decoration" >detection</a> <a href="/tags/segmentation/" class="post-tag no-text-decoration" >segmentation</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text= CS231N chapter 11 - Detection and Segmentation - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/cs231n11/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title= CS231N chapter 11 - Detection and Segmentation - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/cs231n11/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/cs231n11/&amp;text= CS231N chapter 11 - Detection and Segmentation - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/cs231n11/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/cs231n1/"><div class="card-body"> <em class="timeago small" date="2021-09-04 13:00:00 +0900" >Sep 4, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CS231N chapter 1 - Course Introduction</h3><div class="text-muted small"><p> A brief history of Computer Vision Biological Vision 아주 먼 옛날, 눈을 통해 동물들은 진화했다. Human Vision 카메라와 같이 사람의 눈과 유사하게 사진을 담을 수 있도록 만들어졌다. Computer Vision ** David Mars** 이미지를 인식하고 최종...</p></div></div></a></div><div class="card"> <a href="/posts/cs231n2/"><div class="card-body"> <em class="timeago small" date="2021-09-04 13:00:00 +0900" >Sep 4, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CS231N chapter 2 - Image Classification</h3><div class="text-muted small"><p> numpy를 이용한 텐서 사용이 익숙하지 않는 사람들을 위한 튜토리얼 Image Classification 이미지를 볼 때 컴퓨터는 거대한 숫자 그리드로 표현한다. 그것을 통해 컴퓨터는 미리 정의된 class 레이블 중 하나를 지정한다. 이 이미지의 경우 800x600 픽셀로 이루어져 있다. 또한, 3채널 즉, RGB로 표현되기 때문에 8...</p></div></div></a></div><div class="card"> <a href="/posts/cs231n3/"><div class="card-body"> <em class="timeago small" date="2021-09-12 13:00:00 +0900" >Sep 12, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CS231N chapter 3 - Loss function and Optimizations</h3><div class="text-muted small"><p> 2강 리뷰 1) image deformation 2) 그를 해결하기 위한 data-driven approach 3) Nearest Neighbor method, K-Nearest Neighbor(KNN) 4) cross validation 5) hyperparameter 6) linear cl...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/cs231n10/" class="btn btn-outline-primary" prompt="Older"><p> CS231N chapter 10 - Recurrent Neural Networks</p></a> <a href="/posts/handdetection/" class="btn btn-outline-primary" prompt="Newer"><p>2021 Ego-Vision 손동작 인식 경진대회</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>