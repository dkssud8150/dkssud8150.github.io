<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[논문 리뷰] PointCNN: Convolution on X-transformed points" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="pointCNN: Convolution on X-transformed points논문에 대한 리뷰입니다. 이 논문은 2018에 투고된 논문이며 약 1100회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><meta property="og:description" content="pointCNN: Convolution on X-transformed points논문에 대한 리뷰입니다. 이 논문은 2018에 투고된 논문이며 약 1100회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><link rel="canonical" href="https://dkssud8150.github.io/posts/pointcnn/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/pointcnn/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-21T13:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[논문 리뷰] PointCNN: Convolution on X-transformed points" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-01-21T13:00:00+09:00","datePublished":"2022-01-21T13:00:00+09:00","description":"pointCNN: Convolution on X-transformed points논문에 대한 리뷰입니다. 이 논문은 2018에 투고된 논문이며 약 1100회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.","headline":"[논문 리뷰] PointCNN: Convolution on X-transformed points","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/pointcnn/"},"url":"https://dkssud8150.github.io/posts/pointcnn/"}</script><title>[논문 리뷰] PointCNN: Convolution on X-transformed points | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[논문 리뷰] PointCNN: Convolution on X-transformed points</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-src="/assets/img/autodriving/pointcnn/fig4.png" class="preview-img bg" alt="Preview Image" width="800" height="500" data-proofer-ignore><h1 data-toc-skip>[논문 리뷰] PointCNN: Convolution on X-transformed points</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-01-21 13:00:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Fri, Jan 21, 2022, 1:00 PM +0900" >Jan 21, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6690 words"> <em>37 min</em> read</span></div></div></div><div class="post-content"><p><code class="language-plaintext highlighter-rouge">pointCNN: Convolution on X-transformed points</code>논문에 대한 리뷰입니다. 이 논문은 2018에 투고된 논문이며 약 1100회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.</p><p><br /></p><h1 id="abstract">Abstract</h1><p>저자는 point cloud로부터 피쳐 학습에 대한 간단하고 일반적인 프레임워크를 소개하고자 한다. CNN의 성공의 열쇠는 grid에서 조밀하게 표현된 데이터의 공간적-지역적(local) 상관관계를 활용할 수 있는 convolution 연산이다. 그러나 point cloud는 불균일하고 무질서하기 때문에, 점과 관련된 피쳐에 대해 커널을 직접 연산을 하면 형상 정보가 사라지고 점의 정렬에 대한 변형이 발생한다. 이 문제를 해결하기 위해 저자는 두 가지 원인: (1) 점과 관련된 입력 피쳐의 가중치와 (2) 잠재적이고 표준적인 순서에 대한 점들의 순열, 을 동시에 촉진하기 위해 입력 점들로부터 x-transform을 배울 것을 제안한다. 일반적인 convolution 연산자의 요소별 곱과 합 연산은 x-transformation기능 이후에 적용된다. 이 방법은 point cloud로부터 피쳐 학습에 대한 전형적인 CNN의 표본이므로, 저자는 이를 PointCNN이라 부른다.</p><p><br /></p><h1 id="1-introduction">1. Introduction</h1><p>공간적-지역적(spatially-local) 상관관계는 데이터 표현과 독립적인 다양한 유형의 데이터의 아주 흔한 특성이다. 이미지와 같이 일반 도메인에 표시되는 데이터의 경우, convolution 연산은 다양한 작업에서 CNN에서의 주된 성공 요인으로서, 저 상관관계에 활용하면 효과적인 결과를 보여준다. 그러나 무질서하고 불균일한 point cloud 형태의 데이터에 대해서는 convolution 연산이 데이터에서 공간적-지역적 상관관계를 활용하는 것이 적합하지 않다.</p><p><img data-src="/assets/img/autodriving/pointcnn/fig1.png" data-proofer-ignore></p><p>위의 그림은 point cloud에 convolution을 적용할 때의 문제를 설명하고 있다. (i)~(iv)에 대해 C차원의 입력 피쳐을 가진 무질서한 데이터를 F = {fa, fb, fc, fd} 라고 하고, 4xC 형태를 가진 1개의 커널을 K = [kα, kβ, kγ, kδ]^T 를 가진다고 가정해보자.(i)의 경우, 규칙적인 그리드 구조에 의해 규칙적인 정렬을 보여주고 있으며, 이는 2 x 2 patch 지역안의 <code class="language-plaintext highlighter-rouge">fi = Conv(K,[fa,fb,fc,fd]^T)</code>의 형태를 가진 K로부터 convolution되어 4xC 형상의 [fa,fb,fc,fd]^T가 될 수 있다. 이 때, Conv(,)는 요소별 곱을 뜻한다. 그리고, (ii)~(iv)의 경우 점들은 주변 점들에 의해 샘플링되므로 점의 순서가 임시적일 수 있다. 위의 그림에 적힌 순서를 따른다고 생각했을 때, 입력 피쳐 셋 F는 (ii)와 (iii)에서는 [fa,fb,fc,fd]^T이 될 것이고, (iv)에서는 [fc,fa,fb,fd]^T가 될 것이다. 만약 이 상태로 convolution 연산을 직접적으로 적용했을 때, 세 케이스의 출력 피쳐들은 위 그림의 (1a)처럼 계산된다. 모든 케이스에 대해 fii = fiii로 하고, 대부분의 케이스에 대해 fiii != fiv로 고정시킨다. 그렇게 되면, 결과적으로 직접적인 연산이 fii = fiii와 같은 형상 정보는 버리고, fiii != fiv와 같은 순서에 대한 변형은 남겨두는 결과를 초래한다,</p><p>본 논문에서는 X = MLP(p1,p2,…,pk)와 같은 다층 퍼셉트론인 입력 점들(p1,p2,…,pk)인 K의 좌표에 대한 K x K의 X-transformation을 제안한다. 목표는 가중치 부여와 입력 피쳐 변형을 동시에 수행한 다음, 그 뒤에 변형된 피쳐들에게 전형적인 convolution을 적용하는 것이다. 이 과정을 <code class="language-plaintext highlighter-rouge">X-conv</code>라 하고, 이것이 PointCNN에 기본적인 블록이 된다. 이를 통해 fig1의 (ii)~(iv)에 대한 X가 4x4 행렬이고, K = 4인 X-conv를 적용하면 위 그림의 (1b) 방정식처럼 된다. 이 때, xii와 xiii는 각기 다른 점들로부터 학습된 것이기 때문에, 입력 피쳐의 가중치가 달라져 fii != fiii를 만들 수 있다. xiii와 xiv에 대해 ⫪가 (c,a,b,d)를 (a,b,c,d)로 순열하기 위한 순열 행렬일 때, Xiii = xiv x ⫪를 만족한다면, fiii = fiv를 만들 수 있다.</p><p>fig1의 분석을 통해, 이상적인 X-transformation에서 X-conv는 순서를 바꾸지 않고, 점의 형상 정보를 얻을 수 있는 것을 확인했다. 실제로는, 순열 등식 측면에서 x-transformation 학습은 이상과 차이가 있다. 그럼에도 불구하고, X-conv를 통해 만들어진 PointCNN은 point cloud로 일반적인 convolution을 직접 적용하는 것보다 훨씬 좋고, PointNet++와 같이 point cloud 입력 데이터에 대해 설계된 최첨단 신경망보다도 더 좋다.</p><p><br /></p><h1 id="2-related-work">2. Related Work</h1><ul><li>Feature Learning from Regular Domains</ul><p>CNN은 이미지(2D 규칙적인 그리드안의 픽셀들)안의 공간적-지역적 상관관계를 잘 활용해왔다. 3D voxels와 같이 고차원의 규칙적인 도메인에 대한 응용CNN도 잘 나와있다. 그러나 입력과 convolution 커널들은 모두 고차원이고, 그래서 매우 많은 계산 비용과 메모리를 필요로 한다. Octree, Kd-tree, Hash 기반의 접근 방식은 빈 공간에 대해서는 convolution 연산을 하지 않으며 계산을 절약했다. point cloud을 그리드로 분할하고, 3D 커널과 conv연산을 위해 그리드 평균 점과 벡터로 각 그리드를 나타낸다. 이러한 접근 방식에서 커널 그 자체는 밀도있고, 높은 차원이다. 그러나 이 접근 방식은 계측적 특징을 학습하는 데 반복적으로 사용할 수 없다. 이러한 방법들과 비교하여 PointCNN은 입력 표현과 conv커널이 밀도가 희박하다.</p><p><br /></p><ul><li>Feature Learning from Irregular Domains</ul><p>3D 센싱의 급격한 발전에 따라 3D point cloud로부터 특징 학습의 발전이 더뎌졌다. PointNet과 Deep Sets는 입력에 대한 대칭 함수를 사용하면서 입력 순서 불변을 성공시켰다. 커널 상관성과 그래프 풀링은 PointNet과 같은 방법들을 개선시키기 위해 제안되기도 했다. RNN은 정렬된 point cloud 조각들로부터 풀링함으로써 특징들을 프로세싱하는데 사용되었다. 이 많은 대칭 풀링 기반의 방법들이 순서 불변성을 달성하는데 큰 도움을 주었지만 정보를 버리는 대가를 치뤄야만 했다. CNN 커널은 point cloud에 대한 CNN을 일반화하기 위해 이웃 점 위치에 대한 파라미터를 활용한 함수로 표현된다. 그 커널은 독립적으로 파라미터화된 각 점들과 연관이 있지만, 이 논문에서의 방법 안에 x-transformation는 각 이웃으로부터 학습되고, 그래서 지역 구조에 더 적응을 잘 할 수 있다. point cloud이외에 비규칙적인 도메인안의 희박한 데이터는 그래프나 그물망으로 표현되고, 몇몇 작업들은 이러한 표현들을 통해 피쳐 학습을 제안하기도 한다.</p><p><br /></p><ul><li>Invariance Vs Equivariance</ul><p>불변성을 달성하는 데 있어 풀링의 정보 손실 문제를 해결하기 위해 등분할을 목표로 하는 선구적인 방법들이 제안되기도 했다. 저자의 x-transformation은 이상적으로 등가성 실현이 가능하게 했고, 실제로도 효과를 볼 수 있다. 저자는 PointCNN과 SPN(Spatial Transformer Networks)와 유사함을 발견했다. 두 가지 모두 정규화를 진행할 때 명시적인 손실이나 제약없이 입력을 더 많이 처리하기 위해 잠재된 표준 형태로 “변환”하는 메커니즘을 제공한다는 점에서 유사하다. 실제로 이 방법은 네트워크가 학습 메커니즘을 더 잘 활용할 수 있는 방법이라는 것을 밝혀냈다. pointCNN에서 x-transformation은 가중치 부여와 순열을 제공하며, 이는 일반적인 행렬로서 모델화된다. 그래서 순열 행렬이 바람직한 출력인 모델과는 다르게 <strong>이중 확률 행렬</strong>에 의해 근사될 것이다.</p><p><br /></p><h1 id="3-pointcnn">3. PointCNN</h1><p>convolution의 계층적 적용은 CNN을 통한 계층적 표현을 학습하는데 필수적이다. PointCNN은 같은 디자인으로 설계되어 있고, 이를 point cloud로 일반화한다. 먼저, PointCNN에서의 계층적 convolution을 소개한 후, X-conv 연산자에 대해 자세하게 설명하고, 다양한 태스크에 적합한 PointCNN 아키텍쳐를 제시할 것이다.</p><h2 id="31-hierarchical-convolution">3.1 Hierarchical Convolution <a href="#31-hierarchical-convolution" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/autodriving/pointcnn/fig2.png" data-proofer-ignore></p><p>PointCNN의 계층적 convolution을 설명하기 전에, 위 그림과 같이 일반 그리드에 대한 설명을 먼저 보고자 한다. 그리드 기반의 CNN의 입력은 R1xR1xC1의 형태를 가진 F1 특징맵이다. 이 때, R1은 해상도, C1은 피쳐의 채널 깊이이다. 로컬 패치의 형태인 KxKxC1의 커널을 가진 F1과는 달리, KxKxC1xC2의 형태를 가진 convolution의 커널 K은 R2xR2xC2의 형태를 가진 F2의 특징 맵을 만든다. fig2에서 파라미터, R1 = 4, K = 2, R2 = 3이라 가정해보자. F1과 F2를 비교했을 때, 보통 R2 보다 R1의 해상도가 높고, 깊이는 C1보다 C2가 더 깊으며 F2가 F1보다 더 높은 레벨의 정보를 가지고 있다. 위의 그림처럼 4x4 -&gt; 3x3 -&gt; 2x2 의 방식으로 재귀적으로 해상도가 감소하고, 깊이는 더욱 깊어지면서 특징맵을 만들게 된다.</p><p>PointCNN의 입력은 다음과 같다.</p><p><img data-src="/assets/img/autodriving/pointcnn/input.png" data-proofer-ignore></p><p>이 때, 점은 {p1,i : p1, i ∈ R^dim}, 특징은 {f1,i : f1,i ∈ R^C1} 에 해당되고, 그리드 기반의 CNN의 계층적 구조에 따라 X-conv를 F1에 적용하여 높은 레벨의 표현 F2를 얻고자 한다. 이 때, F2는 다음과 같다.</p><p><img data-src="/assets/img/autodriving/pointcnn/input2.png" data-proofer-ignore></p><p>이 때, {p2,i}는 {p1,i}의 대표 점이고, F2는 F1보다 더 작은 해상도와, 깊은 특징 채널을 가진다. F1에서 F2로 변환되는 X-conv 프로세스를 재귀적으로 적용시키면 fig2 하단에서처럼, 입력 점들의 특징은 더 작은 수의 점들로(9 -&gt; 5 -&gt; 2) 투영되나, 특징들은 더 풍부해질 것이다.</p><p>저자는 분류 태스크에서 {p1,i}의 무작위 다운 샘플링을 적용하고, 분할 작업은 균일한 점 분포를 더 요구하기 때문에 분할 작업에서 가장 먼 점과 샘플링한다. 추가적으로 본 모델에서도 적용한 Deep Points와 같이 기하학적 처리에서 좋은 성능을 보여준 더 좋은 점 선택을 의심한다. 그래서 더 나은 대표 점 생성 방법에 대한 탐구를 추후에 진행할 예정이다.</p><p><br /></p><h2 id="32-x-conv-operator">3.2 X-conv Operator <a href="#32-x-conv-operator" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>X-conv는 F1에서 F2로 변환하는 핵심적인 연산이다. 여기서는, 입력과 출력, 연산의 절차를 먼저 설명하고, 연산의 근거를 설명하겠다.</p><p><img data-src="/assets/img/autodriving/pointcnn/xconvop.png" data-proofer-ignore></p><p>그리드 기반 CNN의 convolution과 유사하게 공간적-지역적 상관관계를 활용하기 위해 X-conv는 local 지역에서 작동한다. 출력 특징이 대표점{p2,i}와 연관되어야 하므로 X-conv는 {p1,i}의 인접한 점과 관련 특징을 입력으로 한다. 간편함을 위해 {p2,i}를 p로 두고, p에 대한 특징을 f, {p1,i}의 K neighbor을 N으로 두어, p에 대한 x-conv의 입력은 다음과 같이 정의된다.</p><p><img data-src="/assets/img/autodriving/pointcnn/xconvinput.png" data-proofer-ignore></p><p>이 때, S는 정렬되지 않은 상태다. S는 훈련 가능한 convolution 커널인 K에 대해 K x Dim의 행렬, P = (p1,p2,…,pk)^T 와 K x C1 행렬, F = (f1,f2,…,fk)^T로 표현될 수 있다. 이 입력을 통해 입력 특징들을 대표점 p로 투영한 특징 Fp를 계산할 수 있다.</p><p>위의 과정, 즉 algorithm 1을 요약하면 다음과 같다.</p><p><img data-src="/assets/img/autodriving/pointcnn/algorithmsum.png" data-proofer-ignore></p><p>PointNet에서처럼, MLPδ()는 각 점이 독립적으로 적용된 다층 퍼셉트론이다. x-conv에 포함된 모든 연산들, 예를 들어 <code class="language-plaintext highlighter-rouge">Conv(.,.)(= 행렬 곱 (.) x (.))</code> 와 <code class="language-plaintext highlighter-rouge">MLP(.)(= MLPδ())</code>은 미분이 가능하다. 따라서 x-conv도 미분이 가능하게 되고, 역전파를 통한 훈련을 위해 심층 네트워크에 연결할 수도 있게 된다.</p><p>위의 알고리즘 안에 4-6번째 줄은 섹션 1에서 설명했던 1b방정식에 해당되는 x-transformation의 핵심 코드이다. 그리고 이제, 알고리즘1 안에 1-3번째 줄에 대한 근거를 자세히 설명할 것이다. x-conv는 local점에서 작동하도록 설계되어 있고, 출력은 p와 p의 인접 점들의 절대 좌표와는 독립적이어야 하지만, 상대 좌표에는 의존적이어야 한다. 그러기 위해 fig3-b와 알고리즘1의 1번 줄은 대표 점에 지역 좌표계를 위치시킨다는 뜻이다. 공통된 특징들에 대한 인접 점의 지역 좌표계이고, 이것이 출력 피쳐들을 정의한다.</p><p>하지만, 지역 좌표계는 연관된 특징들과는 다른 차원과 표현이다. 이 문제를 해결하기 위해 알고리즘1의 2번줄에서 좌표를 더 높은 차원과 더 추상적인 표현에 대한 좌표로 만든다. 그리고 나서, 3번줄에서 더 나은 프로세싱을 위해 연관된 특징들과 이 좌표와 결합한다. 이에 대한 그림은 fig3-c에서 볼 수 있다.</p><p>MLP(.)를 통해 지역 좌표계를 형상들과 동일하게 만들 수 있다.</p><p><br /></p><h2 id="33-pointcnn-architectures">3.3 PointCNN Architectures <a href="#33-pointcnn-architectures" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>fig2를 통해 그리드 기반의 CNN에서의 Conv layer과 pointCNN에서의 X-conv의 차이점을 볼 수 있다. 그것은 2가지로 (1) 로컬 지역을 추출하는 방법 (KxK patches Vs 대표 점들 주변의 K neighboring points), (2) 그 로컬 지역으로부터 정보를 배우는 방법 (Conv Vs X-conv) 이다. 그러나 x-conv layer를 통해 심층 네트워크를 합치는 과정은 그리드 기반의 CNN과 유사하다.</p><p><img data-src="/assets/img/autodriving/pointcnn/fig4.png" data-proofer-ignore></p><p>위 그림은 2개의 X-conv layer를 가진 간단한 pointcnn의 구조를 나타낸 것이다. 여기서 N은 출력 대표점 개수, C는 특징 차원, K는 각 대표점마다의 인접 점 개수, D는 x-conv의 팽창률을 의미한다. 그리고, a와 b는 분류를 위한 아키텍처, c는 분할을 위한 아키텍처 구조이다.</p><p>K는 이웃 점 수를, N는 이전 layer에서의 점 개수에 대해 <code class="language-plaintext highlighter-rouge">K/N</code>비로서 각 대표 점의 수용 필드(receptive field)를 정의할 수 있다. 이를 전체적으로 적용하면 최종 점은 1.0의 수용 필드를 가지게 되는데, 이를 통해 전체 모양을 볼 수 있을 것이고, 형상에 대한 의미론적 학습에 도움이 된다. 그 후 마지막 x-conv뒤에 FC layer를 붙이고, 네트워크 훈련을 위한 loss를 구한다.</p><p>fig4-a를 보면, 차원과 점들의 갯수가 점차 줄어듬에 따라 상위 x-conv layer은 훈련 샘플이 급격히 줄어들게 되어 과적합 우려가 발생한다. 이를 해결하기 위해 fig4-b처럼 x-conv layer안에 더 많은 점이 남아 있을 수 있도록 빽빽한 연결이 있는 pointCNN을 제시한다. 하지만 네트워크의 깊이와 수용 필드의 성장률도 유지되어야 하기 때문에, 상위로 올라갈수록 대표 점들이 전체 형상에서의 점점 더 큰 비율을 차지하도록 한다. 그리드 기반의 CNN에서의 확대된 convolution을 사용하여 이를 제작했다. 입력으로 항상 K neighboring 점들을 받는 대신, 팽창률 D에 대한 K x D를 균일하게 샘플링한다. 이 경우 이웃 점의 갯수나 커널 사이즈를 키우지 않고도, 수용 필드가 K/N이 아닌 (K x D)/N로 증가시킬 수 있다.</p><p>fig4-b에서 두번째 x-conv layer을 보면 확장률 D = 2를 사용하여 1개가 아닌 4개의 대표점을 남길 수 있고, 이것들 모두를 예측하는데 사용할 수 있다. 이를 통해 상위의 x-conv layer를 더 철저하게 훈련시킬 수 있으며 더 많은 연결이 네트워크를 더 개선시킬 수 있다. 테스트 시에 다중의 대표 점 출력은 예측을 안정화시키기 위해 softmax바로 직전에 평균화한다.</p><p>분할 태스크에서는 더 높은 해상도의 점 단위 출력이 필요하므로 fig4-c에서처럼, 글로벌 정보, 즉 Conv에 사용된 점들을 고해상도 예측으로 추가로 전달하는 역할을 하는 DeConv가 있는 Conv-DeConv 구조를 사용했다. Conv와 Deconv는 같은 x-conv 연산을 사용한다.</p><p>과적합을 방지하기 위해 FC layer 이전에 dropout을 적용했다. 또한, “subvolume supervision”도 사용하여 과적합을 방지했다. 마지막 x-conv layer에서 수용 필드가 1보다 작도록 세팅되며, 대표 점에 의해 부분적인 정보만 볼 수 있도록 한다. 더 높은 성능을 위해 훈련시에는 부분 정보만 사용하는 방법을 사용하기도 한다.</p><ul><li>Data Augmentation</ul><p>x-conv의 파라미터를 훈련시키기 위해 특정 대표점에 대해 동일한 인접 점 집합을 동일한 순서로 계속 사용하는 것은 좋지 않다. 따라서 무작위 샘플링과 입력 점을 셔플하여 인접 점의 데이터와 순서가 매 반복마다 달라지도록 한다. 입력으로 N개의 점들을 받는 모델을 훈련시키고자 할 때, 가우시안 분포에 따른 <em>N</em>=(N,(N/8)^2)개의 점들이 훈련에 사용된다.</p><p><br /></p><h1 id="4-experiments">4. Experiments</h1><p>분류 작업에서는 총 6개의 데이터셋(ModelNet40, ScanNets,TC-Berlin, Quick Draw, MNIST, CIFAR 10)을 적용하여 pointcnn을 평가했고, 분할 작업에서는 총 3개의 데이터셋(ShapeNet Parts, S3DIS, ScanNet)을 적용하여 pointcnn을 평가했다.</p><h2 id="41-classification-and-segmentation-results">4.1 Classification and Segmentation Results <a href="#41-classification-and-segmentation-results" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/autodriving/pointcnn/table1.png" data-proofer-ignore></p><p>위의 그림은 ModelNet40과 ScanNet의 3D point cloud 분류 결과와 point cloud에 대한 몇몇의 신경망에 대한 비교를 요약한 것이다. ModelNet40의 3D 구조의 많은 부분이 수직 방향과 수평 방향에 미리 맞춰져 있다. 만약 무작위 수평 회전이 훈련이나 테스트에 적용되어 있지 않았다면 상대적으로 일관된 수평 방향만 활용되고, 이에 기반한 방법들은 수평 회전하는 모델과 직접적으로 비교할 수 없다. 그래서 ModelNet40과 ScanNet 둘다에 적용했는데, 두 데이터셋에서 모두 높은 성능을 보였다.</p><p>이제 분할 태스크에 대해 비교를 해볼 것이다.</p><p><img data-src="/assets/img/autodriving/pointcnn/table2.png" data-proofer-ignore></p><p>분할은 ShapeNet Parts, S3DIS, ScanNet 데이터셋에서 비교했고 그 결과는 위의 테이블에서 볼 수 있다. PointCNN은 분할에 특화되어 있는 최첨단 기술인 SSCN, SP-Graph, SGPN등을 포함하여 비교된 모든 방법들보다 뛰어났다.</p><p><img data-src="/assets/img/autodriving/pointcnn/table3.png" data-proofer-ignore></p><p>스케치는 2D 공간에서의 1D 곡선이므로 2D 이미지들보다 point cloud으로 더 효과적으로 표현될 수 있다. pointCNN을 TU-Berlin과 Quick Draw sketches에서 평가했고, 그 결과를 image CNN 기반의 방법들을 포함해 PointNet++와 성능을 비교하여 위의 테이블에 나타냈다. pointCNN은 두 데이터셋에서 PointNET++을 능가했다. TU-Berline 데이터셋에서, pointCNN은 일반적인 image CNN인 AlexNet보다 약간 더 좋은 성능을 보였지만, Sketch-a-Net과는 조금 떨어지는 모습을 보였다. 이는 Sketch-a-Net에서의 구조적 요소들이 현재의 pointCNN보다 뛰어남을 보여준다.</p><p>x-conv가 이상적으로 conv의 일반화 버전이라서 만약 기본 데이터가 같고 단지 다르게 표현된 것이라면, CNN과 성능이 비슷해야 한다. 이를 확인하기 위해 MNIST와 CIFAR10에서 point cloud의 표현에서의 pointCNN의 성능을 평가했다.</p><p><img data-src="/assets/img/autodriving/pointcnn/table4.png" data-proofer-ignore></p><p>MNIST 데이터에서 PointCNN은 다른 방법들과 비슷한 성능을 달성하여 숫자의 모양 정보에 대한 효과적인 학습을 보여줬다. 형상 정보가 대부분 없는 CIFAR10에서는 pointCNN은 RGB 특징에서의 공간적-지역적 상관관계에서 대부분 학습해야 했고, pointCNN과 유명한 image CNN과는 차이가 있긴 하나 꽤나 잘 수행했다. 이를 통해 일반 이미지에 대해서는 원래의 CNN들이 더 나은 선택이라는 결론이 도출된다.</p><p><br /></p><h2 id="42-ablation-experiments-and-visualizations">4.2 Ablation Experiments and Visualizations <a href="#42-ablation-experiments-and-visualizations" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Ablatino test of the core X-conv operator</ul><p>x-transformation의 효과를 확인하기 위해 알고리즘1에서 <code class="language-plaintext highlighter-rouge">Fp &lt;- Conv(K,F*)</code>인 4-6번 줄을 제거한 baseline을 제시하고자 한다. 이를 pointCNN과 비교하면 훈련 가능한 파라미터가 더 적고, 알고리즘1의 4번줄인 MLP(.)을 제거했기 때문에 더 얕다. 공정한 비교를 위해 wider/deeper인 W/D 그리고, X-conv 존재 여부로 나누어 비교하고자 한다. 저 둘은 대량 같은 양의 파라미터를 가진다. pointCNN w/o X (deeper)의 모델의 깊이는 pointCNN에서 MLP(.)의 제거에 대해서 영향을 받는다. 결과는 다음 테이블에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/pointcnn/table5.png" data-proofer-ignore></p><p>PointCNN은 다른 변형들보다 훨씬 뛰어났고, PointCNN과 PointCNN w/o X 의 차이는 모델의 파라미터 수나 모델의 깊이 때문이 아니다.</p><p><br /></p><ul><li>Visualization of X-Conv features</ul><p>각 대표 점은 이웃한 점들을 특정한 순서로 나타내며, 그에 해당하는 <code class="language-plaintext highlighter-rouge">F*</code>과 <code class="language-plaintext highlighter-rouge">C = Cδ + C1</code>에 해당하는 R^(KxC)안의 Fx를 가진다. 같은 대표 점에서 네트워크로 근처 점들을 다른 순서로 넣게되면, <em><code class="language-plaintext highlighter-rouge">F*</code></em>과 <em>Fx</em>를 갖게 된다. 유사하게 <code class="language-plaintext highlighter-rouge">F*</code>을 PointCNN w/o X에서는 <em>F0</em>로 정의한다. 분명한 것은, 입력 점들의 순서의 차이는 다른 <code class="language-plaintext highlighter-rouge">F*</code>을 나타내기 때문에, <em><code class="language-plaintext highlighter-rouge">F*</code></em>은 R^(KxC)공간 안에서 분산될 수 있다. 다른 한편, 학습된 X가 <code class="language-plaintext highlighter-rouge">F*</code>를 완벽하게 표현할 수 있다면 <em>Fx</em>는 공간에서 표준 점에 있어야 한다.</p><p>이를 확인하기 위해 ModelNet40 데이터셋에서의 15개의 무작위로 선택된 대표 점의 <em><code class="language-plaintext highlighter-rouge">F0</code></em>와 <em><code class="language-plaintext highlighter-rouge">F*</code></em>, <em><code class="language-plaintext highlighter-rouge">Fx</code></em>에 대한 T-SNE 시각화할 것이고, 이 결과는 아래 그림에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/pointcnn/fig5.png" data-proofer-ignore></p><p>fig5-a에서처럼 <em><code class="language-plaintext highlighter-rouge">F0</code></em>는 서로 다른 대표 점의 특징이 서로 차별적이지 않음을 나타내는 상당한 “불규칙”을 보인다. fig5-b에서처럼 <em><code class="language-plaintext highlighter-rouge">F*</code></em>은 <em><code class="language-plaintext highlighter-rouge">F0</code></em>보다 더 나은 살짝 모여있지만, fig5-c에서처럼 <em><code class="language-plaintext highlighter-rouge">Fx</code></em>는 X에 의해 집중되어 있음을 볼 수 있고, 따라서 각 대표 점들은 매우 큰 차별성을 보인다. 집중에 대한 정량적 기준을 고려할 때, 먼저 다른 대표점들의 중심 피쳐를 계산하고, 그 후 중심에서 가까운 정도를 기반으로 모든 특징 점을 그들이 속한 대표 점으로 분류한다. 분류 결과는 <em><code class="language-plaintext highlighter-rouge">F0</code></em>, <em><code class="language-plaintext highlighter-rouge">F0</code></em>, <em><code class="language-plaintext highlighter-rouge">Fx</code></em> 각각 76.83%, 89.29%, 94.72%의 분류 정확도를 가진다. 이 결과 한 점에 집중되지는 않지만, 피쳐 학습에서 pointCNN의 성능이 좋다는 것을 볼 수 있다.</p><p><br /></p><ul><li>Optimizer, model size, memory usage and timing</ul><p><img data-src="/assets/img/autodriving/pointcnn/table6.png" data-proofer-ignore></p><p>저자는 tensorflow, lr = 0.01의 ADAM optimizer을 사용했다. 위의 테이블에서 볼 수 있듯이 배치 사이즈 16, 1024개의 입력 점을 nVidia Tesla P100 GPU에서 다른 방법들과 분류 태스크에 대해 작업 시간을 비교했다. PointCNN은 이 셋팅에서 훈련/추론에 대해 각 배치마다 0.031/0.012sec의 시간을 달성했다. 게다가 4.4M개의 파라미터를 가진 2048개의 입력 점을 분할하는 모델에서 배치 사이즈 12, 훈련/추론에 대해 각 배치마다 0.61/0.25sec를 달성했다.</p><p><br /></p><h1 id="5-conclusion">5. Conclusion</h1><p>저자는 point cloud로 표현된 데이터에서 공간적-지역적 상관관계를 활용하도록 CNN을 일반화한 PointCNN을 제안했다. PointCNN의 핵심은 전형적인 convolution으로 작업되기 전에 입력 점과 특징들을 행렬 곱하고 가중치를 부여하는 X-conv 연산이다.</p><p>오픈 소스는 <a href="https://github.com/yangyanli/PointCNN">이 페이지</a>에 올려두었다.</p><p>코드에 대한 리뷰는 <a href="https://dkssud8150.github.io/classlog/pointcnn2.html">다음 페이지</a>에서 진행할 것이다.</p><p><br /></p><h1 id="reference">Reference</h1><ul><li><a href="https://proceedings.neurips.cc/paper/2018/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf">PointCNN: Convolution On X -Transformed Points</a></ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/pointcnn/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/pointcnn.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/review/'>Review</a>, <a href='/categories/autonomous-driving/'>Autonomous Driving</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/autonomous-driving/" class="post-tag no-text-decoration" >Autonomous Driving</a> <a href="/tags/pointcnn/" class="post-tag no-text-decoration" >PointCNN</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[논문 리뷰] PointCNN: Convolution on X-transformed points - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/pointcnn/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[논문 리뷰] PointCNN: Convolution on X-transformed points - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/pointcnn/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/pointcnn/&amp;text=[논문 리뷰] PointCNN: Convolution on X-transformed points - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/pointcnn/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/pointcnn2/"><div class="card-body"> <em class="timeago small" date="2022-01-23 13:00:00 +0900" >Jan 23, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 코드 구현] PointCNN: Convolution on X-transformed points</h3><div class="text-muted small"><p> pointCNN 깃허브에 올라와 있는 코드를 직접 실행하고 리뷰한 내용입니다. 모든 코드는 colab에서 진행하였습니다. 논문에 대한 리뷰가 궁금하신 분들은 이 링크를 참고해주세요. PointCNN 깃허브 주소: https://github.com/yangyanli/PointCNN Classification ModelNet40 먼저 분류 모...</p></div></div></a></div><div class="card"> <a href="/posts/monocular/"><div class="card-body"> <em class="timeago small" date="2022-01-25 23:53:00 +0900" >Jan 25, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] Monocular 3D Object Detection for Autonomous Driving</h3><div class="text-muted small"><p> Monocular 3D Object Detection for Autonomous Driving 논문에 대한 리뷰입니다. 이 논문은 2016 CVPR에 투고된 논문이며 약 669회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstract 본 논문은 자율주행에서 단안 ...</p></div></div></a></div><div class="card"> <a href="/posts/pointrcnn/"><div class="card-body"> <em class="timeago small" date="2022-02-02 01:17:00 +0900" >Feb 2, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</h3><div class="text-muted small"><p> PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstrac...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/voxelnet/" class="btn btn-outline-primary" prompt="Older"><p>[논문 리뷰] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</p></a> <a href="/posts/pointcnn2/" class="btn btn-outline-primary" prompt="Newer"><p>[논문 코드 구현] PointCNN: Convolution on X-transformed points</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>