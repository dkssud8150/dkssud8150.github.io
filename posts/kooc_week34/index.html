<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 3. Motivation and Basics" /><meta property="og:description" content="Chapter 3. Motivation and Basics" /><link rel="canonical" href="https://dkssud8150.github.io/posts/kooc_week34/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/kooc_week34/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-29T01:40:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-10-10T18:24:35+09:00","datePublished":"2022-09-29T01:40:00+09:00","description":"Chapter 3. Motivation and Basics","headline":"[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/kooc_week34/"},"url":"https://dkssud8150.github.io/posts/kooc_week34/"}</script><title>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression </span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-09-29 01:40:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Thu, Sep 29, 2022, 1:40 AM +0900" >Sep 29, 2022</em> </span> <span> Updated <em class="timeago" date="2022-10-10 18:24:35 +0900 " data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 10, 2022, 6:24 PM +0900" >Oct 10, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5380 words"> <em>29 min</em> read</span></div></div></div><div class="post-content"><h1 id="chapter-3-motivation-and-basics">Chapter 3. Motivation and Basics</h1><ul><li>목차</ul><ol><li>optimal classification<ul><li>optimal predictor<li>concept of Bayes risk<li>concept of desicion boundary</ul><li>Naive Bayes Classifier<ul><li>what is classifier?<li>Bayesian version of linear classifier<li>naive assumption</ul><li>naive classifier of text mining<ul><li>bag-of-words</ul></ol><p> </p><p> </p><h2 id="3-1-decision-boundary">3-1. Decision Boundary <a href="#3-1-decision-boundary" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week34/optimal_classification.png" data-proofer-ignore></p><p>optimal predictor 이란 이때까지 해왔던 f(X), 즉 $ \hat{f} $ 이 실제 Y와 같지 않을 확률이 최소가 되는 것을 f* 라는 것이다. 즉 우리가 가정한 값이 틀릴 확률이 최소가 되도록 하고자 한다. 이는 우리가 가정한 값이 정답일 확률이 최대가 되도록 한다는 것과 동일하다.</p><p> </p><p><img data-src="/assets/img/kooc/week34/bayes_risk.png" data-proofer-ignore></p><p>여기 점선과, 실선 총 4개의 값에 대한 그래프가 있다고 해보자. 이 때, 특정 X에 대해 점선은 점선끼리 더해서 1이 되어야 할 것이고, 실선은 실선끼리 더해서 1이 되어야 할 것이다. 이 둘 중 어느 것이 더 나은 것인지에 대한 측정을 해보아야 한다.4개의 선들이 모두 만나는 지점을 Xm이라 했을 때, 이 Xm보다 조금 앞인 Xn의 위치에서 보면 점선의 경우 초록색과 빨간색이 차이가 별로 크지 않다. 그러나 실선의 경우 점선보다는 다소 크다. 즉, 점선의 경우 명확하게 맞는지 틀린지를 구분하게 해주지 못한다는 뜻이다. Xm의 앞부분을 보면 초록색이 true라 판단하므로 빨간색 아래의 영역은 error에 해당된다. 그래서 점선의 빨간색 아랫부분과 실선의 빨간색 아랫부분의 차이가 두 모델의 차이이자 Bayes Risk라 불리는 값이 된다.</p><p>따라서 Bayes optimal classifier 라는 것은 Bayes risk라는 값의 크기를 줄이는 최적화를 수행하는 과정을 나타낸다.</p><p> </p><p><img data-src="/assets/img/kooc/week34/optimal_classifier.png" data-proofer-ignore></p><p>이 때, P(Y = y | X = x ) 를 prior 정보를 조금 더 활용할 수 있도록 Bayes theorem을 통해 변환할 수 있다. Prior 정보를 알아낼 때는 MLE나 MAP 등의 방법으로 데이터셋에서 추출하여 얻을 수 있다. 그리고 class conditional density = P(X = x | Y = y) 는 Y를 먼저 특정한 후, 그곳에서 x가 나올 확률을 구할 수 있다. 그러나 이러한 방식은 x의 feature가 1개 이상이고, 서로 상호적이라면 density를 추정하기 어려울 것이다.</p><p> </p><h2 id="3-2-naive-bayes-classifier">3-2. Naive Bayes Classifier <a href="#3-2-naive-bayes-classifier" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>따라서, 이러한 상호적인 관계를 무시하겠다는 것이 Naive Bayes Classifier이다. 각각의 feature들이 독립적이라는 가정하에 확률을 구해 추정한다.</p><p><img data-src="/assets/img/kooc/week34/weather_dataset.png" data-proofer-ignore></p><p>지난 번에 배웠던 이 데이터를 가지고, 함수를 최적화해보자. 각 x의 feature인 x1,x2,x3,x4,x5,x6가 있고, label인 y=yes가 존재한다. 이 때, prior는 y에 대해서 구하는 것이므로 yes에 대한 prior는 3/4가 된다. 즉, prior를 구하기 위해서 필요한 파라미터는 k-1(이 때 k=2), 1개이다. 그 후, P(X=x|Y=y) 를 구해야 하는데, 이를 구하기 위해서 고려해야할 파라미터의 개수는 각 feature마다 2가지의 선택지만 존재한다고 해도 64개의 값이 더해지면 1이 되어야 한다는 특성으로 이해 63개의 값만 구하면 된다. 그리고 y의 값도 고려해줘야 하므로 (2^d - 1)^k 개의 파라미터가 필요하다. 2개의 선택지만 가정한다 해도 (64-1)*2 = 126 개의 파라미터가 필요해진다.</p><p> </p><p>우리가 직접 모델을 추정하기 위해서는 (2^d - 1)^k 를 잘 해결해야 간편하게 모델을 추정할 수 있다. 이를 위해 conditional independence, 즉 각 feature 별 독립을 가정하여 연산을 줄이고자 한다.</p><p><img data-src="/assets/img/kooc/week34/marginal_independence.png" data-proofer-ignore></p><p>예를 들어, 상사인 commander가 한명 있고, 2명의 officer가 있다고 할 때, 만약 A가 명령을 듣지 못한다해도 B가 수행하는 행동에 따라서 행동할 수 있을 것이다. 이는 독립적이지 않다는 의미가 된다. 그러나 만약 B가 어떻게 행동하든지 상사의 명령을 무조건 들을 수 있어서 따르게 된다면 B의 행동에 영향을 받지 않는다. 이럴 때는 독립적이게 된다. marginal independence라는 것은 상사가 어떻게 말하든 듣지 않고, 행동을 하는 상황을 일컫는다.</p><p> </p><p><img data-src="/assets/img/kooc/week34/conditional_independence.png" data-proofer-ignore></p><p>아까의 데이터셋으로 돌아와 P(X=x|Y=y)를 구하기 위해서는 (2^d -1)^k 개의 파라미터가 필요했다. 그러나 우리가 독립적이라는 것을 가정하게 되면 $ P(X=x|Y=y) = P(X)P(Y) $ 로 계산이 가능하다. 그렇게 되면, 개별 feature들 간 곱셈으로 변하게 되어, (2-1)dk 개의 파라미터 수로 줄어든다.</p><p> </p><p>최종적으로 Naive Bayes Classifier 에 의해 구해진 f는 다음과 같다.</p>\[f_{NB}(x) = argmax_{Y=y} P(Y=y) \prod_{1 \leq i \leq d} P(X_i = x_i | Y = y)\]<p> </p><p>naive bayes classifier는 생성하기 쉽다. 확률만 MLE, MAP를 통해 잘 구해줄 수 있다면 구하기 쉽다. 그러나 그만큼 문제점은 많이 존재한다. 각 feature간이 독립적이라는 것 자체가 부정확한 가정이며, 값이 존재하지 않은 feature가 있다면 0으로 수렴하는 값이 추론될 수 있게 될 것이다.</p><p> </p><p> </p><h1 id="chapter-4-logistic-regression">Chapter 4. Logistic Regression</h1><ul><li>목차</ul><ol><li>Logistic Regression Classifier<ul><li>why the logistic regression is better than linear regression<li>logistic function<li>logistic regression classifier<li>approximation approach for open form solutions</ul><li>Gradient Descent Algorithm<ul><li>tailor expansion<li>gradient descent / ascent algorithm</ul><li>difference between Naive Bayes and logistic regression<ul><li>similarity of two classifier<li>difference of two classifier<li>performance of two classifier</ul></ol><p> </p><p> </p><h2 id="4-1-logistic-regression">4-1. Logistic Regression <a href="#4-1-logistic-regression" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/kooc/week34/optimal_classification.png" data-proofer-ignore></p><p>이러한 그림에서 선들이 겹치는 구간인 Xm에서 결정, desicion이 변화하므로 이 지점을 desicion boundary라 부른다. 점선의 경우 1차원적으로 증가하지만, 실선의 경우 S 커브 형태를 가진다. 이 S커브를 <code class="language-plaintext highlighter-rouge">sigmoid function</code> 이라 부른다. 이러한 sigmoid function은 확률이란 것이 0~1로 값이 매칭되어야 하고, tail 부분에서 거의 일정한 경향이 있으므로 최적화하기 쉽다. 또한, sigmoid function은 desicion boundary의 근처에서 급격하게 변화하는 모습을 보인다.</p><p> </p><p>이전에 사용했던 신용평가 데이터셋을 사용하여 Sigmoid function을 만드는 방법을 설명하고자 한다.</p><p><img data-src="/assets/img/kooc/week34/classification.png" data-proofer-ignore></p><p>A15라는 feature를 사용하여 예측을 했을 때, 왼쪽 그래프의 형태가 된다. x축은 X=A15인 feature이고, y축은 예측 클래스인 0 또는 1이다. 왼쪽 그래프를 보면, 다소 작은 값의 feature를 가진 데이터들은 0, 예외도 존재하긴 하나 대체로 큰 feature 값을 가진 데이터는 1로 구성되어 있는 것을 볼 수 있다.</p><p>조금 더 자세하게 보기 위해 x를 log에 씌워 관찰해본다. log를 씌운 그래프는 오른쪽 그래프가 되고, log를 씌운다는 것은 급격한 변화를 감소시켜 자세하게 보고자 한다는 것이다. 오른쪽 그래프를 볼 때, 방금 가설을 세웠던 작은값들은 0, 다소 큰 값은 1이라는 것이 더 확실하게 바라볼 수 있다. 이 때, 어떤 지점에 Desicion boundary를 그어야 잘 분류할 수 있는지 확인해볼 필요가 있다.</p><p> </p><p><img data-src="/assets/img/kooc/week34/linear_and_logistic.png" data-proofer-ignore></p><p>이러한 A15에 대한 데이터를 linear function으로 투영한 결과가 빨간색 점인데, 이 값들은 0~1을 넘어서는 결과가 되므로 잘못된 함수가 된다. 그리고는 sigmoid function의 한 종류인 logistic function을 사용하여 fitting 해보고자 한다. 그 결과가 왼쪽 그래프의 초록색 그래프이다.</p><p>이 경우는 decision boundary가 매우 앞에 위치하고 있다. 이를 더 자세하게 보기 위해 log를 씌워서 본다. 이 때, 보면 linear regression도 지수함수적인 그래프로 구성된다. 그리고 초록색 그래프의 경우 급격한 변화가 자세하게 보인다.</p><p>linear regression의 경우 y값이 0.5를 지나는 것이 decision boundary가 되므로 그 선이 그어져 있고, logistic regression의 경우 급격한 변화가 생기는 부분인 y값이 0.5를 지나는 부분이 decision boundary이므로 그 선이 마찬가지로 그어져 있다. 회색 선을 확인해봤을 때, logistic regression에 대한 decision boundary가 더 분류가 잘 될 것이다.</p><p> </p><p> </p><p>이제 logisitic function에 대해 자세하게 살펴보자.</p><p><img data-src="/assets/img/kooc/week34/logistic_function.png" data-proofer-ignore></p><p>먼저 sigmoid function은 x가 $ -\infty \sim \infty $ 의 범위를 가진다. 그리고 항상 증가하는 형태를 가져야 한다. tanh(x) 나 argtan(x) 가 sigmoid function의 종류 중 하나이다.</p><p>그리고, logistic function은 sigmoid 와 비슷한 형태를 가지는데, y의 범위가 sigmoid는 -1~1이지만, logistic의 경우 0~1의 y범위를 가진다. 함수는 다음과 같이 정의된다.</p>\[f(x) = \cfrac{1}{1+e^{-x}}\]<p> </p><p>그리고, 이 logistic function을 역함수 한 것을 logit function이라 부르고, 그 형태는 오른쪽 그래프와 같다.</p><p> </p><p><img data-src="/assets/img/kooc/week34/logit_function_fit.png" data-proofer-ignore></p><p>logit function을 통해 A15에 대한 데이터를 fitting 해보면, 다음과 같다. logit function은 $ f(x) = log(\cfrac{x}{1-x}) $ 의 형태를 가지는데, 이를 x를 p와 fitting하고, y를 A15라는 feature에 fitting을 하게 되면 $ x = log(\cfrac{y}{1-y}) $ 가 된다. 오른쪽 그래프는 A15가 x축, p가 y축이다.</p><p>그 후, 이 function을 원래 데이터에 맞게 rescaling과 translation을 해야 하므로, recale에 대한 변수를 <code class="language-plaintext highlighter-rouge">a</code>, translation에 대한 변수를 <code class="language-plaintext highlighter-rouge">b</code>라 지정하여 $ ax + b = log(\cfrac{p}{1-p}) $ 로 나타낼 수 있다.</p><p>또한, linear regression에서 수행했던 matrix로의 변환인 $ X\theta $ 와 같이 ax + b도 1차함수형태이므로 이를 matrix형태인 $ X\theta $ 로 변환할 수 있다. p는 P(Y|X) 즉, X가 주어진 상황에서 Y의 확률인데, 이를 logit 함수의 형태로 regression을 한다하여 logistic regression이라 한다. 따라서 최종적인 형태는 다음과 같다.</p>\[X\theta = log(\cfrac{P(Y|X)}{1-P(Y|X)})\]<p> </p><p>logistic regression은 binomial or multinomial outcome을 예측하는 확률적인 classifier이다. 이러한 상황에서 bernoulli experiment에 logistic function을 사용해보자.</p><p>$ P(y|x) = \mu(x)^y (1-\mu(x))^{1-y} $ 인 상황에서 y가 1인 상황일 때의 $ \mu $ 를 logistic function 인 $ \mu(x) = \cfrac{1}{1 + e^{-\theta^Tx}} $ 로 변환할 수 있다. 이를 위의 식인 $ X\theta $ 에 대입하여 정리해보면 다음과 같다.</p>\[P(Y|X) = \cfrac{e^{X\theta}}{1+e^{X\theta}}\]<p>supervised learning인 상황에서 X, Y는 값을 알고 있으니 최적화해야 하는 부분은 theta가 된다.</p><p> </p><p>기존에 사용했던 MLE 개념을 차용할 것인데, 예전에 사용했던 $ \hat{\theta} = argmax_{\theta}P(D|\theta) $ 는 D, supervised learning 개념이 들어가 있지 않은 간편화된 데이터셋에 대한 식이었다. 이를 더 고도화하여 condition에 따른 MLE, 즉 Maximum Conditional Likelihood Estimation(MCLE)를 구한다. 동일하게 MLE를 구하지만, 주어진 feature이 존재하는 상황에서 클래스에 대한 확률을 구한다.</p><p><img data-src="/assets/img/kooc/week34/find_theta.png" data-proofer-ignore></p><p>동일하게 log를 취하여 연산을 하고, $ log(\prod_{1 \leq i \leq N}) = \sum_{1 \leq i \leq N} $ 로 바꿀 수 있다. 그리고 P에 대해서도 이전에 사용했던 방식을 통해 구체화시킨다. $ P(Y_i | X_i;\theta) = \mu(X_i)^{Y_i} (1-\mu(X_i))^{1-Y_i} $ 로 될 것이고, 여기에 log를 씌우면 간단하게 식을 세울 수 있다.</p><p>이 때, $ log(\cfrac{\mu(X_i)}{1-\mu(X_i)}) $ 가 이전에 사용했던 $ X\theta $ 에 대한 식과 동일한 형태이므로 이를 치환한다. 이 때 X_i에 대한 값이므로 $ X_i \theta $ 로 치환한다. 또한, $ \mu(x) = \cfrac{e^{X\theta}}{1+e^{X\theta}} $ 이므로 이 식을 사용하여 $ log(1-\mu(X_i)) $ 도 치환한다. 최종적으로 theta_hat은 다음과 같다.</p>\[\hat{\theta} = argmax_{\theta} \sum_{1 \leq i \leq N} {Y_iX_i\theta - log(1 + e^{X_i\theta})}\]<p> </p><p><img data-src="/assets/img/kooc/week34/find_theta_contd.png" data-proofer-ignore></p><p>최대가 되는 theta를 찾기 위해 개별 theta(theta_j)에 대해 미분을 한다. $ Y_iX_i\theta $를 theta_j에 대해 미분하면, theta_j에 해당하는 값 이외의 theta_1,theta_2…는 모두 사라지게 될 것이다. 그러면 결국 j에 대한 값만 남게 되므로 $ Y_iX_{i,j} $ 만 남게 된다. 그 후 X_(i,j) 에 대해 묶어줄 수 있고, 그에 따라 생겨나는 식은 또 다시 P(y=1|x) 에 대한 것으로 변환해줄 수 있게 된다. 그러나 이렇게 변환하면 간단하게 업데이트를 할 수 없는 open form 형태가 되므로 근사를 취해야만 한다.</p><p> </p><h2 id="4-2-gradient-method">4-2. Gradient method <a href="#4-2-gradient-method" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>근사를 취하기 위한 방법으로는 gradient를 활용하는 것이다. gradient method를 사용할 때의 가장 기초는 <code class="language-plaintext highlighter-rouge">Taylor Expansion</code> 이다. Taylor series는 함수를 무한대의 함수의 합으로 근사화할 수 있다는 것이다.</p>\[f(x) = f(a) + \cfrac{f'(a)}{1!}(x-a) + \cfrac{f''(a)}{2}(x-a)^2 + \cdots = \sum_{n=0}^\infty \cfrac{f^{(n)}(a)}{n!}(x-a)^n\]<p> </p><p><img data-src="/assets/img/kooc/week34/gradient_method.png" data-proofer-ignore></p><p>이러한 taylor expansion을 활용하여 gradient descent/ascent algorithm을 구현해본다. gradient method는 함수 f(x)와 첫 시작 파라미터인 x1이 주어졌을 때, 올바른 방향으로 가기 위해 f(x)의 값을 높이거나/줄이거나하는 파라미터를 반복적으로 움직여주는 기법이다. 따라서 이 기법에 필요한 값들은 방향/속도이다. 얼마나 빠르게 이동하던 상관없이 방향이 맞으면 언젠가는 정답에 가까워지게 되므로 더 중요한 것은 방향이 된다.</p><p> </p><p>Big-Oh Notation 을 활용하여 taylor expansion을 정의하면 $ f(x) = f(a) + \cfrac{f’(a)}{1!}(x-a) + O(||x-a||^2) $ 이 된다. 이 때, a=x1(initial parameter), x = x1 + hu (h=speed, u=unit direction)을 통해 x를 업데이트를 한다고 하면</p><p>$ f(x_1 + hu) = f(x_1) + hf’(x_1)u + h^2O(1) $ 의 식이 된다. 이 때, h가 작은 값이라는 가정하에 O(1)는 상수이므로 근사를 통해 제거하고, f(x1)를 좌항으로 넘긴다.</p><p>이 때, f(x)를 증가시킬수도 있고, 감수시킬수도 있다. $ argmin_u {f(x1 + hu) - f(x1)} $ 이란, 즉 증가량을 최소화하는 u를 찾는 것이고, 이는 기울기가 감소되는 방향(gradient descent)으로 진행됨을 나타낸다. 그리고 이 식은 이미 위에서 구현을 했다. u는 감소하는 unit vector이므로 $ u = - \cfrac{f’(x_1)}{|f’(x_1)|} $ 로 나타낼 수 있다.</p><p>그렇다면, 처음 값 x1 이 아닌 특정 x_t에 대한 업데이트는 다음과 같다.</p>\[x_{t+1} = x_t + hu* = x_t - h \cfrac{f'(x_1)}{f(x_1)}\]<p>여기서 unit vector에 -를 +로 바꿔주면 gradient ascent로 될 것이다.</p><p> </p><p><img data-src="/assets/img/kooc/week34/gradient_descent_example.png" data-proofer-ignore></p><p>이때까지는 이론을 배웠으니, 실제 적용 사례를 살펴보자. f(x1,x2)에 대한 함수가 존재하고, 이에 대해 미분을 하면 (1,1)의 지점에서 최소값이라는 것이 구해질 것이다. 단순하게 생각해봐도 f(x1,x2)에서 오른쪽 텀에 있는 값들이 모두 square(제곱)처리가 되어 있으므로 항상 양수가 나오게 될 것이다. 그러면 이 function에서의 최소값은 당연히 0이 되어야 하므로 쉽게 (1,1)이라는 지점을 찾아낼 수 있다.</p><p>이 함수에 단순한 미분이 아닌 gradient descent method를 사용해보자.</p><p>순서</p><ol><li>choose random initial point<ul><li>$ x_0 = (x_1^0, x_2^0) = (-1.3, 0.9) $</ul><li>partial derivative vector at the point<ul><li>$ f’(x^0) = (\cfrac{\partial}{\partial x_1} f(x_1, x_2) , \cfrac{\partial}{\partial x2} f(x_1, x_2)) = (-415.4, -158) $</ul><li>select speed, h<ul><li>h = 0.001</ul><li>update the point<ul><li>$ x^1 = x^0 - h \cfrac{f’(x^0)}{|f’(x^0)|} = (-1.2991, 0.9004) $</ul><li>iteration 2~4 for x^i</ol><p> </p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="c1">#!/usr/bin/env bash
# gradient descent code make soon.
# 코드 추가 예정.
</span></pre></table></code></div></div><p> </p><p> </p><p>자 그러면, 이제 gradient ascent method와 logistic regression을 결합하면 어떻게 될까?</p><p><img data-src="/assets/img/kooc/week34/gradient_logisitic_regression.png" data-proofer-ignore></p><p>gradient ascent에 대한 식이 argmax 형태로 나타나 있다. 이 때 argmax부분의 안에 들어있는 식을 f(theta) 라 했을 때, 각각의 theta_j에 대해 편미분을 수행한다. 수행한 결과는 gradient method를 배우기 전에 구해보았다.</p><p>그리고, gradient method에서의 업데이트 식은 $ \theta_{t+1} = \theta_t + h u* = \theta_t + h \cfrac{f’(\theta_t)}{|f’(\theta_t)|} $ 가 되는데, 여기 u*를 logistic function을 적용하면, theta_j에 대한 업데이트 식은 다음과 같이 구성된다.</p><div class="table-wrapper"><table><tbody><tr><td>$$ \theta_j^{t+1} = \theta_j^t + h \cfrac{\partial f(\theta^t)}{\partial \theta_j^t} = \theta_j^t + h{\sum_{1 \leq i \leq N}X_{i,j}(Y_i - P(Y=1<td>X_i;\theta^t))}</table></div><p>그리고, 이 때 ,P(y=1|x)를 치환할 수 있으므로,</p>\[\theta^t + \cfrac{h}{C}{\sum_{1 \leq i \leq N} X_{i,j}(Y_i - \cfrac{e^{X_i \theta^t}}{1 + e^{X_i \theta^t}})}\]<p>이 때 ,C 는 unit vector로의 정규화 상수이다.</p><p> </p><p> </p><h2 id="4-3-difference-between-naive-bayes-and-logistic-regression">4-3. difference between naive bayes and logistic regression <a href="#4-3-difference-between-naive-bayes-and-logistic-regression" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Gaussian Naive Bayes</ul><p>기존에 배웠던 Naive Bayes 방식은 단순히 2개의 class에 대한 classifier이었다. 그러나 logistic regression은 continuous feature에 대해 다뤄지므로 이 둘을 비교하기 위해 Naive Bayes를 gaussian distribution을 따른다는 가정하에 continuous 한 function으로 만들어줄 수 있다. 기존의 Navie Bayes classifier function 은 $ f_{NB}(x) = argmax_{Y=y}P(Y=y) \prod_{1 \leq i \leq d}P(X_i = x_i | Y = y) $ 로 정의된다. 이를 평균이 $ \mu $, 분산이 $ \rho^2 $ 인 gaussian distribution에 대해 정의하면,</p>\[P(X_i | Y, \mu, \rho^2) = \cfrac{1}{\rho \sqrt{2 \pi}}e^{-\cfrac{(X_i - \mu)^2}{2 \rho^2}}\]<p>그리고 prior, P(Y = y)는 MLE 또는 MAP를 통해 구해줄 수 있으므로 상수 $ \pi_1 $ 로 나타낼 수 있다.</p><p>따라서 Gaussian Naive Bayes function은 Gaussian distribution을 feature 개수만큼 곱한 $ P(Y)\prod_{1 \leq i \leq d}P(X_i|Y) = \pi_k \prod_{1 \leq i \leq d} \cfrac{1}{\rho_k^iC} e^{-\frac{1}{2}(\frac{X_i - \mu_k^i}{\rho_k^i})^2} $ 가 된다.</p><p>이 때, C는 sqrt(2pi)_i 를 feature개수인 d개를 모두 곱한 값을 나타낸다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week34/derivation_logistic_regression1.png" data-proofer-ignore></p><p><img data-src="/assets/img/kooc/week34/derivation_logistic_regression2.png" data-proofer-ignore></p><p><img data-src="/assets/img/kooc/week34/derivation_logistic_regression3.png" data-proofer-ignore></p><p><img data-src="/assets/img/kooc/week34/derivation_logistic_regression4.png" data-proofer-ignore></p><p>이제, Gaussian Naive bayes function과 logistic regression이 같아질 수 있는가를 증명해본다. naive Bayes assumption에서 $ P(Y=y|X) = \cfrac{P(X|Y=y)P(Y=y)}{P(X)} $ 인데, 여기서 logistic regression 의 경우 좌항에서 바로 치환하여 연산을 했고, Gaussian Naive Bayes 모델의 경우 우항으로 변환 후 연산하여 function을 구했다. 그래서 이 둘의 관계가 정확하게 어떻게 구현될지 정의해볼 것이다. GNB(Gaussian Naive Bayes), $ \cfrac{P(X|Y=y)P(Y=y)}{P(X)} = \cfrac{P(X|Y=y)P(Y=y)}{P(X|Y=y)P(Y=y) + P(X|Y=n)P(Y=n)} $ 으로 정의할 수 있다. 이 때, n은 Y에 포함되어 있지 않은 X들을 의미한다.</p><p>여기서, 이 P(X|Y=y)는 feature의 개수에 따라 파라미터가 기하급수적으로 증가하게 되므로, feature에 대해 각각을 표현해주기 위해 $ \prod $ 형태로 표현한다. 그 후, P(Xi|Y=y) 가 gaussian distribution을 따른다고 가정했기 때문에, P(Y=y|X)는 다음과 같이 정의된다.</p>\[P(Y=y|X) = \frac{\pi_1 \prod_{1 \leq i \leq d} \frac{1}{\rho_1^iC} e^{-\frac{1}{2}(\frac{X_i - \mu_1^i}{\rho_1^i})^2}}{\pi_1 \prod_{1 \leq i \leq d} \frac{1}{\rho_i^iC} e^{-\frac{1}{2}(\frac{X_i - \mu_1^i}{\rho_1^i})^2} + \pi_2 \prod_{1 \leq i \leq d} \cfrac{1}{\rho_2^iC} e^{-\frac{1}{2}(\frac{X_i - \mu_2^i}{\rho_2^i})^2}} = \frac{1}{1+\frac{\pi_2 \prod_{1 \leq i \leq d} \frac{1}{\rho_2^iC} e^{-\frac{1}{2}(\frac{X_i - \mu_2^i}{\rho_2^i})^2}}{\pi_1 \prod_{1 \leq i \leq d} \frac{1}{\rho_1^iC} e^{-\frac{1}{2}(\frac{X_i - \mu_1^i}{\rho_1^i})^2}}}\]<p> </p><p>이 때, $\rho_2^i = \rho_1^i $ 라 가정하면,</p>\[P(Y=y|X) = \cfrac{1}{1+e^{-\cfrac{1}{2(\rho_1^i)^2}\sum_{1\leq i \leq d}{2(\rho_2^i - \rho_1^i)X_i + {\mu_1^i}^2 - {\mu_2^i}^2} + log\pi_2 - log\pi_1}}\]<p>이 때, sumation 안에 있는 $ 2(\mu_2^i - \mu_1^i)X_i $ 가 linear regression에서 표현되는 $ X\theta $ 와 같은 형태이다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week34/naive_vs_logit.png" data-proofer-ignore></p><p>최종적으로, 우리가 Naive Bayes classifier를 구하기 위해 가정한 것들에는 여러 가지가 있다.</p><ul><li>$ P(Y=y|X) = \cfrac{1}{1+e^{-\cfrac{1}{2(\rho_1^i)^2}\sum_{1\leq i \leq d}{2(\rho_2^i - \rho_1^i)X_i + {\mu_1^i}^2 - {\mu_2^i}^2} + log\pi_2 - log\pi_1}} $<li>Assumption<ul><li>Naive Bayes assumption<li>Gaussian distribution for P(X|Y)<li>Bernoulli distribution for P(Y)</ul><li>num of parameter : 4d + 1</ul><p>Naive Bayes classifier 식에 있는 파라미터는 총 각 feature 마다의 variance 2개, mean 2개씩이므로 4d, 그리고 prior 1개이다. prior는 binomial 인 상황에서 true에 대한 확률을 알고 있으면 false는 구할 수 있으므로 1개이다.</p><p> </p><p>그에 반해 Logistic Regression은 가정한 것이 1개 뿐이다.</p><ul><li>$ P(Y|X) = \cfrac{1}{1+e^{-\theta^Tx}} $<li>Assumption<ul><li>fitting to logistic function</ul><li>num of parameter : d + 1</ul><p>그리고, 파라미터도 개별 feature에 대한 theta와 bias term 이므로 d+1개 뿐이다.</p><p> </p><p>단편적으로 파라미터의 개수만 봤을 때는 logistic regression이 좋지만, 우리가 조정할 수 있는 값이 많다는 자유도 측면에서 봤을 때는 naive bayes가 더 좋다. 따라서 개인의 task에 맞는 모델을 사용해야 할 것이다.</p><p> </p><p> </p><p><img data-src="/assets/img/kooc/week34/gernerative_discriminative_pair.png" data-proofer-ignore></p><ul><li>Generative model</ul><p>Naive Bayes classifier 처럼, P(Y|X)를 P(X|Y)P(Y)/P(X) 로 정의하여 추론하는 방법을 Generative model이라 한다. prior정보를 활용할 수 있고, Bayesian 형태이며, joint probability 에 대해 모델링한다는 점이 장점이다.</p><p> </p><ul><li>Discriminative model</ul><p>Logistic Regression 처럼, P(Y|X)를 곧바로 추론하는 방법을 Discriminative model 이라 한다. 이는 conditional probability 에 대해 모델링하는 것이 특징이다.</p></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week34/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/kooc_week34.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/classlog/'>Classlog</a>, <a href='/categories/kooc/'>kooc</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/kooc/" class="post-tag no-text-decoration" >kooc</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/kooc_week34/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/kooc_week34/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/kooc_week34/&amp;text=[KOOC] 인공지능 및 기계학습 개론 3,4주차 - Naive Bayes Classifier, logistic regression - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/kooc_week34/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/kooc_start/"><div class="card-body"> <em class="timeago small" date="2022-09-27 00:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 - Abstract</h3><div class="text-muted small"><p> KOOC란? KOOC(KAIST Massive Open Online Course)는 KAIST 무료 온라인 강좌를 말하는 것으로, KAIST 교수님들의 강의를 무료로 수강할 수 있는 플랫폼입니다. KOOC에는 다양한 주제로 강좌가 열려 있고, 온라인으로 진행되는 강좌이므로 언제 어디서든 강의를 들을 수 있습니다. 저는 그 중, 인공지능을 전공할 때 ...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week12/"><div class="card-body"> <em class="timeago small" date="2022-09-27 01:40:00 +0900" >Sep 27, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy</h3><div class="text-muted small"><p> Chapter 1. Motivation and Basics 목차 Motivation Machine Learning, AI, Datamining What is Machine Learning? MLE MAP Basics Probabil...</p></div></div></a></div><div class="card"> <a href="/posts/kooc_week56/"><div class="card-body"> <em class="timeago small" date="2022-10-05 00:01:00 +0900" >Oct 5, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[KOOC] 인공지능 및 기계학습 개론 5,6주차 - Support Vector Machine, Training, Regularization</h3><div class="text-muted small"><p> Chapter 5. Support Vector Machine 목차 Support Vector Machine Classifier maximum margin idea of the SVM formulation of the optimization problem soft-margin and pen...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/kooc_week12/" class="btn btn-outline-primary" prompt="Older"><p>[KOOC] 인공지능 및 기계학습 개론 1,2주차 - MLE, MAP, Decision Tree, Entropy</p></a> <a href="/posts/kooc_week56/" class="btn btn-outline-primary" prompt="Newer"><p>[KOOC] 인공지능 및 기계학습 개론 5,6주차 - Support Vector Machine, Training, Regularization</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>