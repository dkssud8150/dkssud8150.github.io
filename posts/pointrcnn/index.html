<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud" /><meta name="author" content="JaeHo YooN" /><meta property="og:locale" content="en" /><meta name="description" content="PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><meta property="og:description" content="PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요." /><link rel="canonical" href="https://dkssud8150.github.io/posts/pointrcnn/" /><meta property="og:url" content="https://dkssud8150.github.io/posts/pointrcnn/" /><meta property="og:site_name" content="JaeHo Yoon" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-02T01:17:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@JaeHo YooN" /><meta name="google-site-verification" content="znvuGsQGYxMZPBslC4XG6doCYao6Y-fWibfGlcaMHH8" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JaeHo YooN"},"dateModified":"2022-02-03T11:13:52+09:00","datePublished":"2022-02-02T01:17:00+09:00","description":"PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.","headline":"[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud","mainEntityOfPage":{"@type":"WebPage","@id":"https://dkssud8150.github.io/posts/pointrcnn/"},"url":"https://dkssud8150.github.io/posts/pointrcnn/"}</script><title>[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud | JaeHo Yoon</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="JaeHo Yoon"><meta name="application-name" content="JaeHo Yoon"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/shin_chan.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JaeHo Yoon</a></div><div class="site-subtitle font-italic">Mamba Mentality</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fab fa-angellist ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/dkssud8150" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hoya58150','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.notion.so/18490713817d403696812c57d0abe730" aria-label="notion" target="_blank" rel="noopener"> <i class="fab fa-battle-net"></i> </a> <a href="https://www.linkedin.com/in/jaeho-yoon-90b62b230" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.instagram.com/jai_ho8150/" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 640 700'%3E%3C/svg%3E" data-src="/assets/img/autodriving/pointrcnn/back.png" class="preview-img bg" alt="Preview Image" width="640" height="700" data-proofer-ignore><h1 data-toc-skip>[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/dkssud8150">JaeHo YooN</a> </em></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-02-02 01:17:00 +0900" data-toggle="tooltip" data-placement="bottom" title="Wed, Feb 2, 2022, 1:17 AM +0900" >Feb 2, 2022</em> </span> <span> Updated <em class="timeago" date="2022-02-03 11:13:52 +0900 " data-toggle="tooltip" data-placement="bottom" title="Thu, Feb 3, 2022, 11:13 AM +0900" >Feb 3, 2022</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="10193 words"> <em>56 min</em> read</span></div></div></div><div class="post-content"><p><code class="language-plaintext highlighter-rouge">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</code> 논문에 대한 리뷰입니다. 이 논문은 2019 CVPR에 투고된 논문이며 약 803회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요.</p><p><br /></p><h1 id="abstract">Abstract</h1><p>본 논문에서는 포인트 클라우드로부터 3D 객체 검출에 대한 PointRCNN을 제안한다. 전체 프레임워크는 2단계로 구성되어 있는데, 1단계는 상향식 3D 제안 발생을 진행하고, 2단계에서는 표준 좌표안에서 제안을 정제하여 최종적인 객체 결과를 얻는다. RGB 이미지로부터 제안을 발생하거나 이전 방식대로 조감도나 복셀들로 포인트 클라우드를 투영하는 것 대신, 이 1단계 서브 네트워크는 전체 포인트 클라우드를 전경/배경으로 분할을 통해 상향식으로 포인트 클라우드에서 작은 수의 하이 퀄리티의 3D 제안을 직접 생성한다. 2단계 서브 네트워크는 각 제안의 풀링된 포인트를 표준 좌표로 변환하여 더 나은 로컬 공간적 특징을 학습하고, 이는 1단계에서 학습된 각 포인트의 전역 의미론적 특징과 결합되어 정확한 박스 세분화 및 신뢰도 예측을 제공한다. KITTI 데이터셋의 3D 객체 벤치마크에서의 실험은 제안된 아키텍쳐가 입력으로서 포인트 클라우드만을 사용하여 최첨단 방법들을 엄청난 차이를 보이면서 능가한다는 것을 보여준다. 코드는 <a href="https://github.com/sshaoshuai/PointRCNN">이 주소</a>를 참고하면 된다.</p><p><br /></p><h1 id="1-introduction">1. Introduction</h1><p>딥러닝은 객체 검출과 인스턴스 세그멘테이션을 포함한 2D 컴퓨터 비전에서 엄청난 진보를 이루고 있다. 2D 장면을 이해하는 것 이상으로 3D 객체 탐지는 중요하고 로봇이나 자율주행과 같은 현실에 적용하기에 필요 불가결하다. 최근 개발된 2D 탐지 알고리즘은 이미지에서 큰 범위의 뷰 포인트 및 배경 클러터를 처리할 수 있지만, <strong>포인트 클라우드가 있는 3D 개체를 감지</strong>하는 것은 여전히 3D 개체의 6DoF(Degrees-of-Freedom)의 불규칙한 데이터 형식과 대규모 검색 공간에서 큰 어려움에 직면해 있다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/fig1.png" data-proofer-ignore></p><p>자율주행에서 보통 3D 센서로 장면들의 3D 구조를 포착하여 3D 포인트 클라우드를 발생시키는 LIDAR 센서를 사용한다. 포인트 클라우드 기반의 3D 객체 탐지의 어려움은 주로 포인트 클라우드의 불규칙성에 있다. 최첨단 3D 객체 탐지 방법들은 위의 그림a처럼 조감도 뷰나 전방뷰 또는 규칙적인 3D 복셀로 포인트 클라우드를 투영하는 좋은 2D 탐지 프레임워크를 활용하는데, 이는 최적이 아니며 정보 손실을 겪을 수 있다. 특징 학습을 위해 포인트 클라우드를 복셀이나 다른 규칙적인 데이터 구조로 변환하는 대신, 포인트 클라우드 분류와 분할에 대한 포인트 클라우드 데이터로부터 직접적으로 3D 표현을 학습하기 위해 PointNet을 제안한다.</p><p>위의 그림b와 같이, 이 아키텍처는 pointNet을 적용하여 2D RGB 탐지 결과로부터 자른 frustum 포인트 클라우드를 기반으로 한 3D 바운딩 박스를 측정했다. 그러나 이 방법의 성능은 2D 탐지에 크게 의존하고, 강인한 바운딩 박스 제안을 생성하기 위한 3D 정보의 장점을 활용할 수 없다.</p><p>2D 이미지로부터 객체 탐지와는 달리 자율주행에서 3D 객체는 주석이 달린 3D 바운딩 박스에 의해 자연스럽고 잘 분리된다. 즉, 3D 객체 탐지에 대한 훈련 데이터는 직접적으로 3D 객체 분할에 대한 의미론적 마스크를 제공한다. 이는 2D 탐지와 3D 탐지 훈련 데이터 사이의 중요한 차이가 된다. 2D 객체 탐지에서 바운딩 박스는 오직 시멘틱 분할에 대한 약한 지도(supervision)를 제공할 수 있다.</p><p>이 관찰을 기반으로 정확하고 강인한 3D 탐지 성능을 달성하고 3D 포인트 클라우드에서 직접적으로 동작하는 <code class="language-plaintext highlighter-rouge">PointRCNN</code>이라는 새로운 2단계 3D 객체 탐지 프레임워크를 제시한다. 이에 대한 설명은 위의 그림c를 통해 볼 수 있다. 이 제시된 프레임워크는 2단계로 구성되어 있는데, 1단계는 상향식으로 3D 바운딩 박스 제안 생서을 목표로 한다. GT 분할 마스크를 생성하기 위한 3D 바운딩 박스를 활용하여 1단계는 배경 점을 분할하고, 분할된 점들로부터 소수의 바운딩 박스 제안을 생성한다. 이러한 전략은 전체 3D 공간 안에서 많은 3D 앵커 박스 사용을 피하고, 계산을 줄인다.</p><p>두번쨰 단계에서는 표준 3D 박스 교정을 수행한다. 3D 제안이 발생된 후에, 포인트 클라우드 영역 풀링 작업을 적용하여 1단계로부터 학습된 점 표현을 풀링한다. 전역 박스 좌표를 직접적으로 측정하는 3D 방법들과 달리 풀링된 3D 점들은 표준 좌표로 변형되고, 상대 좌표 교정을 학습하기 위해 1단계의 분할 마스크뿐만 아니라 풀링 포인트 특징과 결합된다. 이 전략은 1단계의 분할과 제안으로부터 제공된 모든 정보를 활용한다. 더 효과적인 좌표 정제를 위해, 제안 생성 및 개선을 위한 전체 <strong>bin 기반의 3D 상자의 회귀 손실</strong>도 제안하며, 절제 실험 결과 다른 3D 상자의 회귀 손실보다 수렴 속도가 빠르고 회수율이 높은 것으로 나타났다.</p><p>이 논문을 통해 3가지를 기여할 수 있다. (1)첫번째는 포인트 클라우드를 전경 객체 및 배경으로 분할하여 소수의 고품질 3D 제안을 생성하는 새로운 상향식 포인트 클라우드 기반의 3D 바운딩 박스 제안 알고리즘을 제안한다. 분할로부터 학습된 점 표현은 제안 발생에 좋을 뿐만 아니라 추후의 박스 정제에도 도움을 준다. (2)제안된 표준 3D 바운딩 좌표 정제는 1단계로부터 생성된 높은 리콜 박스 제안의 이점을 활용하고, 강력한 bin 기반의 손실로 표준 좌표의 박스 좌표 정제를 예측하는 방법을 학습한다. (3)제안된 3D 탐지 프레임워크인 pointRCNN은 엄청난 차이로 최첨단 기술들을 능가하고, 입력으로 포인트 클라우드만을 사용함으로써 KITTI 3D 탐지 테스트에서 공개된 모든 모델 중 1등을 차지했다.</p><p><br /></p><h1 id="2-related-work">2. Related Work</h1><ul><li>3D object detection from 2D images</ul><p>이미지로부터 3D 바운딩 박스를 추정하는 방법들이 있다. 3D와 2D 바운딩 박스 사이의 공간적 제약 조건을 활용하여 3D 객체 자세를 복구하기도 하며, 이미 정의된 3D 박스들을 평가하기 위한 에너지 함수로서 객체의 3D 구조적 정보를 형성하기도 한다. 이러한 작업들은 깊이 정보가 부족하여 대략적인 3D 감지 결과만 생성할 수 있으며, 외관 변화에 상당한 영향을 받을 수 있다.</p><ul><li>3D object detection from point clouds</ul><p>최첨단 3D 객체 탐지 방법들은 희박한 3D 포인트 클라우드로부터 뚜렷한 특징을 학습하기 위한 다양한 방법들을 제안했다. 포인트 클라우드를 조감도로 투영하고, 2D CNN을 활용하여 3D 박스 발생을 위한 포인트 클라우드 특징을 학습하기도 한다. 점들을 복셀로 그룹화하고, 3D CNN을 활용하여 3D 박스들을 발생시키기 위한 복셀의 특징들을 학습하기도 한다. 그러나 조감도 투영과 복셀화는 데이터 정량화하는 동안에 정보 손실을 겪고, 3D CNN은 메모리와 계산이 너무 많다. 그래서 정확한 2D 탐지기를 활용하여 이미지로부터 2D 제안을 생성하고, 각 잘린 이미지 영역안에서 3D 점의 크기를 줄이기도 했다. 그런 다음 PointNet은 3D 박스 추정에 대한 포인트 클라우드의 특징들을 학습하는데 사용되기도 했다. 그러나 2D 이미지 기반의 제안 생성은 3D 공간에서만 잘 관찰될 수 있는 일부 까다로운 사례에 대한 것은 잘 적용되지 않는다. 그러한 실패는 3D 박스 추정 단계로는 복구할 수 없었다.</p><p>대조적으로 우리의 상향식 3D 제안 생성 방법은 포인트 클라우드에서 직접적으로 강력한 3D 제안을 생성하는데, 이는 효율적이다.</p><ul><li>Learning point cloud representations</ul><p>복셀이나 멀티뷰 포맷으로서 포인트 클라우드를 표현하는 대신, 포인트 클라우드 분류 및 분할의 속도와 정확도를 크게 높이는 포인트 클라우드에서 점들의 특징을 직접 학습할 수 있는 PointNet 아키텍처를 제시한다. 후속 작업에서는 포인트 클라우드의 로컬 구조를 고려하여 추출된 특징 품질을 더욱 개선한다.</p><p>우리의 작업은 점 기반의 특징 추출기를 3D 포인트 클라우드 기반의 객체 탐지로 확장하여 원시 포인트 클라우드에서 3D 박스 제안과 감지 결과를 직접 생성하는 새로운 2단계 3D 탐지 프레임워크를 제시한다.</p><p><br /></p><h1 id="3-pointrcnn-for-point-cloud-3d-detection">3. PointRCNN for Point Cloud 3D Detection</h1><p>이번 섹션에서는 불규칙적인 포인트 클라우드로부터 3D 객체를 검출하기 위한 2단계 검출 프레임워크인 PointRCNN을 제안한다. 전체 구조는 아래 그림과 같다. 1단계는 상향식 제안 발생 단계와 표준 바운딩 박스 정제 단계로 구성되어 있다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/fig2.png" data-proofer-ignore></p><h2 id="31-bottom-up-3d-proposal-generation-via-point-cloud-segmentation">3.1 Bottom-up 3D proposal generation via point cloud segmentation <a href="#31-bottom-up-3d-proposal-generation-via-point-cloud-segmentation" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>현존하는 2D 객체 탐지 방법들은 1단계와 2단계 방법들로 분류할 수 있는데, 1단계 방법들은 일반적으로 빠르지만 정제없이 객체 바운딩 박스를 직접 추정하며, 2단계 방법들은 제안을 먼저 발생시키고 나서 두번째 단계에서 제안과 신뢰도를 정제한다. 그러나, 2D에서 3D로의 2단계의 직접적인 확장은 포인트 클라우드의 불규칙한 포맷과 매우 큰 3D 탐색 공간때문에 필요하다. AVOD는 제안을 생성하기 위해 각 앵커마다 80~100K개의 앵커 박스를 3D 공간에 배치하고 멀티뷰에 각 앵커에 풀링을 배치한다. F-PointNet은 2D 이미지로부터 2D 제안들을 생성하고, 2D 영역으로부터 잘라낸 3D 점들을 기반으로 3D 상자를 추정한다. 이는 3D 공간에서만 명확하게 관찰할 수 있는 까다로운 객체를 놓칠 수 있다. 그래서 전체 포인트 클라우드 분할을 기반으로한 1단계 서브 네트워크로서 정확하고 강인한 3D 제안 발생 알고리즘을 제시한다. 그리고 각기 다른 중첩없이 자연스럽게 분리된 3D 장면들안에서 객체를 관찰한다. 모든 3D 객체 분할 마스크는 그들의 3D 바운딩 박스 주석을 통해 직접적으로 얻어진다. 즉, 3D 상자 내부의 3D 점이 전경 점들로 간주한다.</p><p>그러므로 상향식에서 3D 제안을 생성할 것을 제안한다. 특히, 포인트별 특징을 학습하여 원시의 포인트 클라우드를 분할하는 동시에 분할된 전경 포인트들로부터 3D 제안을 생성한다. 상향식 전량을 기반으로 3D 공간에서 미리 정의된 큰 용량의 3D 박스들을 사용하지 않고, 3D 제안 발생을 위한 탐색 공간을 많이 제한한다. 실험을 통해 제안된 3D 박스 제안 방법이 3D 앵커 기반의 제안 발생 방법들보다 엄청 높은 리콜을 달성한다는 것을 볼 수 있다.</p><ul><li>Learning point cloud representations</ul><p>뚜렷한 포인트별 특징을 학습하여 원시의 포인트 클라우드를 설명하기 위해, 백본 네트워크로서 멀티 스케일 그룹화와 함께 PointNet++을 활용한다. 몇몇의 다른 포인트 클라우드 네트워크 구조에 대한 모델이나 희박한 컨볼루션을 하는 VoxelNet이 있는데, 이 또한 백본 네트워크로서 적용할 수 있다.</p><ul><li>Foreground point Segmentation</ul><p>전경 점들은 객체의 위치와 방향을 예측하는데 많은 정보를 제공한다. 전경 점들을 분할하는 법을 학습하고 나면, 포인트 클라우드 네트워크는 정확한 포인트별 예측을 만들기 위해 맥락적 정보를 포착해야만 하는데, 이는 3D 박스를 발생하는데 효과적이다. 상향식 3D 제안 발생 방법을 설계하여 전경 점들로부터 직접적으로 3D 박스 제안을 생성한다. 즉, 전경 분할과 3D 박스 제안 발생은 동시에 진행된다.</p><p>포인트별 특징을 고려할 떄, 백본 포인트 클라우드 네트워크를 살짝 수정하여 사용하는데, 전경 마스크를 추정하기 위한 1개의 분할 헤드와 3D 제안을 발생시키기 위한 1개의 박스 회귀 헤드를 삽입한다. 점 분할동안 GT 분할 마스크는 3D GT 박스에 의해 자연스럽게 제공된다. 전경 점들의 갯수는 큰 크케일의 바깥 배경보다 훨씬 더 작다. 그래서 우리는 <em>focal loss</em>를 사용하여 불균형 문제를 다룬다. 식은 다음과 같다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/focal.png" data-proofer-ignore></p><p>포인트 클라우드 분할을 훈련하는 동안, 기본값으로 αt = 0.25, γ = 2 로 정한다.</p><ul><li>Bin-based 3D bounding box generation</ul><p>위에서 언급한 것과 같이, 박스 회귀 헤드는 전경 점 분할과 동시에 상향식 3D 제안을 생성하기 위해 추가된다. 훈련동안 오직 전경 점들로부터 3D 바운딩 박스 위치를 회귀하기 위한 박스 회귀 헤드만을 필요로 한다. 이 떄, 배경 점들에 대한 회귀는 하지 않지만, 포인트 클라우드 네트워크의 수용 필드때문에 이러한 점들은 박스 생성을 위한 추가적인 정보도 제공한다.</p><p>3D 바운딩 박스는 LIDAR 좌표계에서 (x,y,z,h,w,l,θ)로서 표기된다. 이 떄, (x,y,z)는 객체의 중심 위치, (h,w,l)은 객체 크기, θ는 조감도에서의 객체 방향이다. 생성된 3D 박스 제안을 제한하기 위해 bin-based 회귀 손실을 제안하여 객체의 3D 바운딩 박스를 추정한다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/fig3.png" data-proofer-ignore></p><p>객체의 중심 위치를 추정하기 위해 위 그림과 같이, 각 전경 점들의 주변 영역을 X축과 Z축을 따라 일련의 연속되지 않는 bin(점)으로 분할한다. 특히, 현재 전경 점의 각 X,Y축에 대해 탐색 범위인 S를 설정하고, 각 1D 탐색 범위는 X-Z평면에서 또 다른 객체 중심(X,Z)를 나타내기 위해 균일한 길이 δ의 bin으로 나뉜다. smooth L1 Loss로의 직접적인 회귀대신에 X,Z축에 대한 cross-entropy loss를 통한 bin 기반의 분류를 사용하게 되면 더 정확하고 강인한 중심 위치를 불러올 수 있다. X,Z축에 대한 위치화 loss는 두 개의 항으로 구성되는데, 1개는 각 X,Z축에 따른 bin 분류에 대한 항이고, 다른 하나는 분류된 bin 내의 남은 회귀 분석에 대한 항이다. 수직의 Y축에 따른 중심 좌표 y에 대해서는 대부분의 객체의 y값이 매우 작은 범위 안에 있기 때문에 smooth L1 loss를 통해 회귀한다. L1 loss를 사용하는 것은 정확한 y값을 얻기에 충분하다.</p><p>그렇게 되면 타겟의 위치는 다음과 같이 형성된다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/local.png" data-proofer-ignore></p><ul><li>(x^(p),y^(p),z^(p)): 전경 관심 지점의 좌표<li>(x^p,y^p,z^p): 해당 객체의 중심 좌표<li>binx^(p), binz^(p): X,Z축을 따라 배치되는 GT bin<li>resx^(p), resz^(p): 할당된 bin안에 추가적인 위치 정제를 위한 잔류 GT<li>C: 정규화를 위한 bin 길이</ul><p>타겟의 방향 θ와 크기 (h,w,l) 추정을 위해 2π를 n개의 bin으로 나누고, x와 z의 예측과 같은 방법으로 bin 분류 타겟인 binθ^(p)와 잔류 회귀 타겟인 resθ^(p)를 계산한다. 객체 크기 (h,w,l)은 전체 훈련 셋에서 각 클래스의 평균 객체 크기에 대한 잔류(resh^(p),resw^(p),resl^(p))를 계산하여 직접적으로 회귀된다.</p><p>추론 단계에서 bin 기반으로 예측된 파라미터 x,z,θ에 대해 먼저 예측된 신뢰도가 가장 높은 bin 중심을 선택하고, 예측된 잔류를 추가하여 정제된 파라미터를 얻는다. y,h,w,l과 같이 직접적으로 회귀된 파라미터들은 그들의 초기값에 예측된 잔류를 더한다.</p><p>전체 3D 바운딩 박스 회귀 손실인 <em>L</em>reg는 다음과 같이 공식화된다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/loss.png" data-proofer-ignore></p><ul><li>Npos: 전경 점들의 갯수<li>binu^(p)_hat, resu^(p)_hat: 전경 점p의 예측된 bin의 수행과 잔류<li>binu^(p), resu^(p): 이전에 계산한 GT 타겟<li>Fcls: cross entropy 분류 loss<li>Freg: smooth L1 loss</ul><p>중복된 제안을 제거하기 위해, 조감도로부터 IoU에 기반한 NMS를 사용하여 고품질 제안의 작은 개수를 발생시킨다. 0.85의 조감도 IoU threshold를 사용하고, NMS이후에 훈련동안 2단계 서브 네트워크 훈련하기 위해 탑 300 제안을 유지한다. 추론동안에는 0.8 IoU threshold를 통한 oriented NMS를 사용하고, 2단계 서브 네트워크의 정제을 위해 오직 상위 100개의 제안을 유지한다.</p><p><br /></p><h2 id="32-point-cloud-region-pooling">3.2 Point Cloud region pooling <a href="#32-point-cloud-region-pooling" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>3D 바운딩 박스 제안을 얻은 후에, 이전에 발생된 박스 제안들을 기반으로 한 박스의 위치와 방향을 정제하는 것을 목표로 한다. 각 제안의 더 구체적인 지역적 특징을 학습하기 위해, 각 3D 제안의 위치에 따라 1단계로부터 3D 점들과 해당 점 특징을 풀링한다.</p><p>각 3D 박스 제안<code class="language-plaintext highlighter-rouge">bi = (xi,yi,zi,hi,wi,li,θi</code>에 대해 이것을 약간 확대하여 새로운 3D 박스<code class="language-plaintext highlighter-rouge">bi^e = (xi,yi,zi,hi + η,wi + η, li + η, θi)</code>를 생성해서 이 것의 문맥으로부터 추가 정보를 인코딩한다. 여기서 η는 상자 크기를 확대하기 위한 상수이다.</p><p>각 점<code class="language-plaintext highlighter-rouge">p = (x^(p),y^(p),z^(p))</code>에 대해 점 p가 확대된 바운딩 박스인 bi^e 안에 있는지에 대한 여부를 결정하기 위해 내부/외부 테스트를 수행한다. <strong>내부에 있다면 그 점과 해당 점의 특징들은 박스bi를 다듬기 위해 유지될 것이다.</strong> 내부 점 p와 연관되어 있는 특징에는 3D 점 좌표<code class="language-plaintext highlighter-rouge">(x^(p),y^(p),z^(p)) ∈ R^3</code>, 레이저 반사 강도<code class="language-plaintext highlighter-rouge">r(p) ∈ R</code>, 1단계에서의 예측된 분할 마스크<code class="language-plaintext highlighter-rouge">m^(p) ∈ {0,1}</code>,그리고 1단계의 C차원의 학습된 점 특징 표현<code class="language-plaintext highlighter-rouge">f^(p) ∈ R^C</code>가 있다.</p><p>또한, 확대된 박스,bi^e 내에서 예측된 전경/배경 점들을 구별하기 위해 분할 마스크,m^(p)를 포함한다. 학습된 점 특징,f^(p)는 세분화 및 제안 생성을 위한 학습을 통해 중요한 정보를 인코딩하므로 여기에도 포함된다. 그런 후, 다음 단계에서 내부 점이 아닌 제안을 제거한다.</p><p><br /></p><h2 id="33-canonical-3d-bounding-box-refinement">3.3 Canonical 3D bounding box refinement <a href="#33-canonical-3d-bounding-box-refinement" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/autodriving/pointrcnn/fig2.png" data-proofer-ignore></p><p>위의 그림b를 보면, 풀링된 점들과 각 제안에 대한 해당 특징들은 2단계 서브 네트워크에 공급되어 3D 박스 위치와 전경 객체의 신뢰도를 정제한다.</p><ul><li>Canonical transformation</ul><p>1단계의 높은 회수율 제안을 활용하고 제안의 박스의 파라미터들의 잔류만을 추청하기 위해, 각 제안에 속하는 풀링된 점들을 해당 3D 제안의 표준 좌표계로 변환한다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/fig4.png" data-proofer-ignore></p><p>위의 그림과 같이 1개의 3D 제안에 대한 표준 좌표계는 몇 가지 특징을 가지는데, (1) 원점은 박스 제안의 중심에 위치된다. (2) 지역적 X’축 및 Z’축은 지면에 대략적으로 평행하며, X’축은 제안의 헤드 방향을 향하고 Z’축은 X’에 수직이다. (3) Y’축은 LIDAR 좌표계의 축과 동일하게 유지된다.</p><p>모든 박스 제안의 풀링된 점들의 좌표p는 적절한 회전과 번역을 통해 p~로서 표준 좌표계로 변환되어야 한다. 제안된 표준 좌표계를 사용하면 박스 개선 단계가 각 제안에 대한 더 나은 지역적인 공간 특징을 학습할 수 있다.</p><ul><li>Feature learning for box proposal refinement</ul><p>3.2 섹션에서 언급한 것과 같이, 정제 서브 네트워크에서는 더 많은 박스와 신뢰도 정제를 위해 변환된 지역적인 공간 특징p~와 1단계의 전역적인 의미론적인 특징 f^(p)를 결합한다.</p><p>표준으로의 변환은 강인한 지역적인 공간 특징 학습을 가능하게 해줌에도 불구하고, 불가피하게 각 물체의 깊이 정보를 잃게 된다. 예를 들어, LIDAR 센서의 고정된 각도 스캔 해상도 때문에, 스캔 멀리 있는 객체는 일반적으로 가까이 있는 객체보다 매우 작은 점들을 가진다. 손실된 깊이 정보를 보완하기 위해 센서까지의 거리<code class="language-plaintext highlighter-rouge">d^(p) = ((x^(p))^2 + (y^(p))^2 + (z^(p))^2)^1/2</code>를 점p의 특징에 포함시킨다.</p><p>각 제안에 대해 해당 점들의 지역적인 공간 특징 p~과 추가적인 특징<code class="language-plaintext highlighter-rouge">(r^(p),m^(p),d^(p))</code>은 연결된 후에 몇몇의 FC layers에 공급되어 그들의 지역적 특징을 전역적 특징f^(p)의 동일한 차원으로 인코딩한다. 즉, 각 local 특징들을 global 특징의 차원으로 바꾸기 위해 지역적 공간 특징과 다른 추가적인 특징들을 연결한 후 FC layer에 공급된다. 그런 다음 지역적 특징과 전역적 특징은 연결되고, 네트워크로 공급되어 신뢰도 분류와 박스 정제를 위한 뚜렷한 특징 벡터를 얻는다.</p><ul><li>Losses for box proposal refinement</ul><p>제안 정제을 위해 bin 기반의 회귀 loss를 선택한다. 만약 3D IoU가 0.55보다 크다면 GT 박스가 3D 박스 제안에 할당되어 박스 정제를 학습한다. 3D 제안과 그들에 따른 GT 박스들은 표준 좌표계로 변환되는데, 이는 3D 제안<code class="language-plaintext highlighter-rouge">bi = (xi,yi,zi,hi,wi,li,θi)</code>와 3D GT 박스<code class="language-plaintext highlighter-rouge">bi^gt = (xi^gt,yi^gt,zi^gt,hi^gt,wi^gt,li^gt,θi^gt)</code>가 다음과 같이 bi~와 bi^gt~로 변환된다는 것이다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/refine.png" data-proofer-ignore></p><p>i번째 박스 제안의 중심좌표에 대한 훈련 타겟인 <code class="language-plaintext highlighter-rouge">(bin∆x^i,bin∆z^i,res∆x^i,res∆z^i,res∆y^i)</code>은 3D 제안의 위치를 정제하기 위한 더 작은 검색 범위 S를 사용하는 것을 제외하고는 위의 eq.(2)과 같은 방식으로 설정된다. 풀링된 희박한 점들은 보통 제안 크기<code class="language-plaintext highlighter-rouge">(hi,wi,li)</code>에 대한 충분한 정보를 주지 못하기 때문에, 크기 잔류<code class="language-plaintext highlighter-rouge">(res∆h^i, res∆w^i, res∆l^i)</code>를 훈련 셋에서 각 클래스의 평균 객체 사이즈에 관해 직접적으로 회귀한다.</p><p>방향을 정제하기 위해, 제안과 GT 박스 사이의 3D IOU가 최소 0.55이상이라는 사실에 기초하여 GT 방향과의 각도 차이<code class="language-plaintext highlighter-rouge">θi^gt - θi</code>가 [-π/4,π/4] 범위 내에 있다고 가정한다. 그러므로, bin 크기 <em>w</em>와 함께 π/2를 개별 bin으로 나누고, 다음과 같이 bin 기반의 방향 타겟을 예측한다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/binorient.png" data-proofer-ignore></p><p>그래서, 2단계 서브 네트워크에 대한 전체적인 loss는 다음과 같이 형성된다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/refineloss.png" data-proofer-ignore></p><ul><li>β: 1단계에서의 3D 제안의 셋<li>βpos: 회귀에 대해 긍정적 제안을 저장<li>prob^i: bi~의 추정된 신뢰도<li>label^i: 레이블<li>Fcls: 예측된 신뢰도를 평가하기 위한 cross entropy loss<li>Lbin^(i)~, Lres^(i)~: bi~와 bi^gt에 의해 계산되는 새로운 타겟, Eq.(3)에서의 Lbin^(p), Lres^(p)와 유사한 역할</ul><p>이를 통해 도출된 결과를 0.01 조감도 IOU threshold의 oriented NMS를 적용하여 중첩된 바운딩 박스를 제거하고, 탐지된 객체에 대한 3D 바운딩 박스를 생성한다.</p><p><br /></p><h1 id="4-experiments">4. Experiments</h1><p>KITTI 데이터셋의 3D 객체 탐지 벤치마크에서 PointRCNN을 평가했다. Sec 4.1에서 PointRCNN의 상세한 구현을 생성한다. Sec 4.2에서 최첨단 3D 감지 방법들과 비교한다. 마지막으로, Sec 4.3에서 PointRCNN을 분석하기 위해 광범위한 절제 연구를 수행한다.</p><h2 id="41-implementation-details">4.1 Implementation Details <a href="#41-implementation-details" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Network Architecture</ul><p>훈련 셋안에서 각 3D 포인트 클라우드 장면에 대해, 입력으로서 각 장면마다 16384개의 점을 서브 샘플링한다. 16384개보다 적은 장면들에 대해서는 무작위로 점을 반복시켜 16384개 점을 얻는다. 1단계 서브 네트워크에 대해, 멀티 스케일로 그룹화하는 각 4개의 레이어를 사용하여 각각 4096, 1024, 256, 64의 크기의 그룹으로 점을 하위 샘플링한다. 그러고 나서 4개의 특징 전파 레이어들은 각 점들의 분할과 제안 발생을 위한 특징 벡터를 얻는데 사용된다. 박스 제안 정제 서브 네트워크동안, 우리는 정제 서브 네트워크의 입력으로서 각 제안의 풀링된 지역에서 512개의 점을 무작위로 샘플링한다. 단일 스케일로 그룹화(128,32,1의 그룹 사이즈)하는 각 3개의 레이어는 객체 신뢰도 분류와 제안 위치 정제를 위한 단일 특징 벡터를 생성하는데 사용된다.</p><ul><li>The training scheme</ul><p>KITTI 데이터셋에 대부분의 샘플이 있기 떄문에 자동차에 대한 훈련 세부 정보를 보고하고, 보행자와 자전거 이용자에 대한 하이퍼 파라미터는 릴리즈된 코드에서 찾을 수 있다.</p><p>1단계 서브 네트워크에서 3D GT 박스안에 있는 모든 점들은 전경 점들로 구성되고, 다른 점들은 배경으로 간주된다. 3D GT 박스가 작은 편차를 가질 수 있기 떄문에 강력한 분할을 위해 3D GT 박스를 물체의 각 면을 0.2m씩 증가시켜 물체 경계 근처의 배경 점을 무시한다. bin 기반의 제안 발생에 대한 하이퍼파라미터를 탐색 범위 <code class="language-plaintext highlighter-rouge">S = 3m</code>, bin 크기 <code class="language-plaintext highlighter-rouge">δ = 0.5m</code>, 방향 bin 개수 <code class="language-plaintext highlighter-rouge">n = 12</code>로 설정한다.</p><p>2단계 서브 네트워크를 훈련시키기 위해, 3D 제안을 무작위로 작은 변형을 통해 데이터를 증가시켜 제안의 다양성을 높였다. 또한, 박스 분류 헤드를 훈련시키기 위해 제안이 GT 박스와의 3D IOU가 0.6보다 크다면 positive, 0.45보다 작으면 negative로 간주한다. 그리고 박스 회귀 헤드의 훈련을 위한 제안의 촤소 threshold로서 0.55 3D IOU를 사용한다. bin 기반의 제안 정제에서는 탐색 범위는 S = 1.5m, 위치화의 bin 크기는 δ = 0.5m, 방향의 bin 크기는 ω = 10◦로 설정한다. 포인트 클라우드 풀링의 맥락적 길이는 η = 1.0m이다.</p><p>PointRCNN의 두 단계의 서브 네트워크는 개별적으로 훈련된다. 1단계 서브 네트워크는 배치사이즈 = 16, lr = 0.002에서 200epochs동안 훈련되지만, 2단계 서브 네트워크는 배치사이즈 = 256, lr = 0.002에서 50epochs동안 훈련된다. 훈련동안 데이터 증강으로서 [0.95,1.05]의 범위에 대한 <strong>random flip</strong>, [-10◦,10◦] 범위에서 수직 Y축을 중심으로 <strong>rotation</strong>을 사용했다. 다양한 환경에서 객체를 형성하기 위해 무작위로 겹치지 않는 박스들을 선택하여 다른 장면의 여러 새로운 GT 박스들과 그들 안에 있는 점들을 현재 훈련 장면에 배치한다. 이 증강을 다음 세션에서 <strong>GT-AUG</strong>로 정의한다.</p><p><br /></p><h2 id="42-3d-object-detection-on-kitti">4.2 3D Object Detection on KITTI <a href="#42-3d-object-detection-on-kitti" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>KITTI의 3D 객체 탐지 벤치마크는 7481개의 훈련 샘플과 7518개의 테스트 샘플로 구성되어 있다. 이 훈련 샘플을 3712개의 훈련 샘플과 3769개의 검증 샘플로 분할한다.</p><p>KITTI 데이터셋에서 검증 셋과 테스트 셋에서 최첨단 3D 객체 탐지 방법들과 PointRCNN을 비교한다.</p><ul><li>Evaluation of 3D object detection</ul><p>3D 탐지 벤치마크의 KITTI 테스트 서버에서 방법을 평가했다. 그 결과는 아래 테이블에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/table1.png" data-proofer-ignore></p><p>차와 자전거 이용자의 3D 탐지에서 이 논문의 방법은 이전의 최첨단 방법들을 모든 3가지 난이도에서 엄청난 차이로 능가하고, 제출 당시에 출시된 모든 방법들 중에서 KITTI 테스트 보드에서 1등을 했다. 대부분의 이전 방법들은 입력으로 포인트 클라우드와 RGB 이미지를 둘다 사용함에도 불구하고, 이 논문에서의 방법이 입력으로 오직 포인트 클라우드만을 사용하면서 더 효과적인 성취를 했다. 보행자 탐지에서 LIDAR만을 사용하는 방법들과 비교했을 때, 이 논문에서의 방법이 더 좋거나 비슷한 결과를 달성했다. 하지만, 다중 센서를 사용하는 방법보다는 약간 안좋은 성능을 보였다. 이는 입력으로 오직 희박한 포인트 클라우드만을 사용했기 때문이라 생각한다. 하지만 보행자는 매우 작은 크기를 가지고, 이미지는 포인트 클라우드보다 3D 탐지에 사용할 더 상세한 정보를 포착할 수 있기도 하다. 가장 중요한 차량 카테고리의 경우 검증 셋에서의 3D 탐지 결과의 성능을 보고했고, 이는 아래 테이블에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/table2.png" data-proofer-ignore></p><p>이를 통해 검증셋에서 이전의 최첨단 방법들을 큰 차이로 능가했다. 특히 어려움 난이도에서 이전의 최고 AP보다 8.28% AP더 높게 받았고, 이는 PointRCNN이 효과적이라는 것을 보여준다.</p><ul><li>Evaluation of 3D proposal generation</ul><p>상향식 제안 발생 네트워크의 성능은 제안의 갯수와 3D IOU threshold를 통한 3D 바운딩 박스의 리콜을 계산을 통해 평가된다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/table3.png" data-proofer-ignore></p><p>위의 테이블에서 볼 수 있듯이, GT-AUG없이도 이전의 방법들보다 엄청 더 높은 리콜을 달성했다. 50개의 제안만을 사용했을 때, 차량 분류에 대해 중간 난이도에서 0.5 IOU threshold에서 96.01% 리콜을 얻었는데 반해 AVOD는 91% 리콜로 약 5.01%이 더 낮지만, 중요한 것은 이 방법은 입력으로 포인트 클라우드만을 사용하는 PointRCNN과 달리 입력으로 2D 이미지와 포인트 클라우드를 둘 다 사용한다.</p><p>300개의 제안을 사용했을 때, 0.5 IoU threshold에서 98.21% 리콜을 달성했다. 여기서 이미 최대 리콜을 달성했기 때문에 제안 개수를 더 증가시키는 것은 의미가 없다.</p><p>대조적으로 위의 테이블을 보면, 추론에서는 0.7 IOU threshold를 사용하여 3D 바운딩 박스의 리콜을 작성했다. 300개의 제안을 사용할 때, 0.7 IOU threshold에서 82.29% 리콜을 달성했다. 이는 다소 낮은 수치일 수 있으나 아직 강인하고 정확한 상향식 제안 발생 네트워크라고 볼 수 있다.</p><p><br /></p><h2 id="43-ablation-study">4.3 Ablation Study <a href="#43-ablation-study" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>이번 세션에서는 pointRCNN의 또 다른 구성들의 효과를 분석하기 위해 확장된 절제 실험을 한다. 모든 실험은 GT-AUG 없이 차량 클래스에 대해 훈련 셋에서 훈련시키고, 검증 셋에서 평가했다.</p><ul><li>Different inputs for the refinement sub-network</ul><p>세션 3.3에서 언급했듯이, 정제 서브 네트워크의 입력은 표죽적으로 변형된 좌표와 각 풀링된 점들의 풀링된 특징으로 구성된다.</p><p>1개씩 제거함으로서 정제 서브 네트워크에서 각 특징들의 효과를 분석한다. 모든 실험은 공정한 비교를 위해 같은 1단계 서브 네트워크를 사용한다. 이 결과는 다음과 같다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/table4.png" data-proofer-ignore></p><p>제안된 표준적 변형없이, 정제 서브네트워크의 성능은 상당히 떨어지는데, 이는 표준 좌표계로의 변환은 많은 회전과 위치 변동성을 많이 제거하고, 2단계에 대한 특징 학습의 효율성을 향상시킨다. 분할과 제안 발생으로부터 학습되는 1단계 특징인 f^(p)를 제거하는 것이 중간 난이도에서는 2.71% mAP정도를 감소시키는데, 이는 1단계에서 의미론적 분할에 대한 학습의 이점을 보여준다.</p><p>또한, 위의 테이블에서, 3D 점인 p에 대한 카메라 깊이 정보인 d^(p)와 분할 마스크인 m^(p)가 표준 변환 중에 제거된 거리 정보를 나타내고, 분할 마스크가 풀링된 영역의 전경 지점을 나타내기 떄문에 최종 성능에 약간 기여안다는 것을 볼 수 있다.</p><ul><li>Context-aware point cloud pooling</ul><p>세션 3.2에서 각 제안의 신뢰도 추정과 위치 회귀를 위한 더 맥락적 점들을 풀링하기 위해 bi를 마진 η를 통해 bi^e로 확장하는 것을 소개했다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/table5.png" data-proofer-ignore></p><p>위의 테이블에서 서로 다른 풀링된 맥락적 넓이인 η의 영향을 볼 수 있다. η = 1.0m일 때, 제안된 프레임워크가 최고 성능을 보였다. 맥락적 정보가 없이 풀링되면 특히 어려움 난이도에서, 정확도가 상당히 떨어진다. 어려움 난이도는 객체가 폐색되거나 센서로부터 멀리 떨어져있어 제안 정제와 분류를 위한 더 많은 맥락적 정보가 필요하기 떄문에, 일반적으로 제안안에 점이 별로 없다. 위의 테이블을 보면, 매우 큰 η는 현재 제안의 풀링된 지역이 다른 객체들의 전경 점들을 포함하기 때문에 성능을 떨어뜨린다.</p><ul><li>Losses of 3D bounding box regression</ul><p>세션 3.1에서 3D 박스 제안을 생성하기 위한 bin 기반의 위치화 loss를 제시했다. 이번 파트에서는 각기 다른 타입의 3D 박스 회귀 loss를 사용했을 때의 성능을 평가하고자 한다. loss의 종류는 RB loss(residual-based loss), RCB loss(residual-cos-based loss), CN loss(corner loss), PBB loss(partial-bin-based loss), BB loss(full bin-based loss)이 있다. 여기서 RCB loss는 각도 회귀의 모호성을 제거하기 위해 (cos(∆θ),sin(∆θ))에 의한 residual based loss의 ∆θ를 인코딩한다.</p><p>1단계에서 100개의 제안에 대한 마지막 리콜(0.5,0.7 IOU)는 평가 방식으로서 사용되고, 이 결과는 아래 그림에서 볼 수 있다.</p><p><img data-src="/assets/img/autodriving/pointrcnn/fig5.png" data-proofer-ignore></p><p>그래프는 전체 bin 기반의 3D 바운딩 박스 회귀 loss의 영향을 보여준다. 특히, BB loss를 사용한 1단계 서브 네트워크는 더 높은 리콜을 달성하고, 다른 loss보다 더 빠르게 수렴하는데, 사전 지식으로 타겟 즉 위치화를 제한함으로서 효과를 얻는다. 부분 bin 기반의 loss는 비슷한 리콜을 달성하지만, 수렴 속도가 다소 느리다. 전체와 부분 bin 기반의 loss는 0.7 IOU threshold에서 다른 함수들보다 더 높은 리콜을 가진다. RCB loss는 각도 회귀 타겟을 개선함으로서 RB loss보다 더 높은 리콜을 얻는다.</p><p><br /></p><h1 id="5-conclusion">5. Conclusion</h1><p>원시 포인트 클라우드로부터 3D 객체를 탐지하기 위해 새로운 3D 객체 탐지기인 PointRCNN을 제시했다. 제시된 1단계 서브 네트워크는 상향식으로 포인트 클라우드로부터 3D 제안을 직접 생성하는데, 이 제안 발생방법이 이전의 방법들보다 상당히 더 높은 리콜을 가진다. 2단계 서브 네트워크에서는 지역적인 공간 특징과 의미론적 특징을 결합하여 그 제안을 표준좌표로 정제한다. 게다가 새로 제안된 bin 기반의 loss는 3D 바운딩 박스 회귀에 대한 효과를 볼 수 있었다. 실험을 통해 KITTI 데이터셋의 3D 탐지 벤치마크에서 PointRCNN이 이전의 최첨단 방법들을 큰 차이로 능가했다.</p><p><br /></p><h1 id="reference">Reference</h1><ul><li><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.pdf">PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud</a></ul></div><div class="post-tail-wrapper text-muted"><div style="text-align: left;"> <a href="http://hits.dwyl.com/dkssud8150.github.io/posts/pointrcnn/" target="_blank"> <img data-src="http://hits.dwyl.com/dkssud8150.github.io/posts/pointrcnn.svg" data-proofer-ignore> </a></div><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/review/'>Review</a>, <a href='/categories/autonomous-driving/'>Autonomous Driving</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/autonomous-driving/" class="post-tag no-text-decoration" >Autonomous Driving</a> <a href="/tags/pointrcnn/" class="post-tag no-text-decoration" >PointRCNN</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud - JaeHo Yoon&amp;url=https://dkssud8150.github.io/posts/pointrcnn/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud - JaeHo Yoon&amp;u=https://dkssud8150.github.io/posts/pointrcnn/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://dkssud8150.github.io/posts/pointrcnn/&amp;text=[논문 리뷰] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud - JaeHo Yoon" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://dkssud8150.github.io/posts/pointrcnn/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6611243953442169" crossorigin="anonymous"></script></div><script src="https://utteranc.es/client.js" repo="dkssud8150/dkssud8150.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/gitpage/">[깃허브 프로필 꾸미기] github 프로필 만들기</a><li><a href="/posts/product/">[깃허브 프로필 꾸미기] productive box 만들기</a><li><a href="/posts/regex/">[데브코스] 2주차 - linux 기초(REGEX)</a><li><a href="/posts/Kfold/">KFold Cross Validation 과 StratifiedKFold</a><li><a href="/posts/cmake/">[데브코스] 17주차 - CMake OpenCV, Eigen, Pangolin install </a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/pointcnn2/"><div class="card-body"> <em class="timeago small" date="2022-01-23 13:00:00 +0900" >Jan 23, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 코드 구현] PointCNN: Convolution on X-transformed points</h3><div class="text-muted small"><p> pointCNN 깃허브에 올라와 있는 코드를 직접 실행하고 리뷰한 내용입니다. 모든 코드는 colab에서 진행하였습니다. 논문에 대한 리뷰가 궁금하신 분들은 이 링크를 참고해주세요. PointCNN 깃허브 주소: https://github.com/yangyanli/PointCNN Classification ModelNet40 먼저 분류 모...</p></div></div></a></div><div class="card"> <a href="/posts/monocular/"><div class="card-body"> <em class="timeago small" date="2022-01-25 23:53:00 +0900" >Jan 25, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] Monocular 3D Object Detection for Autonomous Driving</h3><div class="text-muted small"><p> Monocular 3D Object Detection for Autonomous Driving 논문에 대한 리뷰입니다. 이 논문은 2016 CVPR에 투고된 논문이며 약 669회 인용되었다고 합니다. 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstract 본 논문은 자율주행에서 단안 ...</p></div></div></a></div><div class="card"> <a href="/posts/yolo/"><div class="card-body"> <em class="timeago small" date="2022-03-30 01:17:00 +0900" >Mar 30, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[논문 리뷰] YOLO-based lane detection system</h3><div class="text-muted small"><p> YOLO based lane detection system 논문에 대한 리뷰입니다. 본 논문은 혼자 공부하기 위해 정리한 내용이니 이해가 안되는 부분이 많을 수 있습니다. 참고용으로만 봐주세요. Abstract 자율주행과 반자율주행 자동차가 개발되고 이러한 기술들은 주변 환경 문제로 인해 차선 이탈 경우와 자율주행 자동차에서 판단하지 못하는 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/monocular/" class="btn btn-outline-primary" prompt="Older"><p>[논문 리뷰] Monocular 3D Object Detection for Autonomous Driving</p></a> <a href="/posts/gitpage/" class="btn btn-outline-primary" prompt="Newer"><p>[깃허브 프로필 꾸미기] github 프로필 만들기</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">JaeHo YooN</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/devcourse/">devcourse</a> <a class="post-tag" href="/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/tags/cs231n/">CS231N</a> <a class="post-tag" href="/tags/ros/">ros</a> <a class="post-tag" href="/tags/coding-test/">coding test</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/kooc/">kooc</a> <a class="post-tag" href="/tags/opencv/">OpenCV</a> <a class="post-tag" href="/tags/pytorch-tutorial/">pytorch tutorial</a> <a class="post-tag" href="/tags/autonomous-driving/">Autonomous Driving</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-NC7MWHVXJE"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-NC7MWHVXJE'); }); </script>